[
  {
    "objectID": "ch_01_Introduction_to_Differential_Equations.html",
    "href": "ch_01_Introduction_to_Differential_Equations.html",
    "title": "1  Introduction to Differential Equations",
    "section": "",
    "text": "1.1 Definitions and Terminology",
    "crumbs": [
      "**First Semester**",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction to Differential Equations</span>"
    ]
  },
  {
    "objectID": "ch_01_Introduction_to_Differential_Equations.html#definitions-and-terminology",
    "href": "ch_01_Introduction_to_Differential_Equations.html#definitions-and-terminology",
    "title": "1  Introduction to Differential Equations",
    "section": "",
    "text": "A differential equation (DE) is an equation containing the derivatives of one or more dependent variables with respect to one or more independent variables\nNotation\n\nLeibniz notation \\(~\\displaystyle\\color{blue}{\\frac{d^2y}{dx^2}}\\)\nPrime notation \\(~\\color{blue}{y''}\\)\nNewton’s dot notation \\(~\\color{blue}{\\ddot{y}}\\)\nSubscript notation \\(~\\color{blue}{u_{xx} + u_{yy} = 0}\\)\n\n\n\n1.1.1 Classification by Type\n\nOrdinary differential equation (ODE): \\(\\,\\) Derivatives are with respect to a single independent variable\n\\[\\frac{dx}{d\\color{blue}{t}} + \\frac{dy}{d\\color{blue}{t}} = 3x + 2y\\]\nPartial differential equation (PDE): \\(\\,\\) Derivatives are with respect to two or more independent variables\n\\[\\frac{\\partial u}{\\partial \\color{blue}{y}} = -\\frac{\\partial u}{\\partial \\color{blue}{x}}\\]\n\n\n\n1.1.2 Classification by Order\n\nThe order of an ODE or PDE is the order of the highest derivatives in the equation\n\\[\\underbrace{\\color{blue}{\\frac{d^2y}{dx^2}}}_{\\mathrm{highest\\; order}} +5\\left ( \\frac{dy}{dx} \\right )^3 - 4y = e^x\\]\n\\[\\underbrace{2\\color{blue}{\\frac{\\partial^4 u}{\\partial x^4}}}_{\\mathrm{highest\\; order}} +\\frac{\\partial^2 u}{\\partial t^2} = 0\\]\n\n\n\n1.1.3 Classification by Linearity\n\nAn \\(n\\)-th order ODE, \\(F\\left( x, y, y', \\cdots, y^{(n)} \\right) = 0\\), \\(\\,\\)is linear in the variable \\(y\\,\\) if \\(\\,F\\) is linear in \\(y, y',\\cdots,y^{(n)}\\)\nAn ODE is nonlinear if\n\nThe coefficients of \\(y, y',\\cdots,y^{(n)}\\) contain the dependent variable \\(y\\) or its derivatives\nPowers of \\(y, y',\\cdots,y^{(n)}\\) appear in the equation or\nNonlinear functions of the dependent variable or its derivatives (e.g., \\(\\sin y\\) or \\(e^{y'}\\)) appear in the equation\n\n\n\n\n1.1.4 Solution of an ODE\n\nAny function \\(\\phi\\), defined on an interval \\(I\\) and possessing at least \\(n\\) derivatives that are continuous on \\(I\\), which when substituted into an ODE reduces the equation to an identity, is a solution\nInterval \\(I\\) can be an open interval \\((a,b)\\), a closed interval \\([a,b]\\), an infinite interval \\((a,\\infty)\\), etc.\nA solution of a differential equation that is identically zero on an interval \\(I\\) is a trivial solution\nThe graph of a solution \\(\\phi\\) of an ODE is a solution curve and it is continuous on its interval \\(I\\) while the domain of \\(\\phi\\) may differ from the interval \\(I\\)\nAn explicit solution is one in which the dependent variable is expressed solely in terms of the independent variable and constants\n\\(G(x,y)=0\\,\\) is an implicit solution, if at least one function \\(\\phi\\) exists, that satisfies the relation \\(G\\) and the ODE on \\(I\\)\n\n\n\n1.1.5 Families of Solutions\n\nSimilar to integration, we obtain a solution to a first-order differential equation containing an arbitrary constant \\(c\\)\nA solution with a constant \\(c\\) represents a set \\(G(x,y,c)=0\\) of solutions, called a one-parameter family of solutions\nAn \\(n\\)-parameter family of solutions \\[ G(x,y,c_1,c_2,\\cdots,c_n)=0 \\]\nsolves an \\(n\\)-th order differential equation\n\n\n\n1.1.6 Systems of Differential Equations\n\nTwo or more equations involving the derivatives of two or more unknown functions of a single independent variable are called the system of differential equations\n\\[\n\\begin{aligned}\n  \\frac{dx}{dt} &= f(t,x,y) \\\\\n   \\frac{dy}{dt} &= g(t,x,y)\n\\end{aligned}\\]\nA solution of a system is a set of functions defined on a common interval \\(I\\) that satisfies each equation of the system on this interval",
    "crumbs": [
      "**First Semester**",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction to Differential Equations</span>"
    ]
  },
  {
    "objectID": "ch_01_Introduction_to_Differential_Equations.html#sec-1-2",
    "href": "ch_01_Introduction_to_Differential_Equations.html#sec-1-2",
    "title": "1  Introduction to Differential Equations",
    "section": "1.2 Initial-Value Problems - IVP",
    "text": "1.2 Initial-Value Problems - IVP\nIn an IVP, we seek a solution \\(y(x)\\) of a differential equation so that \\(y(x)\\) satisfies initial conditions at \\(x_0\\)\n\n\\(n\\)-th order differential equation and Initial conditions\n\\[\n\\begin{aligned}\n  \\frac{d^ny}{dx^n} &= f \\left ( x,y,y',\\cdots,y^{(n-1)} \\right ) \\\\\n  y(x_0) &= y_0, \\;y'(x_0) = y_1, \\cdots, \\;y^{(n-1)}(x_0) = y_{n-1} \\\\ \\\\\n  &\\Downarrow \\\\ \\\\\n  \\frac{d\\mathbf{y}}{dx} &= \\mathbf{f}(x, \\mathbf{y}),\n     \\;\\;\\mathbf{y}=\\mathbf{y}_0\n\\end{aligned}\\]\n\nWhen solving an IVP, consider whether a solution exists and whether the solution is unique\n\nExistence: \\(\\,\\) Does the differential equation possess solutions and do any of the solution curves pass through the initial point \\((x_0, y_0)\\)?\nUniqueness: \\(\\,\\) When can we be certain there is precisely one solution curve passing through the initial point \\((x_0, y_0)\\)?\n\n\nTheorem 1.1 gives conditions that are sufficient to guarantee the existence and uniqueness of a solution \\(y(x)\\) to a first-order IVP\n\n\n\n\n\n\n\\(f(x,y)\\) and \\(\\displaystyle\\frac{\\partial f}{\\partial y}\\) are continuous on the region \\(\\,R\\,\\) for the interval \\(\\,I_0\\)\n\n\nExample \\(\\,\\) First-order IVP\n\n\\(y(x)=ce^x\\) is a one-parameter family of solutions of the first-order DE \\(~y'=y\\)\nFind a solution of the first-order IVP with initial condition \\(y(0)=3\\)\n\nSolution\n\nFrom the initial condition, we obtain \\(3=ce^0\\)\nSolving, we find \\(c=3\\)\nThe solution of the IVP is \\(y=3e^x\\)\n\nExample \\(\\,\\) Second-order IVP\n\n\\(x(t) = c_1 \\cos 4t +c_2 \\sin 4t\\) \\(\\,\\)is a two-parameter family of solutions of the second-order DE \\(~x''+16x=0\\)\nFind a solution of the second-order IVP with initial conditions \\(~x\\left( \\frac{\\pi}{2} \\right) =-2, \\;x'\\left (\\frac{\\pi}{2} \\right)=1\\)\n\nSolution\n\nSubstituting for initial conditions and solving for constants, we find \\(c_1=-2\\) and \\(c_2=1/4\\)\nThe solution of the IVP is \\(~x(t)=-2\\cos 4t +\\frac{1}{4}\\sin 4t\\)",
    "crumbs": [
      "**First Semester**",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction to Differential Equations</span>"
    ]
  },
  {
    "objectID": "ch_01_Introduction_to_Differential_Equations.html#sec-1-3",
    "href": "ch_01_Introduction_to_Differential_Equations.html#sec-1-3",
    "title": "1  Introduction to Differential Equations",
    "section": "1.3 Differential Equations as Mathematical Models",
    "text": "1.3 Differential Equations as Mathematical Models\n\nA mathematical model is a description of a system or a phenomenon\nDifferential equation models are used to describe behavior in various fields\n\nBiology\nChemistry\nPhysics\n\nSteps of the modeling process\n\n\n\n\n\n\nExample \\(\\,\\) Radioactive Decay\n\nIn modeling the phenomenon, it is assumed that the rate \\(\\displaystyle\\frac{dA}{dt}\\) at which the nuclei of a substance decays is proportional to the amount \\(A(t)\\) remaining at time \\(t\\) given an initial amount of radioactive substance on hand \\(A_0\\)\n\\[\\frac{dA}{dt} = -kA, \\;A(0)=A_0\\]\nThis differential equation also describes a first-order chemical reaction\n\nExample \\(\\,\\) Draining Tank\n\n\n\n\n\n\nToricelli’s law states that the exit speed \\(v\\) of water through a sharp-edged hole at the bottom of a tank filled to a depth \\(h\\) is the same as the speed it would acquire falling from a height \\(h\\), \\(\\,\\)\\(v=\\sqrt{2gh}\\)\nVolume \\(V\\) leaving the tank per second is proportional to the area of the hole \\(A_h\\) \\[\\frac{dV}{dt}=-A_h\\sqrt{2gh}\\]\nThe volume in the tank at \\(t\\) \\(\\,\\)is\\(\\,\\) \\(V(t)=A_w h\\), \\(\\,\\) in which \\(\\,\\) \\(A_w\\) is the constant area of the upper water surface\nCombining expressions gives the differential equation for the height of water at time \\(t\\) \\[\\frac{dh}{dt}=-\\frac{A_h}{A_w}\\sqrt{2gh}\\]",
    "crumbs": [
      "**First Semester**",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction to Differential Equations</span>"
    ]
  },
  {
    "objectID": "ch_01_Introduction_to_Differential_Equations.html#worked-exercises",
    "href": "ch_01_Introduction_to_Differential_Equations.html#worked-exercises",
    "title": "1  Introduction to Differential Equations",
    "section": "Worked Exercises",
    "text": "Worked Exercises\n1.2: 1. \\(~y=1/(1 +c_1 e^{-x})\\) is a one-parameter family of solutions of the first-order DE \\(y'=y -y^2\\). \\(\\,\\) Find a solution of the first-order IVP consisting of this differential equation and the given initial condition:\n\\[y(0)=-\\frac{1}{3}\\]\nSolution\n\\[\n\\begin{aligned}\ny &= \\frac{1}{1 +c_1 e^{-x}}\\\\\n&\\Downarrow {\\scriptstyle \\text{at } x = 0}\\\\\n-\\frac{1}{3} &= \\frac{1}{1 +c_1} \\\\\n&\\Downarrow \\\\\nc_1 &= -4 \\\\\n&\\Downarrow \\\\\ny &= \\frac{1}{1 -4 e^{-x}}\n\\end{aligned}\\]\n1.2: 3. \\(~y=1/(x^2 +c)\\) is a one-parameter family of solutions of the first-order DE \\(y' +2xy^2=0\\). Find a solution of the first-order IVP consisting of this differential equation and the given initial condition. Give the largest interval \\(I\\) over which the solution is defined\n\\[y(2)=\\frac{1}{3}\\]\nSolution\n\\[\n\\begin{aligned}\n  y &= \\frac{1}{x^2 +c}\\\\\n     &\\Downarrow {\\scriptstyle \\text{at } x = 2}\\\\\n  \\frac{1}{3} &= \\frac{1}{4 +c} \\\\\n     &\\Downarrow \\\\\n  c = -1 \\, &, \\;\\; I =(1,\\infty) \\\\\n   &\\Downarrow \\\\\n   y &= \\frac{1}{x^2 -1}\n\\end{aligned}\\]\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nx = np.linspace(-5, 5, 200)\ny = 1.0 /(x**2 -1)\n\nthreshold = 20\ny[y &gt; threshold] = np.inf\ny[y &lt;-threshold] = np.inf\n\nplt.plot(x,y)\nplt.scatter(2, 1/3, color='red')\n\nplt.title(r'$y=\\frac{1}{x^2 -1}$')\nplt.xlabel('$x$')\nplt.ylabel('$y$')\nplt.grid()\n\nplt.show()\n\n\n\n\nExercises 1.2: 3\n\n\n\n\n1.2: 7. \\(\\,x(t)=c_1\\cos t +c_2\\sin t\\) is a two-parameter family of solutions of the second-order DE \\(x''+x=0\\). Find a solution of the second-order IVP consisting of this differential equation and the given initial conditions\n\\[x(0)=-1, \\; x'(0)=8\\]\nSolution\n\\[\n\\begin{aligned}\n  x(t) &= c_1\\cos t +c_2\\sin t \\\\\n       &\\Downarrow \\;{\\scriptstyle x(0)=-1, \\; x'(0)=8}\\\\\n    -1 &= c_1 \\\\\n     8 &= c_2 \\\\\n       &\\Downarrow \\\\\n  x(t) &= -\\cos t +8\\sin t\n\\end{aligned}\\]\n1.2: 17. \\(\\,\\) Determine a region of the xy-plane for which the given differential equation would have a unique solution whose graph passes through a point \\((x_0, y_0)\\) in the region.\n\\[\\frac{dy}{dx}=y^{2/3}\\]\nSolution\nStep 1: \\(~\\)Identify the Function and Its Partial Derivative\nLet:\n\\[f(x, y) = y^{2/3}\\]\nThen the partial derivative with respect to \\(y\\) is:\n\\[\\frac{\\partial f}{\\partial y} = \\frac{2}{3} y^{-1/3}\\]\nStep 2: \\(~\\)Analyze Continuity\n\n\\(f(x, y) = y^{2/3}\\) is continuous for all real values of \\(x\\) and \\(y\\)\nHowever, \\(\\frac{\\partial f}{\\partial y} = \\frac{2}{3} y^{-1/3}\\) is undefined at \\(y = 0\\), since \\(y^{-1/3}\\) is not defined at \\(y = 0\\)\n\nStep 3: \\(~\\)Apply the Existence and Uniqueness Theorem\n\nThe Existence and Uniqueness Theorem requires both \\(f(x, y)\\) and \\(\\frac{\\partial f}{\\partial y}\\) to be continuous in a neighborhood of the initial point \\((x_0, y_0)\\)\nSo, a unique solution exists through \\((x_0, y_0)\\) only if \\(y_0 \\ne 0\\)\n\n\\(~\\)\n1.2: 29.\n\nBy inspection, find a one-parameter family of solutions of the differential equation \\(xy'=y\\). Verify that each member of the family is a solution of the initial value problem \\(xy'=y, \\,y(0)=0\\)\nExplain part 1. by determining a region \\(R\\) in the \\(xy\\)-plane for which the differential equation \\(xy'=y\\) would have a unique solution through a point \\((x_0, y_0)\\) in \\(R\\)\nVerify that the piecewise-defined function\n\\[y= \\begin{cases}\n  0, & x &lt; 0 \\\\\n  x, & x \\ge 0\n\\end{cases}\\]\nsatisfies the condition \\(y(0)=0\\). Determine whether this function is also a solution of the initial-value problem in part 1\n\nSolution\nStep 1: \\(~\\)Solve by Inspection\nLet’s try separating variables or guessing solutions:\nGiven: \\[xy’ = y \\quad \\Rightarrow \\quad y’ = \\frac{y}{x}\\]\nThis is separable:\n\\[\\frac{dy}{y} = \\frac{dx}{x}\\]\nIntegrate both sides:\n\\[\\ln |y| = \\ln |x| + C_1\\]\nExponentiate:\n\\[|y| = C_2|x| \\quad \\Rightarrow \\quad y = Cx\\]\nfor some constant \\(C \\in \\mathbb{R}\\), because the sign of \\(C_2\\) absorbs the absolute values. So the one-parameter family of solutions is:\n\\[y = Cx\\]\nStep 2: \\(~\\)Verify with the Initial Condition\nWe are given the initial value problem:\n\\[xy’ = y, \\quad y(0) = 0\\]\nLet’s plug \\(x = 0\\) into the general solution:\n\\[y = Cx \\Rightarrow y(0) = C \\cdot 0 = 0\\]\nSo for any value of \\(C\\), the condition \\(y(0) = 0\\) is satisfied.\n❗ But Wait – What About Uniqueness?\nLet’s look back at the original differential equation:\n\\[y’ = \\frac{y}{x}\\]\nThis function \\(f(x, y) = \\frac{y}{x}\\) is not defined at \\(x = 0\\), so the existence and uniqueness theorem does not apply at \\(x = 0\\)\nThat means:\n\nInfinitely many solutions of the form \\(y = Cx\\) pass through \\((0, 0)\\),\nBut there is no unique solution to the initial value problem,\nSo although each \\(y = Cx\\) satisfies both the differential equation and the initial condition, the IVP does not have a unique solution\n\n\\(~\\)\n1.3: 5. \\(\\,\\) A cup of coffee cools according to Newton’s law of cooling. Use data from the graph of the temperature \\(T(t)\\) in figure to estimate the constants \\(T_m\\), \\(T_0\\), and \\(k\\) in a model of the form of the first-order initial-value problem\n\\[ \\frac{dT}{dt}=k(T -T_m), \\;\\;T(0)=T_0 \\]\n\n\n\n\n\nSolution\n\\[\n\\begin{aligned}\n\\frac{dT}{dt} &=k(T -T_m), \\;\\;T(0)=T_0 \\\\\n    &\\Downarrow \\\\\nT(0) &= T_0 \\\\\nT(\\infty) &= T_m \\\\\n\\ln \\left( \\frac{T -T_m}{T_0 -T_m} \\right) &= k t \\\\\n    &\\Downarrow \\\\  \n    k \\text{ is the slope of the graph of }\\, & t \\text{ vs. } \\ln \\left( \\frac{T -T_m}{T_0 -T_m}\\right)\n\\end{aligned}\\]\n1.3: 19. After a mass \\(m\\) is attached to a spring, it stretches \\(s\\) units and then hangs at rest in the equilibrium position as shown in Figure (b). After the spring/mass system has been set in motion, let \\(x(t)\\) denote the directed distance of the mass beyond the equilibrium position. As indicated in Figure (c), assume that the downward direction is positive, that the motion takes place in a vertical straight line through the center of gravitiy of the mass, and that the only forces acting on the system are the weight of the mass and the restoring force of the stretched spring. Use Hooke’s law: The restoring force of a spring is proportional to its total elongation. Determine a differential equation for the displacement \\(x(t)\\) a time \\(t\\)\n\n\n\n\n\nSolution\nTo model the vertical motion of a spring-mass system as described, we need to:\n\nUse Newton’s Second Law: \\(~F = ma\\),\nUse Hooke’s Law: The restoring force is proportional to the total elongation of the spring,\nLet \\(y(t)\\) denote the displacement from equilibrium (positive downward),\nAssume only gravity and the spring’s restoring force act\n\nStep 1: \\(~\\)Define the Forces\nLet:\n\n\\(m =\\) mass (kg),\n\\(g =\\) acceleration due to gravity (m/s²),\n\\(k =\\) spring constant (N/m),\n\\(y(t) =\\) displacement from equilibrium (m), with positive direction downward\n\nFrom equilibrium condition:\n\nWhen the mass is at rest (before any motion), gravity is balanced by the spring force:\n\n\\[mg = k \\Delta l \\quad \\text{(equilibrium stretch)}\\]\nStep 2: \\(~\\)After the System is Set in Motion\nWhen displaced by \\(y(t)\\) from equilibrium:\n\nWeight (constant force downward): \\(~F_g = mg\\)\nSpring force (upward restoring force): \\(~F_s = -k(\\Delta l + y)\\)\n\nBut since we’re measuring \\(y(t)\\) from equilibrium, the net restoring force becomes:\n\\[F_{\\text{net}} = -k y(t)\\]\nThus, from Newton’s second law:\n\\[m \\ddot{y}(t) = -k y(t)\\]\nStep 3: \\(~\\)Final Differential Equation\n\\[m \\ddot{y} + k y = 0\\]\nThis is a second-order linear differential equation that governs the vertical motion of the mass from equilibrium",
    "crumbs": [
      "**First Semester**",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction to Differential Equations</span>"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Engineering Mathematics",
    "section": "",
    "text": "Welcome\nChemical engineering occupies a unique position at the interface between molecular sciences and engineering. Intimately linked with the fundamental subjects of chemistry, biology, mathematics, and physics — and in close collaboration with fellow engineering disciplines like materials science, computer science, and mechanical, electrical, and civil engineering — chemical engineering offers unparalleled opportunities to do great things",
    "crumbs": [
      "Welcome"
    ]
  },
  {
    "objectID": "index.html#contents",
    "href": "index.html#contents",
    "title": "Engineering Mathematics",
    "section": "Contents",
    "text": "Contents\n\nPart I\n\nIntroduction to Differential Equations\nFirst-Order Differential Equations\nHigher-Order Differential Equations\nThe Laplace Transform\nSeries Solutions of Linear Differential Equations\nMatrices\nSystems of Linear Differential Equations\nSystems of Nonlinear Differential Equations\n\n\n\n\nPart II\n\nVectors\nVector Calculus\nOrthogonal Functions and Fourier Series\nParabolic Partial Differential Equations\nHyperbolic Partial Differential Equations\nElliptic Partial Differential Equations\n\n\n\n\nScientific Computing in Python\n\nNumpy: Vectors, Matrices, and Multidimensional Arrays\nSympy: Symbolic Computing\nSympy: Laplace Transform\nEquation Solving\nOptimization\nInterpolation\nIntegration\nOrdinary Differential Equations\nDouble Pendulum\nPartial Differential Equations - Dedalus\nPartial Differential Equations - FEniCS\n\n\n\n\nPloting and Data Analysis in Python\n\nMatplotlib\nMandelbrot Set in Python",
    "crumbs": [
      "Welcome"
    ]
  },
  {
    "objectID": "index.html#references",
    "href": "index.html#references",
    "title": "Engineering Mathematics",
    "section": "References",
    "text": "References\n\nD.G. Zill and W.S. Wright, Advanced Engineering Mathematics, 7th ed., Jones and Bartlett, 2022.\nE.A. Coddington, An Introduction to Ordinary Differential Equations, Prentice-Hall, 1961. (Dover Publications Inc., 1989 - Unabridged, corrected republication).\nG.E. Shilov, Linear Algebra, Prentice-Hall, 1971. (Dover Publications Inc., 1977 - Unabridged republication)\nS.J. Farlow, Partial Differential Equations for Scientists and Engineers, John Wiely & Sons, 1982. (Dover Publications Inc., 1993 – Unabridged, corrected republication).\nH.F. Weinberger, A First Course in Partial Differential Equations with Complex Variables and Transform Methods, Blaisdell Publishing Company, 1965. (Dover Publications Inc., 1995 - New edition).\nR. Johansson, Numerical Python: Scientific Computing and Data Science Applications with Numpy, Scipy and Matplotlib, 2nd ed., Apress, 2018.\nJørgen S. Dokken, The FEniCSx tutorial, https://jsdokken.com/dolfinx-tutorial/\nDedalus Project, Tutorial & Examples, https://dedalus-project.readthedocs.io/en/latest/pages/tutorials.html",
    "crumbs": [
      "Welcome"
    ]
  },
  {
    "objectID": "ch_02_First-Order_Differential_Equations.html",
    "href": "ch_02_First-Order_Differential_Equations.html",
    "title": "2  First-Order Differential Equations",
    "section": "",
    "text": "2.1 Solution Curves Without a Solution\nFor example, \\(~\\displaystyle\\frac{dy}{dx}=\\sin y\\)\nimport numpy as np\nimport matplotlib.pyplot as plt\nplt.style.use('ggplot')\n\nx = np.linspace(-2, 2, 20)\ny = np.linspace(-2, 2, 20)\nX, Y = np.meshgrid(x, y)\n\ndy = (X*X -Y*Y)\ndx = np.ones(dy.shape)\n\nplt.figure(figsize=(5, 5))\nplt.quiver(X, Y, dx, dy, color='red')\nplt.plot([-2, -1, 0, 0], [1, -1, 2, 0], 'o', color=\"blue\")\nplt.xlim([-2.2, 2.2])\nplt.ylim([-2.3, 2.2])\nplt.xticks(np.arange(-2, 2.5, 0.5))\nplt.yticks(np.arange(-2, 2.5, 0.5))\nplt.xlabel('x')\nplt.ylabel('y')\n\nplt.show()\n\\(~\\)\nExample \\(\\,\\) Phase portrait and solution curves\n\\[ \\displaystyle \\frac{dP}{dt} = P(a-bP) \\]\n\\(~\\)\nExample \\(\\,\\) Consider the autonomous first-order differential equation\n\\[\\frac{dy}{dx}=y^2-y^4\\]\nand the initial condition \\(y(0)=y_0\\). Sketch the graph of a typical solution \\(y(x)\\) when \\(y_0\\) has the given values\n\\[\n  \\begin{aligned}\n    &(a) && \\phantom{-1 &lt; }\\; y_0 &lt; {-1} \\\\\n    &(b) && {-1} &lt; y_0 &lt; 0 \\\\\n    &(c) && \\phantom{-}0 &lt; y_0 &lt; 1 \\\\\n    &(d) && \\phantom{-}1 &lt; y_0 \\\\\n  \\end{aligned}\\]",
    "crumbs": [
      "**First Semester**",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>First-Order Differential Equations</span>"
    ]
  },
  {
    "objectID": "ch_02_First-Order_Differential_Equations.html#sec-2-1",
    "href": "ch_02_First-Order_Differential_Equations.html#sec-2-1",
    "title": "2  First-Order Differential Equations",
    "section": "",
    "text": "DEs can be analyzed qualitatively, allowing us to approximate a solution curve without solving the problem\nTwo approaches are:\n\nDirection fields\nAutonomous first-order DEs\n\nDirection fields:\n\nSlope of the lineal element at \\((x,y(x))\\) on a solution curve is the value of \\(\\,\\frac{dy}{dx}\\,\\) at this point\nDirection/slope fields of \\(\\,\\frac{dy}{dx}=f(x,y)\\,\\) are collections of lineal slope elements that visually suggest the shape of a family of solution curves\n\n\n\n\n\n\n\n\n\nUsing the given computer-generated direction field, sketch, by hand, an approximate solution curve that passes through each of the indicated points:\n\\[\n\\begin{aligned}\n    \\frac{df}{dx} &= x^2 -y^2 \\\\ \\\\\n    (a) &\\;\\; y(-2)=1 \\\\\n    (b) &\\;\\; y(-1)=-1 \\\\\n    (c) &\\;\\; y(0)=2 \\\\\n    (d) &\\;\\; y(0)=0\n\\end{aligned}\\]\n\n\n\nAutonomous first-order DEs, \\(~\\displaystyle\\color{red}{\\frac{dy}{dx}=f(y)}\\)\nAn ODE in which the independent variable does not appear explicitly\n\\[\\begin{aligned}\n    \\frac{dy}{dx} &= 1+y^2 && \\mathrm{autonomous} \\\\\n    \\frac{dy}{dx} &= 0.2\\,xy && \\mathrm{nonautonomous}\n  \\end{aligned}\\]\nCritical points, \\(~f(c)=0\\), \\(~\\) are constant (or equilibrium) solutions of autonomous DEs\nA phase portrait is made by putting critical points on a vertical line with phase lines pointing up or down, depending on the sign of the function over intervals between the points\nSome conclusions can be drawn about nonconstant solution curves to autonomous DEs\n\nIf a solution \\(y(x)\\) passes through \\((x_0,y_0)\\) in sub-region \\(R_i\\), \\(~\\)then \\(y(x)\\) remains in \\(R_i\\) \nBy continuity, \\(~f(y)\\) cannot change signs in a sub-region \\(R_i\\)\nSince \\(f(y)\\) is either positive or negative in \\(R_i\\), \\(~\\)a solution is either increasing or decreasing and has no relative extremum",
    "crumbs": [
      "**First Semester**",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>First-Order Differential Equations</span>"
    ]
  },
  {
    "objectID": "ch_02_First-Order_Differential_Equations.html#sec-2-2",
    "href": "ch_02_First-Order_Differential_Equations.html#sec-2-2",
    "title": "2  First-Order Differential Equations",
    "section": "2.2 Separable Equations",
    "text": "2.2 Separable Equations\n\nConsider \\(~\\displaystyle\\frac{dy}{dx}=f(x)\\)\n\nWhen \\(f\\) does not depend on \\(y\\), \\(~\\displaystyle\\frac{dy}{dx}=f(x)\\), \\(~\\)which can be solved by integration\nThe solution \\(\\displaystyle y=\\int f(x) dx = F(x) +c\\), \\(~\\)where \\(F(x)\\) is an antiderivative (indefinite integral)\nSome functions, termed nonelementary, \\(~\\)do not possess an antiderivative that is an elementary function\n\nA first-order DE of the form \\(\\displaystyle\\frac{dy}{dx}=g(x)h(y)\\) is said to be separable, or have separable variables\n\nA separable equation can be rewritten in the form\n\\[ \\color{red}{\\frac{1}{h(y)}dy=g(x)dx}\\]\nwhich is solved by integrating both sides\n\n\n\\(~\\)\nExample \\(\\,\\) Solve a separable equation \\(\\displaystyle\\frac{dy}{dx}=y^2-9\\), \\(\\;y(0)=0\\)\n\nSeparating and using partial fractions\n\n\\[\n  \\begin{aligned}\n    \\frac{dy}{(y-3)(y+3)} &= dx \\\\\n    &\\Downarrow \\\\\n    \\frac{1}{6} \\left [ \\frac{1}{y-3} -\\frac{1}{y+3} \\right ] dy &= dx\n  \\end{aligned}\\]\n\nIntegrating and solving for \\(y\\,\\) yields\n\n\\[\n  \\begin{aligned}\n    \\frac{1}{6} \\ln \\left | \\frac{y-3}{y+3} \\right | &= x+c_1\\\\\n    &\\Downarrow c=e^{6c_1} \\\\\n    y &= 3 \\frac{1-ce^{6x}}{1+ce^{6x}}\n  \\end{aligned}\\]\n\nFinally, \\(~\\) applying \\(y(0)=0~\\) gives \\(c=1\\)\n\n\\(~\\)\nExample \\(\\,\\) Solve the given differential equation by separation of variables\n\n\\(\\displaystyle \\frac{dy}{dx}=\\sin 5 x\\)\n\\(\\displaystyle dx +e^{3x} dy = 0\\)\n\\(\\displaystyle \\frac{dy}{dx} = x\\sqrt{1 -y^2}\\)\n\n\\(~\\)\n\nimport sympy\nfrom sympy import pi, dsolve\nsympy.init_printing()\n\nx = sympy.Symbol('x')\ny = sympy.Function('y')\n\neq = y(x).diff(x) -x *sympy.sqrt(1 -y(x) *y(x))\n\ndsolve(eq, hint='separable')\n\n\\(\\displaystyle y{\\left(x \\right)} = \\sin{\\left(C_{1} + \\frac{x^{2}}{2} \\right)}\\)\n\n\n\\(~\\)\nExample \\(\\,\\) Find an implicit and an explicit solution of the given initial-value problem\n\n\\(\\displaystyle x^2 \\frac{dy}{dx} = y -xy, \\;\\;y(-1)=-1\\)\n\\(\\displaystyle \\frac{dx}{dt}=4(x^2 + 1), \\;\\;x(\\pi/4)=1\\)\n\n\\(~\\)\n\nt = sympy.Symbol('t')\nx = sympy.Function('x')\n\neq = x(t).diff(t) -4 *(t *t +1)\n\ndsolve(eq, ics={x(pi/4): 1}, hint='separable')\n\n\\(\\displaystyle x{\\left(t \\right)} = \\frac{4 t^{3}}{3} + 4 t - \\pi - \\frac{\\pi^{3}}{48} + 1\\)\n\n\n\\(~\\)",
    "crumbs": [
      "**First Semester**",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>First-Order Differential Equations</span>"
    ]
  },
  {
    "objectID": "ch_02_First-Order_Differential_Equations.html#sec-2-3",
    "href": "ch_02_First-Order_Differential_Equations.html#sec-2-3",
    "title": "2  First-Order Differential Equations",
    "section": "2.3 Linear Equations",
    "text": "2.3 Linear Equations\nA first-order DE of the form \\(\\displaystyle a_1(x) \\frac{dy}{dx} +a_0(x)y = g(x)~\\) is a linear equation in the dependent variable \\(y\\)\n\nThe DE is homogeneous when \\(g(x)=0\\) ; \\(~\\)otherwise, \\(~\\)it is nonhomogeneous\nThe standard form of a linear DE is obtained by dividing both sides by the lead coefficient\n\\[\\color{red}{\\frac{dy}{dx}+P(x)y=f(x)}\\]\n\nThe standard form equation has the property that its solution \\(y\\) is the sum of the solution of the associated homogeneous equation \\(y_h\\) and the particular solution of the nonhomogeneous equation \\(y_p\\) :\n\\[\\color{red}{y=y_h +y_p}\\]\n\nThe homogeneous equation \\(\\displaystyle\\frac{dy_h}{dx} +P(x)y_h= 0~\\) is separable, allowing us to solve for \\(y_h\\)\n\n\\[\n\\begin{aligned}\n  \\frac{dy_h}{y_h} &= -P(x)dx \\\\\n  &\\Downarrow \\\\\n  \\ln |y_h| &= -\\int P(x)\\,dx +c \\,\\Rightarrow\\, y_h = \\bar{c} \\exp\\left( -\\int P(x) \\,dx \\right)    \n\\end{aligned}\\]\n\nVariation of parameters \\(\\,\\color{blue}{y_p=u(x)y_h}~\\) can be used to solve the nonhomogeneous equation of \\(\\,y_p\\)\n\n\\[\n\\begin{aligned}\n    y_h \\frac{du}{dx} +& \\underbrace{\\left (\\frac{dy_h}{dx} +P(x) y_h  \\right )}_{=\\,0} u = f(x)\\\\\n    du &= \\frac{f(x)}{y_h} dx \\;\\Rightarrow\\; u(x) = \\displaystyle\\int \\frac{f(x)}{y_h(x)} dx \\\\\n    &\\Downarrow \\\\\n    y_p &= y_h \\displaystyle\\int \\frac{f(x)}{y_h(x)} dx\n\\end{aligned}\\]\n\\(~\\)\nExample \\(\\,\\) Find the general solution of the given differential equation:\n\n\\(~\\displaystyle \\frac{dy}{dx} + 2y=0\\)\n\\(~\\displaystyle y' +2xy=x^3\\)\n\\(~\\displaystyle x\\frac{dy}{dx} +2y=3\\)\n\\(~\\displaystyle xy' +(1+x)y=e^{-x} \\sin 2x\\)\n\n\\(~\\)\nExample \\(\\,\\) Solve the given initial-value problem. Give the largest interval \\(~I\\) over which the solution is defined\n\n\\(~\\displaystyle y\\frac{dx}{dy} -x=2y^2, \\;\\; y(1)=5\\)\n\\(~\\displaystyle (x+1)\\frac{dy}{dx}+y = \\ln x, \\;\\;y(1)=10\\)\n\n\\(~\\)\n\nx = sympy.Symbol('x')\ny = sympy.Function('y')\n\neq = (x + 1) *y(x).diff(x) +y(x) -sympy.log(x)\n\ndsolve(eq, ics={y(1): 10}, hint='1st_linear')\n\n\\(\\displaystyle y{\\left(x \\right)} = \\frac{x \\log{\\left(x \\right)} - x + 21}{x + 1}\\)\n\n\n\\(~\\)\nExample \\(\\,\\) The given differential equation is not linear in \\(y\\). \\(~\\)Nevertheless, find a general solution of the equation\n\n\\(~dx=(x+y^2)dy\\)\n\\(~ydx + (2x + xy-3)dy=0\\)\n\n\\(~\\)\nExample \\(\\,\\) The sine integral function is defined as\n\\[ \\mathrm{Si}(x)=\\int_0^x \\frac{\\sin t }{t} \\,dt\\]\nwhere the integrand is defined to be 1 at \\(x=0\\). Express the solution of the initial value problem\n\\[x^3 \\frac{dy}{dx} + 2x^2 y = 10 \\sin x, \\;\\; y(1)=0\\]\nin terms of \\(\\mathrm{Si}(x)\\)\n\\(~\\)",
    "crumbs": [
      "**First Semester**",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>First-Order Differential Equations</span>"
    ]
  },
  {
    "objectID": "ch_02_First-Order_Differential_Equations.html#sec-2-4",
    "href": "ch_02_First-Order_Differential_Equations.html#sec-2-4",
    "title": "2  First-Order Differential Equations",
    "section": "2.4 Exact Equations",
    "text": "2.4 Exact Equations\n\nA differential expression \\(~M(x,y)dx + N(x,y)dy~\\) is an exact differential in a region \\(R\\) of the \\(xy\\)-plane if it corresponds to the differential of some function \\(f(x,y)\\):\n\\[ \\color{red}{df(x,y)=\\frac{\\partial f}{\\partial x} dx +\\frac{\\partial f}{\\partial y} dy}\\]\nand a condition of exact differentials is:\n\\[ \\frac{\\partial M}{\\partial y}=\\frac{\\partial N}{\\partial x}\\]\n\\(M(x,y)\\,dx + N(x,y)\\, dy=0 ~\\) is an exact equation if the left side is an exact differential\n\n\\(~\\)\nExample \\(\\,\\) Solving an exact DE, \\(\\;2xy\\,dx+(x^2-1)\\,dy=0\\)\n\\(~\\)\n\nIntegrating Factor of the first-order linear DE\n\\[\n\\begin{aligned}\n  \\frac{dy}{dx} +P(x)y &= f(x)\\\\\n  &\\Downarrow \\\\\n  \\left ( P(x)y -f(x) \\right )dx +dy &= 0\\\\\n  &\\Downarrow \\times \\; I(x): \\text{ Integrating Factor}\\\\\n     I(x)\\left ( P(x)y -f(x) \\right )dx +I(x)dy &= 0 \\\\ \\\\\n    {\\small \\text{To be an exact equation }} &\\big\\Downarrow \\\\\n  \\frac{\\partial}{\\partial y}\n   \\left\\{I(x)\\left( P(x){\\color{blue}{y}} -f(x) \\right) \\right \\} &= I(x)\n     P(x) =\\frac{d}{d x} I(x) \\\\\n  &\\big\\Downarrow \\;\\; {\\color{red}{I(x) = \\exp\\left(\\int P(x) dx\\right)}}\n\\end{aligned}\\]\nThen\n\\[\n\\begin{aligned}\n   I(x) \\frac{dy}{dx} +I(x) P(x)y &= I(x)f(x) \\; \\Rightarrow \\; \\frac{d} {dx}\\left\\{ I(x)y \\right \\} = I(x)f(x) \\\\\n   &\\Downarrow \\\\\n  \\color{red}{y(x) = I(x)^{-1}y(x_0)I(x_0)}  &\\color{red}{\\,+\\,I(x)^{-1} \\int_{x_0}^x I(x) f(x) dx}  \n\\end{aligned}\\]\n\nExample \\(\\,\\) Solve \\(\\displaystyle\\frac{dy}{dx} -2xy = 2, \\;y(0)=1\\)\n\\[\n\\begin{aligned}\n    \\frac{dy}{dx} -2xy &= 2\\\\\n    &\\Downarrow \\times \\;e^{-x^2} \\\\\n    \\frac{d}{dx}[e^{-x^2}y] &= 2e^{-x^2}\\\\\n    y &= c e^{x^2} +2e^{x^2} \\int_0^x e^{-t^2} dt\\\\\n    & \\big\\Downarrow \\;{\\small y(0) = 1 \\rightarrow c=1} \\\\\n     y &= e^{x^2} \\left[ 1 +\\sqrt{\\pi} \\underbrace{\\left(\\frac{2}{\\sqrt{\\pi}} \\int_0^x e^{-t^2} dt \\right)}_{\\mathrm{erf}(x)} \\right ] \\\\\n     &= e^{x^2} \\left[1 +\\sqrt{\\pi} \\,\\mathrm{erf} (x) \\right]\n\\end{aligned}\\]\nExample \\(\\,\\) Determine whether the given differential equation is exact. If it is exact, solve it\n\n\\(~(2x - 1)dx + (3y+7)dy=0\\)\n\\(~(5x + 4y)dx + (4x-8y^2)dy=0\\)\n\\(~(2xy^2-3)dx +(2x^2y+4)dy=0\\)\n\\(~(x^2 -y^2)dx+(x^2-2xy)dy=0\\)\n\n\\(~\\)\n\nx = sympy.Symbol('x')\ny = sympy.Function('y')\n\neq = (2 *x -1) +(3 *y(x) +7) *y(x).diff(x)\n\ndsolve(eq, hint='1st_exact')\n\n\\(\\displaystyle \\left[ y{\\left(x \\right)} = - \\frac{\\sqrt{C_{1} - 6 x^{2} + 6 x}}{3} - \\frac{7}{3}, \\  y{\\left(x \\right)} = \\frac{\\sqrt{C_{1} - 6 x^{2} + 6 x}}{3} - \\frac{7}{3}\\right]\\)\n\n\n\\(~\\)\nExample \\(\\,\\) Solve the given initial-value problem\n\n\\(~(x+y)^2 dx + (2xy +x^2-1)dy = 0, \\;\\;y(1)=1\\)\n\\(~(4y + 2t -5)dt + (6y +4t-1)dy=0, \\;\\;y(-1)=2\\)\n\n\\(~\\)\nExample \\(\\,\\) Solve the given differential equation by finding an appropriate integrating factor\n\n\\(~y(x+y+1)dx + (x+2y)dy=0\\)",
    "crumbs": [
      "**First Semester**",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>First-Order Differential Equations</span>"
    ]
  },
  {
    "objectID": "ch_02_First-Order_Differential_Equations.html#sec-2-5",
    "href": "ch_02_First-Order_Differential_Equations.html#sec-2-5",
    "title": "2  First-Order Differential Equations",
    "section": "2.5 Solutions by Substitutions",
    "text": "2.5 Solutions by Substitutions\nSubstitution is often used to get a DE in a form that a known procedure can be used to find a solution\n\nReduction to separation of variables can be facilitated in the DE\n\\[\\frac{dy}{dx}=f(Ax+By+C)~\\]\nby substituting \\(\\,u=Ax+By+C, \\;B \\neq 0\\)\n\n\\(~\\)\nExample \\(\\,\\) Solve the IVP \\(~\\displaystyle\\frac{dy}{dx} = (-2x +y)^2 -7, \\;y(0)=0\\)\nLet \\(\\,u=-2x+y\\), then \\(~\\displaystyle\\frac{du}{dx}=-2 + \\frac{dy}{dx}~\\) giving \\(~\\displaystyle\\frac{du}{dx} = u^2 -9\\)\n\\(~\\)\n\nx = sympy.Symbol('x')\ny = sympy.Function('y')\n\neq = y(x).diff(x) -(-2 *x +y(x))**2 +7\n\ndsolve(eq, ics={y(0): 0})\n\n\\(\\displaystyle y{\\left(x \\right)} = \\frac{- 2 x e^{6 x} - 2 x + 3 e^{6 x} - 3}{- e^{6 x} - 1}\\)\n\n\n\\(~\\)\n\nHomogeneous first-order DE\nA first-order ordinary DE in the form:\n\\[\\frac{dy}{dx}=f(x,y) = - \\frac{M(x,y)}{N(x,y)}\\]\nis a homogeneous type if both function \\(M(x,y)\\) and \\(N(x,y)\\) are homogeneous functions of the same degree \\(n\\):\n\\[M(\\lambda x, \\lambda y) = \\lambda^n M(x, y), \\;\\; N(\\lambda x, \\lambda y) = \\lambda^n N(x, y)\\]\nThus, we can let \\(\\color{blue}{t=1/x}~\\) to simplify this quotient to a function \\(f\\) of the single variable \\(y/x\\):\n\\[\\frac{M(x, y)}{N(x, y)}=\\frac{M(t x,t y)}{N(t x, t y)} = \\frac{M\\left(1, \\tfrac{y}{x}\\right)}{N\\left(1,\\tfrac{y}{x}\\right)}\n=-f\\left(\\frac{y}{x}\\right) = -\\frac{dy}{dx}\\]\nThe change of variables \\(y=ux\\) transforms the original differential equation into the separable form:\n\\[\\frac{du}{f(u)-u}=\\frac{dx}{x}\\]\n\n\\(~\\)\nExample \\(\\,\\) Solve \\((x^2 +y^2) dx +(x^2 -xy) dy = 0\\)\n\\(~\\)\n\nBernoulli DE:\n\\[~y'+P(x)y =Q(x)y^{n} \\;\\text{ where }\\; n \\neq 0~ \\text{ and } n \\neq 1\\]\n\nBernoulli equations are special because they are nonlinear differential equations with known exact solutions\nThe substitution \\(u=y^{1-n}\\) reduces any Bernoulli equation to a linear differential equation\n\n\n\\(~\\)\nExample \\(\\,\\) Solve \\(\\displaystyle y'-\\frac{2}{x}y=-x^2y^2\\)\nChanging variables \\(\\displaystyle u=\\tfrac{1}{y}\\), \\(~\\)\\(\\displaystyle u'=-\\tfrac{1}{y^2}y'\\) gives the equation\n\\[ u'+\\frac{2}{x}u=x^2\\]\n\\(~\\)\n\nx = sympy.Symbol('x')\ny = sympy.Function('y')\n\neq = y(x).diff(x) -2 /x *y(x) +x**2 *y(x)**2\n\ndsolve(eq, hint='Bernoulli')\n\n\\(\\displaystyle y{\\left(x \\right)} = \\frac{5 x^{2}}{C_{1} + x^{5}}\\)\n\n\n\\(~\\)\n\nRiccati DE\n\nRiccati equation is any first-order ordinary differential equation that is quadratic in the unknown function. In other words, it is an equation of the form\n\\[y'=q_0(x)+q_1(x)y+q_2(x)y^2\\]\nwhere \\(q_0(x)\\neq 0\\) and \\(q_2(x)\\neq 0\\). If \\(q_0(x)=0\\), the equation is Bernoulli one, and if \\(q_2(x)=0\\), the equation is linear one\nThe new variable \\(v=yq_2\\) satisfies an equation\n\\[v'=v^2+R(x)v+S(x)\\]\nwhere \\(S(x)=q_2 q_0\\) and \\(\\displaystyle R(x)=q_{1}+\\left({\\tfrac{q_{2}'}{q_{2}}}\\right)\\)\nSubstituting \\(\\displaystyle v=-\\tfrac{u'}{u\\,}\\), \\(~u\\) satisfies the linear 2nd order ODE\n\\[u''-R(x)u'+S(x)u=0\\] A solution of this equation will lead to a solution \\(\\displaystyle y=-\\tfrac{u'}{q_2u}\\) of the original Riccati equation\n\n\n\\(~\\)\nExample \\(\\,\\) Solve the given differential equation by using an appropriate substitution\n\n\\(~(x-y)dx+xdy=0\\)\n\\(~xdx+(y-2x)dy=0\\)\n\\(~\\displaystyle \\frac{dy}{dx}=\\frac{y-x}{y+x}\\)\n\\(~\\displaystyle x\\frac{dy}{dx}=y+\\sqrt{x^2-y^2}, \\;\\;x&gt;0\\)\n\n\\(~\\)\n\nx = sympy.Symbol('x')\ny = sympy.Function('y')\n\neq = x *y(x).diff(x) -y(x) -sympy.sqrt(x**2 -y(x)**2)\n\ndsolve(eq, hint='1st_homogeneous_coeff_best')\n\n\\(\\displaystyle y{\\left(x \\right)} = - x \\sin{\\left(C_{1} - \\log{\\left(x \\right)} \\right)}\\)\n\n\n\\(~\\)\nExample \\(\\,\\) Solve the given initial-value problem\n\n\\(~\\displaystyle xy^2 \\frac{dy}{dx}=y^3-x^3, \\;\\;y(1)=2\\)\n\\(~\\displaystyle (x^2 +2y^2) \\frac{dx}{dy}=xy, \\;\\; y(-1)=1\\)\n\n\\(~\\)\nExample \\(\\,\\) Solve the given differential equation by using an appropriate substitution\n\n\\(~\\displaystyle x\\frac{dy}{dx} +y =\\frac{1}{y^2}\\)\n\\(~\\displaystyle 3(1+t^2)\\frac{dy}{dt}=2ty(y^3-1)\\)\n\\(~\\displaystyle \\frac{dy}{dx}=\\cos (x+y), \\;\\;y(0)=\\frac{\\pi}{4}\\)\n\n\\(~\\)",
    "crumbs": [
      "**First Semester**",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>First-Order Differential Equations</span>"
    ]
  },
  {
    "objectID": "ch_02_First-Order_Differential_Equations.html#sec-2-6",
    "href": "ch_02_First-Order_Differential_Equations.html#sec-2-6",
    "title": "2  First-Order Differential Equations",
    "section": "2.6 A Numerical Method",
    "text": "2.6 A Numerical Method\nNumerical method: \\(\\,\\) an algorithm for approximating the unknown solution to a DE\n\n\n\nLinearization approximates solutions within a small area around lineal elements in direction fields\nThe procedure of evaluating successive tangent lines is \\(~\\)Euler’s method\n\n\n\n\n\n\n\n\n\nExample \\(\\,\\) Consider \\(y'=0.1\\sqrt{y}+0.4x^2, \\;y(2)=4~\\) and approximate \\(y(2.5)\\) using \\(h=0.1\\)\n\nSubstituting into the general formula for Euler’s method gives\n\\[y_{n+1}=y_n +h\\left(0.1\\sqrt{y_n} + 0.4x_n^2 \\right)\\]\nConsidering the initial condition and \\(n=0\\)\n\\[\\scriptsize\n  \\begin{aligned}\n      y_1 &= 4+0.1\\left( 0.1\\sqrt{4} + 0.4 \\cdot2^2\\right ) =4.1800 \\\\\n      y_2 &= 4.18 +0.1\\left( 0.1\\sqrt{4.18} + 0.4 \\cdot2.1^2\\right ) =4.3768 \\\\\n      &\\, \\vdots \\\\\n      y_5 &= 5.0768\n\\end{aligned}\\]\n\n\nx0 = 2.0\ny0 = 4.0\nxf = 2.5\n\nh = 0.1\nn = int((xf -x0) /h) + 1\n\nx = np.linspace(x0, xf, n)\ny = np.zeros(n)\n\ny[0] = y0\nfor i in range(1, n):\n    y[i] = y[i -1] +h *(0.1 *np.sqrt(y[i -1]) +0.4 *x[i -1]**2)\n    print(f'x = {x[i]: 3.1f}, y = {y[i]: 5.4f}')\n\nplt.figure(figsize=(4, 4))\nplt.plot(x, y, 'o')\nplt.xlabel(\"$x$\")\nplt.ylabel(\"$y$\")\nplt.title(\"$h=0.1$\")\n\nplt.show()\n\nx =  2.1, y =  4.1800\nx =  2.2, y =  4.3768\nx =  2.3, y =  4.5914\nx =  2.4, y =  4.8244\nx =  2.5, y =  5.0768\n\n\n\n\n\nEuler’s Method\n\n\n\n\nExample \\(\\,\\) Use Euler’s method to obtain a four-decimal approximation of the indicated value. Carry out the recursion, first using \\(h=0.1\\) and then using \\(h=0.05\\)\n\\[~y'=2x-3y+1, \\;\\; y(1)=5; \\;\\;y(1.2)\\]",
    "crumbs": [
      "**First Semester**",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>First-Order Differential Equations</span>"
    ]
  },
  {
    "objectID": "ch_02_First-Order_Differential_Equations.html#sec-2-7",
    "href": "ch_02_First-Order_Differential_Equations.html#sec-2-7",
    "title": "2  First-Order Differential Equations",
    "section": "2.7 Linear Models",
    "text": "2.7 Linear Models\nExample \\(\\,\\) Series Circuits - For a series circuit containing a resister \\(R\\) and an inductor \\(L\\),\n\\(~\\)\n\n\n\n\n\nKirchoff’s second law gives\n\\[L\\frac{di}{dt}+Ri=E(t)\\]\nFor a series circuit containing a resister and a capacitor,\n\n\n\n\n\nKirchoff’s second law gives\n\\[Ri+\\frac{1}{C}q=E(t)\\]\nwhere \\(\\displaystyle i=\\frac{dq}{dt}\\). \\(~\\)Then \\(~\\)\\(\\displaystyle R\\frac{dq}{dt}+\\frac{1}{C}q=E(t)\\)\n\\(~\\)\nExample \\(\\,\\) The population of a community is known to increase at a rate proportional to the number of people present at time \\(t\\). If an initial population \\(P_0\\) has doubled in 5 years, how long will it take to triple? To quadruple?",
    "crumbs": [
      "**First Semester**",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>First-Order Differential Equations</span>"
    ]
  },
  {
    "objectID": "ch_02_First-Order_Differential_Equations.html#sec-2-8",
    "href": "ch_02_First-Order_Differential_Equations.html#sec-2-8",
    "title": "2  First-Order Differential Equations",
    "section": "2.8 Nonlinear Models",
    "text": "2.8 Nonlinear Models\n\\(~\\)\nExample \\(\\,\\) The logistic model\n\nVerhulst proposed a model, called the logistic model, for population growth in 1838. It does not assume unlimited resources. Instead, it assumes there is a carrying capacity \\(K\\) for the population\nThis carrying capacity is the stable population level. If the population is above \\(K\\), then the population will decrease, but if below, then it will increase\nFor this model, it is assumed that ther rate of change \\(\\frac{dy}{dt}\\) of the population \\(y\\) is proportional to the product of the current population \\(y\\) and \\(K − y\\)\n\\[\\frac{dy}{dt}=\\alpha y(K-y)\\]",
    "crumbs": [
      "**First Semester**",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>First-Order Differential Equations</span>"
    ]
  },
  {
    "objectID": "ch_02_First-Order_Differential_Equations.html#sec-2-9",
    "href": "ch_02_First-Order_Differential_Equations.html#sec-2-9",
    "title": "2  First-Order Differential Equations",
    "section": "2.9 Modeling with Systems of First-Order DEs",
    "text": "2.9 Modeling with Systems of First-Order DEs\n\\(~\\)\nExample \\(\\,\\) Radioactive Series:\n\\[X \\xrightarrow{k_1} Y \\xrightarrow{k_2} Z\\]\n\n\nThe decay of the initial element \\(X\\) is described by\n\\[\\frac{dx}{dt}=-k_1 x\\]\nThe second element \\(Y\\) is produced by the decay of \\(X\\) and loses from its own spontaneous decay\n\\[\\frac{dy}{dt}=k_1 x -k_2 y\\]\nThe stable element \\(z\\) is generated from the decay of \\(Y\\)\n\\[\\frac{dz}{dt}=k_2 y\\]\n\n\n\\(~\\)\nExample \\(\\,\\) Consider the Lotka-Volterra predator-prey model defined by\n\\[  \n\\begin{aligned}\n   \\frac{dx}{dt} &= -0.1x +0.02xy \\\\\n   \\frac{dy}{dt} &= 0.2y -0.025xy\n\\end{aligned}\\]\nwhere the populations \\(x(t)\\) (predator) and \\(y(t)\\) (prey) are measured in the thousands and \\(x(0)=6\\) and \\(y(0)=6\\)\n\nUse a numerical solver to graph \\(x(t)\\) and \\(y(t)\\)\n\n\n# Lotka-Volterra predator-prey model\nfrom scipy import integrate\n\na, b, c, d = -0.1, 0.02, 0.2, -0.025\n\ndef f(t, xy):\n    x, y = xy\n    return [a *x +b *x *y, c *y +d *x *y]\n\n# Initial condition and Time span\nxy0 = [6, 6]\ntf = 400\nt_eval = np.linspace(0, tf, 5*tf)\n\n# Numerical Solver\nsol = integrate.solve_ivp(f, [0, tf], xy0, t_eval=t_eval)\n\nt = sol.t\nxy_t = sol.y.T\n\n\n\nUse the graphs to approximate the period of each population\n\n\nfig, ax = plt.subplots(1, 1, figsize=(6, 5))\n\nax.plot(t, xy_t[:, 0], 'r', label=\"Predator\")\nax.plot(t, xy_t[:, 1], 'b', label=\"Prey\")\n\nax.set_xlabel(\"Time\")\nax.set_ylabel(\"Number of animals\")\nax.set_xlim(0, tf)\nax.set_ylim(0, 12)\nax.legend()\n\nplt.show()\n\n\n\n\nLotka-Volterra predator-prey model",
    "crumbs": [
      "**First Semester**",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>First-Order Differential Equations</span>"
    ]
  },
  {
    "objectID": "ch_02_First-Order_Differential_Equations.html#worked-exercises",
    "href": "ch_02_First-Order_Differential_Equations.html#worked-exercises",
    "title": "2  First-Order Differential Equations",
    "section": "Worked Exercises",
    "text": "Worked Exercises\nSolve the given differential equation by separation of variables\n2.2: 1. \\(\\,\\displaystyle \\frac{dy}{dx}=\\sin 5 x\\)\nSolution\n\\[\n\\begin{aligned}\n  \\frac{dy}{dx} &=\\sin 5x\\\\\n   &\\Downarrow \\\\\n   dy &= \\sin 5x \\,dx \\\\\n   &\\Downarrow \\\\\n   y &=-\\frac{1}{5} \\cos 5x +c\n\\end{aligned}\\]\n2.2: 3. \\(\\,\\displaystyle dx +e^{3x} dy = 0\\)\nSolution\n\\[\n\\begin{aligned}\n  dy &=-e^{-3x} \\,dx\\\\\n  &\\Downarrow \\\\\n  y &= \\frac{1}{3}e^{-3x} +c\n\\end{aligned}\\]\n2.2: 21. \\(\\,\\displaystyle \\frac{dy}{dx} = x\\sqrt{1 -y^2}\\)\nSolution\n\\[\n\\begin{aligned}\n\\frac{1}{\\sqrt{1 -y^2}}dy &= x \\,dx\\\\\n&\\Downarrow \\\\\n\\arcsin y &= \\frac{1}{2}x^2 +c \\\\\n&\\Downarrow \\\\\ny &= \\sin \\left(\\frac{1}{2}x^2 +c \\right)\n\\end{aligned}\\]\n2.2: 25. Find an implicit or an explicit solution of the given initial-value problem\n\\[ x^2 \\frac{dy}{dx} = y -xy, \\;\\;y(-1)=-1\\]\nSolution\n\\[\n\\begin{aligned}\n  \\frac{1}{y}dy &= \\frac{1-x}{x^2} \\,dx\\\\\n  &\\Downarrow \\\\\n  \\ln |y| &= -\\left(\\frac{1}{x} +\\ln |x| \\right) +c \\\\\n  &\\Downarrow {\\small y(-1)=-1 \\; \\Rightarrow \\; c=-1}\\\\\n  \\ln |y| &= -\\left(1 +\\frac{1}{x} \\right) -\\ln |x| \\\\\n  &\\Downarrow \\\\\n  y &= \\frac{1}{x} e^{-\\left( 1 +\\frac{1}{x}\\right)}\n\\end{aligned}\\]\n2.7: 41. \\(\\,\\) Evaporating Raindrop \\(\\,\\) As a raindrop falls, it evaporates while retaining its spherical shape. If we make the further assumptions that the rate at which the raindrop evaporates is proportional to its surface area and that air resistance is negligible, then a model for the velocity \\(v(t)\\) of the raindrop is\n\\[\n\\frac{dv}{dt} + \\frac{3(k/\\rho)}{(k/\\rho) t +r_0} v = g\n\\]\nHere \\(\\rho\\) is the density of water, \\(r_0\\) is the radius of the raindrop at \\(t=0\\), \\(k &lt; 0\\) is the constant of proportionality \\(\\displaystyle \\frac{dm}{dt}=4\\pi r^2 k\\), and the downward direction is taken to be the positive direction\n(a)\\(\\,\\) Solve for \\(v(t)\\) if the raindrop falls from the rest\n(b)\\(\\,\\) Show that the radius of the raindrop at time \\(t\\) is \\(r(t)=(k/\\rho)t +r_0\\)\n(c)\\(\\,\\) If \\(r_0=0.01\\) ft and \\(r =0.007\\) ft at time \\(t=10\\) sec after the raindrop falls from a cloud, determine the time at which the raindrop has evaporated completely\nSolution\n(a)\\(\\,\\)\n\\[\n\\begin{aligned}\n\\frac{dv}{dt} &+ \\frac{3(k/\\rho)}{(k/\\rho) t +r_0} v = g \\\\\n&\\Downarrow {\\small \\text{ multiply by the integral factor }} \\\\\n\\frac{d}{dt} \\left[ \\left(\\frac{k}{\\rho}t +r_0 \\right)^3 v  \\right] &= \\left(\\frac{k}{\\rho}t +r_0 \\right)^3 g\\\\\n&\\Downarrow \\\\\nv(t)=\\frac{g\\rho}{4k} \\left( \\frac{k}{\\rho} t +r_0 \\right) &+c\\left(\\frac{k}{\\rho}t +r_0 \\right)^{-3} \\\\\n&\\Downarrow {\\small \\text{ } v(0)=0 \\; \\rightarrow \\;c = -\\frac{g\\rho}{4k} r_0^4} \\\\\nv(t) = \\frac{g\\rho}{4k} \\left(\\frac{k}{\\rho}t +r_0 \\right) & \\left[ 1 - \\left( \\frac{r_0}{ \\frac{k}{\\rho}t +r_0 }\n\\right)^4 \\right] \\\\\n\\end{aligned}\\]\n(b)\\(\\,\\)\n\\[\n\\begin{aligned}\nm &= \\frac{4}{3}\\pi r^3 \\rho\\\\\n&\\Downarrow \\\\\n\\frac{dm}{dt} &= 4\\pi r^2 \\rho \\frac{dr}{dt}\\\\\n&\\Downarrow  \\text{ } {\\scriptstyle \\frac{dm}{dt}=4\\pi r^2 k}\\\\\n  \\frac{dr}{dt} &= \\frac{k}{\\rho} \\\\\n&\\Downarrow \\text{ } {\\scriptstyle r=r_0 \\text{ at } t=0} \\\\\nr(t) &= \\frac{k}{\\rho} t +r_0  \n\\end{aligned}\\]\n(c)\\(\\,\\)\n\\[\n\\begin{aligned}\nr(10) &= \\frac{k}{\\rho} \\times 10 +0.01 =0.007 \\; \\Rightarrow \\; \\frac{k}{\\rho} = -0.0003\\\\\n&\\Downarrow \\\\\nr(t)  &= -0.0003 t + 0.01 = 0 \\; \\Rightarrow \\; t =\\frac{0.01}{0.0003} \\approx 33.3 \\;\\text{sec}\n\\end{aligned}\\]\n\\(~\\)\n1. \\(~\\)Find an explicit solution of the given initial value problem\n\\[\\sqrt{1-y^2} \\,dx -\\sqrt{1-x^2}\\,dy=0, \\;y(0)=\\sqrt{3}/2\\]\nSolution\nStep 1: \\(~\\)Rearrange the Equation\nWe write the equation in differential form:\n\\[\\sqrt{1 - y^2} \\, dx = \\sqrt{1 - x^2} \\, dy\\]\nSeparate variables:\n\\[\\frac{dx}{\\sqrt{1 - x^2}} = \\frac{dy}{\\sqrt{1 - y^2}}\\]\nStep 2: \\(~\\)Integrate Both Sides\n\\[\\int \\frac{dx}{\\sqrt{1 - x^2}} = \\int \\frac{dy}{\\sqrt{1 - y^2}}\\]\nThese are standard integrals:\n\n\\(\\displaystyle\\scriptsize \\int \\frac{dx}{\\sqrt{1 - x^2}} = \\arcsin x + C\\)\n\\(\\displaystyle\\scriptsize \\int \\frac{dy}{\\sqrt{1 - y^2}} = \\arcsin y + C\\)\n\nSo:\n\\[\\arcsin x = \\arcsin y + C\\]\nStep 3: \\(~\\)Solve for the Constant Using the Initial Condition\nWe’re given \\(y(0) = \\frac{\\sqrt{3}}{2}\\), so plug in \\(x = 0\\), \\(y = \\frac{\\sqrt{3}}{2}\\):\n\\[\\arcsin(0) = \\arcsin\\left(\\frac{\\sqrt{3}}{2}\\right) + C\\]\n\\[0 = \\frac{\\pi}{3} + C \\Rightarrow C = -\\frac{\\pi}{3}\\]\nStep 4: \\(~\\)Write the Explicit Solution\nFrom earlier:\n\\[\\arcsin x = \\arcsin y - \\frac{\\pi}{3}\\]\nSolve for \\(y\\):\n\\[\\arcsin y = \\arcsin x + \\frac{\\pi}{3}\\]\nNow apply \\(\\sin\\) to both sides:\n\\[y = \\sin\\left(\\arcsin x + \\frac{\\pi}{3}\\right)\\]\n\\(~\\)\n2. \\(~\\)Find an explicit solution of the given initial value problem\n\\[\\frac{dy}{dx} =-y\\ln y, \\;y(0)=e\\]\nSolution\nStep 1: \\(~\\)Separate Variables\nWe want to isolate \\(y\\) and \\(x\\):\n\\[\\frac{dy}{dx} = -y \\ln y\n\\quad \\Rightarrow \\quad\n\\frac{1}{y \\ln y} \\, dy = -dx\\]\nStep 2: \\(~\\)Integrate Both Sides\nLet’s handle the left-hand side:\nLet \\(u = \\ln y\\), then:\n\\[\\frac{du}{dy} = \\frac{1}{y} \\Rightarrow du = \\frac{1}{y} \\, dy\\]\nSo:\n\\[\\frac{1}{y \\ln y} \\, dy = \\frac{1}{u} \\cdot du = \\frac{du}{u}\\]\nThus, we have:\n\\[\\int \\frac{dy}{y \\ln y} = \\int \\frac{du}{u} = \\ln |\\ln y|\\]\nSo now integrate both sides:\n\\[\\int \\frac{dy}{y \\ln y} = \\int -dx \\quad \\Rightarrow \\quad \\ln|\\ln y| = -x + C\\]\nStep 3: \\(~\\)Solve for \\(y(x)\\)\nStart with:\n\\[\\ln|\\ln y| = -x + C\\]\nExponentiate both sides:\n\\[|\\ln y| = e^{-x + C} = A e^{-x}, \\quad A &gt; 0\\]\nSo:\n\\[\\ln y = \\pm A e^{-x}\\]\nWe drop the \\(\\pm\\) by using a general constant \\(B = \\pm A\\), so:\n\\[\\ln y = B e^{-x} \\Rightarrow y = e^{B e^{-x}}\\]\nStep 4: \\(~\\)Apply Initial Condition \\(y(0) = e\\)\nPlug in:\n\\[y(0) = e^{B e^0} = e^{B} = e\n\\Rightarrow B = 1\\]\nFinal Answer:\n\\[y(x) = e^{e^{-x}}\\]\nThis is the explicit solution satisfying the differential equation and the initial condition \\(y(0) = e\\)\n\\(~\\)\n3. \\(~\\)Solve the given initial-value problem\n\\[\\frac{dy}{dx} = \\frac{3x+2y}{3x+2y+2}, \\;y(-1)=-1\\]\nSolution\nStep 1: \\(~\\)Make a Substitution\nLet us simplify the expression by setting:\n\\[u = 3x + 2y\\]\nDifferentiate both sides with respect to x:\n\\[\\frac{du}{dx} = 3 + 2\\frac{dy}{dx}\\]\nNow solve for from this:\n\\[\\frac{dy}{dx} = \\frac{1}{2} \\left( \\frac{du}{dx} - 3 \\right)\\]\nBut also, from the original equation:\n\\[\\frac{dy}{dx} = \\frac{u}{u + 2}\\]\nSo equating both expressions:\n\\[\\frac{1}{2} \\left( \\frac{du}{dx} - 3 \\right) = \\frac{u}{u + 2}\\]\nSo:\n\\[\\frac{du}{dx} = 3 + \\frac{2u}{u + 2}\\]\nStep 2: \\(~\\)Simplify the Right-Hand Side\nWe simplify and so:\n\\[\\frac{du}{dx} = \\frac{5u + 6}{u + 2}\\]\nStep 3: \\(~\\)Separate Variables and Integrate\nWe separate:\n\\[\\frac{u + 2}{5u + 6} \\, du = dx\\]\nLet’s integrate the left-hand side. Let’s simplify:\n\\[\\int \\frac{u + 2}{5u + 6} \\, du = \\int \\left[ \\frac{1}{5} + \\frac{4}{5(5u + 6)} \\right] du\n= \\frac{1}{5} u + \\frac{4}{25} \\ln |5u + 6| + C\\]\nSo:\n\\[x = \\frac{1}{5} u + \\frac{4}{25} \\ln |5u + 6| + C\\]\nStep 4: \\(~\\)Substitute Back \\(u = 3x + 2y\\)\nRecall \\(u = 3x + 2y\\). So we substitute:\n\\[x = \\frac{1}{5}(3x + 2y) + \\frac{4}{25} \\ln |5(3x + 2y) + 6| + C\\]\nThis is an implicit solution\nStep 5: \\(~\\)Apply the Initial Condition \\(y(-1) = -1\\)\nRecall \\(u = 3x + 2y\\), so at \\(x = -1\\), \\(y = -1\\):\n\\[u = 3(-1) + 2(-1) = -5\n\\Rightarrow 5u + 6 = -25 + 6 = -19\\]\nThen:\n\\[-1 = \\frac{1}{5}(-5) + \\frac{4}{25} \\ln(19) + C\n\\Rightarrow C = - \\frac{4}{25} \\ln 19\\]\nFinal Answer:\n\\[\nx = \\frac{1}{5}(3x + 2y) + \\frac{4}{25} \\ln |5(3x + 2y) + 6| - \\frac{4}{25} \\ln 19\n\\]\n\\(~\\)\n4. \\(~\\)Solve the given differential equation\n\\[\\frac{dy}{dx} = \\tan^2(x+y)\\]\nSolution\nStep 1: \\(~\\)Make a Substitution\nLet’s simplify the expression using substitution.\nLet:\n\\[u = x + y \\quad \\Rightarrow \\quad \\frac{du}{dx} = 1 + \\frac{dy}{dx}\\]\nFrom the original equation:\n\\[\\frac{dy}{dx} = \\tan^2(x + y) = \\tan^2 u\\]\nSo:\n\\[\\frac{du}{dx} = 1 + \\tan^2 u = \\sec^2 u\\]\nStep 2: \\(~\\)Separate Variables and Integrate\n\\[\\frac{du}{\\sec^2 u} = dx \\quad \\Rightarrow \\quad \\cos^2 u \\, du = dx\\]\nSo integrate both sides:\n\\[\\int \\cos^2 u \\, du = \\int \\frac{1 + \\cos(2u)}{2} \\, du = \\frac{1}{2} \\left[ u + \\frac{1}{2} \\sin(2u) \\right]\\]\nSo:\n\\[\\frac{1}{2} u + \\frac{1}{4} \\sin(2u) = x + C\\]\nStep 3: \\(~\\)Substitute Back \\(u = x + y\\)\n\\[\\frac{1}{2}(x + y) + \\frac{1}{4} \\sin(2(x + y)) = x + C\\]\nwhere \\(C\\) is a constant of integration. This is an implicit solution to the original differential equation\n\\(~\\)\n5. \\(~\\) Find the solution of period \\(2\\pi\\) of the equation\n\\[ y' +(\\cos x) y = \\sin 2x\\]\nSolution\nStep 1: \\(~\\)Solve Using the Integrating Factor Method\nIntegrating Factor\n\\[\\mu(x) = e^{\\int \\cos x \\, dx} = e^{\\sin x}\\]\nMultiply both sides of the ODE by \\(\\mu(x) = e^{\\sin x}\\):\n\\[e^{\\sin x} y’ + e^{\\sin x} \\cos x \\, y = e^{\\sin x} \\sin 2x\\]\nLeft-hand side becomes:\n\\[\\frac{d}{dx}\\left( e^{\\sin x} y \\right) = e^{\\sin x} \\sin 2x\\]\nStep 2: \\(~\\)Integrate Both Sides\n\\[\\int \\frac{d}{dx}(e^{\\sin x} y) \\, dx = \\int e^{\\sin x} \\sin 2x \\, dx\\]\nSo:\n\\[y(x) = e^{-\\sin x} \\left( \\int e^{\\sin x} \\sin 2x \\, dx + C \\right)\\]\nStep 3: \\(~\\)Impose Periodicity\nLet’s denote:\n\\[I(x) := \\int e^{\\sin x} \\sin 2x \\, dx\n\\Rightarrow y(x) = e^{-\\sin x} (I(x) + C)\\]\nLet’s write the condition:\n\\[\n\\begin{aligned}\ny(x + 2\\pi) = y(x) \\; &\\Rightarrow \\; e^{-\\sin x} (I(x + 2\\pi) + C) = e^{-\\sin x} (I(x) + C)\\\\\n&\\Rightarrow I(x + 2\\pi) = I(x)\n\\end{aligned}\\]\nBut:\n\\[\nI(x + 2\\pi) = \\int_{0}^{x + 2\\pi} e^{\\sin t} \\sin 2t \\, dt = \\int_{0}^{x} e^{\\sin t} \\sin 2t \\, dt + \\int_{x}^{x+2\\pi} e^{\\sin t} \\sin 2t \\, dt\\]\nTherefore, periodicity condition is:\n\\[I(x + 2\\pi) - I(x) = \\int_{x}^{x + 2\\pi} e^{\\sin t} \\sin 2t \\, dt = 0\\]\nSo the \\(2\\pi\\)-periodic solution is:\n\\[y(x) = e^{-\\sin x} \\int_0^x e^{\\sin t} \\sin 2t \\, dt\\]\n\\(~\\)\n6. \\(~\\) Find the solution of period \\(2\\pi\\) of the equation\n\\[ y' +3 y = \\cos x\\]\nSolution\nStep 1: \\(~\\)General Solution via Integrating Factor\nThis is a first-order linear ODE. Let’s solve it using the integrating factor method.\nLet:\n\\(\\mu(x) = e^{\\int 3 \\, dx} = e^{3x}\\)\nMultiply both sides of the equation by \\(e^{3x}\\):\n\\[e^{3x} y’ + 3 e^{3x} y = e^{3x} \\cos x\n\\Rightarrow \\frac{d}{dx}(e^{3x} y) = e^{3x} \\cos x\\]\nNow integrate both sides:\n\\[e^{3x} y = \\int e^{3x} \\cos x \\, dx\\]\nStep 2: \\(~\\)Compute the Integral\n\\[\\int e^{3x} \\cos x \\, dx = \\frac{e^{3x}}{10}(3 \\cos x + \\sin x) + C\\]\nThus:\n\\[e^{3x} y = \\frac{e^{3x}}{10}(3 \\cos x + \\sin x) + C\n\\Rightarrow y(x) = \\frac{1}{10}(3 \\cos x + \\sin x) + C e^{-3x}\\]\nStep 3: \\(~\\)Impose Periodicity\nWe are asked to find a solution with period \\(2\\pi\\). Let’s analyze:\n\nThe term \\(\\frac{1}{10}(3 \\cos x + \\sin x)\\) is clearly \\(2\\pi\\)-periodic\nBut the term \\(C e^{-3x}\\) is not periodic unless \\(C = 0\\)\n\nSo to obtain a periodic solution, we must take: \\(C = 0\\)\nFinal Answer:\n\\[y(x) = \\frac{1}{10}(3 \\cos x + \\sin x)\\]\n\\(~\\)\n7. \\(~\\) Solve the given differential equation by using an appropriate substitution\n\\[ \\frac{dy}{dx} = \\sin(x+y)\\]\nSolution\nStep 1: \\(~\\)Use Substitution\nLet:\n\\[u = x + y \\quad \\Rightarrow \\quad \\frac{du}{dx} = 1 + \\frac{dy}{dx}\\]\nSubstitute into the derivative of \\(u\\): \\[\\frac{du}{dx} = 1 + \\sin u\\]\nStep 2: \\(~\\)Separate Variables\nWe now have the separable equation:\n\\[\\frac{du}{1 + \\sin u} = dx\\]\nWe simplify the left-hand side:\n\\[\\frac{1}{1 + \\sin u} = \\frac{1 - \\sin u}{(1 + \\sin u)(1 - \\sin u)}\n= \\frac{1 - \\sin u}{1 - \\sin^2 u} = \\frac{1 - \\sin u}{\\cos^2 u}\\]\nNow integrate:\n\\[\\int \\sec^2 u \\, du = \\tan u, \\quad\n\\int \\sec u \\tan u \\, du = \\sec u\\]\nSo:\n\\[\\int \\frac{du}{1 + \\sin u} = \\tan u - \\sec u + C = x + C\\]\nStep 3: \\(~\\)Substitute Back \\(u = x + y\\)\n\\[\\tan(x + y) - \\sec(x + y) = x + C\\]\nThis is the implicit general solution to the differential equation\n\\(~\\)\n8. \\(~\\) Find a general solution of the equation\n\\[ydx+(2x+xy-3)dy=0\\]\nSolution\n\\[\\begin{aligned}\nydx +(2x+xy-3)&dy =0 \\\\\n&\\Downarrow \\\\\n\\frac{dx}{dy} + \\left(1 + \\frac{2}{y} \\right)x &= \\frac{3}{y} \\\\\n&\\Downarrow \\\\  \n\\frac{d}{dy} \\left[ x \\,y^2 e^y \\right ] &= 3y e^y \\\\\n&\\Downarrow \\\\\n\\color{red}{x = \\frac{3}{y}\\left(1-\\frac{1}{y} \\right)}\\, & \\color{red}{\\, + \\,Cy^{-2} e^{-y}}\n\\end{aligned}\\]\n\\[~\\text{or}~\\]\n\\[\\begin{aligned}\nydx \\;+\\;&(2x+xy-3)dy =0 \\\\\n&\\Downarrow \\;\\; \\times \\, y e^y\\\\\ny^2 e^y dx \\;+\\;&y e^y (2x+xy-3)dy =0 \\\\\n&\\Downarrow \\\\  \n\\frac{\\partial f}{\\partial x} = y^2 e^y \\;\\;&\\rightarrow \\;\\; f(x,y) =y^2 e^y x + h(y)\\\\\n&\\Downarrow \\\\\n\\frac{\\partial f}{\\partial y} = ye^y(2x +xy) +\\frac{dh}{dy} \\;\\;&\\rightarrow \\;\\; \\frac{dh}{dy} = -3y e^y\\\\\n&\\Downarrow \\\\  \nh(y) =-3e^y (y-1) - C \\;\\;&\\rightarrow \\;\\; f(x,y) =y^2 e^y x -3e^y (y-1) = C \\\\\n&\\Downarrow \\\\\n\\color{red}{x = \\frac{3}{y}\\left(1-\\frac{1}{y} \\right)}\\, & \\color{red}{\\, + \\,Cy^{-2} e^{-y}}\n\\end{aligned}\\]\n\\(~\\)\n9. \\(~\\) Solve the differential equation\n\\[ xv\\frac{dv}{dx} +v^2=32x \\]\nSolution \\[\n\\begin{aligned}\nxv\\frac{dv}{dx} +v^2 &=32x \\\\\n&\\Downarrow \\\\\n\\frac{dv}{dx} +\\frac{1}{x} v &= \\frac{32}{v}\\\\\n&\\Downarrow \\;\\; u = v^2, \\; \\frac{du}{dx} = 2v \\frac{dv}{dx}\\\\\n\\frac{du}{dx} +\\frac{2}{x}u &= 64\\\\\n&\\Downarrow \\\\\n\\frac{d}{dx} \\left( u x^2 \\right ) &= 64x^2 \\\\\n&\\Downarrow \\\\\nu = \\frac{64}{3} x + \\frac{C}{x^2} \\;\\;&\\rightarrow \\;\\; \\color{red}{v = \\pm\\sqrt{\\frac{64}{3} x + \\frac{C}{x^2}}}\n\\end{aligned}\n\\]\n\\(~\\)\n10. \\(~\\) Solve the initial-value problem\n\\[\\frac{dy}{dt} + 2(t+1) y^2=0, \\;\\;y(0)=-\\frac{1}{8}\\]\nand give the largest interval \\(I\\) on which the solution is defined:\nSolution\n\\[\\begin{aligned}\n\\frac{dy}{dt} &+2(t+1)y^2 = 0 \\\\\n&\\Downarrow \\\\\n\\frac{dy}{y^2} &= -2(t+1) \\, dt \\\\\n&\\Downarrow \\\\\ny &= \\frac{1}{t^2 + 2t + c}, \\;\\; y(0)=-\\frac{1}{8} \\\\\n&\\Downarrow \\\\\n\\color{red}{y} &\\color{red}{= \\frac{1}{(t-2)(t+4)}, \\;\\;-4&lt;t&lt;2}\n\\end{aligned}\n\\]\n\\(~\\)",
    "crumbs": [
      "**First Semester**",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>First-Order Differential Equations</span>"
    ]
  },
  {
    "objectID": "ch_03_Higher-Order_Differential_Equations.html",
    "href": "ch_03_Higher-Order_Differential_Equations.html",
    "title": "3  Higher-Order Differential Equations",
    "section": "",
    "text": "3.1 Theory of Linear Equations\n\\(~\\)\nExample \\(\\,\\) Given that \\(\\,x(t)=c_1\\cos\\omega t +c_2 \\sin\\omega t~\\) is the general solution of \\(x''+\\omega^2x=0~\\) on the interval \\((-\\infty,\\infty)\\), \\(\\,\\) show that a solution satisfying the initial conditions \\(x(0)=x_0\\), \\(x'(0)=x_1\\), is given by\n\\[x(t)=x_0\\cos\\omega t+\\frac{x_1}{\\omega} \\sin\\omega t\\]\nExample \\(\\,\\) Determine whether the given set of functions is linearly dependent or linearly independent on the interval \\((-\\infty,\\infty)\\)\n\\[f_1(x)=x, ~f_2(x)=x^2, ~f_3(x)=4x-3x^2\\]\n\\[f_1(x)=5, ~f_2(x)=\\cos^2 x, ~f_3(x)=\\sin^2 x\\]\nExample \\(\\,\\) Verify that the given functions form a fundamental set of solutions of the differential equation on the indicated interval. Form the general solution of the equation\n\\[y''-y'-12y=0; \\;\\; e^{-3x}, \\; e^{4x}, \\;\\;(-\\infty,\\infty)\\]\n\\[y''-2y'+5y=0; \\;\\;e^x \\cos 2x, \\;\\; e^x\\sin 2x, \\;\\;(-\\infty,\\infty)\\]\nExample \\(\\,\\) Verify that the given two-parameter family of functions is the general solution of the nonhomogeneous differential equation on the indicated interval\n\\[y''-7y'+10y=24e^x; \\;\\;y=c_1 e^{2x} +c_2 e^{5x} +6e^x; \\;\\;(-\\infty,\\infty)\\]\n\\[y''-4y'+4y=2e^{2x} +4x-12; \\;\\;y=c_1 e^{2x} +c_2 xe^{2x} +x^2e^{2x} +x -2; \\;\\;(-\\infty,\\infty)\\]",
    "crumbs": [
      "**First Semester**",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Higher-Order Differential Equations</span>"
    ]
  },
  {
    "objectID": "ch_03_Higher-Order_Differential_Equations.html#sec-3-1",
    "href": "ch_03_Higher-Order_Differential_Equations.html#sec-3-1",
    "title": "3  Higher-Order Differential Equations",
    "section": "",
    "text": "In an initial-value problem (IVP), \\(\\,\\) we seek a solution \\(y(x)\\) of a \\(n\\)th order linear DE so that \\(y(x)\\) satisfies initial conditions at \\(x_0\\)\n\n\\(n\\)th order linear DE: \\(\\;a_n(x) \\neq 0\\)\n \\[\\underbrace{a_n(x) \\frac{d^ny}{dx^n} +a_{n-1}(x) \\frac{d^{n-1}y}{dx^{n-1}} +\\cdots + a_1(x) \\frac{dy}{dx} +a_0(x) y}_{L(y): ~\\mathrm{Linear\\;Operator}}= g(x)\n\\] \nInitial conditions\n \\[y(x_0) = y_0, \\; y'(x_0) = y_1, \\cdots, \\; y^{(n-1)}(x_0) = y_{n-1}\\] \n\nBoundary-value problem (BVP) consists of solving a linear DE of order 2 or greater, in which the dependent variable \\(y\\) or its derivatives are specified at different points\n\nFor example,\n\\[a_2(x) \\frac{d^2y}{dx^2} +a_1(x) \\frac{dy}{dx} +a_0(x)y=g(x)\\]\nBoundary conditions\n\\[y(x_0)=y_0, \\;y(x_1)=y_1\\]\n\nThe sum, or  superposition , of two or more solutions of a homogeneous linear DE is also a solution\nAny set of \\(n\\) linearly independent solutions \\(y_1, y_2, \\cdots, y_n\\) of the \\(n\\)th order homogeneous linear DE on interval \\(~I\\) is a fundamental set of solutions\nIf two functions are linearly dependent, then one is a constant multiple of the other (otherwise, they are linearly independent)\nIf \\(\\{y_1, y_2, \\cdots, y_n\\}\\) are a set of linearly independent functions, the Wronskian function is not singular:\n\\[ W(y_1, y_2, \\cdots, y_n)= \\begin{vmatrix}\n     y_1 & y_2 & \\cdots & y_n \\\\\n     y_1' & y_2' & \\cdots & y_n'\\\\\n     \\vdots & \\vdots & \\ddots & \\vdots\\\\\n     y_1^{(n-1)} & y_2^{(n-1)} & \\cdots & y_n^{(n-1)}\n  \\end{vmatrix} \\neq 0 \\]\nGeneral solution of \\(~n\\)th order homogeneous linear DE is\n\\[y(x)=c_1 y_1(x) +c_2 y_2(x) + \\cdots + c_n y_n(x)\\]\nwhere \\(\\,y_1, y_2, \\cdots, y_n\\,\\) is a fundamental set of solutions and \\(c_i, \\;i=1,2,\\cdots,n\\;\\) are arbitrary constants\nGeneral solution of \\(n\\)th order nonhomogeneous linear DE is\n \\[y(x)=c_1 y_1(x) +c_2 y_2(x) + \\cdots + c_n y_n(x) +y_p(x)\\] \nwhere \\(\\,y_1, y_2, \\cdots, y_n\\,\\) is a fundamental set of solutions, \\(\\,y_p\\) is a particular solution, and \\(\\,c_i, \\;i=1,2,\\cdots,n\\;\\) are arbitrary constants",
    "crumbs": [
      "**First Semester**",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Higher-Order Differential Equations</span>"
    ]
  },
  {
    "objectID": "ch_03_Higher-Order_Differential_Equations.html#sec-3-2",
    "href": "ch_03_Higher-Order_Differential_Equations.html#sec-3-2",
    "title": "3  Higher-Order Differential Equations",
    "section": "3.2 Reduction of Order",
    "text": "3.2 Reduction of Order\n\nReduction of order can be used to reduce a linear second-order DE with known solution \\(y_1\\) into a linear first-order DE, which can be solved for a second solution \\(y_2\\)\nApplying reduction of order \\(y_2 = u(x) y_1\\) to the standard form of a second-order linear homogeneous DE\n\\[y''+P(x)y' +Q(x)y=0\\]\ngives\n\\[\n\\begin{aligned}\n   y_1 u'' & +\\left(2y_1' +P(x)y_1\\right) u' = 0\\\\\n   &\\Downarrow \\;{\\scriptsize\\times\\, y_1, \\; u'=w}\\\\\n   (y_1^2 w)' &= -(y_1^2 w) P(x) \\\\\n   &\\Downarrow \\\\\n   \\color{red}{y_2(x)} &\\color{red}{= y_1(x) {\\LARGE\\int} {\\small\\frac{\\exp\\left(-{\\displaystyle\\int} P(x) \\,dx \\right)}{y_1^2(x)} \\,dx}}\n\\end{aligned}\\]\n\nExample \\(\\,\\) The indicated function \\(y_1(x)\\) is a solution of the given equation. Use the reduction of order to find a second solution \\(y_2(x)\\)\n\n\\(y'' -4y' +4y=0; \\;\\;y_1=e^{2x}\\)\n\\(y''+16y=0; \\;\\; y_1=\\cos 4x\\)\n\\(y''-y=0; \\;\\; y_1=\\cosh x\\)\n\nExample \\(\\,\\) The indicated function \\(y_1(x)\\) is a solution of the associated homogeneous equation. Use the reduction of order to find a second solution \\(y_2(x)\\) of homogeneous equation and a particular solution \\(y_p(x)\\) of the given nonhomogeneous equation\n\\[y''-4y=2; \\;\\;y_1=e^{-2x}\\]",
    "crumbs": [
      "**First Semester**",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Higher-Order Differential Equations</span>"
    ]
  },
  {
    "objectID": "ch_03_Higher-Order_Differential_Equations.html#sec-3-3",
    "href": "ch_03_Higher-Order_Differential_Equations.html#sec-3-3",
    "title": "3  Higher-Order Differential Equations",
    "section": "3.3 Homogeneous Linear Equations with Constant Coefficients",
    "text": "3.3 Homogeneous Linear Equations with Constant Coefficients\nThe general solution of \\(ay''+by'+cy=0\\,\\) is found by substituting \\(\\,y=e^{\\,px}\\) and solving the resulting characteristic equation \\(\\,ap^2+bp+c=0\\,\\) for roots \\(\\color{red}{p_1}\\) and \\(\\color{red}{p_2}\\)\n\nCase I \\(~\\) \\(p_1\\) and \\(\\,p_2\\) are real and distinct\n\\[y = c_1 {\\color{red}{e^{\\,p_1 x}}} +c_2 {\\color{red}{e^{\\,p_2 x}}}\\]\nCase II \\(~\\) \\(p_1\\) and \\(\\,p_2\\) are real and equal\n\\[y=c_1 {\\color{red}{e^{\\,p_1x}}} +c_2 \\color{red}{x e^{\\,p_1 x}}\\]\nCase III \\(~\\) \\(p_1\\) and \\(\\,p_2\\) are complex conjugate: \\(~p_1, p_2 = \\color{red}{\\alpha} \\pm i\\color{red}{\\beta}\\)\n\\[y={\\color{red}{e^{\\alpha x}}} \\left(c_1 {\\color{red}{\\cos\\beta x}} +c_2 {\\color{red}{\\sin\\beta x}} \\right)\\]\n\n\\(~\\)\nExample \\(\\,\\) Solve \\(~y'' +\\omega^2y=0\\,\\) and \\(\\,y'' -\\omega^2y=0\\)\n\\(~\\)\nExample \\(\\,\\) Solve \\(~3y'''+5y''+10y'-4y=0\\)\n\nfrom sympy import Symbol, init_printing\nfrom sympy.solvers import solve\ninit_printing()\n\nx = Symbol('x')\nsolve(3*x**3 +5*x**2 +10*x -4, x)\n\n\\(\\displaystyle \\left[ \\frac{1}{3}, \\  -1 - \\sqrt{3} i, \\  -1 + \\sqrt{3} i\\right]\\)\n\n\n\\(~\\)\nExample \\(\\,\\) Find the general solution of the given second-order differential equation\n\n\\(4y'' +y'=0\\)\n\\(y''-y'-6y=0\\)\n\\(y'' +8y' +16y=0\\)\n\n\\(~\\)\nExample \\(\\,\\) Find the general solution of the given higher-order differential equation\n\n\\(y''' -4y'' -5y'=0\\)\n\\(y''' -5y'' +3y' +9y=0\\)\n\n\\(~\\)\nExample \\(\\,\\) Solve the initial-value problem\n\\(y''' +12y'' +36y'=0, \\;\\;y(0)=0, \\;y'(0)=1, \\; y''(0)=-7\\)\n\\(~\\)",
    "crumbs": [
      "**First Semester**",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Higher-Order Differential Equations</span>"
    ]
  },
  {
    "objectID": "ch_03_Higher-Order_Differential_Equations.html#sec-3-4",
    "href": "ch_03_Higher-Order_Differential_Equations.html#sec-3-4",
    "title": "3  Higher-Order Differential Equations",
    "section": "3.4 Undetermined Coefficients",
    "text": "3.4 Undetermined Coefficients\nMethod of undetermined coefficients can be used to obtain a particular solution \\(y_p\\)\n\nThe underlying idea is a conjecture about the form of \\(y_p\\) based on the kinds of functions making up the input function \\(g(x)\\)\nLimited to nonhomogeneous linear DEs where\n\nCoefficients \\(a_i\\), \\(i=1,\\cdots,n\\), are constants\n\\(g(x)\\) is a constant, a polynominal function, \\(e^{\\alpha x}\\), \\(\\sin\\beta x\\) or \\(\\cos\\beta x\\), or finite sums and products of these functions\n\nThere are models of \\(y_p\\) for various functions\n\n\n\n\n\nFinally, the general solution is obtained from the superposition of \\(y_h\\) and \\(y_p\\)\n\n\\(~\\)\nExample \\(\\,\\) Solve \\(~y'' +4y = x\\cos x\\)\n\\(~\\)\n\nIf \\(g(x)\\) consists of a sum of, say, \\(m\\) terms of the kind listed in the table, \\(~\\)then the assumption for a particular solution \\(y_p\\) consists of the sum of the trial forms\nIf \\(\\,y_p\\) contains terms that duplicate terms in \\(y_h\\), \\(\\,\\)then that \\(y_p\\) must be multiplied by \\(x^n\\), \\(~\\)where \\(n\\) is the smallest positive integer that eliminate that duplication\n\n\\(~\\)\nExample \\(\\,\\) Solve \\(~y'' -6y' +9y = 6x^2 +2 -12e^{3x}\\)\n\\(~\\)\nExample \\(\\,\\) Solve \\(~y^{(4)}+y'''= 1 -x^2e^{-x}\\)\n\\(~\\)\nExample \\(\\,\\) Solve the given differential equation by undetermined coefficients\n\n\\(y'' +2y' +y=\\sin x +3 \\cos 2x\\)\n\\(y'' +2y' -24y=16 - (x+2)e^{4x}\\)\n\\(y''' -6y''=3-\\cos x\\)\n\n\\(~\\)\nExample \\(\\,\\) Solve the given initial-value problem\n\n\\(y'' +4y=-2, \\;\\;y(\\pi/8)=1/2, \\;\\;y'(\\pi/8) = 2\\)\n\\(5y'' +y'=-6x, \\;\\;y(0)=0, \\; y'(0)=-10\\)\n\\(\\displaystyle \\frac{d^2 x}{dt^2} +\\omega^2x=F_0 \\sin\\omega t, \\;\\; x(0)=0, \\;x'(0)=0\\)\n\n\\(~\\)",
    "crumbs": [
      "**First Semester**",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Higher-Order Differential Equations</span>"
    ]
  },
  {
    "objectID": "ch_03_Higher-Order_Differential_Equations.html#sec-3-5",
    "href": "ch_03_Higher-Order_Differential_Equations.html#sec-3-5",
    "title": "3  Higher-Order Differential Equations",
    "section": "3.5 Variation of Parameters",
    "text": "3.5 Variation of Parameters\nThe method of variation of parameters can be used with linear higher-order DEs\n\nThis method always yields a \\(y_p\\) provided the homogeneous equation can be solved\nThis method is not limited to the types of input functions constraining the method of undetermined coefficients\n\nTo adapt the method of variation of parameters to a linear second-order DE\n\\[a_2(x)y'' +a_1(x)y' +a_0(x)y = g(x),\\]\nwe put the above equation in the standard form\n\\[y'' +P(x)y' +Q(x)y = f(x)\\] To solve,\n\nFind the homogeneous solutions \\(y_1\\), \\(y_2\\)\nSeek a particular solution of the form\n\\[y_p = \\color{red}{u_1(x)} \\color{blue}{y_1} +\\color{red}{u_2(x)} \\color{blue}{y_2}\\]\n\n\\(~\\)\n\\[\n\\begin{aligned}\n     y_p''+P(x)y_p' &+Q(x)y_p = f(x)\\\\\n     &{\\Big\\Downarrow} \\; {\\small y_p = u_1 y_1 +u_2 y_2} \\\\\n     \\frac{d}{dx}[\\color{blue}{y_1 u_1' +y_2 u_2'}] +P(x) [\\color{blue}{y_1 u_1' +y_2 u_2'}] &+\\color{red}{y_1' u_1'} +\\color{red}{y_2' u_2'}= f(x) \\\\\n     {\\small\\text{let } y_1 u_1' +y_2 u_2' =0,}\\; &{\\Big\\Downarrow} \\; {\\small\\text{then }\n     y_1' u_1' +y_2' u_2'= f(x)} \\\\\n     \\color{red}{\\begin{bmatrix}\n        y_1 & y_2\\\\\n        y_1' & y_2'\n     \\end{bmatrix}\n     \\begin{bmatrix}\n        u_1' \\\\\n        u_2'\n     \\end{bmatrix}} &=\n     \\color{red}{\\begin{bmatrix}\n        0 \\\\\n        f(x)\n     \\end{bmatrix}} \\\\\n     &\\Downarrow \\\\          \n     \\qquad\\quad\\color{blue}{u_1 = \\displaystyle\\int \\frac{W_1}{W\\;} \\,dx =-\\int \\frac{y_2}{W}\\,f(x)\\,dx}&,\n        \\;\\; \\color{blue}{u_2} \\color{blue}{= \\displaystyle\\int \\frac{W_2}{W\\;} \\,dx =\\int \\frac{y_1}{W}\\,f(x)\\,dx}\\\\ \\\\\n     \\mathrm{where} \\;\\;\n       W = \\begin{vmatrix}\n             y_1 & y_2\\\\\n             y_1' & y_2'\n           \\end{vmatrix}, \\;\n       W_1 =& \\begin{vmatrix}\n               0 & y_2\\\\\n               f(x) & y_2'\n             \\end{vmatrix}, \\;\n       W_2 = \\begin{vmatrix}\n               y_1 & 0\\\\\n               y_1' & f(x)\n             \\end{vmatrix}             \n\\end{aligned}\\]\n\\(~\\)\nExample \\(\\,\\) Solve \\(~\\displaystyle y'' -y = \\frac{1}{x}\\)\n\\(~\\)\nThe method can be generalized to the standard form of \\(n\\)th order linear DE. \\(\\,\\) A particular solution is\n\\[y_p = u_1(x) y_1 +u_2(x) y_2 +\\cdots +u_n(x) y_n\\]\nwhere\n\\[u_k = \\displaystyle\\int \\frac{W_k}{W\\;}\\, dx, \\;k=1, 2, \\cdots, n\\]\n\\(~\\)\nExample \\(\\,\\) Solve each differential equation by variation of parameters\n\n\\(y'' +y=\\sec x\\)\n\\(y'' +y=\\sin x\\)\n\\(y'' +y=\\cos^2 x\\)\n\\(3y''-6y'+6y=e^x \\sec x\\)\n\n\\(~\\)\nExample \\(\\,\\) Solve each differential equation by variation of parameters subject to the initial conditions \\(y(0)=1, \\;y'(0)=0\\)\n\n\\(4y'' -y=xe^{x/2}\\)\n\\(y''-2y'+y=e^x \\sec^2 x\\)\n\\(y'' +2y' -8y=2e^{-2x} -e^{-x}\\)\n\n\\(~\\)\nExample \\(\\,\\) Solve each differential equation by variation of parameters subject to the initial conditions \\(y(0)=1, \\;y'(0)=0\\)\n\n\\(\\displaystyle y''-4y=\\frac{e^{2x}}{x}\\)\n\\(2y'' +2y' +y=4\\sqrt{x}\\)\n\n\\(~\\)",
    "crumbs": [
      "**First Semester**",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Higher-Order Differential Equations</span>"
    ]
  },
  {
    "objectID": "ch_03_Higher-Order_Differential_Equations.html#sec-3-6",
    "href": "ch_03_Higher-Order_Differential_Equations.html#sec-3-6",
    "title": "3  Higher-Order Differential Equations",
    "section": "3.6 Cauchy-Euler Equation",
    "text": "3.6 Cauchy-Euler Equation\nThe Cauchy-Euler equation is a linear DE of the form\n\\[ a_n {\\color{red}{x^n}} \\frac{d^n y}{dx^n} +a_{n-1} {\\color{red}{x^{n-1}}} \\frac{d^{n-1} y}{dx^{n-1}} +\\cdots +a_1 {\\color{red}{x}} \\frac{dy}{dx} +a_0 y = g(x)\\]\nwhere \\(a_n, a_{n-1}, \\cdots, a_0\\) are constants and the exponent of the coefficient matches the order of differentiation\n\\(\\color{red}{y =x^{\\,p}}\\) is a solution of second order Cauchy-Euler equation whenever \\(p\\) is a solution of the auxiliary equation\n\\[a_2 p^2 +(a_1 -a_2)p +a_0 =0\\]\n\nCase I: \\(~\\) Distinct Real Roots, \\(~p_1, \\,p_2\\)\n\\[ y = c_1 {\\color{red}{x^{\\,p_1}}} +c_2 {\\color{red}{x^{\\,p_2}}} \\]\nCase II: \\(~\\) Repeated Real Roots, \\(~p_1=p_2\\)\n\\[y = c_1 {\\color{red}{x^{\\,p_1}}} +c_2 {\\color{red}{x^{\\,p_1}\\ln x}}\\]\nCase III: \\(~\\) Complex Conjugate Roots, \\(~p_1, p_2 = \\color{red}{\\alpha \\pm i\\beta}\\)\n\\[y = {\\color{red}{x^{\\alpha}}} \\left[ c_1 {\\color{red}{\\cos(\\beta\\ln x)}} + c_2 {\\color{red}{\\sin(\\beta\\ln x)}} \\right]\\]\n\n\\(~\\)\nExample \\(\\,\\) Solve \\(~x^2y'' -3xy' +3y = 2x^4 e^x\\)\n\\(~\\)\nExample \\(\\,\\) Solve the given differential equation\n\n\\(x^2y''-2y=0\\)\n\\(xy''+y'=0\\)\n\\(x^2y''+xy'+4y=0\\)\n\n\\(~\\)\nExample \\(\\,\\) Solve the given differential equation by variation of parameters\n\n\\(xy''-4y'=x^4\\)\n\\(x^2y''+xy'-y=\\ln x\\)\n\n\\(~\\)",
    "crumbs": [
      "**First Semester**",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Higher-Order Differential Equations</span>"
    ]
  },
  {
    "objectID": "ch_03_Higher-Order_Differential_Equations.html#sec-3-7",
    "href": "ch_03_Higher-Order_Differential_Equations.html#sec-3-7",
    "title": "3  Higher-Order Differential Equations",
    "section": "3.7 Nonlinear Equations",
    "text": "3.7 Nonlinear Equations\n\nNonlinear equations do not possess superposability\n\n\\(~\\)\nExample \\(\\,\\) Verify that \\(y_1\\) and \\(y_2\\) are solutions of the given DE but that \\(c_1 y_1 +c_2 y_2\\) is, in general, not a solution\n\\[\\left(y''\\right)^2 = y^2,\\; y_1=e^x,\\; y_2=\\cos x\\]\n\\(~\\)\n\nThe major difference between linear and nonlinear DEs of order two or higher lies in the realm of solvability. Nonlinear higher-order DEs virtually defy solution. This means that there are no analytical methods whereby either an explicit or implicit solution can be found\nThere are still things that can be done; \\(~\\)we can always analyze a nonlinear DE qualitatively and numerically\nNonlinear second-order DE \\(F(y, y', y'')=0\\,\\) can be reduced to two first-order equations by means of the substitution \\(u=y'\\) and can sometimes be solved using first-order methods\n\n\\(~\\)\nExample \\(\\,\\) Solve \\(~y''=2x(y')^2\\) and \\(yy''=(y')^2\\)\n\\(~\\)\n\nIn some instances, a solution of a nonlinear IVP can be approximated by a Taylor series centered at \\(x_0\\)\n\n\\(~\\)\nExample \\(\\,\\) Solve \\(~y''=x +y -y^2\\), \\(\\,y(0)=-1\\), \\(\\,y'(0)=1\\) by using\n\\[y(x) = y(0) +\\frac{y'(0)}{1!}x +\\frac{y''(0)}{2!}x^2 +\\frac{y'''(0)}{3!} x^3 + \\cdots\\]\n\\(~\\)\nExample \\(\\,\\) The dependent variable \\(y\\) is missing in the given differential equation. Solve the equation by using the substitution \\(u=y'\\)\n\\[y''+(y')^2+1=0\\]\n\\(~\\)\nExample \\(\\,\\) The independent variable \\(x\\) is missing in the given differential equation. Solve the equation by using the substitution \\(u=y'\\)\n\\[yy''+(y')^2+1=0\\]\n\\(~\\)\nExample \\(\\,\\) Consider the initial-value problem\n\\[y''+yy'=0,\\;\\;y(0)=1,\\;y'(0)=-1\\]\n1. Use the DE and a numerical solver to graph the solution curve\n2. Find an explicit solution of the IVP. \\(~\\)Use a graphing utility to graph this solution\n3. Find an interval of definition for the solution in part 2\n\\(~\\)\nExample \\(\\,\\) Show that the substitution \\(u=y'\\) leads to a Bernoulli equation. Solve this equation\n\\[xy''=y'+(y')^3\\]\n\\(~\\)",
    "crumbs": [
      "**First Semester**",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Higher-Order Differential Equations</span>"
    ]
  },
  {
    "objectID": "ch_03_Higher-Order_Differential_Equations.html#sec-3-8",
    "href": "ch_03_Higher-Order_Differential_Equations.html#sec-3-8",
    "title": "3  Higher-Order Differential Equations",
    "section": "3.8 Linear Models: Initial-Value Problems",
    "text": "3.8 Linear Models: Initial-Value Problems\nSeveral dynamical systems are modeled with linear 2nd-order DEs with constant coefficients and initial conditions at \\(t_0\\)\n\nSpring/Mass Systems\n\nFree Undamped Motion\nFree Damped Motion\nDriven Motion\n\n\n\\[\n\\begin{aligned}\n  m\\frac{d^2x}{dt^2} &= -kx -\\beta\\frac{dx}{dt} +f(t)\\\\\n  x(0) &= x_0 \\\\\n  \\dot{x}(0) &= x_1\n\\end{aligned}\\]\n\n\n\n\n\n\\(~\\)\nExample \\(\\,\\) Show that the solution of the initial-value problem\n\\[\\frac{d^2x}{dt^2}+\\omega^2x=F_0\\cos\\gamma t, \\;x(0)=0, \\;x'(0)=0\\]\nis \\(~\\displaystyle x(t)=\\frac{F_0}{\\omega^2-\\gamma^2}(\\cos\\gamma t -\\cos\\omega t)\\)\n\\(~\\)\nExample \\(\\,\\) Evaluate \\(\\,\\displaystyle \\lim_{\\gamma \\to \\omega} \\frac{F_0}{\\omega^2-\\gamma^2} (\\cos\\gamma t - \\cos\\omega t)\\)\n\\(~\\)",
    "crumbs": [
      "**First Semester**",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Higher-Order Differential Equations</span>"
    ]
  },
  {
    "objectID": "ch_03_Higher-Order_Differential_Equations.html#sec-3-9",
    "href": "ch_03_Higher-Order_Differential_Equations.html#sec-3-9",
    "title": "3  Higher-Order Differential Equations",
    "section": "3.9 Linear Models: Boundary-Value Problems",
    "text": "3.9 Linear Models: Boundary-Value Problems\n\\(~\\)\nExample \\(\\,\\) Temperature in a Ring\n\n\n\n\n\nThe temperature \\(u(r)\\) in circular ring is determined from the boundary-value problem\n\\[r\\frac{d^2 u}{dr^2} +\\frac{du}{dr}=0,\\; u(a)=u_0,\\; u(b)=u_1\\]",
    "crumbs": [
      "**First Semester**",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Higher-Order Differential Equations</span>"
    ]
  },
  {
    "objectID": "ch_03_Higher-Order_Differential_Equations.html#sec-GF",
    "href": "ch_03_Higher-Order_Differential_Equations.html#sec-GF",
    "title": "3  Higher-Order Differential Equations",
    "section": "3.10 Green’s Functions",
    "text": "3.10 Green’s Functions\n\n3.10.1 Influence Function\n\\(~\\)\n\nWe shall consider the self-adjoint form\n\\[\n\\color{red}{\\frac{d}{dx} \\left[ p(x)\\frac{du}{dx} \\right] +q(x)u = -f(x)} \\tag{SA}\\label{eq:SA}\n\\]\nThe function \\(p(x)\\) is continuously differentiable and positive, and \\(q(x)\\) and \\(f(x)\\) are continuous for \\(\\alpha &lt; x &lt; \\beta\\)\nThe homogeneous second-order differential equation\n\\[\n\\frac{d}{dx} \\left[ p(x)\\frac{dv}{dx} \\right] +q(x)v = 0 \\tag{SH}\\label{eq:SH}\n\\]\nhas exactly two linearly independent solutions \\(v_1(x)\\) and \\(v_2(x)\\): any solution of \\(\\eqref{eq:SH}\\) can be written in the form\n\\[v(x) = c_1 v_1(x) +c_2 v_2(x)\\]\nwhere \\(c_1\\) and \\(c_2\\) are constants\nWe now consider the function\n\\[ w(x) = v_1(x) {\\color{blue}{\\int_\\alpha^x v_2(\\xi) f(\\xi)\\, d\\xi}} -v_2(x) {\\color{blue}{\\int_\\alpha^x v_1(\\xi) f(\\xi)\\, d\\xi}}\\]\nDifferentiating \\(w\\), \\(~\\)we have\n\\[\\scriptsize\n\\begin{aligned}\n  \\frac{dw}{dx} &=  v_1'(x) {\\color{blue}{\\int_\\alpha^x v_2(\\xi) f(\\xi)\\, d\\xi}} -v_2'(x) {\\color{blue}{\\int_\\alpha^x v_1(\\xi) f(\\xi)\\, d\\xi}}  \n  + \\underbrace{\\left[ v_1(x) {\\color{blue}{v_2(x)}} -v_2(x) {\\color{blue}{v_1(x)}} \\right]}_{=0} {\\color{blue}{f(x)}}\\\\\n  &= v_1'(x) {\\color{blue}{\\int_\\alpha^x v_2(\\xi) f(\\xi)\\, d\\xi}} -v_2'(x) {\\color{blue}{\\int_\\alpha^x v_1(\\xi) f(\\xi)\\, d\\xi}}\n\\end{aligned}\\]\nThen\n\\[ \\scriptsize\n\\begin{aligned}\n  \\frac{d}{dx}\\left[ p(x)\\frac{dw}{dx} \\right]\n   &=\\underbrace{\\frac{d}{dx}\\left[ p(x)v_1'(x) \\right]}_{-q(x)v_1} \\int_\\alpha^x v_2(\\xi) f(\\xi)\\, d\\xi -\\underbrace{\\frac{d}{dx}\\left[ p(x)v_2'(x) \\right]}_{-q(x) v_2} \\int_\\alpha^x v_1(\\xi) f(\\xi)\\, d\\xi \\\\\n   &\\phantom{=}+\\underbrace{p(x) \\left[ v_1'(x) v_2(x) -v_2'(x) v_1(x) \\right]}_{-K} f(x) \\\\\n   &=-q(x) w -Kf(x)\n\\end{aligned}\\]\nwhere\n\\[\\scriptsize\n\\begin{aligned}\n   \\frac{d}{dx} &\\left\\{p(x) \\left[ v_1'(x) v_2(x) -v_2'(x) v_1(x) \\right] \\right\\} \\\\\n    &= \\underbrace{\\frac{d}{dx} \\left[ p(x) v_1'(x) \\right]}_{-q(x) v_1(x)} v_2(x) -\\underbrace{\\frac{d}{dx} \\left[ p(x) v_2'(x) \\right]}_{-q(x)v_2(x)} v_1(x) +p(x)v_1'(x)v_2'(x) -p(x)v_2'(x)v_1'(x) = 0 \\\\\n    &\\Downarrow \\\\ \\\\\n  K & \\text{ is constant}\n  \\end{aligned}\\]\nWe have shown that \\(w\\) satisfies the equation:\n\\[\\frac{d}{dx} \\left[ p(x)\\frac{dw}{dx} \\right] +q(x)w = -Kf(x)\\]\nMoreover, \\(\\,\\) as \\(x \\rightarrow \\alpha\\)\n\\[w(\\alpha)=w'(\\alpha)=0\\]\nDividing by the constant \\(K\\), \\(\\,\\) we find that the function\n\\[u(x) = \\int_\\alpha^x R(x,\\xi)\\,f(\\xi)\\,d\\xi \\tag{IN}\\label{eq:IN}\\]\nwhere\n\\[\\scriptsize\n\\color{blue}{R(x,\\xi)=\\frac{v_1(x)v_2(\\xi) - v_2(x)v_1(\\xi)}{K}} =-\\frac{v_1(x)v_2(\\xi) - v_2(x)v_1(\\xi)}{p(x) \\left[ v_1'(x) v_2(x) -v_2'(x) v_1(x) \\right]}\\]\nis the solution of the initial value problem\n \\[\n\\begin{aligned}\n  \\frac{d}{dx} \\left[ p(x) \\frac{du}{dx}\\right] +qu\n    &= -f(x)\\;\\; \\text{ for } x &gt; \\alpha \\\\\n  u(\\alpha) = u'(\\alpha) &= 0\n\\end{aligned}\\]\nSince the denominator of \\(\\,R(x,\\xi)\\) is a constant, \\(\\,\\)the function \\(R(x,\\xi)\\) satisfies the homogeneous equation \\(\\eqref{eq:SH}\\) as either a function of \\(x\\) or \\(\\xi\\). In fact,\n\\[R(x,\\xi)=-R(\\xi,x)\\]\nFor a fixed value of \\(\\xi\\), \\(~R(x,\\xi)\\) is completely characterized as the solution of the homogeneous initial value problem\n \\[\n\\begin{aligned}\n    \\frac{d}{dx} \\left[ p(x) \\frac{dR}{dx}\\right] &+q(x)R = 0\\;\\; \\text{ for } x &gt; \\xi \\\\\n    \\left. R \\right|_{x=\\xi} &= 0 \\\\\n    \\left. R' \\right|_{x=\\xi} &= -\\frac{1}{p(\\xi)} \\\\\n\\end{aligned}\\]\nThe function \\(R(x,\\xi)\\) describes the influence on the value of \\(u\\) at \\(x\\) of a disturbance(impulse) concentrated at \\(\\xi\\). It is sometimes called the influence function, or the one-sided Green’s function\nIf the values of \\(u(\\alpha)\\) and \\(u'(\\alpha)\\) are prescribed to be other than zero, \\(~\\)we must simply add a suitable solution \\(c_1 v_1(x) +c_2 v_2(x)\\) to the expression \\(\\eqref{eq:IN}\\)\n\n\\(~\\)\nExample \\(\\,\\) Consider the problem\n\\[\n\\begin{aligned}\nu'' +u &= -f(x)\\;\\; \\text{ for } x &gt; 0 \\\\\nu(0) &= 1 \\\\\nu'(0) &=-1 \\\\\n\\end{aligned}\\]\nSolution \\(~u(x) = w(x) +v(x)\\)\n(1) \\(~w(x)\\)\n\\[\n\\begin{aligned}\nw'' +w &= -f(x)\\;\\; \\text{ for } x &gt; 0 \\\\\nw(0) &= 0 \\\\\nw'(0) &= 0 \\\\\n\\end{aligned}\\]\nFor a fixed value of \\(\\xi\\), \\(~\\)the influence function \\(R(x,\\xi)\\) satisfies\n\\[\n\\begin{aligned}\n\\frac{d^2R}{dx^2} +R &= 0\\;\\; \\text{ for } x &gt; \\xi \\\\\n\\left. R \\right|_{x=\\xi} &= 0 \\\\\n\\left. R' \\right|_{x=\\xi} &= -1\n\\end{aligned}\\]\nThus \\(~R(x,\\xi)=\\sin(\\xi -x)\\,\\) and the solution is\n\\[w(x)=\\int_0^x \\sin(\\xi -x)\\,f(\\xi)\\,d\\xi\\]\n(2) \\(~v(x)\\)\n\\[\n\\begin{aligned}\nv'' +v &= 0\\;\\; \\text{ for }\\, x &gt; 0\\;\\; \\\\\nv(0) &= 1 \\\\\nv'(0) &=-1 \\\\\n\\end{aligned}\\]\n\n\\[\n\\begin{aligned}\n    &\\Downarrow \\\\\n    v(x)&=\\cos x -\\sin x\n\\end{aligned}\\]\n\n\n\n3.10.2 Green’s Function\n\nHere, we research a two-point boundary value problem:\n\\[\n\\begin{aligned}\n  \\frac{d}{dx} \\left[ p(x)\\frac{du}{dx} \\right] +q(x)u\n   &= -f(x)\\;\\; \\text{ for } \\alpha &lt; x &lt; \\beta \\\\\n  {\\color{blue}{u(\\alpha)=u(\\beta)}}\n   & {\\color{blue}{=0}}\n\\end{aligned} \\tag{P1}\\label{eq:P1}\\]\nWriting the general solution in the form\n\\[u(x) = \\int_\\alpha^x R(x,\\xi)\\,f(\\xi)\\,d\\xi +c_1 v_1(x) +c_2 v_2(x)\\]\nwe obtain the two equations\n\\[\n\\begin{aligned}\n  c_1 v_1(\\alpha) +c_2 v_2(\\alpha) &= 0\\\\\n  c_1 v_1(\\beta) +c_2 v_2(\\beta) &= -\\int_\\alpha^\\beta R(\\beta,\\xi)\\,f(\\xi)\\,d\\xi\n\\end{aligned}\\]\nThese two equations determine a unique pair of constants \\(c_1\\) and \\(c_2\\), provided the determinant of their coefficients is not zero; \\(~\\)that is, provided\n\\[D \\equiv v_1(\\alpha)v_2(\\beta)-v_2(\\alpha)v_1(\\beta) \\neq 0\\]\nWe assume this for the moment. Then\n\\[\\scriptsize\n\\begin{aligned}\n  c_1 &= \\frac{v_2(\\alpha)}{D} \\int_\\alpha^\\beta R(\\beta,\\xi)\\,f(\\xi)\\,d\\xi \\\\\n      &=\\frac{v_2(\\alpha)}{D} \\int_\\alpha^x R(\\beta,\\xi)\\,f(\\xi)\\,d\\xi\n  +\\frac{v_2(\\alpha)}{D} \\int_x^\\beta R(\\beta,\\xi)\\,f(\\xi)\\,d\\xi\\\\   \n  c_2 &= -\\frac{v_1(\\alpha)}{D} \\int_\\alpha^\\beta R(\\beta,\\xi)\\,f(\\xi)\\,d\\xi \\\\\n      &= -\\frac{v_1(\\alpha)}{D} \\int_\\alpha^x R(\\beta,\\xi)\\,f(\\xi)\\,d\\xi\n  -\\frac{v_1(\\alpha)}{D} \\int_x^\\beta R(\\beta,\\xi)\\,f(\\xi)\\,d\\xi  \n\\end{aligned}\\]\nThe solution can then be written as\n\\[\n\\begin{aligned}\n   u(x) &= \\int_\\alpha^x \\left[ R(x,\\xi) +\\frac{v_2(\\alpha)v_1(x) -v_1(\\alpha)v_2(x)}{D} R(\\beta,\\xi) \\right]\\,f(\\xi)\\,d\\xi\\\\\n        &+\\int_x^\\beta \\frac{v_2(\\alpha)v_1(x) -v_1(\\alpha)v_2(x)}{D}  R(\\beta,\\xi)\\,f(\\xi)\\,d\\xi\n\\end{aligned}\\]\nWe find after some algebraic manipulation that\n\\[\\scriptsize\n\\begin{aligned}\n  G(x,\\xi) &=R(x,\\xi) +\\frac{v_2(\\alpha)v_1(x) -v_1(\\alpha)v_2(x)}{D} R(\\beta,\\xi)  \\\\\n   &= \\frac{v_1(x)v_2(\\xi) - v_2(x)v_1(\\xi)}{K} \\frac{D}{D}\n    +\\frac{v_2(\\alpha)v_1(x) -v_1(\\alpha)v_2(x)}{D} \\frac{v_1(\\beta)v_2(\\xi) - v_2(\\beta)v_1(\\xi)}{K}\\\\\n   &= \\frac{\\left[v_1(\\alpha)v_2(\\xi) -v_2(\\alpha)v_1(\\xi)\\right] \\left[v_1(x)v_2(\\beta) -v_2(x)v_1(\\beta) \\right]}{KD}\\;\\;\\text{ for } \\xi \\leq x \\\\ \\\\\n  G(x,\\xi) &=\\frac{v_2(\\alpha)v_1(x) -v_1(\\alpha)v_2(x)}{D} R(\\beta,\\xi)  \\\\\n    &= \\frac{\\left[v_1(x)v_2(\\alpha) -v_2(x)v_1(\\alpha)\\right] \\left[v_1(\\beta)v_2(\\xi) -v_2(\\beta)v_1(\\xi) \\right]}{KD}\\;\\;\\text{ for } \\xi \\geq x\n\\end{aligned}\\]\nThen the solution of the two-point boundary value problem \\(\\eqref{eq:P1}\\) can be written in the form\n\\[u(x)=\\int_\\alpha^\\beta G(x,\\xi)\\,f(\\xi)\\,d\\xi \\tag{GR}\\label{eq:GR}\\]\nThe function \\(G(x,\\xi)\\) is called the Green’s function of the problem \\(\\eqref{eq:P1}\\). It is symmetric. That is\n\\[G(x,\\xi)=G(\\xi,x)\\]\nTo determine the Green’s function, we note that for each \\(\\xi\\) it satisfies the following boundary value problem\n \\[\\begin{aligned}\n\\frac{d}{dx} \\left[ p(x) \\frac{dG}{dx}\\right] &+q(x)G = 0\\;\\; \\text{ for } x \\neq \\xi \\\\\n\\left. G \\right|_{x=\\alpha} &= \\left. G \\right|_{x=\\beta}=0 \\\\\n\\left. G \\right|_{x=\\xi+0} &-\\left. G \\right|_{x=\\xi -0}=0 \\\\  \n\\left. G' \\right|_{x=\\xi+0} &-\\left. G'\n  \\right|_{x=\\xi-0}=-\\frac{1}{p(\\xi)} \\\\\n\\end{aligned}\\]\n\n\\(~\\)\nExample \\(\\,\\) Consider the problem\n\\[\n\\begin{aligned}\nu'' &= -f(x)\\;\\; \\text{ for}\\; 0 &lt; x &lt; 1\\\\\nu(0) &= 0 \\\\\nu(1) &= 0 \\\\\n\\end{aligned}\\]\nSolution\n\\[\n\\begin{aligned}\nG''= 0 \\;&\\Rightarrow \\; G = ax+b\\\\\n&\\Downarrow {\\scriptstyle G|_{x=0} = G|_{x=1}=0} \\\\\nG(x,\\xi) &=\n\\begin{cases}\na_1(\\xi)\\, x& \\text{ for } x &lt; \\xi \\\\\na_2(\\xi)(1-x)& \\text{ for } x &gt; \\xi\n\\end{cases}\\\\\n&\\Downarrow {\\scriptstyle G(x,\\xi)=G(\\xi,x)}\\\\\na_1(\\xi)&= A(1 -\\xi) \\\\\na_2(\\xi)&=A\\xi \\\\\n&\\Downarrow {\\scriptstyle \\left. G' \\right|_{x=\\xi+0} -\\left. G' \\right|_{x=\\xi-0}=-1}\\\\\n-A\\xi -A(1 -\\xi) &=-1 \\rightarrow A = 1 \\\\\n&\\Downarrow \\\\\nG(x,\\xi) &=\n\\begin{cases}\n(1-\\xi) x& \\text{ for } x \\leq \\xi \\\\\n\\xi(1-x)& \\text{ for } x \\geq \\xi\n\\end{cases}\n\\end{aligned}\\]\nThe solution is\n\\[u(x)= \\int_0^x \\xi(1-x)\\,f(\\xi)\\,d\\xi +\\int_x^1 (1-\\xi)x \\,f(\\xi)\\,d\\xi\\]\n\\(~\\)\n\nTwo-point boundary value problems with more general boundary conditions can be treated in the same manner. We consider the problem\n\\[\n\\begin{aligned}\n  \\frac{d}{dx} &\\left[ p(x)\\frac{du}{dx} \\right] +q(x)u = -f(x)\\;\\; \\text{ for } \\alpha &lt; x &lt; \\beta \\\\\n   -&\\mu_1 u'(\\alpha) +\\sigma_1 u(\\alpha)=0 \\\\\n    &\\mu_2 u'(\\beta) +\\sigma_2 u(\\beta)=0\n\\end{aligned}\\tag{P2}\\label{eq:P2} \\]\nThe Green’s function \\(G(x,\\xi)\\) is derived as before, provided the condition\n\\[\\scriptsize\n\\begin{aligned}\n  D \\equiv &\\left[ -\\mu_1 v_1'(\\alpha) +\\sigma_1v_1(\\alpha)\\right]\\left[ \\mu_2 v_2'(\\beta) +\\sigma_2v_2(\\beta)\\right] \\\\\n&-\\left[ -\\mu_1 v_2'(\\alpha) +\\sigma_1v_2(\\alpha)\\right]\\left[ \\mu_2 v_1'(\\beta) +\\sigma_2v_1(\\beta)\\right]\\neq0\n\\end{aligned}\\]\nis satisfied\nThe Green’s function \\(G(x,\\xi)\\) is the solution of the problem\n\\[\n\\begin{aligned}\n  \\frac{d}{dx} \\left[ p(x) \\frac{dG}{dx}\\right] &+q(x)G = 0\\;\\; \\text{ for } x \\neq \\xi \\\\\n  -\\mu_1\\left. G' \\right|_{x=\\alpha} &+\\sigma_1 \\left. G\\right|_{x=\\alpha}  = \\mu_2\\left. G' \\right|_{x=\\beta} +\\sigma_2\\left. G \\right|_{x=\\beta}=0 \\\\\n  \\left. G \\right|_{x=\\xi+0} &- \\left. G \\right|_{x=\\xi -0}=0 \\\\  \n  \\left. G' \\right|_{x=\\xi+0} &-\\left. G' \\right|_{x=\\xi-0}=-\\frac{1}{p(\\xi)} \\\\\n\\end{aligned}\\]\nIt still satisfies the symmetry relation\n\\[G(x,\\xi)=G(\\xi,x)\\]\n\n\\(~\\)\nExample \\(\\,\\) Consider the problem\n\\[\n\\begin{aligned}\nu'' &= -f(x)\\;\\; \\text{ for }\\, 0 &lt; x &lt; 1\\\\\nu(0) &= 0 \\\\\nu'(1) &+\\sigma_2 u(1)= 1 \\\\\n\\end{aligned}\\]\nSolution \\(~u(x) = w(x) +v(x)\\)\n(1) \\(~w(x)\\)\n\\[\n\\begin{aligned}\nw'' &= -f(x)\\;\\; \\text{ for } 0 &lt; x &lt; 1\\\\\nw(0) &= 0 \\\\\nw'(1) &+\\sigma_2 w(1)= 0 \\\\\n\\end{aligned}\\]\n\\(~\\)\n\\[\n\\begin{aligned}\nG''&= 0\\\\\n&\\Downarrow \\\\\nG &= ax+b\\\\\n&\\Downarrow {\\scriptstyle G|_{x=0} \\,=\\, G'|_{x=1} \\,+\\,\\sigma_2  G|_{x=1}\\,=\\,0} \\\\\nG(x,\\xi) &=\n\\begin{cases}\na_1(\\xi)\\, x& \\text{ for } x &lt; \\xi \\\\\na_2(\\xi)\\left[1+\\sigma_2(1 -x)\\right]& \\text{ for } x &gt; \\xi\n\\end{cases}\\\\\n&\\Downarrow {\\scriptstyle G(x,\\xi)\\,=\\,G(\\xi,x)}\\\\\na_1(\\xi)&= A\\left[1 +\\sigma_2(1 -\\xi)\\right] \\\\\na_2(\\xi)&=A\\xi \\\\\n&\\Downarrow {\\scriptstyle \\left. G' \\right|_{x=\\xi+0} \\,-\\,\\left. G' \\right|_{x=\\xi-0}\\,=\\,-1}\\\\\nA\\xi\\cdot-\\sigma_2 -A\\left[1 +\\sigma_2(1 -\\xi)\\right] &=-1 \\rightarrow A = \\frac{1}{1+\\sigma_2} \\\\\n&\\Downarrow \\\\\nG(x,\\xi) &=\n\\begin{cases}\n\\frac{\\left[ 1+\\sigma_2(1 -\\xi)\\right]\\,x}{1+\\sigma_2} & \\text{ for } x \\leq \\xi \\\\\n\\frac{\\xi \\,\\left[ 1+\\sigma_2(1 -x)\\right]}{1+\\sigma_2} & \\text{ for } x \\geq \\xi\n\\end{cases}\n\\end{aligned}\\]\nThe solution is\n\\[ w(x)= \\int_0^x \\frac{\\xi \\,\\left[ 1+\\sigma_2(1 -x)\\right]}{1+\\sigma_2}\\,f(\\xi)\\,d\\xi\n  +\\int_x^1 \\frac{\\left[ 1+\\sigma_2(1 -\\xi)\\right]\\,x}{1+\\sigma_2} \\,f(\\xi)\\,d\\xi\\]\n(2) \\(~v(x)\\)\n\\[\n\\begin{aligned}\nv'' &= 0\\;\\; \\text{ for }\\, 0 &lt; x &lt; 1\\\\\nv(0) &= 0 \\\\\nv'(1) +\\sigma_2 v(1)&= 1 \\\\\n\\end{aligned}\\]\n\\[\n\\begin{aligned}\n  &\\Downarrow \\\\\n  v(x)&=\\frac{x}{1+\\sigma_2}\n\\end{aligned}\\]\n\\(~\\)\nExample \\(\\,\\) Consider the problem\n\\[\\begin{aligned}\nr^2 &u'' +2r u' -n(n +1) u = -r^2 F(r), \\;\\; 0 &lt; r &lt; R \\\\\n&u(R)= 0, \\;\\left| u \\right| &lt; \\infty\n\\end{aligned}\\]\nSolution\n\\[\\scriptsize\n\\begin{aligned}\n  \\frac{d}{dr}\\left[r^2\\frac{dG}{dr} \\right] &-n(n+1) G = 0\\\\\n  &\\Downarrow \\\\\n  G &= c_1r^n +c_2r^{-(n+1)} \\\\\n  &\\Downarrow {\\tiny G|_{r=0} \\;= \\text{ bounded },\\;\n   G|_{r=R} \\;= 0} \\\\\n  G(r,\\rho) &=\n  \\begin{cases}\n    a_1(\\rho) r^n\\, & \\text{ for } r &lt; \\rho \\\\\n    a_2(\\rho)\\left[\\left(\\frac{r}{R} \\right)^{-(n+1)} -\\left(\\frac{r}{R} \\right)^{n} \\right]& \\text{ for } r &gt; \\rho\n  \\end{cases}\\\\\n  &\\Downarrow {\\tiny G(r,\\rho)=G(\\rho,r)}\\\\\n  a_1(\\rho)&= A\\left[\\left(\\frac{\\rho}{R} \\right)^{-(n+1)} -\\left(\\frac{\\rho}{R} \\right)^{n} \\right] \\\\\n  a_2(\\rho)&=A\\rho^n \\\\\n  &\\Downarrow {\\tiny \\left. G' \\right|_{r=\\rho+0} \\;\\;-\\;\\left. G' \\right|_{r=\\rho-0} \\;=-1/\\rho^2}\\\\\n   {\\tiny \\frac{A\\rho^n}{R} \\left[-(n +1)\\left(\\frac{\\rho}{R} \\right)^{-(n+2)} -n \\left(\\frac{\\rho}{R} \\right)^{n-1}\\right]\n-An\\rho^{n-1}}\n&{\\tiny \\left[\\left(\\frac{\\rho}{R} \\right)^{-(n+1)} -\\left(\\frac{\\rho}{R} \\right)^{n}\\right] =-\\frac{1}{\\rho^2} \\;\\;\\rightarrow\\;\\; A=\\frac{1}{(2n +1)R^{n+1}}}\\\\\n  &\\Downarrow \\\\\n  G(r,\\rho) &=\n\\begin{cases}\n   \\frac{1}{(2n +1)R}\\left(\\frac{r}{R} \\right)^n\\left[\\left(\\frac{\\rho}{R} \\right)^{-(n+1)} -\\left(\\frac{\\rho}{R} \\right)^{n} \\right] & \\text{ for } r \\leq \\rho \\\\\n   \\frac{1}{(2n +1)R}\\left(\\frac{\\rho}{R} \\right)^n\\left[\\left(\\frac{r}{R} \\right)^{-(n+1)} -\\left(\\frac{r}{R} \\right)^{n} \\right]& \\text{ for } r \\geq \\rho\n\\end{cases}\\\\\n\\end{aligned}\\]\nThe solution is\n\\[ u(x)= \\int_0^R G(r,\\rho)\\,F(\\rho) \\rho^2 d\\rho\\]\n\\(~\\)",
    "crumbs": [
      "**First Semester**",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Higher-Order Differential Equations</span>"
    ]
  },
  {
    "objectID": "ch_03_Higher-Order_Differential_Equations.html#sec-3-11",
    "href": "ch_03_Higher-Order_Differential_Equations.html#sec-3-11",
    "title": "3  Higher-Order Differential Equations",
    "section": "3.11 Solving Nonlinear Model",
    "text": "3.11 Solving Nonlinear Model\n\nIn order to analyze an \\(n\\)th order IVP numerically, \\(~\\)we express the \\(n\\)th order ODE as a system of \\(n\\) first-order equations\nFor example, \\[\n\\begin{aligned}\n    \\frac{d^2y}{dx^2} = f(x,y,y')&, \\;\\;y(x_0)=y_0, \\;y'(x_0) = y_1\\\\[5pt]\n    &\\big\\Downarrow \\;{y'=u}\\\\[5pt]\n    \\mathrm{Solve:} &\\;\n    \\begin{cases} \\;y' = u \\\\ \\;u' = f(x,y,u) \\end{cases}\\\\[8pt]\n    \\mathrm{subject \\;to:} & \\;y(x_0)=y_0, \\;u(x_0)=y_1\n\\end{aligned}\\]\n\n\\(~\\)\nExample \\(\\,\\) Use a numerical solver to obtain the solution curves satisfying the given initial conditions:\n\\[\\frac{d^2 x}{dt^2} +\\frac{dx}{dt} +x +x^3 = 0\\]\n\\[x(0)=-3,\\; \\dot{x}(0)=4,\\; \\text{ or }\\; x(0)=0,\\; \\dot{x}(0)=-8\\]\nSolution\n\\[\n\\begin{aligned}\n  \\frac{d^2 x}{dt^2} +\\frac{dx}{dt}&= -x -x^3 \\\\\n  &\\Downarrow \\\\\n  \\dot{x} &= u\\\\\n  \\dot{u} &= -x -x^3 -u\n\\end{aligned}\\]\n\nimport numpy as np\nfrom scipy.integrate import solve_ivp\n\ndef func(t, y):\n    return [y[1], -y[0] -y[0]**3 -y[1]]\n\ntf = 14\nt_eval = np.linspace(0, tf, 200)\n\nsol1 = solve_ivp(func, [0, tf], [-3, 4], t_eval=t_eval)\nsol2 = solve_ivp(func, [0, tf], [0, -8], t_eval=t_eval)\n\n\n\nimport matplotlib.pyplot as plt\nplt.style.use('ggplot')\n\nplt.figure(figsize=(6, 4))\n\nplt.plot(sol1.t, sol1.y[0], 'b-', label=r'$x_0=-3,\\; \\dot{x}_0=4$')\nplt.plot(sol2.t, sol2.y[0], 'r-', label=r'$x_0=0,\\; \\dot{x}_0=-8$')\nplt.axis((0, tf, -3, 3))\nplt.xlabel('t')\nplt.ylabel('x')\nplt.legend()\n\nplt.show()\n\n\n\n\n\n\n\nFigure 3.1: \\(\\displaystyle\\frac{d^2x}{dt^2} +\\frac{dx}{dt} +x +x^3=0\\)",
    "crumbs": [
      "**First Semester**",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Higher-Order Differential Equations</span>"
    ]
  },
  {
    "objectID": "ch_03_Higher-Order_Differential_Equations.html#sec-3-12",
    "href": "ch_03_Higher-Order_Differential_Equations.html#sec-3-12",
    "title": "3  Higher-Order Differential Equations",
    "section": "3.12 Solving System of Linear Equations",
    "text": "3.12 Solving System of Linear Equations\nWhen physical systems are coupled, the mathematical model of the system usually consists of a set of coupled DEs\n\n\n\n\n\n\\[\n\\begin{aligned}\n    m_1 \\ddot{x}_1 &=-k_1 x_1 +k_2 (x_2 -x_1) \\\\\n    m_2 \\ddot{x}_2 &=-k_2 (x_2 -x_1)\n\\end{aligned}\\]\nLinear systems with constant coefficients can be solved by uncoupling the system into distinct linear ODEs in each dependent variable\n\\(~\\)\nExample \\(\\,\\) Solve the above equation under the assumption that \\(k_1=6\\), \\(k_2=4\\), \\(m_1=1\\), and \\(m_2=1\\) subject to\n\\[x_1(0)=0,\\; x'_1(0)=1,\\; x_2(0)=0,\\; x'_2(0)=-1\\]\n\nk1, k2, m1, m2 = 6, 4, 1, 1\n\ndef func(t, y):\n    return [y[1], -(k1 + k2)/m1 * y[0] + k2/m1 * y[2], \n            y[3], k2/m2 * y[0] - k2/m2 * y[2]]\n\ntf = 14\nt_eval = np.linspace(0, tf, 200)\n\nsol = solve_ivp(func, [0, tf], [0, 1, 0, -1], t_eval=t_eval)\n\n\nplt.figure(figsize=(6, 4))\n\nplt.plot(sol.t, sol.y[0], 'b-', label='$x_1$')\nplt.plot(sol.t, sol.y[2], 'r-', label='$x_2$')\nplt.axis((0, tf, -0.6, 0.6))\nplt.legend()\nplt.xlabel('t')\nplt.ylabel('x')\n\nplt.show()\n\n\n\n\n\n\n\nFigure 3.2: Coupled Spring/Mass System",
    "crumbs": [
      "**First Semester**",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Higher-Order Differential Equations</span>"
    ]
  },
  {
    "objectID": "ch_03_Higher-Order_Differential_Equations.html#worked-exercises",
    "href": "ch_03_Higher-Order_Differential_Equations.html#worked-exercises",
    "title": "3  Higher-Order Differential Equations",
    "section": "Worked Exercises",
    "text": "Worked Exercises\n1. \\(\\phantom{1}\\) Solve the given differential equation\n\\[3y'' -6y' +30y = 15\\sin x +e^x \\tan 3x\\]\nSolution\nStep 1: \\(~\\)Solve the homogeneous equation\n\\[3y’’ - 6y’ + 30y = 0\\]\nDivide through by \\(3\\):\n\\[y’’ - 2y’ + 10y = 0\\]\nThis is a linear equation with constant coefficients. The characteristic equation is:\n\\[p^2 - 2p + 10 = 0\\]\nSolve using the quadratic formula:\n\\[p = 1 \\pm 3i\\]\nSo the general solution to the homogeneous equation is:\n\\[y_h(x) = e^{x} \\left(c_1 \\cos 3x + c_2 \\sin 3x\\right)\\]\nStep 2: \\(~\\)Find a particular solution\nThe nonhomogeneous term is:\n\\[15\\sin x + e^x \\tan 3x\\]\nWe’ll treat each part separately:\n\\((a)\\) \\(~15 \\sin x\\):\nWe use the method of undetermined coefficients\nGuess:\n\\[y_{p1}(x) = A \\cos x + B \\sin x\\]\nPlug into the left-hand side of the homogeneous operator and simplify\n\\[3y’’ -6y’ + 30y = (27A + 6B)\\cos x + (27B - 6A)\\sin x\\]\nWe want this equal to the right-hand side: \\(15\\sin x\\)\nSo:\n\\[\\begin{aligned}\n  A &= -\\frac{2}{17} \\\\\n  B &= \\frac{9}{17}\n\\end{aligned}\\]\n\\((b)\\) \\(~e^x \\tan 3x\\):\nWe use the method of variation of parameters or note that the particular solution involving \\(e^x \\tan 3x\\) will not be expressible in elementary functions directly\nWe now solve:\n\\[y’’ - 2y’ + 10y = \\frac{1}{3} e^x \\tan 3x\\]\nThe fundamental solutions to the homogeneous part are:\n\n\\(y_1 = e^x \\cos 3x\\)\n\\(y_2 = e^x \\sin 3x\\)\n\nWe apply the method of variation of parameters:\n\\[y_{p2}(x) = u_1(x)y_1 (x) + u_2(x)y_2(x)\\]\nwhere:\n\\[\\begin{aligned}\nu_1’ &= -\\frac{y_2 f}{W} \\\\\nu_2’ &= \\frac{y_1 f}{W}\n\\end{aligned}\\]\nCompute the Wronskian \\(W\\)\n\\[W =\n\\begin{vmatrix}\ny_1 & y_2 \\\\\ny_1’ & y_2’\n\\end{vmatrix}\n= 3e^{2x}\\]\nCompute \\(u_1’\\), \\(u_2’\\)\n\\[\\begin{aligned}\nu_1’ &= \\frac{-e^x \\sin 3x \\cdot \\frac{1}{3} e^x \\tan 3x}{3e^{2x}} = -\\frac{1}{9} \\sin 3x \\tan 3x\\\\\nu_2’ &= \\frac{e^x \\cos 3x \\cdot \\frac{1}{3} e^x \\tan 3x}{3e^{2x}} = \\frac{1}{9} \\sin 3x\n\\end{aligned}\\]\nNow integrate:\n\\[\\begin{aligned}\nu_1(x) &= -\\frac{1}{9} \\int \\sin 3x \\tan 3x\\, dx =-\\frac{1}{27} e^x \\cos 3x \\ln|\\sec 3x + \\tan 3x| \\\\\nu_2(x) &= \\frac{1}{9} \\int \\sin 3x dx = -\\frac{1}{27} \\cos 3x\n\\end{aligned}\\]\n\\(~\\)\n2. \\(\\phantom{1}\\) Solve the given differential equation\n\\[y'' -2y' +y = 4x^2 -3 +x^{-1} e^x\\]\nSolution\nStep 1: \\(~\\)Solve the homogeneous equation\nThe associated homogeneous equation is:\n\\[y’’ - 2y’ + y = 0\\]\nThis is a linear equation with constant coefficients. Its characteristic equation is:\n\\[p^2 - 2p + 1 = 0 \\quad \\Rightarrow \\quad (p - 1)^2 = 0\\]\nSo we have a repeated root: \\(p = 1\\)\nThus, the general solution to the homogeneous equation is:\n\\[y_h(x) = (c_1 + c_2 x)e^x\\]\nStep 2: \\(~\\)Find a particular solution\nWe now look for a particular solution to:\n\\[ y’’ - 2y’ + y = 4x^2 - 3 + \\frac{e^x}{x}\\]\nThis is a nonhomogeneous term with two parts:\n\n\\(f_1(x) = 4x^2 - 3\\) (a polynomial)\n\\(f_2(x) = e^x/x\\) (a non-polynomial term)\n\nWe’ll compute the particular solution as:\n\\[y_p = y_{p1} + y_{p2}\\]\n\\((a)\\) \\(~4x^2 - 3\\)\nSince the right-hand side is a polynomial, we use undetermined coefficients. Try a solution of the form:\n\\[y_{p1} = Ax^2 + Bx + C\\]\nSubstitute into the differential operator:\n\\[y_{p1}’’ - 2y_{p1}’ + y_{p1} = 2A - 2(2Ax + B) + (Ax^2 + Bx + C)\\]\nSet this equal to the right-hand side:\n\\[Ax^2 + (-4A + B)x + (2A - 2B + C) = 4x^2 - 3\\]\nMatch coefficients and so:\n\\[y_{p1}(x) = 4x^2 + 16x + 21\\]\n\\((b)\\) \\(\\displaystyle~\\frac{e^x}{x}\\)\nThis is not suitable for undetermined coefficients — instead, we use variation of parameters.\nWe already have two linearly independent solutions to the homogeneous equation:\n\n\\(y_1 = e^x\\)\n\\(y_2 = x e^x\\)\n\nLet:\n\\[y_{p2}(x) = u_1(x) y_1(x) + u_2(x) y_2(x)\\]\nWhere:\n\\[\\begin{aligned}\nu_1’ &= \\frac{-y_2(x) \\cdot f(x)}{W(y_1, y_2)} \\\\\nu_2’ &= \\frac{y_1(x) \\cdot f(x)}{W(y_1, y_2)}\n\\end{aligned}\\]\nCompute the Wronskian\n\\[W(y_1, y_2) = \\begin{vmatrix}\ne^x & x e^x \\\\\ne^x & (1 + x)e^x\n\\end{vmatrix}\n= e^x (1 + x)e^x - x e^x \\cdot e^x = e^{2x}(1 + x - x) = e^{2x}\\]\nThen:\n\\[\\begin{aligned}\nu_1’ &= \\frac{-x e^x \\cdot \\frac{e^x}{x}}{e^{2x}} = \\frac{-e^{2x}}{e^{2x}} = -1 \\quad \\Rightarrow \\quad u_1 = -x \\\\\nu_2’ &= \\frac{e^x \\cdot \\frac{e^x}{x}}{e^{2x}} = \\frac{1}{x} \\quad \\Rightarrow \\quad u_2 = \\ln|x|\n\\end{aligned}\\]\nSo:\n\\[y_{p2}(x) = u_1 y_1 + u_2 y_2 = (-x)e^x + \\ln|x| \\cdot (x e^x)\n= x e^x (\\ln|x| - 1)\\]\n\\(~\\)\n3. \\(\\phantom{1}\\) Find a member of the family of solutions of\n\\[xy''+y'+\\sqrt{x}=0\\]\nwhose graph is tangent to the x-axis at \\(x=1\\)\nSolution\nWe are given the second-order linear nonhomogeneous differential equation:\n\\[xy’’ + y’ + \\sqrt{x} = 0\\]\nand the condition that the solution’s graph is tangent to the x-axis at \\(x = 1\\)\nThis implies:\n\nThe solution satisfies \\(y(1) = 0\\)\nThe slope is zero at \\(x = 1\\), so \\(y’(1) = 0\\)\n\nStep 1: \\(~\\)Solve the homogeneous differential equation\nWe can treat this as a linear second-order ODE:\n\\[xy'' + y' = -\\sqrt{x}\\]\nThis is a linear ODE in standard form:\n\\[y'' + \\frac{1}{x} y' = -\\frac{\\sqrt{x}}{x}\n= -x^{-1/2}\\]\nLet’s solve the homogeneous equation first:\n\\[y'' + \\frac{1}{x} y' = 0\\]\nUse standard method for linear homogeneous equations\nLet \\(y' = u\\), then \\(y'' = u'\\), so:\n\\[u' + \\frac{1}{x} u = 0\\]\nThis is a separable first-order equation:\n\\[\\begin{aligned}\n\\frac{du}{u} &= -\\frac{dx}{x}\n\\;\\Rightarrow\\;\n\\ln|u| = -\\ln|x| + C \\\\\n&\\;\\Rightarrow\\;\nu = \\frac{c_1}{x}\n\\;\\Rightarrow\\;\ny' = \\frac{c_1}{x}\n\\;\\Rightarrow\\;\ny = c_1 \\ln x + c_2\n\\end{aligned}\\]\nSo the general solution of the homogeneous equation is:\n\\[y_h(x) = c_1 \\ln x + c_2\\]\nStep 2: \\(~\\)Find a particular solution to the nonhomogeneous equation\nWe need a particular solution \\(y_p\\) to:\n\\[y'' + \\frac{1}{x} y' = -x^{-1/2}\\]\nWe can use variation of parameters, since we already have two independent solutions to the homogeneous equation:\n\\[y_1 = \\ln x,\\quad y_2 = 1\\]\nCompute the Wronskian:\n\\[W =\n\\begin{vmatrix}\n\\ln x & 1 \\\\\n\\frac{1}{x} & 0 \\\\\n\\end{vmatrix}\n= -\\frac{1}{x}\\]\nThe particular solution is:\n\\[ y_p = -\\ln x \\int \\frac{1 \\cdot (-x^{-1/2})}{-1/x} \\, dx +   1 \\cdot \\int \\frac{\\ln x \\cdot (-x^{-1/2})}{-1/x} \\, dx = -\\frac{4}{9} x^{3/2}\\]\nStep 3: \\(~\\)General solution\n\\[y(x) = c_1 \\ln x + c_2 - \\frac{4}{9} x^{3/2}\\]\nWe are told the graph is tangent to the x-axis at \\(x=1\\). Compute \\(y(1)\\) and \\(y'(x)\\):\n\\[y(1) = c_1 \\ln 1 + c_2 - \\frac{4}{9} (1)^{3/2} = 0 + c_2 - \\frac{4}{9} = 0 \\Rightarrow c_2 = \\frac{4}{9}\\]\n\\[y'(1) = c_1 - \\frac{2}{3} = 0 \\Rightarrow c_1 = \\frac{2}{3}\\]\nThe solution is\n\\[y(x) = \\frac{2}{3} \\ln x + \\frac{4}{9} - \\frac{4}{9} x^{3/2}\\]\n\\(~\\)\n4. \\(~\\) Find all solutions of the following equation:\n\\[y'' +y = \\tan x, \\;\\; (-\\pi/2 &lt; x &lt; \\pi/2)\\]\nSolution\nStep 1: \\(~\\)Solve the Homogeneous Equation\nFirst, solve the homogeneous part:\n\\[y_h'' + y_h = 0\\]\nThe characteristic equation is:\n\\[p^2 + 1 = 0 \\Rightarrow p = \\pm i\\]\nSo the general solution of the homogeneous equation is:\n\\[y_h(x) = c_1 \\cos x + c_2 \\sin x\\]\nStep 2: \\(~\\)Find a Particular Solution \\(y_p(x)\\)\nWe now need a particular solution to the nonhomogeneous equation:\n\\[y'' + y = \\tan x\\]\nWe use the method of variation of parameters. So the particular solution is:\n\\[y_p = u_1(x) \\cos x + u_2(x) \\sin x\\]\nWhere: \\[\nu_1’(x) = -\\frac{\\sin x \\cdot \\tan x}{W} = -\\sin x \\tan x, \\quad\nu_2’(x) = \\frac{\\cos x \\cdot \\tan x}{W} = \\cos x \\tan x\\]\nIntegrate:\n\\[\nu_1(x) = -\\int \\frac{\\sin^2 x}{\\cos x} dx, \\quad\nu_2(x) = \\int \\sin x \\, dx = -\\cos x\\]\nNow compute \\(u_1(x)\\):\n\\[\\int \\frac{\\sin^2 x}{\\cos x} dx = \\int \\frac{1 - \\cos^2 x}{\\cos x} dx = \\int \\left( \\frac{1}{\\cos x} - \\cos x \\right) dx = \\int \\sec x \\, dx - \\int \\cos x \\, dx\\]\nTherefore:\n\\[u_1(x) = -\\left( \\ln |\\sec x + \\tan x| - \\sin x \\right) = \\sin x - \\ln |\\sec x + \\tan x|\\]\nNow plug into:\n\\[y_p = u_1(x) \\cos x + u_2(x) \\sin x\\]\nSo we get:\n\\[ y_p(x) = -\\cos x \\ln |\\sec x + \\tan x| \\]\nStep 4: \\(~\\)General Solution\nCombine homogeneous and particular solutions:\n\\[\ny(x) = c_1 \\cos x + c_2 \\sin x - \\cos x \\ln |\\sec x + \\tan x|, \\quad -\\frac{\\pi}{2} &lt; x &lt; \\frac{\\pi}{2}\\]\n\\(~\\)\n5. \\(~\\) Solve the given initial-value problem on the interval \\((-\\infty, 0)\\)\n\\[4x^2 y'' +y = 0, \\;\\; y(-1)=2, \\; y'(-1)=4\\]\nSolution\nStep 1: \\(~\\)Identify the Type of Equation\nThis is a Cauchy-Euler equation. Since the interval is \\(x &lt; 0\\), define:\n\\[x = -e^t \\quad \\text{so } t = \\ln(-x)\\]\nThen:\n\n\\(y(x) = Y(t)\\)\nChain rule:\n\n\\[\\frac{dy}{dx} = \\frac{dY}{dt} \\cdot \\frac{dt}{dx} = \\frac{1}{x} \\frac{dY}{dt}, \\quad\n\\frac{d^2y}{dx^2} = \\frac{d}{dx}\\left( \\frac{1}{x} \\frac{dY}{dt} \\right)\n= -\\frac{1}{x^2} \\frac{dY}{dt} + \\frac{1}{x^2} \\frac{d^2Y}{dt^2}\\]\nSo:\n\\[y'' = \\frac{1}{x^2} \\left( \\frac{d^2Y}{dt^2} - \\frac{dY}{dt} \\right)\\]\nNow plug into the original equation:\n\\[4x^2 y'' + y = 0 \\Rightarrow\n4x^2 \\cdot \\left[ \\frac{1}{x^2} \\left( Y’’ - Y’ \\right) \\right] + Y = 0\n\\Rightarrow 4(Y’’ - Y’) + Y = 0\\]\nThus we get the constant-coefficient equation:\n\\[4Y’’ - 4Y’ + Y = 0\\]\nStep 2: \\(~\\) Solve the Constant-Coefficient ODE\n\\[4Y’’ - 4Y’ + Y = 0\n\\Rightarrow 4p^2 - 4p + 1 = 0\n\\Rightarrow p = \\frac{1}{2} \\text{ (repeated root)}\\]\nSo the general solution is:\n\\[Y(t) = (A + B t) e^{t/2}\\]\nRecall \\(x = -e^t \\Rightarrow t = \\ln(-x)\\), so:\n\\[Y(t) = \\left[A + B \\ln(-x)\\right] \\cdot (-x)^{1/2}\\]\nThus:\n\\[y(x) = \\sqrt{-x} \\left[ A + B \\ln(-x) \\right], \\quad x &lt; 0\\]\nStep 3: \\(~\\)Apply Initial Conditions\nStep 3.1: \\(~\\) Evaluate \\(y(-1)\\)\n\\[ y(-1) = \\sqrt{1}(A + B \\ln 1) = A \\Rightarrow A = 2 \\]\nStep 3.2: \\(~\\) Compute \\(y'(x)\\)\n\\[y'(x) = -\\frac{1}{2\\sqrt{-x}} (A + B \\ln(-x)) + \\sqrt{-x} \\cdot \\left( -\\frac{B}{x} \\right)\n= -\\frac{A + B \\ln(-x)}{2\\sqrt{-x}} - \\frac{B \\sqrt{-x}}{x}\\]\nNow evaluate at \\(x = -1\\):\n\\[y'(-1) = -\\frac{A + B \\cdot 0}{2} - B \\cdot \\frac{1}{-1}\n= -\\frac{A}{2} + B\\]\nWe already found \\(A = 2\\), so:\n\\[4 = -1 + B \\Rightarrow B = 5\\]\nFinal Answer:\n\\[y(x) = \\sqrt{-x} \\left( 2 + 5 \\ln(-x) \\right), \\quad x &lt; 0\n\\]\nThis is the unique solution on the interval \\((-\\infty, 0)\\) satisfying the initial conditions\n\\(~\\)\n6. \\(~\\) Find all solutions of the following equation:\n\\[y'' +y = \\sec x, \\;\\; (-\\pi/2 &lt; x &lt; \\pi/2)\\]\nSolution\nStep 1: \\(~\\) Solve the Homogeneous Equation\nFirst, solve the homogeneous equation:\n\\[y_h'' + y_h = 0\\]\nIts characteristic equation is:\n\\[p^2 + 1 = 0 \\quad \\Rightarrow \\quad p = \\pm i\\]\nSo the general solution to the homogeneous part is:\n\\[y_h(x) = c_1 \\cos x + c_2 \\sin x\\]\nStep 2: \\(~\\) Find a Particular Solution Using Variation of Parameters\nThe Wronskian is:\n\\[W = \\begin{vmatrix}\n\\cos x & \\sin x \\\\\n-\\sin x & \\cos x\n\\end{vmatrix}\n= \\cos^2 x + \\sin^2 x = 1\\]\nFormulas for variation of parameters:\n\\[ y_p = u_1(x)\\cos x + u_2(x)\\sin x \\]\nwhere:\n\\[u_1’(x) = -\\frac{\\sin x \\cdot \\sec x}{W} = -\\tan x, \\qquad\nu_2’(x) = \\frac{\\cos x \\cdot \\sec x}{W} = 1\\]\nIntegrate both:\n\\[u_1(x) = -\\int \\tan x \\, dx = \\ln |\\cos x|, \\qquad\nu_2(x) = \\int 1 \\, dx = x\\]\nSo:\n\\[y_p = \\cos x \\cdot \\ln |\\cos x| + \\sin x \\cdot x\\]\nStep 3: \\(~\\) General Solution\nThe general solution is:\n\\[\ny(x) = c_1 \\cos x + c_2 \\sin x + \\cos x \\ln |\\cos x| + x \\sin x\n\\quad \\text{for } -\\frac{\\pi}{2} &lt; x &lt; \\frac{\\pi}{2}\\]\nThis is the full set of solutions to the given equation\n\\(~\\)\n7. \\(~\\) Find a homogeneous Cauchy-Euler differential equation whose general solution is given:\n\\[y = c_1 + c_2x + c_3 x \\ln x\\]\nSolution\nStep 1: \\(~\\)Recall the Form of a Cauchy-Euler Equation\nA homogeneous Cauchy-Euler equation of order 3 has the form:\n\\[x^3 y''' + a_2 x^2 y''  + a_1 x y' + a_0 y = 0\\]\nOur goal is to determine such coefficients \\(a_2\\), \\(a_1\\), \\(a_0\\) so that the general solution matches the given one\nStep 2: \\(~\\) Find Linearly Independent Solutions\nThe given general solution:\n\\[y(x) = c_1 + c_2 x + c_3 x \\ln x\\]\nhas three linearly independent components:\n\n\\(y_1 = 1\\)\n\\(y_2 = x\\)\n\\(y_3 = x \\ln x\\)\n\nWe note:\n\n\\(y_1 = 1\\) is constant\n\\(y_2 = x\\) is a power of \\(x\\)\n\\(y_3 = x \\ln x\\) suggests a repeated root\n\nThe general solution of a Cauchy-Euler equation with constant coefficients is of the form:\n\\[y(x) = x^r, \\quad x^r \\ln x, \\quad \\text{etc}\\]\nGiven the presence of \\(x \\ln x\\), this suggests the repeated root \\(r = 1\\). So the solution corresponds to the roots of the indicial (auxiliary) equation:\n\\[(r - 0)(r - 1)^2 = 0\n\\Rightarrow r^3 - 2r^2 + r = 0\\]\nTherefore, the corresponding differential equation is:\n\\[\nx^3 y''' - 2x^2 y'' + x y' = 0\n\\]\nStep 3: \\(~\\) Use the General Indicial Form\nFor a third-order Cauchy-Euler equation, the general form of the indicial equation is:\n\\[r(r - 1)(r - 2) + a_2 r(r - 1) + a_1 r + a_0 = 0\\]\nComparing coefficients:\n\\[\\begin{aligned}\nr^3 &: 1 = 1 \\\\\nr^2 &: -3 + a_2 = -2 \\Rightarrow a_2 = 1 \\\\\nr^1 &: 2 - a_2 + a_1 = 1 \\Rightarrow 2 - 1 + a_1 = 1 \\Rightarrow a_1 = 0 \\\\\nr^0 &: a_0 = 0\n\\end{aligned}\\]\nSo the equation is:\n\\[\nx^3 y’’’ + x^2 y’’ = 0\n\\]\n\\(~\\)\n7. \\(~\\) Solve \\(~2y'' + 2y' +y = 4\\sqrt{x}\\)\nSolution\nStep 1: \\(~\\) Homogeneous part\n\\[\n\\begin{aligned}\n2y'' +& 2y' + y =0 \\\\\n&\\Downarrow \\;\\; y=e^{px} \\\\\n2p^2 + 2p + 1 = 0 \\;\\; &\\rightarrow p = -\\frac{1}{2} \\pm\\frac{1}{2}i \\\\\n&\\Downarrow \\\\\ny_1 = e^{-\\frac{1}{2}x} \\cos \\frac{1}{2}x, &\\;\\; y_2 = e^{-\\frac{1}{2}x} \\sin \\frac{1}{2}x\n\\end{aligned}\\]\n\\(~\\)\nStep 2: \\(~\\) Nonhomogeneous part: Method of variation of parameters\n\\[\nW = \\begin{vmatrix}\ne^{-\\frac{1}{2}x} \\cos \\frac{1}{2}x & e^{-\\frac{1}{2}x} \\sin \\frac{1}{2}x\\\\\n-\\frac{1}{2} e^{-\\frac{1}{2}x} \\left(\\cos \\frac{1}{2}x +\\sin \\frac{1}{2}x  \\right ) &\n  \\frac{1}{2} e^{-\\frac{1}{2}x} \\left(\\cos \\frac{1}{2}x -\\sin \\frac{1}{2}x  \\right )\n\\end{vmatrix} = \\frac{1}{2} e^{-x}\n\\]\n\\[\nW_1 = \\begin{vmatrix}\n0 & e^{-\\frac{1}{2}x} \\sin \\frac{1}{2}x\\\\\n2\\sqrt{x} &\n  \\frac{1}{2} e^{-\\frac{1}{2}x} \\left(\\cos \\frac{1}{2}x -\\sin \\frac{1}{2}x  \\right )\n\\end{vmatrix} = -2\\sqrt{x} \\,e^{-\\frac{1}{2}x} \\sin \\frac{1}{2}x\n\\]\n\\[\nW_2 = \\begin{vmatrix}\ne^{-\\frac{1}{2}x} \\cos \\frac{1}{2}x & 0 \\\\\n-\\frac{1}{2} e^{-\\frac{1}{2}x} \\left(\\cos \\frac{1}{2}x +\\sin \\frac{1}{2}x  \\right ) & 2\\sqrt{x}  \n\\end{vmatrix} = 2\\sqrt{x} \\,e^{-\\frac{1}{2}x} \\cos \\frac{1}{2}x\n\\]\n\\[\n\\begin{aligned}\n&\\Downarrow \\\\\nu_1 &= \\int \\frac{W_1}{W}\\, dx = -4\\int \\sqrt{x} \\,e^{\\frac{1}{2}x} \\sin \\frac{1}{2}x \\, dx\\\\\nu_2 &= \\int \\frac{W_2}{W}\\, dx = 4\\int \\sqrt{x} \\,e^{\\frac{1}{2}x} \\cos \\frac{1}{2}x \\, dx\\\\\n&\\Downarrow \\\\\n\\color{red}{y} & \\color{red}{= c_1 y_1 + c_2 y_2 + u_1 y_1 + u_2 y_2}\n\\end{aligned}\\]\n\\(~\\)",
    "crumbs": [
      "**First Semester**",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Higher-Order Differential Equations</span>"
    ]
  },
  {
    "objectID": "ch_04_The_Laplace_Transform.html",
    "href": "ch_04_The_Laplace_Transform.html",
    "title": "4  The Laplace Transform",
    "section": "",
    "text": "4.1 Definition of the Laplace Transform\n\\(~\\)\nExample \\(\\,\\) Evaluate \\(\\mathcal{L}\\{1\\}\\), \\(~\\mathcal{L}\\{t\\}\\), \\(~\\)and \\(~\\mathcal{L}\\{e^{-3t}\\}\\)\n\\(~\\)\nExample \\(\\,\\) Evaluate \\(\\mathcal{L}\\{f(t)\\}~\\) for \\(~\\displaystyle f(t) = \\left\\{\\begin{matrix}\n0, & 0 \\leq t &lt; 3\\\\\n2, & \\phantom{0 \\leq }\\; t \\geq 3\n\\end{matrix}\\right.\\)\n\\(~\\)\n\\(~\\)\nExample \\(\\,\\) Find \\(\\mathcal{L}\\{f(t)\\}\\) by first using an appropriate trigonometric identity \\(~f(t)=\\sin^2 2t\\)\n\\(~\\)\nExample \\(\\,\\) Find \\(\\mathcal{L}\\{f(t)\\}\\)\n\\(~\\)\nExample \\(\\,\\) Find \\(\\mathcal{L}\\{f(t)\\}\\)\n\\(~\\)",
    "crumbs": [
      "**First Semester**",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>The Laplace Transform</span>"
    ]
  },
  {
    "objectID": "ch_04_The_Laplace_Transform.html#sec-4-1",
    "href": "ch_04_The_Laplace_Transform.html#sec-4-1",
    "title": "4  The Laplace Transform",
    "section": "",
    "text": "If \\(f\\) be a function defined for \\(t \\geq 0\\), \\(~\\)then the integral\n\\[\\mathcal{L}\\{f(t)\\} =\\int_0^\\infty f(t) e^{-st}\\, dt =F(s)\\]\nis the Laplace Transform of \\(~f\\) provided the integral converges. The result is a function of \\(s\\)\n\n\n\n\n\n\n\n\\(\\mathcal{L}\\) is a linear transform\n\\[\\mathcal{L}\\{\\alpha f(t) +\\beta g(t)\\} = \\alpha \\mathcal{L} \\{f(t)\\} +\\beta\\mathcal{L}\\{g(t)\\} =\\alpha F(s) +\\beta G(s)\\]\n\n\n\n\n\n\n\\(~f(t)=\\left\\{\\begin{matrix}\n-1, & 0 \\leq t &lt; 1\\\\\n\\phantom{-}1, & t \\geq 1\\quad\\;\\;\n\\end{matrix}\\right.\\)\n\\(~f(t)=\\left\\{\\begin{matrix}\n\\phantom{-}t, & 0 \\leq t &lt; 1\\\\\n\\phantom{-}1, & t \\geq 1\\quad\\;\\;\n\\end{matrix}\\right.\\)\n\\(~f(t)=\\left\\{\\begin{matrix}\n\\sin t,& 0 \\leq t &lt; \\pi\\\\\n0,& t \\geq \\pi \\quad\\;\\;\n\\end{matrix}\\right.\\)\n\n\n\n\n\\(~f(t)=e^{t+7}\\)\n\\(~f(t)=e^t \\cos t\\)\n\\(~f(t)=t\\cos t\\)\n\\(~f(t)=\\sin 3t \\cos 3t\\)\n\\(~f(t)=\\sin^4 t\\)",
    "crumbs": [
      "**First Semester**",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>The Laplace Transform</span>"
    ]
  },
  {
    "objectID": "ch_04_The_Laplace_Transform.html#sec-4-2",
    "href": "ch_04_The_Laplace_Transform.html#sec-4-2",
    "title": "4  The Laplace Transform",
    "section": "4.2 The Inverse Transform and Transforms of Derivatives",
    "text": "4.2 The Inverse Transform and Transforms of Derivatives\n\nIf \\(F(s)\\) represents the Laplace transform of \\(~f(t)\\), \\(~\\)then \\(f(t)\\) is the inverse Laplace transform of \\(F(s)\\)\n\\[f(t)=\\mathcal{L}^{-1}\\{F(s)\\}\\]\n\\(\\mathcal{L}^{-1}\\) is a linear transform\n\\[\\mathcal{L}^{-1}\\{\\alpha F(s) +\\beta G(s)\\} = \\alpha \\mathcal{L}^{-1} \\{F(s)\\} +\\beta\\mathcal{L}^{-1}\\{G(s)\\} =\\alpha f(t) +\\beta g(t)\\]\n\n\\(~\\)\nExample \\(\\,\\) Find the given inverse transform\n\n\n\n\\(\\displaystyle \\mathcal{L}^{-1} \\left\\{ \\frac{1}{s^3} \\right\\}\\)\n\\(~\\)\n\\(\\displaystyle \\mathcal{L}^{-1} \\left\\{ \\frac{1}{s^2} - \\frac{48}{s^5} \\right\\}\\)\n\\(~\\)\n\\(\\displaystyle \\mathcal{L}^{-1} \\left\\{ \\frac{(s+1)^3}{s^4} \\right\\}\\)\n\n\n\n\\(\\displaystyle \\mathcal{L}^{-1} \\left\\{ \\frac{1}{4s^2+1} \\right\\}\\)\n\\(~\\)\n\\(\\displaystyle \\mathcal{L}^{-1} \\left\\{ \\frac{s+1}{s^2+2} \\right\\}\\)\n\n\n\n\\(~\\)\nExample \\(\\,\\) Evaluate \\(\\displaystyle\\mathcal{L}^{-1}\\left\\{\\frac{-2s +6}{s^2 +4}\\right\\}\\)\n\\(~\\)\n\nTransforms of derivatives\n\\[\n\\begin{aligned}\n  \\mathcal{L}\\{f'(t)\\} &= sF(s) -f(0)\\\\\n  \\mathcal{L}\\{f''(t)\\} &= s^2F(s) -sf(0) -f'(0)\\\\\n  &\\; \\vdots  \n\\end{aligned}\\]\n\\(\\displaystyle\\mathcal{L}\\left\\{\\frac{d^n f}{dt^n}\\right\\}\\) depends on \\(F(s)=\\mathcal{L}\\{f(t)\\}\\) and the \\(n-1\\) derivatives of \\(~f(t)\\) evaluated at \\(t=0\\)\nIf \\(~f\\) is piecewise continuous on \\([0, \\infty]\\) and of exponential order, then\n\\[\\lim_{s \\to \\infty} \\mathcal{L}\\{f(t)\\}=0\\]\nThe Laplace transform of a linear DE with constant coefficients becomes an algebraic equation in \\(Y(s)\\)\n\n\\(~\\)\nExample \\(\\,\\) Use the Laplace transform to solve the IVP\n\\[\\frac{dy}{dt} +3y = 13\\sin 2t, \\;y(0)=6\\]\n\\(~\\)\nExample \\(\\,\\) Use the Laplace transform to solve the given initial-value problem\n\n\\(~y''+y=\\sqrt{2} \\sin \\sqrt{2}t, \\;\\;y(0)=10,\\;y'(0)=0\\)\n\\(~2y'''+3y''-3y'-2y=e^{-t}, \\;\\;y(0)=0, \\;y'(0)=0, \\; y''(0)=1\\)\n\n\\(~\\)",
    "crumbs": [
      "**First Semester**",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>The Laplace Transform</span>"
    ]
  },
  {
    "objectID": "ch_04_The_Laplace_Transform.html#sec-4-3",
    "href": "ch_04_The_Laplace_Transform.html#sec-4-3",
    "title": "4  The Laplace Transform",
    "section": "4.3 Translation Theorems",
    "text": "4.3 Translation Theorems\n\nFirst Translation Theorem\nIf \\(\\mathcal{L}\\{f(t)\\}=F(s)~\\) and \\(~a\\) is any real number, \\(~\\)then\n\\[\\mathcal{L}\\{e^{-at}f(t)\\}=F(s+a)\\]\n\n\\(~\\)\nExample \\(\\,\\) Evaluate \\(\\mathcal{L}\\{e^{-2t}\\cos 4t\\}\\) and \\(\\displaystyle\\mathcal{L}^{-1}\\left\\{\\frac{2s +5}{(s +3)^2}\\right\\}\\)\n\\(~\\)\n\nSecond Translation Theorem\nIf \\(\\mathcal{L}\\{f(t)\\}=F(s)~\\) and \\(~a &gt;0\\), then\n\\[\\mathcal{L}\\{f(t -a)\\mathcal{U}(t -a)\\}=e^{-as}F(s)\\]\nAlternative Form\n\\[\n\\begin{aligned}\n  \\mathcal{L}\\{g(t)\\mathcal{U}(t -a)\\} &= {\\small\\int_a^\\infty e^{-st} g(t)\\,dt}\\\\\n     &={\\small \\int_0^\\infty e^{-s(t'+a)} g(t' +a) \\,dt'} \\\\&= e^{-as} \\mathcal{L}\\{g(t+a)\\}\n\\end{aligned}\\]\n\n\\(~\\)\nExample \\(\\,\\) Find either \\(~F(s)\\) or \\(~f(t)\\)\n\n\n\n\\(\\mathcal{L}\\{ te^{10t} \\}\\)\n\\(\\mathcal{L}\\{ t^{10}e^{-7t}\\}\\)\n\\(\\mathcal{L}\\{ e^t \\sin 3t \\}\\)\n\n\n\n\\(\\displaystyle \\mathcal{L}^{-1} \\left\\{ \\frac{s}{(s+1)^2} \\right\\}\\)\n\\(\\displaystyle \\mathcal{L}^{-1} \\left\\{ \\frac{2s-1}{s^2(s+1)^3} \\right\\}\\)\n\n\n\n\\(~\\)\nExample \\(\\,\\) Solve \\(~y' +y = f(t)\\), \\(~\\)\\(y(0)=5\\), where\n\\[f(t) =\n    \\begin{cases}\n       0, & 0 \\leq t &lt; \\pi\\\\\n       3\\cos t ,& t \\geq \\pi\n    \\end{cases}\\]\n\\(~\\)",
    "crumbs": [
      "**First Semester**",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>The Laplace Transform</span>"
    ]
  },
  {
    "objectID": "ch_04_The_Laplace_Transform.html#sec-4-4",
    "href": "ch_04_The_Laplace_Transform.html#sec-4-4",
    "title": "4  The Laplace Transform",
    "section": "4.4 Additional Operational Properties",
    "text": "4.4 Additional Operational Properties\n\nDerivatives of Transforms: \\(~\\) If \\(F(s)=\\mathcal{L}\\{f(t)\\}~\\) and \\(n=1,2,\\cdots,\\) then\n\\[\\mathcal{L}\\{t^nf(t)\\}=(-1)^n \\frac{d^n}{ds^n} F(s)\\]\n\n\\(~\\)\n\nConvolution Theorem: \\(\\,\\) \\(\\displaystyle f*g=\\int_0^t f(\\tau)g(t -\\tau)\\, d\\tau\\)\n\\[\\mathcal{L}\\{f*g\\}=\\mathcal{L}\\{f(t)\\} \\mathcal{L}\\{g(t)\\} =F(s) G(s)\\]\n\n\\(~\\)\n\nTransform of a Periodic Function: \\(~\\) \\(f(t+T)=f(t)\\)\n\\[\\mathcal{L}\\{f(t)\\}=\\frac{1}{1-e^{-sT}} \\int_0^T e^{-st} f(t) \\,dt\\]\n\n\\(~\\)\nExample \\(\\,\\) Evaluate \\(~\\mathcal{L}\\{t\\sin \\omega t\\}\\)\n\\(~\\)\nExample \\(\\,\\) Solve \\(~x'' +16x =\\cos 4t, \\; x(0)=1, \\; x'(0)=1\\)\n\\(~\\)\nExample \\(\\,\\) Evaluate \\(\\displaystyle\\mathcal{L}^{-1}\\left\\{\\frac{1}{(s^2 +\\omega^2)^2}\\right\\}\\)\n\\(~\\)\nExample \\(\\,\\) Evaluate \\(\\displaystyle\\mathcal{L} \\left\\{ \\int_0^t f(\\tau)\\, d\\tau \\right\\}\\)\n\\(~\\)\nExample \\(\\,\\) Solve \\(\\displaystyle \\,f(t) =3t^2 -e^{-t} -\\int_0^t f(\\tau)\\, e^{t -\\tau}\\, d\\tau\\;\\) for \\(f(t)\\)\n\\(~\\)\nExample \\(\\,\\) Find the Laplace transform of the periodic function\n\n\n\n\n\n\\[\\scriptsize\\mathcal{L}\\{E(t)\\}=\\frac{1}{1 -e^{-2s}} \\int_0^2 e^{-st} E(t)\\,dt=\\frac{1}{s(1 +e^{-s})}\\]\n\\(~\\)\nExample \\(\\,\\) Evaluate the given Laplace transform\n\n\\(~\\mathcal{L}\\left\\{ te^{-10t} \\right\\}\\)\n\\(~\\mathcal{L}\\left\\{ t\\cos 2t \\right\\}\\)\n\\(~\\mathcal{L}\\left\\{ te^{2t}\\sin 6t \\right\\}\\)\n\n\\(~\\)\nExample \\(\\,\\) Use the Laplace transform to solve the given initial-value problem\n\n\\(~y'+y=t \\sin t, \\;y(0)=0\\)\n\\(~y''+9y=\\cos 3t, \\;y(0)=2, \\;y'(0)=5\\)\n\n\\(~\\)\nExample \\(\\,\\) Find the convolution \\(~f*g\\) of the given functions. After integrating find the Laplace transform \\(~f*g\\)\n\n\\(~f(t)=4t, \\;g(t)=3t^2\\)\n\\(~f(t)=e^{-t}, \\;g(t)=e^t\\)\n\n\\(~\\)\nExample \\(\\,\\) Find the Laplace transform\n\n\\(~\\mathcal{L} \\left\\{ e^{-t}* e^t \\cos t \\right\\}\\)\n\n\\(~\\)\nExample \\(\\,\\) Evaluate the given inverse transform\n\n\\(~\\displaystyle \\mathcal{L}^{-1} \\left\\{ \\frac{1}{s^3(s-1)} \\right\\}\\)\n\n\\(~\\)\nExample \\(\\,\\) Use the Laplace transform to solve the given integral or integrodifferential equation\n\n\\(~\\displaystyle f(t) +2\\int_0^t f(\\tau)\\cos (t-\\tau)\\,d\\tau=4e^{-t}+\\sin t\\)\n\\(~\\displaystyle \\frac{dy}{dt}=10-\\int_0^t e^{-4\\tau} y(t-\\tau)\\,d\\tau, \\;y(0)=5\\)\n\n\\(~\\)\nExample \\(\\,\\) The Laplace transform \\(\\mathcal{L} \\left\\{ e^{-t^2} \\right\\}\\) exists, but without finding it solve the initial-value problem\n\n\\(~y''+9y=3e^{-t^2}, \\;\\;y(0)=0, \\;y'(0)=0\\)\n\n\\(~\\)\nExample \\(\\,\\) Solve the integral equation\n\n\\(~\\displaystyle f(t)=e^t+e^t \\int_0^t e^{-\\tau} f(\\tau)\\, d\\tau\\)\n\n\\(~\\)",
    "crumbs": [
      "**First Semester**",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>The Laplace Transform</span>"
    ]
  },
  {
    "objectID": "ch_04_The_Laplace_Transform.html#sec-4-5",
    "href": "ch_04_The_Laplace_Transform.html#sec-4-5",
    "title": "4  The Laplace Transform",
    "section": "4.5 The Dirac Delta Function",
    "text": "4.5 The Dirac Delta Function\n\n\n\\[\\textbf{Unit Pulse}\\]\n\\[\\small\n     \\delta_a(t-t_0) =\n       \\left\\{\\begin{matrix}\n          0, & \\;\\;\\;\\;\\;\\;\\; 0 \\leq t &lt; t_0 -a\\\\\n          \\frac{1}{2a}, & t_0 -a \\leq t \\leq t_0 +a\\\\\n          0, & \\;\\; t \\geq t_0 +a\n       \\end{matrix}\\right.\\]\n\n\n\n\n\n\n\n\n\nThe Dirac Delta Function\n\\[\n\\begin{aligned}\n   \\delta(t -t_0) &= \\lim_{a \\to 0} \\,\\delta_a(t -t_0) \\\\\n   &\\Downarrow \\\\\n   \\mathcal{L}\\{\\delta(t -t_0)\\} &= \\lim_{a \\to 0} \\mathcal{L}\\{\\delta_a(t -t_0)\\}=e^{-st_0}\\lim_{a \\to 0} \\left(\\frac{e^{sa} -e^{-sa}}{2sa}\\right)\\\\\n   &= e^{-st_0}\n\\end{aligned}\\]\nWhen \\(~t_0=0\\), \\(~\\)\\(\\displaystyle\\mathcal{L}\\{\\delta(t)\\}=1\\)\n\n\\(~\\)\nExample \\(\\,\\) Solve \\(~y'' +y=4\\delta(t -2\\pi)\\) \\(~\\)subject to \\(y(0)=1, \\;y'(0)=0\\)\n\\(~\\)\nExample \\(\\,\\) Use the Laplace transform to solve the given differential equation subject to the indicated initial conditions\n\n\\(~y'-3y=\\delta(t-2), \\;y(0)=0\\)\n\\(~y''+y=\\delta(t-2\\pi), \\;y(0)=0, \\,y'(0)=1\\)\n\\(~y''+y=\\delta(t-\\pi/2)+\\delta(t-3\\pi/2),\\;y(0)=0,\\,y'(0)=0\\)\n\n\\(~\\)",
    "crumbs": [
      "**First Semester**",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>The Laplace Transform</span>"
    ]
  },
  {
    "objectID": "ch_04_The_Laplace_Transform.html#sec-4-6",
    "href": "ch_04_The_Laplace_Transform.html#sec-4-6",
    "title": "4  The Laplace Transform",
    "section": "4.6 Systems of Linear Differential Equations",
    "text": "4.6 Systems of Linear Differential Equations\n\nWhen initial conditions are specified, \\(~\\)the Laplace transform reduces a system of linear DEs to a set of simultaneous algebraic equations in the transformed functions\n\n\\(~\\)\nExample \\(\\,\\) Double Pendulum\n\n\n\n\n\nLinearization \\(\\,\\) For small displacements \\(\\theta_1\\) and \\(\\theta_2\\),\n\\[\n   \\begin{aligned}\n      (m_1 +m_2) l_1 \\ddot{\\theta_1} +m_2 l_2 \\ddot{\\theta_2} +(m_1 +m_2) g \\,\\theta_1 &= 0\\\\\n      l_2 \\ddot{\\theta_2} +l_1 \\ddot{\\theta_1} +g \\,\\theta_2 &= 0\n  \\end{aligned}\\]\nSolve the system when\n\\[m_1=3, m_2=1, l_1=l_2=5, \\text{ and } ~g=10\\]\n\\[\\theta_1(0) = 1, \\theta_2(0)=-1, \\dot{\\theta_1}(0)=0,  \\dot{\\theta_2}(0)=0\\]\n\\(~\\)\nExample \\(\\,\\) Use the Laplace transform to solve the given system of differential equations\n\\[\n\\begin{aligned}\n     \\dot{x} &= -x+y \\\\\n     \\dot{y} &= 2x, \\;\\;x(0)=0,\\;y(0)=1\n\\end{aligned}\\]\n\\[\n\\begin{aligned}\n    \\dot{x} &= x-2y \\\\\n    \\dot{y} &= 5x-y, \\;\\;x(0)=-1,\\;y(0)=2\n\\end{aligned}\\]\n\\[\n\\begin{aligned}\n    2&\\dot{x}+\\dot{y} -2x= 1 \\\\\n    &\\dot{x}+\\dot{y} -3x -3y= 2, \\;\\;x(0)=0,\\;y(0)=0\n\\end{aligned}\\]",
    "crumbs": [
      "**First Semester**",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>The Laplace Transform</span>"
    ]
  },
  {
    "objectID": "ch_04_The_Laplace_Transform.html#worked-exercises",
    "href": "ch_04_The_Laplace_Transform.html#worked-exercises",
    "title": "4  The Laplace Transform",
    "section": "Worked Exercises",
    "text": "Worked Exercises\n1. \\(\\phantom{1}\\) Solve the system\n\\[\\begin{aligned}\nx' +y &= t\\\\\n4x +y' &=0\n\\end{aligned} \\quad \\text{at} \\;\\; x(0)=0, \\;y(0)=2\\]\n\\(~\\)\nSolution\nStep 1: Take Laplace transforms\n\\[\\scriptsize \\mathcal{L}\\{x’\\} + Y(s) = \\mathcal{L}\\{t\\}\n\\Rightarrow sX(s) - x(0) + Y(s) = \\frac{1}{s^2}\n\\Rightarrow sX(s) + Y(s) = \\frac{1}{s^2} \\tag{a}\\label{ch04:e1}\\]\n\\[\\scriptsize 4X(s) + \\mathcal{L}\\{y’\\} = 0\n\\Rightarrow 4X(s) + (sY(s) - y(0)) = 0\n\\Rightarrow 4X(s) + sY(s) = 2\\tag{b}\\label{ch04:e2}\\]\nStep 2: Solve the algebraic system\nFrom equation \\(\\eqref{ch04:e1}\\):\n\\[Y(s) = \\frac{1}{s^2} - sX(s)\\]\nSubstitute into equation \\(\\eqref{ch04:e2}\\):\n\\[4X(s) + s\\left( \\frac{1}{s^2} - sX(s) \\right) = 2\n\\Rightarrow X(s) = \\frac{2s - 1}{s(4 - s^2)}\\]\nThen, from \\(\\eqref{ch04:e1}\\):\n\\[ Y(s) = \\frac{1}{s^2} - sX(s) = \\frac{1}{s^2} - \\frac{2s - 1}{4 - s^2}\\]\nStep 3: Partial fraction decomposition\nFor \\(X(s)\\),\n\\[ X(s) = \\frac{-1}{4s} + \\frac{3}{8(2 - s)} + \\frac{5}{8(2 + s)}\\]\nFor \\(Y(s)\\),\n\\[ Y(s) = \\frac{1}{s^2} + \\frac{3}{4(s - 2)} + \\frac{5}{4(s + 2)}\\]\nStep 4: Inverse Laplace Transform\n\\[ \\begin{aligned}\nx(t) &= -\\frac{1}{4} -\\frac{3}{8} e^{2t} + \\frac{5}{8} e^{-2t} \\\\\ny(t) &= t + \\frac{3}{4} e^{2t} + \\frac{5}{4} e^{-2t}\n\\end{aligned}\\]\n\\(~\\)\n2. \\(~\\) Solve the system\n\\[\\begin{aligned}\nx'' +y'' &= e^{2t}\\\\\n2x' +y'' &=-e^{2t}\n\\end{aligned} \\quad \\text{at} \\;\\; x(0)=0, \\;y(0)=0, \\;x'(0)=0, \\;y'(0)=0\\]\n\\(~\\)\nSolution\nStep 1: Apply Laplace Transforms\n\\[\\mathcal{L}\\{x’’ + y’’\\} = \\mathcal{L}\\{e^{2t}\\}\n\\Rightarrow s^2 X(s) + s^2 Y(s) = \\frac{1}{s - 2}\\tag{a}\\label{eq:ch04e3}\\]\n\\[\\mathcal{L}\\{2x’ + y’’\\} = \\mathcal{L}\\{-e^{2t}\\}\n\\Rightarrow 2sX(s) + s^2 Y(s) = -\\frac{1}{s - 2}\n\\tag{b}\\label{eq:ch04e4}\\]\nStep 2: Solve the system algebraically\nSubtract \\(\\eqref{eq:ch04e4}\\) from \\(\\eqref{eq:ch04e3}\\):\n\\[\\begin{aligned}(s^2 X &+ s^2 Y) - (2s X + s^2 Y) = \\frac{1}{s - 2} - \\left(-\\frac{1}{s - 2}\\right) \\\\\n&\\Rightarrow X(s) = \\frac{2}{(s - 2)(s^2 - 2s)}\n\\end{aligned}\n\\]\nNow substitute into \\(\\eqref{eq:ch04e3}\\) to find \\(Y(s)\\):\n\\[ Y(s) = \\frac{1}{s^2(s - 2)} - \\frac{2}{(s - 2)(s^2 - 2s)}\\]\nStep 3: Simplify and compute inverse Laplace transforms\n\\[X(s) = \\frac{2}{(s - 2)(s^2 - 2s)} = \\frac{1}{2s} - \\frac{1}{2(s - 2)} + \\frac{1}{(s - 2)^2}\\]\nTake inverse Laplace:\n\\[ x(t) = \\frac{1}{2} - \\frac{1}{2} e^{2t} + t e^{2t}\n= \\frac{1}{2}(1 - e^{2t}) + t e^{2t}\\]\nRecall:\n\\[ Y(s) = \\frac{1}{s^2(s - 2)} - \\frac{2}{s(s - 2)^2} = -\\frac{5}{4s} - \\frac{1}{2s^2} + \\frac{5}{4(s - 2)} - \\frac{2}{(s - 2)^2}\\]\nThus:\n\\[ y(t) = -\\frac{5}{4} - \\frac{1}{2} t + \\frac{5}{4} e^{2t} - 2t e^{2t}\\]\n3. \\((a)\\) Show that \\(y = e^{-t^2}\\) is a solution of the initial value problem\n\\[ \\frac{dy}{dt} +2ty=0,\\;\\;y(0)=1 \\]\n\\((b)\\) Find \\(Y(s)=\\mathcal{L}\\left[ e^{-t^2} \\right]\\) by applying the Laplace transform to the above differential equation\n\\[\\phantom{**}\\] Hint) \\(\\displaystyle \\mathrm{erfc}(t) = \\frac{2}{\\sqrt{\\pi}} \\int_t^\\infty \\, e^{-u^2}\\, du\\)\nSolution (a)\nStep 1: \\(~\\) Plug into the differential equation\nLet \\(y(t) = e^{-t^2}\\)\nThen:\n\\[\\frac{dy}{dt} = \\frac{d}{dt}(e^{-t^2}) = -2t e^{-t^2}\\]\nNow substitute into the equation:\n\\[\\frac{dy}{dt} + 2t y = -2t e^{-t^2} + 2t e^{-t^2} = 0\\]\nStep 2: \\(~\\) Check initial condition\n\\[y(0) = e^{0} = 1\\]\nSolution (b)\nStep 1: \\(~\\) Take Laplace transforms\nLet \\(Y(s) = \\mathcal{L}\\{y(t)\\}\\)\nApply Laplace transform:\n\\[\\mathcal{L}\\left\\{ \\frac{dy}{dt} + 2t y \\right\\}\n= sY(s) - y(0) - 2 \\frac{dY}{ds} = 0\\]\nGiven \\(y(0) = 1\\), the transformed equation becomes:\n\\[\\frac{dY}{ds} = \\frac{1}{2}\\left[sY(s) - 1\\right]\\]\nStep 2: \\(~\\) Solve the first-order linear ODE\nWe solve:\n\\[\\frac{dY}{ds} = \\frac{1}{2}sY(s) - \\frac{1}{2}\\]\nThis is a first-order linear ODE for \\(Y(s)\\). Standard form:\n\\[\\frac{dY}{ds} - \\frac{1}{2}sY = -\\frac{1}{2}\\]\nUse integrating factor:\n\\[\\mu(s) = e^{-\\frac{1}{4}s^2}\\]\nMultiply both sides:\n\\[e^{-\\frac{1}{4}s^2} \\frac{dY}{ds} - \\frac{1}{2}s e^{-\\frac{1}{4}s^2} Y = -\\frac{1}{2} e^{-\\frac{1}{4}s^2}\\]\nLHS becomes derivative of the product:\n\\[\\frac{d}{ds} \\left( e^{-\\frac{1}{4}s^2} Y \\right) = -\\frac{1}{2} e^{-\\frac{1}{4}s^2}\\]\nNow integrate both sides:\n\\[\\int \\frac{d}{ds} \\left( e^{-\\frac{1}{4}s^2} Y \\right) ds = \\int -\\frac{1}{2} e^{-\\frac{1}{4}s^2} ds\\]\n\\[e^{-\\frac{1}{4}s^2} Y(s) = -\\frac{1}{2} \\int e^{-\\frac{1}{4}s^2} ds + C\\]\nThere’s no elementary antiderivative for \\(e^{-\\frac{1}{4}s^2}\\), so we leave the integral unevaluated:\n\\[Y(s) = e^{\\frac{1}{4}s^2} \\left( C - \\frac{1}{2} \\int e^{-\\frac{1}{4}s^2} ds \\right)\\]\nThe constant \\(C\\) that appears when solving a differential equation via the Laplace transform is determined by the initial condition (e.g., \\(y(0) = 1\\))\n\\[C = \\frac{\\sqrt{\\pi}}{2} \\operatorname{erfc}\\left( \\frac{s}{2} \\right) + \\frac{1}{2} \\int e^{-s^2/4} ds\\]\nThen:\n\\[\n\\mathcal{L} \\{ e^{-t^2} \\} = \\frac{\\sqrt{\\pi}}{2} e^{\\frac{s^2}{4}} \\operatorname{erfc}\\left( \\frac{s}{2} \\right)\n\\]\n\\(~\\)\n4. \\(~\\) One definition of the gamma function \\(\\Gamma(\\alpha)\\) is given by the improper integral\n\\[ \\Gamma(\\alpha) = \\int_0^\\infty t^{\\alpha-1} \\,e^{-t} \\, dt, \\;\\alpha &gt; 0 \\]\n\\((a)\\) Use this definition to show that \\(\\Gamma(\\alpha+1) = \\alpha \\Gamma(\\alpha)\\)\n\\((b)\\) and find that \\(\\mathcal{L}\\left[ t^\\alpha \\right]\\)\nSolution (a)\nStart with \\(\\Gamma(\\alpha + 1)\\)\n\\[ \\Gamma(\\alpha + 1) = \\int_0^\\infty t^{\\alpha} e^{-t} dt \\]\nWe’ll use integration by parts:\n\nLet \\(u = t^\\alpha \\Rightarrow du = \\alpha t^{\\alpha - 1} dt\\)\nLet \\(dv = e^{-t} dt \\Rightarrow v = -e^{-t}\\)\n\nThen:\n\\[\n\\begin{aligned}\n\\Gamma(\\alpha + 1) &= \\left. -t^{\\alpha} e^{-t} \\right|_0^\\infty + \\int_0^\\infty \\alpha t^{\\alpha - 1} e^{-t} dt \\\\\n&= 0 + \\alpha \\int_0^\\infty t^{\\alpha - 1} e^{-t} dt \\\\\n&= \\alpha \\Gamma(\\alpha)\n\\end{aligned}\\]\nSolution (b)\n\\[ \\mathcal{L} \\{ t^\\alpha \\} = \\int_0^\\infty e^{-s t} t^\\alpha dt \\]\nMake substitution \\(\\displaystyle u = s t \\Rightarrow t = \\frac{u}{s}, \\; dt = \\frac{du}{s}\\)\nThen:\n\\[\n\\begin{aligned}\n\\mathcal{L} \\{ t^\\alpha \\}\n&= \\int_0^\\infty e^{-s t} t^\\alpha dt\n= \\int_0^\\infty e^{-u} \\left( \\frac{u}{s} \\right)^\\alpha \\cdot \\frac{du}{s} \\\\\n&= \\frac{1}{s^{\\alpha + 1}} \\int_0^\\infty u^\\alpha e^{-u} du \\\\\n&= \\frac{1}{s^{\\alpha + 1}} \\Gamma(\\alpha + 1)\n\\end{aligned}\\]\n\\(~\\)\n5. \\(~\\) Solve the given differential equation\n\\[y''+2y'+y=(t-1) u(t-1) +\\delta(t-5), \\;y(0)=0, \\;y'(0)=0\\]\nSolution\n\\[\\begin{aligned}\ny''+2y'+y=(t-1) u(t-1) &+\\delta(t-5), \\;\\;y(0)=0,\\; y'(0)=0 \\\\\n&\\Downarrow \\\\\n(s^2 +2s +1)\\,Y &= \\frac{e^{-s}}{s^2} + e^{-5s}\\\\\n&\\Downarrow \\\\\nY = \\frac{e^{-s}}{s^2(s+1)^2} + \\frac{e^{-5s}}{(s+1)^2} &= \\left[ -\\frac{2}{s} +\\frac{1}{s^2} +\\frac{2}{s+1} +\\frac{1}{(s+1)^2}\\right] e^{-s} + \\frac{e^{-5s}}{(s+1)^2} \\\\\n&\\Downarrow \\\\\n\\color{red}{y(t) = \\left[-3 +t +(t+1)e^{-(t-1)} \\right]} \\; & \\color{red}{\\;u(t -1) +(t-5) e^{-(t-5)} u(t-5)}\n\\end{aligned}\\]\n\\(~\\)\n6. \\(~\\) Solve \\(~y'' + 4y' + 3y =e^t \\delta (t-1), \\;\\;y(0)=0, \\;y'(0)=2\\)\nSolution\n\\[\n\\begin{aligned}\ny'' + 4y' + 3y &= e^t \\delta (t-1), \\;\\;y(0)=0, \\;y'(0)=2\\\\\n&\\Downarrow \\\\\n(s^2 + 4s +3) Y(s) - 2 &= e e^{-s} \\\\\nY(s) &= \\frac{2}{(s+1)(s+3)} + \\frac{e}{(s+1)(s+3)} e^{-s}\\\\\n&\\Downarrow \\\\\n\\color{red}{y(t)} &\\color{red}{= e^{-t} - e^{-3t} +\\frac{e}{2} \\left[e^{-(t-1)} - e^{-3(t-1)}\\right] u(t-1)}\n\\end{aligned}\\]\n\\(~\\)\n7. \\(~\\) Use the Laplace transform to find the numerical value of the improper integral \\[\\int_0^\\infty t e^{-2t} \\sin 4t \\,dt\\]\nSolution\n\\[\\begin{aligned}\nF(s) &= \\mathcal{L} \\left[ t\\, \\sin 4t\\right ] = \\int_0^\\infty t\\, \\sin 4t \\, e^{-st} \\,dt\\\\\n&\\Downarrow \\\\\nF(s) &= -\\frac{d}{ds} \\left(\\frac{4}{s^2 + 4^2} \\right) = \\frac{8s}{(s^2 + 4^2)^2}\\\\\n&\\Downarrow \\\\\n\\color{red}{F(2)} \\; &\\color{red}{= \\frac{1}{25}}\n\\end{aligned}\\]",
    "crumbs": [
      "**First Semester**",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>The Laplace Transform</span>"
    ]
  },
  {
    "objectID": "ch_05_Series_Solutions_of_Linear_Differential_Equations.html",
    "href": "ch_05_Series_Solutions_of_Linear_Differential_Equations.html",
    "title": "5  Series Solutions of Linear Differential Equations",
    "section": "",
    "text": "5.1 Solutions about Ordinary Points\n\\(~\\)\nExample: \\(\\,\\) Solve \\(y'' +xy =0\\)\n\\(~\\)",
    "crumbs": [
      "**First Semester**",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Series Solutions of Linear Differential Equations</span>"
    ]
  },
  {
    "objectID": "ch_05_Series_Solutions_of_Linear_Differential_Equations.html#sec-5-1",
    "href": "ch_05_Series_Solutions_of_Linear_Differential_Equations.html#sec-5-1",
    "title": "5  Series Solutions of Linear Differential Equations",
    "section": "",
    "text": "A power series in \\(x -a\\) is an infinite series of the form\n\\[\\sum_{n=0}^\\infty c_n(x -a)^n = c_0 +c_1(x -a) +c_2(x -a)^2 +\\cdots\\]\nSuch a series is also said to be a power series centered at \\(a\\)\nA power series is convergent if its sequence of partial sums converges\n\nConvergence of power series can often be determined by the ratio test. Suppose that \\(c_n \\neq 0\\) for all \\(n\\), and that\n\\[\\lim_{n \\to \\infty} \\left| \\frac{c_{n+1} (x -a)^{n +1}}{c_n (x -a)^n} \\right|\n   =|x -a| \\lim_{n \\to \\infty} \\left| \\frac{c_{n +1}}{c_n} \\right| =L\\]\nIf \\(L&lt;1\\), the series converges absolutely, if \\(L&gt;1\\), the series diverges, and if \\(L=1\\), the test is inconclusive\nEvery power series has a radius of convergence, \\(R\\). If \\(R&gt;0\\), a power series \\(\\sum_{n=0}^\\infty c_n (x -a)^n\\) converges for \\(|x -a| &lt; R\\)\nA function \\(\\,f\\) ia analytic at point \\(a\\) if it can be represented by a power series in \\(x -a\\) with a positive radius of convergence\nPower series can be combined through the operations of addition, multiplication, and division\n\nConsider the linear second-order DE\n\\[a_2(x) y'' +a_1(x) y' +a_0(x)y = 0,\\;\\;a_2(x)\\neq0\\]\n\nDivide by \\(a_2(x)\\) to put into standard form\n\\[y'' +P(x)y' +Q(x)y = 0\\]\nPoint \\(x_0\\) is an ordinary point of the DE if both \\(P(x)\\) and \\(Q(x)\\) are analytic at \\(x_0\\). \\(~\\)A point that is not an ordinary point is a singular point of the equation\nIf \\(x=x_0\\) is an ordinary point of the DE, \\(~\\)we can always find two linearly independent solutions in the form of a power series centered at \\(x_0\\)\nA series solution converges at least on some interval defined by \\(|x -x_0|&lt;R\\), \\(~\\)where \\(R\\) is the distance from \\(x_0\\) to the closest singular point\n\n\n\n\n\nSince there are no finite singular points, \\(~\\)two power series solutions are guaranteed, centered at \\(0\\), convergent for \\(|x|&gt;\\infty\\)\nSubstituting \\(~y=\\sum_{n=0}^\\infty c_n x^n\\) and the second derivative \\(y''=\\sum_{n=2}^\\infty c_n n(n -1) x^{n -2}\\) into the DE gives\n\\[\n\\begin{aligned}\n   y'' +xy &= \\sum_{n=2}^\\infty c_n n(n-1) x^{n -2} +\\sum_{n=0}^\\infty c_n x^{n +1}\\\\\n   &= 2c_2 +\\sum_{k=1}^{\\infty} \\left[(k +1)(k +2) c_{k +2} +c_{k -1} \\right] x^k =0\n\\end{aligned}\\]\nThe coefficient of each power of \\(x\\) be set equal to zero:\n\\[c_2 =0 \\;\\;\\text{and}\\;\\; \\displaystyle c_{k +2} =-\\frac{c_{k -1}}{(k+1)(k+2)}, \\;k=1,2,3,\\cdots\\]\n\\[\\scriptsize\n\\begin{aligned}\nc_3 &= -\\frac{c_0}{2\\cdot3} \\\\\nc_4 &= -\\frac{c_1}{3\\cdot4} \\\\\nc_5 &= -\\frac{c_2}{4\\cdot5} = 0 \\\\\nc_6 &= -\\frac{c_3}{5\\cdot6} = \\frac{c_0}{2\\cdot3\\cdot5\\cdot6} \\\\\nc_7 &= -\\frac{c_4}{6\\cdot7} = \\frac{c_1}{3\\cdot4\\cdot6\\cdot7}\\\\\nc_8 &= -\\frac{c_5}{7\\cdot8} = 0 \\\\\n     &\\;\\;\\vdots\n\\end{aligned}\\]\nAfter grouping the terms containing \\(c_0\\) and the terms containing \\(c_1\\), \\(~\\)we obtain \\(y = c_0 y_1(x) +c_1 y_2(x)\\)\n\\[  \n\\begin{aligned}\n  y_1(x) &= 1 +\\sum_{n=1}^\\infty \\frac{(-1)^n}{2\\cdot3 \\cdots (3n -1)(3n)}x^{3n}\\\\\n  y_2(x) &= x +\\sum_{n=1}^\\infty \\frac{(-1)^n}{3\\cdot4 \\cdots (3n)(3n +1)}x^{3n +1}\n\\end{aligned}\\]",
    "crumbs": [
      "**First Semester**",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Series Solutions of Linear Differential Equations</span>"
    ]
  },
  {
    "objectID": "ch_05_Series_Solutions_of_Linear_Differential_Equations.html#sec-5-2",
    "href": "ch_05_Series_Solutions_of_Linear_Differential_Equations.html#sec-5-2",
    "title": "5  Series Solutions of Linear Differential Equations",
    "section": "5.2 Solutions about Singular Points",
    "text": "5.2 Solutions about Singular Points\n\nConsider the linear second-order DE\n\\[(x -x_0)^2 y'' +(x -x_0) p(x) y' +q(x) y = 0\\]\n\nPoint \\(x_0\\) is a regular singular point of the DE if both \\(p(x)\\) and \\(q(x)\\) are analytic at \\(x_0\\)\nA singular point that is not regular is an irregular singular point of the equation\n\nTo solve a DE about a regular singular point, \\(~\\)we employ Frobenius’ Theorem\n\nIf \\(x_0\\) is a regular singular point of the standard DE, \\(~\\)there exists at least one nonzero solution of the form\n\\[y=(x -x_0)^r \\sum_{n=0}^\\infty c_n (x -x_0)^n = \\sum_{n=0}^\\infty c_n (x -x_0)^{n+r}\\]\nwhere \\(r\\) is a constant, and the series converges at least on some interval, \\(~0 &lt; x -x_0 &lt; R\\)\nAfter substituting \\(y =\\sum_{n=0}^\\infty c_n (x -x_0)^{n+r}\\) into a DE and simplifying, the indicial equation is obtained, \\(~\\)a quadratic equation in \\(r\\) that results from equating the total coefficient of the lowest power of \\(x\\) to zero\nThe indicial roots are the solutions to the quadratic equation and are then substituted into a recurrence relation\n\n\nSuppose that \\(x=x_0\\) ia a regular singular point of a DE and the indicial roots are \\(r_1\\) and \\(r_2\\): \\(\\;r_1 \\geq r_2\\)\n\nCase I\n\\(r_1\\) and \\(r_2\\) are distinct  and do not differ by an integer,\n\\[\n\\begin{aligned}\n   y_1(x) &= \\sum_{n=0}^\\infty {\\color{red}{c_n}} (x -x_0)^{n +{\\color{red}{r_1}}} \\\\\n   y_2(x) &= \\sum_{n=0}^\\infty {\\color{red}{b_n}} (x -x_0)^{n +{\\color{red}{r_2}}}\n\\end{aligned}\\]\nCase II\n\\(r_1 -r_2 = N\\), \\(~\\)where \\(N\\) is a positive integer,\n\\[   \n\\begin{aligned}\n    y_1(x) &= \\sum_{n=0}^\\infty {\\color{red}{c_n}} (x -x_0)^{n +{\\color{red}{r_1}}}, \\; c_0 \\neq 0\\\\\n    y_2(x) &= {\\color{red}{C}}y_1(x)\\ln (x -x_0) +\\sum_{n=0}^\\infty {\\color{red}{b_n}} (x -x_0)^{n +{\\color{red}{r_2}}}, \\; b_0 \\neq 0\n\\end{aligned}\\]\nCase III\n\\(r_1=r_2\\),\n\\[\n\\begin{aligned}\n   y_1(x) &= \\sum_{n=0}^\\infty {\\color{red}{c_n}} (x -x_0)^{n +{\\color{red}{r_1}}}, \\; c_0 \\neq 0\\\\\n    y_2(x) &= y_1(x)\\ln (x -x_0) +\\sum_{n=0}^\\infty {\\color{red}{b_n}} (x -x_0)^{n +{\\color{red}{r_1}}}\n\\end{aligned}\\]\n\n\\(~\\)\nExample: \\(\\,\\) Solve \\(\\,2xy'' +(1 +x)y' +y = 0\\)\n\nSubstituting \\(y = \\sum_{n=0}^\\infty c_n x^{n +r}\\) gives\n\\[\\scriptsize  \n\\begin{aligned}\n  2xy'' & +(1 +x)y' +y \\\\\n  & = 2\\sum_{n=0}^\\infty (n +r)(n +r -1)c_n x^{n +r -1}\n   +\\sum_{n=0}^\\infty (n +r) c_n x^{n +r -1}\n   +\\sum_{n=0}^\\infty (n +r) c_n x^{n +r}\n   +\\sum_{n=0}^\\infty c_n x^{n +r}\\\\\n   & = x^r\\left[r(2r -1) c_0 x^{-1} +\\sum_{k=0}^\\infty [(k + r +1)(2k +2r +1) c_{k +1} +(k +r +1) c_k] x^k \\right] = 0\n\\end{aligned}\\]\nwhich implies\n\\[\n\\begin{aligned}\n   &r(2r -1) = 0\\\\\n   (k + r +1)(2k +2r +1) &c_{k +1} +(k +r +1) c_k =0, \\;\\;k=0,1,\\cdots\n\\end{aligned}\\]\nWe see that the indicial roots are \\(r_1=\\frac{1}{2}\\) and \\(r_2=0\\)\n\\[\n\\begin{aligned}\n  r_1 = \\frac{1}{2}, &\\;\\;c_{k+1} =-\\frac{c_k}{2(k +1)}, \\;k=0,1,2, \\cdots\\, \\\\\n  r_2 = 0,\\; &\\;\\;c_{k+1} =-\\frac{c_k}{2k +1}, \\;k=0,1,2, \\cdots \\,\\\\\n\\end{aligned}\\]\n\n\n\n\nFor \\(r_1=\\frac{1}{2}\\),\n\\[ \\small\n\\begin{aligned}\n  c_1 &= -\\frac{c_0}{2\\cdot1}\\\\\n  c_2 &= -\\frac{c_1}{2\\cdot2}=\\frac{c_0}{2^2\\cdot2!}\\\\\n  c_3 &= -\\frac{c_2}{2\\cdot3}=\\frac{-c_0}{2^3\\cdot3!}\\\\\n  c_4 &= -\\frac{c_3}{2\\cdot4}=\\frac{c_0}{2^4\\cdot4!}\\\\\n   &\\;\\vdots \\\\\n  c_n &= \\frac{(-1)^n c_0}{2^n n!}\n\\end{aligned}\\]\n\n\n\nFor \\(r_2=0\\),\n\\[ \\small\n\\begin{aligned}\n   c_1 &= -\\frac{c_0}{1}\\\\\n   c_2 &= -\\frac{c_1}{3}=\\frac{c_0}{1\\cdot3}\\\\\n   c_3 &= -\\frac{c_2}{5}=\\frac{-c_0}{1\\cdot3\\cdot5}\\\\\n   c_4 &= -\\frac{c_3}{7}=\\frac{c_0}{1\\cdot3\\cdot5\\cdot7}\\\\ &\\;\\vdots \\\\\n   c_n &= \\frac{(-1)^n c_0}{1\\cdot3\\cdot5\\cdot7\\cdots (2n -1)}\n\\end{aligned}\\]\n\n\n\n\nThe series solutions are\n\\[\n\\begin{aligned}\n    y_1(x) &= x^{1/2} \\left[ 1 +\\sum_{n=1}^\\infty \\frac{(-1)^n}{2^n n!} x^n \\right ]\\\\\n    y_2(x) &= 1 +\\sum_{n=1}^\\infty \\frac{(-1)^n}{1\\cdot3\\cdot5\\cdot7\\cdots(2n -1)} x^n\n\\end{aligned}\\]\n\n\\(~\\)",
    "crumbs": [
      "**First Semester**",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Series Solutions of Linear Differential Equations</span>"
    ]
  },
  {
    "objectID": "ch_05_Series_Solutions_of_Linear_Differential_Equations.html#sec-5-3",
    "href": "ch_05_Series_Solutions_of_Linear_Differential_Equations.html#sec-5-3",
    "title": "5  Series Solutions of Linear Differential Equations",
    "section": "5.3 Special Functions",
    "text": "5.3 Special Functions\nThe following DEs occur frequently in advanced studies in applied mathematics, physics, and engineering\n\nBessel’s equation of order \\(\\nu\\), \\(~\\)solutions are Bessel functions\n\\[x^2 y'' +xy' +(x^2 -\\nu^2) y =0\\]\nLegendre’s equation of order \\(n\\), \\(~\\)solutions are Legendre polynomials\n\\[(1 -x^2)y'' -2xy' +n(n +1)y =0\\]\n\n\n5.3.1 Bessel Functions\n\\(~\\)\n\nBecause \\(x=0~\\) is a regular singular point of Bessel’s equation, there exists at least one solution of the form \\(y=\\sum_{n=0}^\\infty c_n x^{n +r}\\)\n\\[\n\\begin{aligned}\n  x^2 &y'' +xy' +(x^2 -\\nu^2)y \\\\\n  &= \\sum_{n=0}^\\infty c_n (n +r)(n +r -1) x^{n +r}  \n   +\\sum_{n=0}^\\infty c_n (n +r) x^{n +r} \\\\\n  &\\qquad+\\sum_{n=0}^\\infty c_n x^{n +r +2}\n   -\\nu^2 \\sum_{n=0}^\\infty c_n x^{n +r}\\\\\n  &= {\\color{red}{c_0 (r^2 -\\nu^2) x^r}} +x^r\n  \\sum_{n=1}^\\infty c_n [(n +r)^2 -\\nu^2] x^n +x^r\n  \\sum_{n=0}^\\infty c_n x^{n +2} = 0\n\\end{aligned}\\]\nThe indicial equation is \\(~r^2 -\\nu^2=0\\) \\(~\\)so that the indicial roots are \\(r_1=\\nu\\) and \\(r_2=-\\nu\\). When \\(\\,r_1=\\nu \\geq 0\\),\n\\[\\scriptsize\n\\begin{aligned}\n   x^\\nu \\sum_{n=1}^\\infty & c_n n(n +2\\nu) x^n +x^\\nu \\sum_{n=0}^\\infty c_n x^{n +2} \\\\\n   &  = x^\\nu \\left[ (1 +2\\nu) c_1 x +\\sum_{k=0}^\\infty \\left[(k +2)(k +2 +2\\nu) c_{k+2}\n   +c_k\\right]x^{k +2} \\right] = 0 \\\\\n   &\\Downarrow \\\\\n   c_1 & = 0,\\;\\; c_{k+2} = \\frac{-c_k}{(k +2)(k +2 +2\\nu)}, \\;\\;k=0,1,2,\\cdots \\\\\n   &\\Downarrow\\\\\n     c_3 &  =c_5=c_7=\\cdots=0, \\\\\n     c_{2n} & =-\\frac{c_{2n -2}}{2^2 n(n +\\nu)}  \n       \\;\\;\\leftarrow\\;\\;k+2=2n\n\\end{aligned}\\]\nThus\n\\[\\scriptsize\n\\begin{aligned}\nc_2 & = -\\frac{c_0}{2^2\\cdot1\\cdot(1 +\\nu)}\\\\\nc_4 & = -\\frac{c_2}{2^2\\cdot2\\cdot(2 +\\nu)} \\\\\n     & = \\frac{c_0}{2^4 \\cdot 1 \\cdot 2(1 +\\nu)(2 +\\nu)}\\\\\nc_6 & = -\\frac{c_4}{2^2\\cdot3\\cdot(3 +\\nu)} \\\\\n     & =-\\frac{c_0}{2^6 \\cdot 1 \\cdot 2 \\cdot 3 (1 +\\nu)(2 +\\nu)(3 +\\nu)}\\\\\n     &\\;\\vdots \\\\\nc_{2n} & = \\frac{(-1)^n c_0}{2^{2n} n! (1 +\\nu)(2 +\\nu)\\cdots(n +\\nu)}\\\\    \n\\end{aligned}\\]\nIt is standard practice to choose\n\\[\n\\begin{aligned}\n  c_0 & =\\frac{1}{2^\\nu \\Gamma(1 +\\nu)} \\\\\n  & \\text{thus, } \\\\\n  c_{2n} & = \\frac{(-1)^n}{2^{2n +\\nu} n!\\, \\Gamma(1 +\\nu +n)}\n\\end{aligned}\\]\n\nNote \\(\\,\\) \\(\\displaystyle\\Gamma(\\nu)=\\;\\int_0^\\infty x^{\\nu -1} e^{-x} \\,dx\\), \\(\\;\\Gamma(\\nu +1)=\\nu\\Gamma(\\nu)\\)\n\nimport numpy as np\nfrom scipy.special import gamma, factorial\n\nimport matplotlib.pyplot as plt\n\nx = np.linspace(-3.5, 5.5, 2251)\ny = gamma(x)\n\ndef gamma_plot():\n    fig = plt.figure(figsize=(6, 4))\n\n    plt.plot(x, y, 'b', alpha=0.6, label=r'$\\Gamma(x)$')    \n    k = np.arange(1, 7)\n    plt.plot(k, factorial(k -1), 'ko', alpha=0.6,\n         label=r'$(x-1)!,\\; x = 1, 2, 3\\cdots$')\n    plt.xlim(-3.5, 5.5)\n    plt.ylim(-10, 25)\n    plt.grid('true')\n    plt.xlabel('x')\n    plt.ylabel(r'$\\Gamma(x)$')\n    plt.legend(loc='lower right')\n\n    plt.show()\n\ngamma_plot()    \n\n\n\n\n\n\n\nFigure 5.1: Gamma Function\n\n\n\n\n\n\nBessel Functions of the First Kind\nThe series solution \\(y_1=\\sum_{n=0}^\\infty c_{2n} x^{2n +\\nu}\\) is usually denoted by \\(J_\\nu(x)\\)\n\\[J_\\nu(x)=\\sum_{n=0}^\\infty \\frac{(-1)^n}{n!\\, \\Gamma(1 +\\nu +n)} \\left( \\frac{x}{2} \\right)^{2n +\\nu}\\]\nAlso, for the second exponent \\(r_2=-\\nu\\)\n\\[J_{-\\nu}(x)=\\sum_{n=0}^\\infty \\frac{(-1)^n}{n!\\, \\Gamma(1 -\\nu +n)} \\left( \\frac{x}{2} \\right)^{2n -\\nu}\\]\n\nWhen \\(~\\nu=0\\), \\(~J_0(x)\\)\nWhen \\(r_1 -r_2 = 2\\nu~\\) is not positive integer, \\(~J_\\nu(x)\\) and \\(J_{-\\nu}(x)\\) are linearly independent\nWhen \\(r_1 -r_2 = 2\\nu~\\) is positive integer, \\(~\\)there are two possibilities\n\nWhen \\(\\nu = m =\\) positive integer, \\(~J_{-m}(x)\\) is a constant multiple of \\(J_m(x)\\) : \\(~J_m(x)=(-1)^m J_{-m}(x)\\)\nWhen \\(\\nu\\) is half an odd positive integer, \\(~J_\\nu(x)\\) and \\(J_{-\\nu}(x)\\) are linearly independent\n\n\n\n\\(~\\)\nExample \\(\\,\\) Solve \\(~x^2y'' +xy' +(x^2 -\\frac{1}{4})y=0\\)\n\\(~\\)\n\nBessel Functions of the Second Kind\nIf \\(\\nu \\neq\\) integer\n\\[Y_\\nu(x)=\\frac{\\cos \\nu \\pi J_\\nu(x) -J_{-\\nu}(x)}{\\sin \\nu \\pi}\\]\n\\(J_\\nu(x)\\) and \\(Y_\\nu(x)\\) are linearly independent solutions of\n\\[x^2 y'' +xy' +(x^2 -\\nu^2) y =0\\]\nAs \\(\\nu \\rightarrow m\\,(\\text{an integer})\\),\n\\[Y_m(x)=\\lim_{\\nu \\to m} Y_\\nu(x)\\]\n\\(J_m(x)\\) and \\(Y_m(x)\\) are linearly independent solutions of\n\\[x^2 y'' +xy' +(x^2 -m^2) y =0\\]\nHence for any value of \\(\\nu\\), \\(~\\)the general solution of Bessel equation can be written as\n\\[y=c_1 J_\\nu(x) +c_2 Y_\\nu(x)\\]\n\\(Y_\\nu(x)\\) is called the Bessel function of the second kind of order \\(\\nu\\)\n\n\\(~\\)\nExample \\(\\,\\) Solve \\(~x^2y'' +xy' +(x^2 -9)y=0\\)\n\\(~\\)\n\n5.3.1.1 Properties of Bessel Functions\n\\(~\\)\n\nWhen \\(m\\) is integer\n\n\\(\\displaystyle \\color{blue}{J_m(x) = \\sum_{n=0}^\\infty \\frac{(-1)^n}{n!(n + m)!} \\left( \\frac{x}{2} \\right)^{2n + m}}\\)\n\\(J_{-m}(x)=(-1)^m J_m(x)\\)\n\\(J_{m}(-x)=(-1)^m J_m(x)\\)\n\\(J_m(0)=\\left\\{\\begin{matrix} 0, & m &gt; 0\\\\ 1, & m = 0 \\end{matrix}\\right.\\)\n\\(\\displaystyle\\lim_{x \\to 0^+} Y_m(x)=-\\infty\\)\n\n\n\nfrom scipy.special import jv, yv\n\nplt.style.use('ggplot')\n\nfig = plt.figure(figsize=(6, 8))\n\nax1 = fig.add_subplot(211)\n\nx = np.linspace(0, 20, 200)\nfor m in range(5):\n    y = jv(m, x)\n    ax1.plot(x, y, label=f'$J_{m}(x)$')\n\nax1.axis((0, 20, -0.6, 1))\n\nax1.set_ylabel('$J_m(x)$')\nax1.legend()\n\nax2 = fig.add_subplot(212)\n\nfor m in range(5):\n    y = yv(m, x)\n    ax2.plot(x, y, label=f'$Y_{m}(x)$')\n\nax2.axis((0, 20, -3, 1))\n\nax2.set_xlabel('x')\nax2.set_ylabel('$Y_m(x)$')\nax2.legend()\n\nplt.show()\n\n\n\n\n\n\n\nFigure 5.2: Bessel functions\n\n\n\n\n\n\nDifferential Recurrence Relation\n\\[\n\\begin{aligned}\n   x&J_\\nu'(x)= {\\scriptsize \\sum_{n=0}^\\infty \\frac{(-1)^n(2n +\\nu)}{n! \\, \\Gamma(1 +\\nu +n)}\n   \\left( \\frac{x}{2}\\right )^{2n +\\nu}}\\\\\n   &= {\\scriptsize\\nu \\sum_{n=0}^\\infty \\frac{(-1)^n}{n! \\, \\Gamma(1 +\\nu +n)} \\left( \\frac{x}{2}\\right )^{2n +\\nu}\n   +x\\sum_{n=1}^\\infty \\frac{(-1)^n}{(n -1)! \\, \\Gamma(1 +\\nu +n)} \\left( \\frac{x}{2}\\right )^{2n +\\nu -1} }\\\\\n   &= {\\scriptsize \\nu J_\\nu(x) -x\\sum_{k=0}^\\infty \\frac{(-1)^k}{k! \\, \\Gamma(2 +\\nu +k)}\n   \\left( \\frac{x}{2}\\right )^{2k +\\nu +1} }\\\\\n   &= {\\scriptsize \\nu J_\\nu(x) -x J_{\\nu+1}(x) }\\\\\n   &\\;\\big\\Downarrow\\;{\\scriptsize\\times\\, x^{-\\nu -1}} \\\\\n   \\color{blue}{\\frac{d}{dx}} & \\color{blue}{[x^{-\\nu}J_\\nu(x)]} \\color{blue}{=-x^{-\\nu} J_{\\nu+1}(x)}\n\\end{aligned}\\]\nand\n\\[  \n\\begin{aligned}\n   xJ_\\nu'(x)&= -\\nu J_\\nu(x) +x J_{\\nu -1}(x)\\\\\n   & \\;\\big\\Downarrow\\;{\\scriptsize\\times\\, x^{\\nu -1}} \\\\\n   \\color{blue}{\\frac{d}{dx} [x^{\\nu}J_\\nu(x)]}\n   & \\color{blue}{=x^{\\nu} J_{\\nu -1}(x)}\n\\end{aligned}\\]\n\n\\(~\\)\nExample \\(\\,\\) \\(J_0'(x)=-J_1(x)\\)\n\\(~\\)\n\n\n5.3.1.2 DEs Solvable in Terms of Bessel Functions\n\nParametric Bessel equation of order \\(\\nu\\)\n\\[\n\\begin{aligned}\n   x^2 y'' +x y' &+({\\color{red}{\\alpha^2}} x^2 -\\nu^2) y = 0 \\\\\n   &\\;\\Big\\Downarrow \\;t=\\alpha x, \\;\\alpha&gt;0 \\\\\n   t^2\\frac{d^2y}{dt^2} +t\\frac{dy}{dt} &+(t^2 -\\nu^2)y = 0 \\\\\n   &\\Downarrow \\\\\n   y = c_1 J_\\nu({\\color{red}{\\alpha}} x) &+c_2 Y_\\nu({\\color{red}{\\alpha}} x)\n\\end{aligned}\\]\nModified Bessel equation of order \\(\\nu\\)\n\\[\n\\begin{aligned}\n    x^2 y'' +x y' &{\\color{red}{-}}(x^2 +\\nu^2) y = 0 \\\\\n    &\\;\\Big\\Downarrow \\;t=ix \\\\\n    t^2\\frac{d^2y}{dt^2} +t\\frac{dy}{dt} &+(t^2 -\\nu^2)y = 0 \\\\\n    &\\;\\Bigg\\Downarrow \\;{\\scriptsize I_\\nu(x)=i^{-\\nu} J_\\nu(ix),\n    \\;K_\\nu(x)=\\frac{\\pi}{2}\\frac{I_{-\\nu}(x) -I_{\\nu}(x)}{\\sin\\nu\\pi}, \\;K_m(x)=\\lim_{\\nu\\to m} K_\\nu(x) } \\\\\n    y = c_1 {\\color{red}{I_\\nu(x)}}\n      &+c_2 {\\color{red}{K_\\nu(x)}}\n\\end{aligned}\\]\nYet another equation\n\\[\n\\begin{aligned}\n     { y'' +\\frac{1 -2a}{x} y' }&{ +\\left( b^2 c^2 x^{2c -2} +\\frac{a^2 -p^2 c^2}{x^2}\\right)y=0, \\;p \\geq 0 } \\\\\n     &\\Bigg\\Downarrow \\;{\\scriptsize z=bx^c, \\;y(x)=\\left( \\frac{z}{b} \\right )^{a/c} w(z) }\\\\\n     y=x^a &\\left[c_1 J_p(bx^c) +c_2 Y_p(bx^c)\\right]\n\\end{aligned}\\]\nThe aging spring\n\\[\n\\begin{aligned}\n     m\\ddot{x} &+ke^{-\\alpha t}x = 0, \\;\\alpha &gt; 0 \\\\\n     &\\,\\Bigg\\Downarrow \\;{\\scriptsize s = \\frac{2}{\\alpha} \\sqrt{\\frac{k}{m}} e^{-\\alpha t/2} }\\\\\n     s^2 \\frac{d^2 x}{ds^2} &+s\\frac{dx}{ds} +s^2 x = 0\n\\end{aligned}\\]\nSpherical Bessel Functions:\nWhen solving the Helmholtz equation in spherical coordinates by separation of variables, \\(~\\)the radial equation has the form\n\\[x^2 y'' +2xy' +\\left(x^2 -n(n+1)\\right)y=0\\]\nThe two linearly independent solutions to this equation are called the spherical Bessel functions \\(j_n\\) and \\(y_n\\)\n\\[{\\scriptsize j_n(x)=\\color{blue}{\\sqrt{\\frac{\\pi}{2x}}} J_{n +\\frac{1}{2}}(x) }\\]\n\\[{\\scriptsize y_n(x)=\\color{blue}{\\sqrt{\\frac{\\pi}{2x}}} Y_{n +\\frac{1}{2}}(x)=(-1)^{n +1}\\sqrt{\\frac{\\pi}{2x}} J_{-n -\\frac{1}{2}}(x) }\\]\nwhen \\(\\nu=n +\\frac{1}{2}\\) is half an odd integer, that is, \\(~\\pm\\frac{1}{2}\\), \\(\\pm\\frac{3}{2}\\), \\(\\pm\\frac{5}{2}\\), \\(\\cdots\\)\nLet’s consider the case when \\(~\\nu=\\frac{1}{2}\\),\n\\[\\scriptsize J_{1/2}(x)=\\sum_{n=0}^\\infty \\frac{(-1)^n}{n!\\, \\Gamma(1 +\\frac{1}{2} +n)} \\left( \\frac{x}{2} \\right)^{2n +1/2}\\]\nwhere\n\\[\\tiny\n\\begin{aligned}\n  \\Gamma\\left(\\frac{1}{2} \\right ) & = \\sqrt{\\pi}\\\\\n  \\Gamma\\left(\\frac{3}{2} \\right ) & = \\Gamma\\left(1 +\\frac{1}{2} \\right )\n    = \\frac{1}{2}\\Gamma\\left(\\frac{1}{2} \\right )=\\frac{1}{2} \\sqrt{\\pi}\\\\\n  \\Gamma\\left(\\frac{5}{2} \\right ) & = \\Gamma\\left(1 +\\frac{3}{2} \\right )\n    = \\frac{3}{2}\\Gamma\\left(\\frac{3}{2} \\right )=\\frac{3}{2^2} \\sqrt{\\pi}\n    = \\frac{3\\cdot2}{2^3} \\sqrt{\\pi} = \\frac{3!}{2^3} \\sqrt{\\pi}\\\\\n  \\Gamma\\left(\\frac{7}{2} \\right ) & = \\Gamma\\left(1 +\\frac{5}{2} \\right )\n    = \\frac{5}{2}\\Gamma\\left(\\frac{5}{2} \\right ) = \\frac{5\\cdot3}{2^3} \\sqrt{\\pi}\n    = \\frac{5\\cdot4\\cdot3\\cdot2}{2^3\\cdot4\\cdot2} \\sqrt{\\pi} = \\frac{5!}{2^5 2!} \\sqrt{\\pi}\\\\\n  &\\; \\vdots \\\\\n  \\Gamma\\left(1 +\\frac{1}{2} +n \\right ) &= \\frac{(2n +1)!}{2^{2n +1} n!} \\sqrt{\\pi}\n\\end{aligned}\\]\nHence\n\\[J_{1/2}(x)={\\tiny \\sum_{n=0}^\\infty \\frac{(-1)^n}{n!\\, \\frac{(2n +1)!}{2^{2n +1} n!} \\sqrt{\\pi}} \\left( \\frac{x}{2} \\right)^{2n +1/2} = \\sqrt{\\frac{2}{\\pi x}}\\sum_{n=0}^\\infty \\frac{(-1)^n}{(2n +1)!} x^{2n +1} } =\\sqrt{\\frac{2}{\\pi x}}\\sin x\\]\n\n\\(~\\)\nExample \\(\\,\\) Show that \\(\\displaystyle \\, J_{-1/2}(x)=\\sqrt{\\frac{2}{\\pi x}}\\cos x\\)\n\nfrom scipy.special import spherical_jn\n\nplt.style.use('ggplot')\n\nfig, ax = plt.subplots(figsize=(6, 4))\n\nx = np.linspace(np.finfo(np.float32).eps, 10, 200)\nfor n in range(5):\n    y = spherical_jn(n, x)\n    ax.plot(np.append(0, x), np.append(0, y),\n        label=f'$j_{n}(x)$')\n\nax.axis((0, 10, -0.4, 1.0))\n\nax.set_xlabel('$x$')\nax.set_ylabel('$j_n(x)$')\nax.legend()\n\nplt.show()\n\n\n\n\n\n\n\nFigure 5.3: Spherical Bessel Function\n\n\n\n\n\n\\(~\\)\nExample \\(\\,\\) Find the general solution of the given differential equation on \\((0,\\infty)\\)\n\n\\(x^2y''+xy'+(x^2-\\frac{1}{9})y=0\\)\n\\(xy''+y'+xy=0\\)\n\\(x^2y'' +xy'+(9x^2-4)y=0\\)\n\\(x^2y''+xy'-(16x^2+\\frac{4}{9})y=0\\)\n\n\\(~\\)\nExample \\(\\,\\) Find the general solution of the given differential equation on \\((0,\\infty)\\)\n\n\\(x^2y''+2xy'+\\alpha^2x^2y=0; \\;y=x^{-1/2}u(x)\\)\n\\(xy''+2y'+4y=0\\)\n\n\\(~\\)\nExample \\(\\,\\) Verify that \\(I_\\nu(x)=i^{-\\nu}J_\\nu(ix)~\\) is a real function\n\\(~\\)\nExample \\(\\,\\) Express the general solution of the given differential equation in terms of the modified Bessel functions\n\\[xy''+y'-7x^3y=0\\]\n\\(~\\)\nExample \\(\\,\\) First express the general solution of the given differential equation in terms of Bessel functions and then express the general solution in terms of elementary functions\n\\[x^2y''+4xy'+(x^2+2)y=0\\]\n\\(~\\)\nExample \\(\\,\\) Derive the following result\n\\[\\int_0^x rJ_0(r)\\,dr=xJ_1(x)\\]\n\\[J_0'(x)=J_{-1}(x)=-J_1(x)\\]\n\\(~\\)\nExample \\(\\,\\) Use the solution of the aging spring equation \\(mx''+ke^{-at}x=0\\) to discuss the behavior of \\(x(t)\\) as \\(t \\to\\infty\\) in the three cases\n\n\\(c_1\\neq 0, \\;c_2=0\\)\n\\(c_1=0, \\;c_2\\neq 0\\)\n\\(c_1\\neq 0, \\;c_2 \\neq 0\\)\n\n\\(~\\)\n\n\n\n5.3.2 Legendre Polynomials\n\\(~\\)\n\nSince \\(x=0\\) is an ordinary point of Legendre’s equation, \\(~\\)we substitute the series \\(y=\\sum_{j=0}^\\infty c_j x^j~\\) to get\n\\[\\scriptsize\n\\begin{aligned}\n{\\normalsize (1 -x^2)} & {\\normalsize y'' -2xy' +n(n +1)y = }\\\\[8pt]\n[n(n+1)c_0 +2c_2] &+[(n -1)(n +2)c_1 +6c_3]x \\\\\n    +\\sum_{j=2}^\\infty & [(j +2)(j +1) c_{j +2} +(n -j)(n +j +1)c_j] x^j = 0\\\\[3pt]\n&\\Downarrow \\\\\nc_2 \\, & { = -\\frac{n(n+1)}{2} c_0 }\\\\\nc_3 \\, & { = -\\frac{(n -1)(n +2)}{6} c_1 }\\\\\n&\\vdots \\\\\nc_{j +2} \\, & { =-\\frac{(n -j)(n +j +1)}{(j +2)(j +1)} c_j, \\;\\; j=2,3,4,\\cdots }\n\\end{aligned}\\]\nThe recurrence relation yields\n\\[ \\scriptsize\n\\begin{aligned}\nc_2 & = -\\frac{n(n+1)}{2!} c_0\\\\\nc_4 & = -\\frac{(n -2)(n +3)}{4\\cdot3}c_2=\\frac{(n -2)n(n+1)(n +3)}{4!} c_0\\\\\nc_6 & = -\\frac{(n -4)(n +5)}{6\\cdot5}c_4=-\\frac{(n -4)(n -2)n(n+1)(n +3)(n +5)}{6!} c_0\\\\\n& \\,\\vdots \\\\\nc_3 & = -\\frac{(n -1)(n+2)}{3!} c_1\\\\\nc_5 & = -\\frac{(n -3)(n +4)}{5\\cdot4}c_3=\\frac{(n -3)(n -1)(n+2)(n +4)}{5!} c_1\\\\\nc_7 & = -\\frac{(n -5)(n +6)}{7\\cdot6}c_5=-\\frac{(n -5)(n -3)(n -1)(n+2)(n +4)(n +6)}{7!} c_1\\\\\n& \\,\\vdots\n\\end{aligned}\\]\nThus for at least \\(|x|&lt;1\\), \\(\\,\\)we obtain two linearly independent power series solutions:\n\\[\\tiny\n\\begin{aligned}\ny_1(x) &= c_0 \\left[1 -\\frac{n(n+1)}{2!} x^2\n   +\\frac{(n -2)n(n+1)(n +3)}{4!} x^4\n      -\\frac{(n -4)(n -2)n(n+1)(n +3)(n +5)}{6!} x^6 \\cdots \\right] \\\\\ny_2(x) &= c_1 \\left[x - \\frac{(n -1)(n+2)}{3!} x^3 +\\right. \\left. \\frac{(n -3)(n -1)(n+2)(n +4)}{5!} x^5 \\right. \\left. -\\frac{(n -5)(n -3)(n -1)(n+2)(n +4)(n +6)}{7!} x^7 \\cdots \\right]\n\\end{aligned}\\]\n\nIf \\(n\\) is an even integer, \\(~\\)the series \\(y_1(x)\\) reduces to a polynomial of degree \\(n\\) with only even powers of \\(x\\) and the series \\(y_2(x)\\) diverges\nIf \\(n\\) is an odd integer, \\(~\\)the series \\(y_2(x)\\) reduces to a polynomial of degree \\(n\\) with only odd powers of \\(x\\) and the series \\(y_1(x)\\) diverges\n\nThe general solution for an integer \\(n\\) is then given by the polynomials\nFor example, if \\(n=4\\), then\n\\[\\scriptsize\ny_1(x) = c_0 \\left[1 -\\frac{4\\cdot5}{2!} x^2 +\\frac{2\\cdot4\\cdot5\\cdot7}{4!} x^4 \\right]\n= c_0 \\left[1 -10x^2 +\\frac{35}{3} x^4 \\right]\\]\nIt is traditional to choose specific values for \\(c_0\\) or \\(c_1\\), depending on whether \\(n\\) is an even or odd positive integer, respectively\n\nFor \\(n=0\\), \\(~c_0=1\\), and for \\(n=2,4,6,\\cdots\\),\n\\(\\displaystyle\\scriptsize c_0=(-1)^{n/2} \\frac{1\\cdot 3\\cdots (n -1)}{2 \\cdot 4 \\cdots n}\\)\nFor \\(n=1\\), \\(~c_1=1\\), and for \\(n=3,5,7,\\cdots\\),\n\\(\\displaystyle\\scriptsize c_1=(-1)^{(n -1)/2} \\frac{1\\cdot 3\\cdots n}{2 \\cdot 4 \\cdots (n -1)}\\)\n\nFor example, \\(\\,\\) when \\(n=4\\), \\(~\\)we have\n\\[\\scriptsize y_1(x) = (-1)^{4/2} \\frac{1\\cdot3}{2\\cdot4} \\left[1 -10x^2 +\\frac{35}{3} x^4 \\right]\n=\\frac{1}{8} (35x^4 -30x^2 +3)\\]\nLegendre Polynomials\nThere specific \\(n\\)-th degree polynomials are called Legendre polynomials and denoted by \\(P_n(x)\\)\n\\[\\scriptsize\n\\begin{aligned}\n   P_0(x) & = 1\\\\\n   P_1(x) & = x\\\\\n   P_2(x) & = \\frac{1}{2}(3x^2 -1)\\\\\n   P_3(x) & = \\frac{1}{2}(5x^3-3x)\\\\\n   P_4(x) & = \\frac{1}{8}(35x^4 -30x^2 +3)\\\\\n   P_5(x) & = \\frac{1}{8}(63x^5 -70x^3 +15x)\n\\end{aligned}\\]\n\n\nfrom scipy.special import eval_legendre\n\nfig, ax = plt.subplots(figsize=(5, 5))\n\nx = np.linspace(-1, 1, 100)\nfor n in range(6):\n    y = eval_legendre(n, x)\n    ax.plot(x, y, label=f'$P_{n}(x)$')\n\nax.axis((-1.1, 1.1, -1.1, 1.1))\nax.set_xlabel('x')\nax.set_ylabel('$P_n(x)$')\nax.legend(loc='lower left', bbox_to_anchor=(1, 0))\n\nplt.show()\n\n\n\n\n\n\n\nFigure 5.4: Legendre polynomials\n\n\n\n\n\n\nProperties of Legendre Polynomials\n\n\\(P_n(-x)=(-1)^n P_n(x)\\)\n\\(P_n(1)=1\\)\n\\(P_n(-1)=(-1)^n\\)\n\\(P_n(0)=0\\), \\(~n\\) odd\n\\(P_n'(0)=0\\), \\(~n\\) even\n\nRodrigues’ Formula\nThe Legendre polynomials can also be represented using Rodrigues’ Formula,\n\\[P_n(x)=\\frac{1}{2^n n!} \\frac{d^n}{dx^n} \\left(x^2 -1\\right)^n \\tag{RF}\\label{eq:RF}\\]\nThis can be demonstrated through the following observations\n\nThe right hand side of \\(\\eqref{eq:RF}\\) is an \\(n\\)-th order polynomial\nTreating \\(\\left(x^2 -1\\right)^n = (x -1)^n (x + 1)^n\\) as a product and using Leibnitz’ rule to differentiate \\(n\\) times, \\(~\\)we have\n\\[\\frac{d^n}{dx^n} (x -1)^n (x +1)^n = n!(x+1)^n + {\\scriptsize\\text{terms with } (x -1) \\text{ as a factor}}\\]\nso that \\(\\displaystyle ~P_n(1) = \\frac{n! 2^n}{2^n n!} =1\\)\nIf \\(~h(x) = \\left(1 -x^2\\right)^n\\), \\(~\\)then \\(h'(x) = -2nx(1 -x^2)^{n-1}\\), \\(~\\)so that\n\\[\\left(1 -x^2\\right)h' + 2nx\\,h = 0\\]\nNow differentiate \\(n +1\\) times, using Leibnitz, to get\n\\[\\scriptsize\n\\begin{aligned}\n   \\left(1 -x^2\\right)h^{n +2}\n    & -2(n +1)x h^{n +1} -2\\frac{(n + 1)n}{2} h^n + 2nxh^{n +1} + 2n(n + 1)h^n = 0 \\\\\n    &\\Downarrow\\\\\n   \\left(1 -x^2\\right)h^{n +2}\n    & -2x h^{n +1} +n(n +1) h^n = 0\n\\end{aligned}\\]\nAs the equation is linear and \\(P_n(x) \\propto h^n(x)\\), \\(~P_n(x)\\) satisfies the Legendre equation of order \\(n\\)\n\\[\\left(1 -x^2\\right) P_n''(x) -2x P_n'(x) +n(n +1) P_n(x) = 0\\]\n\nIntegral Relations\n\\[\\int_{-1}^1 P_k(x) P_l(x) \\,dx = \\delta_{k,l} \\frac{2}{2l +1},\\;\\;k \\leq l\\]\n\\[\\int_{-1}^1 x P_s(x) P_r(x) \\,dx = \\delta_{s +1,r} \\frac{2r}{(2r -1)(2r +1)}, \\; s \\leq r \\]\nRecurrence Relation\nThe Legendre polynomials satisfy the following recurrence relation\n\\[(n + 1) P_{n +1}(x) = (2n + 1) x P_n(x) -n P_{n -1}(x)\\]\nConsider the polynomial \\(x P_n(x)\\). It has degree \\(n +1\\) and is thus in the linear span of \\(P_0, \\cdots, P_{n +1}\\). We can hence write \\(x P_n(x)\\) as a linear combination of the first \\(~n +2\\) Legendre polynomials:\n\\[\nxP_n(x) = c_0 P_0(x) +c_1 P_1(x) +\\cdots +c_{n +1}P_{n +1}(x)\n\\]\n\nThus\n\n\\[\\scriptsize\n\\begin{aligned}\n    \\int_{-1}^1 x P_n(x) P_k(x)\\,dx & = c_k \\int_{-1}^1 P_k^2(x)\\, dx \\\\\n    \\rightarrow  c_k\n    & =\\frac{2k +1}{2} \\int_{-1}^1 x P_n(x) P_k(x)\\,dx\n\\end{aligned}\\]\nThese integrals vanish unless \\(k = n \\pm 1\\) and for this case, we can use\n\\[ \\scriptsize\n   c_{n -1}=\\frac{2n -1}{2} \\int_{-1}^1 x P_n(x) P_{n -1}(x)\\,dx=\\frac{n}{2n +1}\n\\]\n\\[ \\scriptsize\n   c_{n +1}=\\frac{2n +3}{2} \\int_{-1}^1 x P_n(x) P_{n +1}(x)\\,dx=\\frac{n +1}{2n +1}\n\\]\n\nHence\n\\[\n  \\begin{aligned}\n    xP_n(x) &= c_{n -1} P_{n -1}(x) +c_{n +1}P_{n +1}(x)\\\\\n    &= \\frac{n}{2n +1}P_{n -1}(x)+\\frac{n +1}{2n +1}P_{n +1}(x)\n\\end{aligned}\\]\nThis is what we wanted to prove\n\n\n\\(~\\)\nExample \\(\\,\\) Use the recurrence relation and \\(P_0(x)=1\\), \\(P_1(x)=x\\), to generate the next six Legendre polynomials\n\\(~\\)\nExample \\(\\,\\) Show that the differential equation\n\\[\\sin\\theta \\frac{d^2y}{d\\theta^2}+\\cos\\theta \\frac{dy}{d\\theta}+n(n+1)\\,\\sin\\theta\\,y=0\\]\ncan be transformed into Legendre’s equation by means of the substitution \\(x=\\cos\\theta\\)\n\\(~\\)\nExample \\(\\,\\) Find the first three positive values of \\(~\\lambda~\\) for which the problem\n\\[(1-x^2)y''-2xy'+\\lambda y=0\\]\n\\[y(0)=0, \\;y(x),\\;y'(x)\\;\\text{ bounded on } [-1, 1]\\]\nhas nontrivial solutions\n\\(~\\)\n\nThe differential equation\n\\[(1-x^2)y''-2xy'+\\left[ n(n+1) -\\frac{m^2}{1-x^2} \\right]y=0\\]\nis known as the associated Legendre equation. When \\(~m=0\\), this equation reduces to Legendre’s equation. \\(~\\)A solution of the associated equation is\n\\[P_n^m(x)=(1-x^2)^{m/2} \\frac{d^m}{dx^m} P_n(x)\\]\nwhere \\(P_n(x), \\;n=0, 1,2, \\cdots\\) are the Legendre polynomials. The solutions \\(P_n^m(x)\\) for \\(m=0,1,2,\\cdots,\\) are called associated Legendre functions\n\n\\(~\\)\nExample \\(\\,\\) Answer the following questions\n\nFind the associated Legendre functions \\(P_0^0(x)\\), \\(P_1^0(x)\\), \\(P_1^1(x)\\), \\(P_2^1(x)\\), \\(P_2^2(x)\\), \\(P_3^1(x)\\), \\(P_3^2(x)\\), and \\(P_3^3(x)\\)\n\\(\\,\\) What can you say about \\(P_n^m(x)\\) when \\(m\\) is an even non-negative integer?\n\\(\\,\\) What can you say about \\(P_n^m(x)\\) when \\(m\\) is a non-negative integer and \\(m&gt;n\\)?\n\\(\\,\\) Verify that \\(y=P_1^1(x)\\) satisfies the associated Legendre equation when \\(n=1\\) and \\(m=1\\)",
    "crumbs": [
      "**First Semester**",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Series Solutions of Linear Differential Equations</span>"
    ]
  },
  {
    "objectID": "ch_05_Series_Solutions_of_Linear_Differential_Equations.html#worked-exercises",
    "href": "ch_05_Series_Solutions_of_Linear_Differential_Equations.html#worked-exercises",
    "title": "5  Series Solutions of Linear Differential Equations",
    "section": "Worked Exercises",
    "text": "Worked Exercises\n1. \\(\\phantom{1}\\) The differential equation\n\\[ xy'' +(1-x) y' +\\alpha y=0\\]\n\\(~\\) is known as Laguerre’s equation\n(a)\\(~\\) Find two linearly independent solutions for \\(0 \\le x &lt;\\infty\\)\n(b)\\(~\\) Show that there is a polynomial solution of degree \\(n\\), in case \\(\\alpha=n\\) is a non-negative integer\n(c)\\(~\\) These solutions are naturally called Laguerre polynomials and are denoted by \\(L_n(x)\\). Rodrigues’ formula for the Laguerre polynomials is\n\\[ L_n(x) = \\frac{e^x}{n!} \\frac{d^n}{dx^n} x^n e^{-x}\\]\nUse this formula to find the Laguerre polynomials corresponding to \\(n=0,1,2,3,4\\)\n\\(~\\)\nSolution (a)\nLet’s apply the Frobenius method to solve the differential equation near the regular singular point at \\(x = 0\\)\nStep 1: Frobenius Series Assumption\nAssume a solution of the form:\n\\[ \\color{blue}{y(x) = x^r \\sum_{k=0}^{\\infty} a_k x^k = \\sum_{k=0}^{\\infty} a_k x^{k + r}, \\quad a_0 \\ne 0}\\]\nThen,\n\\[ y'(x) = \\sum_{k=0}^{\\infty} a_k (k + r) x^{k + r - 1}\\]\n\\[\ny''(x) = \\sum_{k=0}^{\\infty} a_k (k + r)(k + r - 1) x^{k + r - 2}\\]\nStep 2: Substitute into the Equation\nSubstitute into the original equation:\n\\[\\scriptsize\\begin{aligned}\n& x y'' + (1 - x) y' + n y \\\\\n&= \\sum_{k=0}^{\\infty} a_k (k + r)(k + r - 1) x^{k + r - 1}\n+\\sum_{k=0}^{\\infty} a_k (k + r) x^{k + r - 1}\n-\\sum_{k=0}^{\\infty} a_k (k + r) x^{k + r}\n+\\sum_{k=0}^{\\infty} \\alpha a_k x^{k + r} = 0\n\\end{aligned}\\]\nGroup and simplify:\n\\[\n\\sum_{k=0}^{\\infty} a_k (k + r)^2 x^{k + r - 1}\n+\\sum_{k=0}^{\\infty} a_k (\\alpha - k - r) x^{k + r} = 0\n\\]\nStep 3: Indicial Equation\nThe lowest power of \\(x\\) is \\(x^{r - 1}\\). Its coefficient is:\n\\[\na_0 r^2\\]\nSet this to zero:\n\\[\n\\color{blue}{r^2 = 0 \\quad \\Rightarrow \\quad r = 0}\\]\nStep 4: Recurrence Relation\nNow use \\(r = 0\\), so:\n\\[ \\sum_{k=0}^{\\infty} \\left[ a_{k+1} (k + 1)^2 + a_k (\\alpha - k) \\right] x^k = 0\\]\nSo the recurrence relation is:\n\\[ \\color{blue}{a_{k+1} = - \\frac{(\\alpha - k)}{(k + 1)^2} a_k}\\]\nChoose \\(a_0 = 1\\) (arbitrary constant), this gives a general form:\n\\[\\color{blue}{a_k = \\frac{(-1)^k \\alpha(\\alpha - 1) \\cdots (\\alpha - k + 1)}{(k!)^2}}\n\\]\nThus the first solution is\n\\[ \\color{blue}{y_1(x) = \\sum_{k=0}^{\\infty} a_k x^k}\\]\nWhen the indicial equation has a repeated root \\(r=0\\), the second solution has the form:\n\\[ \\color{blue}{y_2(x) = y_1(x) \\ln x + \\sum_{k=0}^{\\infty} b_k x^k}\\]\nWe plug this into the original equation. This leads to\n\\[ 2y_1' -y_1 +\\sum_{k=0}^\\infty \\left[ (k+1)^2 b_{k+1} +(\\alpha -k) b_k \\right] x^k = 0\\]\nSolution (b)\nWhen \\(\\alpha = n\\),\n\\[\na_k = \\frac{(-1)^k n!}{(n - k)! (k!)^2}\n\\quad \\text{for } k \\le n\\]\nFor \\(k &gt; n\\), we get \\(a_k = 0\\), so the series terminates. The series becomes a polynomial of degree \\(n\\):\n\\[\\color{blue}{y_1(x) = \\sum_{k=0}^{n} \\frac{(-1)^k n!}{(n - k)! (k!)^2} x^k = L_n(x)}\\]\nThe second linearly independent solution is of the form:\n\\[ y_2(x) = L_n(x) \\ln x + \\sum_{k=0}^{\\infty} d_k x^k\\]\nSolution (c)\nTo find the Laguerre polynomials \\(L_n(x)\\) for \\(n = 0, 1, 2, 3, 4\\), we will use Rodrigues’ formula.\n\\[\\color{blue}{L_0(x) = \\frac{e^x}{0!} \\frac{d^0}{dx^0} \\left( x^0 e^{-x} \\right) = e^x \\cdot e^{-x} = 1}\\]\n\\[ \\color{blue}{L_1(x) = \\frac{e^x}{1!} \\frac{d}{dx} \\left( x e^{-x} \\right) = e^x \\cdot (1 - x) e^{-x} = 1 - x}\\]\n\\[ \\color{blue}{L_2(x) = \\frac{e^x}{2!} \\frac{d^2}{dx^2} \\left( x^2 e^{-x} \\right) = 1 - 2x + \\frac{1}{2}x^2}\\]\n\\[\\color{blue}{L_3(x) = 1 - 3x + \\frac{3}{2}x^2 - \\frac{1}{6}x^3}\\]\n\\[ \\color{blue}{ L_4(x) = 1 - 4x + 3x^2 - \\frac{2}{3}x^3 + \\frac{1}{24}x^4}\\]\n\\(~\\)\n2. \\(\\phantom{1}\\) The differential equation\n\\[ (1 -x^2) y'' -xy' +\\alpha^2 y=0\\]\n\\(~\\) where \\(\\alpha\\) is a parameter, is known as Chebyshev’s equation\n(a)\\(~\\) Find two power series solutions centered at the ordinary point \\(0\\)\n(b)\\(~\\) When \\(\\alpha=n\\) is a nonnegative integer, show that Chebyshev’s differential equation always possesses a polynomial solution of degree \\(n\\)\n(c)\\(~\\) These solutions are naturally called Chebyshev polynomials and are denoted by \\(T_n(x)\\). Rodrigues’ formula for the Chebyshev polynomials is\n\\[T_n(x) = (-1)^n \\frac{2^n n!}{(2n)!} \\sqrt{1-x^2}\\frac{d^n}{dx^n} (1-x^2)^{n-1/2}\\]\nUse this formula to find the Chebyshev polynomials corresponding to \\(n=0,1,2,3\\)\n\\(~\\)\nSolution (a)\nWe are to find two linearly independent power series solutions centered at \\(x = 0\\), which is an ordinary point (i.e., the coefficients of \\(y''\\) and \\(y'\\) are analytic at \\(x = 0\\))\nStep 1: Assume a Power Series Solution\nLet’s assume a solution of the form:\n\\[\\color{blue}{y(x) = \\sum_{k=0}^{\\infty} a_k x^k}\\]\nThen compute the derivatives:\n\\[y'(x) = \\sum_{k=1}^{\\infty} k a_k x^{k-1}, \\quad\ny''(x) = \\sum_{k=2}^{\\infty} k(k-1) a_k x^{k-2}\\]\nNow substitute \\(y, y', y''\\) into the differential equation:\n\\[\\scriptsize\\sum_{k=2}^{\\infty} k(k-1) a_k x^{k-2}\n    -\\sum_{k=2}^{\\infty} k(k-1) a_k x^k\n    -\\sum_{k=1}^{\\infty} k a_k x^k\n    +\\sum_{k=0}^{\\infty} \\alpha^2 a_k x^k = 0\\]\nShift the first term to match powers of \\(x^k\\) and combine all terms and simplify the coefficients:\n\\[\\color{blue}{\\sum_{k=0}^{\\infty} \\left[\n(k+2)(k+1) a_{k+2}\n    +(\\alpha^2 - k^2) a_k\n\\right] x^k = 0}\\]\nSet each coefficient of \\(x^k\\) to \\(0\\), we get the recurrence relation:\n\\[\n\\color{blue}{a_{k+2} = -\\frac{\\alpha^2 - k^2}{(k+2)(k+1)} a_k}\\]\nStep 2: Build Two Linearly Independent Solutions\nWe choose \\(a_0\\) and \\(a_1\\) arbitrarily to construct two independent solutions\n\nFirst solution: even powers only (let \\(a_0 = 1\\), \\(a_1 = 0\\))\nThen the series becomes:\n\\[y_1(x) = a_0 + a_2 x^2 + a_4 x^4 + \\cdots\\]\nUse recurrence:\n\\[a_2 = -\\frac{\\alpha^2 - 0^2}{2 \\cdot 1} a_0 = -\\frac{\\alpha^2}{2}\\]\n\\[a_4 = -\\frac{\\alpha^2 - 2^2}{4 \\cdot 3} a_2 = -\\frac{\\alpha^2 - 4}{12} a_2 = \\frac{(\\alpha^2 - 4)(\\alpha^2)}{24}\\]\nAnd so on\nSecond solution: odd powers only (let \\(a_0 = 0\\), \\(a_1 = 1\\))\nThen the series becomes:\n\\[y_2(x) = a_1 x + a_3 x^3 + a_5 x^5 + \\cdots\\]\nUse recurrence:\n\\[a_3 = -\\frac{\\alpha^2 - 1^2}{3 \\cdot 2} a_1 = -\\frac{\\alpha^2 - 1}{6}\\]\n\\[a_5 = -\\frac{\\alpha^2 - 3^2}{5 \\cdot 4} a_3 = \\frac{(\\alpha^2 - 9)(\\alpha^2 - 1)}{120}\\]\nAnd so on\n\nTwo linearly independent power series solutions centered at \\(x = 0\\) are:\n\\[\n\\color{blue}{y_1(x) = a_0 \\left[ 1 - \\frac{\\alpha^2}{2} x^2 + \\frac{(\\alpha^2 - 4)\\alpha^2}{24} x^4 - \\cdots \\right]}\n\\]\n\\[\n\\color{blue}{y_2(x) = a_1 \\left[ x - \\frac{\\alpha^2 - 1}{6} x^3 + \\frac{(\\alpha^2 - 1)(\\alpha^2 - 9)}{120} x^5 - \\cdots \\right]}\n\\]\nIn general,\n\\[ \\begin{aligned}\na_{2k} &= (-1)^k \\frac{\\left(\\alpha^2 - (2k-2)^2\\right) \\left(\\alpha^2 - (2k-4)^2\\right) \\cdots \\left(\\alpha^2 - 2^2\\right) \\alpha^2}{(2k)!} a_0 = \\hat{a}_{2k} a_0 \\\\\na_{2k+1} &= (-1)^k \\frac{\\left(\\alpha^2 - (2k-1)^2\\right) \\left(\\alpha^2 - (2k-3)^2\\right) \\cdots \\left(\\alpha^2 - 1\\right)}{(2k+1)!} a_1 =\\hat{a}_{2k+1} a_1 \\\\\n&\\Downarrow \\\\\ny &= a_0 \\sum_{k=1}^\\infty \\hat{a}_{2k} x^{2k} +a_1 \\sum_{k=1}^\\infty \\hat{a}_{2k+1} x^{2k+1} = a_0 y_0(x) + a_1 y_1(x)\n\\end{aligned}\\]\nSolution (b)\nLet \\(\\alpha = n\\), a nonnegative integer. Then the recurrence becomes:\n\\[ a_{k+2} = -\\frac{n^2 - k^2}{(k+2)(k+1)} a_k\\]\nObserve that when \\(\\color{blue}{k = n}\\), we get:\n\\[ \\color{blue}{a_{n+2} = -\\frac{n^2 - n^2}{(n+2)(n+1)} a_n = 0}\\]\nIf we start with \\(a_0 \\ne 0\\), and define the even-power solution:\n\\[\\color{red}{y(x) = a_0 + a_2 x^2 + a_4 x^4 + \\cdots}\\]\nthen for even \\(n\\), the recurrence will produce:\n\\[\\color{red}{a_{n+2} = 0 \\Rightarrow \\text{ the series terminates at } x^n}\\]\nSo, the solution is a polynomial of degree \\(n\\)\nSimilarly, if \\(a_1 \\ne 0\\), the odd-power series will terminate at \\(x^n\\) when \\(n\\) is odd. Thus, we get a polynomial solution of degree exactly \\(n\\)\nThese solutions are known as the Chebyshev polynomials of the first kind, usually denoted \\(T_n(x).\\) However the recurrence continues — unless we start with initial coefficients \\(a_0\\), \\(a_1\\) that make the series terminate\nSolution (c)\nLet’s define:\n\\[ f_n(x) = (1 - x^2)^{n - \\frac{1}{2}}, \\quad \\text{then} \\quad T_n(x) = (-1)^n \\frac{2^n n!}{(2n)!} \\sqrt{1 - x^2} \\cdot \\frac{d^n}{dx^n} f_n(x)\\]\n\n\\(n = 0\\)\nWe have: \\(\\displaystyle f_0(x) = (1 - x^2)^{-1/2}, \\quad \\text{but} \\quad \\frac{d^0}{dx^0} f_0 = f_0\\)\n\\(\\color{red}{T_0(x) = \\frac{1}{1} \\cdot \\sqrt{1 - x^2} \\cdot (1 - x^2)^{-1/2} = 1}\\)\n\\(n = 1\\)\n\\(f_1(x) = (1 - x^2)^{1 - \\frac{1}{2}} = (1 - x^2)^{1/2}\\)\nDifferentiate:\n\\(\\displaystyle \\frac{d}{dx} (1 - x^2)^{1/2} = \\frac{1}{2}(1 - x^2)^{-1/2} \\cdot (-2x) = -\\frac{x}{\\sqrt{1 - x^2}}\\)\nNow plug into the formula:\n\\(\\displaystyle \\color{red}{T_1(x)} = (-1)^1 \\cdot \\frac{2^1 \\cdot 1!}{2!} \\cdot \\sqrt{1 - x^2} \\cdot \\left( -\\frac{x}{\\sqrt{1 - x^2}} \\right) = x\\)\n\\(n = 2\\)\n\\(f_2(x) = (1 - x^2)^{3/2}\\)\nDifferentiate twice:\nFirst derivative:\n\\(f_2'(x) = \\frac{3}{2}(1 - x^2)^{1/2} \\cdot (-2x) = -3x (1 - x^2)^{1/2}\\)\nSecond derivative:\n\\(f_2''(x) = \\frac{d}{dx} [-3x (1 - x^2)^{1/2}]\n  = -3(1 - x^2)^{1/2} + 3x^2 (1 - x^2)^{-1/2}\\)\nNow plug into Rodrigues’ formula:\n\\(\\color{red}{\n    \\begin{aligned}\n    T_2(x) &= \\frac{2^2 \\cdot 2!}{(4)!} \\cdot \\sqrt{1 - x^2} \\cdot f_2''(x) \\\\\n   &= {\\scriptsize\\frac{4 \\cdot 2}{24} \\cdot \\sqrt{1 - x^2} \\cdot \\left[ -3(1 - x^2)^{1/2} + 3x^2(1 - x^2)^{-1/2} \\right]} \\\\\n   &= 2x^2 - 1\n   \\end{aligned}}\\)\n\\(n = 3\\)\n\\(f_3(x) = (1 - x^2)^{5/2}\\)\nWe’ll compute the 3rd derivative and plug it into the formula:\n\\(\\color{red}{\n  \\begin{aligned}\n    T_3(x) &= (-1)^3 \\cdot \\frac{2^3 \\cdot 3!}{(6)!} \\cdot \\sqrt{1 - x^2} \\cdot f_3’’’(x)\\\\\n    &= {\\scriptsize-\\frac{8 \\cdot 6}{720} \\cdot \\sqrt{1 - x^2} \\cdot \\left[ 45x(1 - x^2)^{1/2} - 15x^3 (1 - x^2)^{-1/2} \\right]}\\\\\n    &= 4x^3 - 3x\n  \\end{aligned}}\\)\n\\(~\\)\n\n3. \\(~\\) If \\(n\\) is an integer, you can use the substitution \\(R(x)=(\\alpha x)^{-1/2} \\, Z(x)\\) to solve that the differential equation\n\\[ x^2 \\frac{d^2R}{dx^2} +2x \\frac{dR}{dx} +\\left[ \\alpha^2 x^2 - n(n+1) \\right] R = 0 \\]\nFind the general solution of the differential equation on the interval \\((0, \\infty)\\)\nSolution\nStep 1: \\(~\\) Apply the substitution\nLet:\n\\[R(x) = (\\alpha x)^{-1/2} Z(x) = \\frac{1}{\\sqrt{\\alpha x}} Z(x)\\]\nWe compute derivatives of \\(R(x)\\):\n\nFirst derivative:\nUsing the product rule:\n\\[\\frac{dR}{dx} = \\frac{d}{dx} \\left( \\frac{1}{\\sqrt{\\alpha x}} Z(x) \\right)\n= \\frac{d}{dx} \\left( \\alpha^{-1/2} x^{-1/2} Z(x) \\right)\\]\nNow differentiate:\n\\[\\frac{dR}{dx} = \\alpha^{-1/2} \\left( -\\frac{1}{2} x^{-3/2} Z(x) + x^{-1/2} Z’(x) \\right)\\]\nSecond derivative:\nDifferentiate again:\n\\[\\frac{d^2R}{dx^2} = \\alpha^{-1/2}\n\\left[ \\frac{3}{4} x^{-5/2} Z(x) - x^{-3/2} Z’(x) + x^{-1/2} Z’’(x) \\right]\\]\n\nStep 2: \\(~\\) Plug into the original equation\nNow substitute \\(R\\), \\(R'\\), \\(R''\\) into the equation:\n\\[x^2 R'' + 2x R' + \\left( \\alpha^2 x^2 - n(n+1) \\right) R = 0\\]\nThus the equation becomes:\n\\[\\alpha^{-1/2} \\left[ x^{3/2} Z'' + x^{1/2} Z' + \\left( \\alpha^2 x^{3/2} - \\left(n(n+1) + \\frac{1}{4} \\right) x^{-1/2} \\right) Z \\right] = 0\\]\nFactor out \\(x^{1/2}\\):\n\\[\\alpha^{-1/2} x^{1/2} \\left[\nx Z'' + Z' + \\left( \\alpha^2 x - \\frac{n(n+1) + \\tfrac{1}{4}}{x} \\right) Z\n\\right] = 0\\]\nSo the reduced equation is:\n\\[x Z'' + Z' + \\left( \\alpha^2 x - \\frac{n(n+1) + \\tfrac{1}{4}}{x} \\right) Z = 0\\]\nStep 3: \\(~\\) Change variable to standard form\nLet’s define:\n\\[u = \\alpha x\n\\;\\Rightarrow\\;\n\\frac{dZ}{dx} = \\alpha \\frac{dZ}{du}, \\quad \\frac{d^2Z}{dx^2} = \\alpha^2 \\frac{d^2Z}{du^2}\\]\nPlug into the reduced equation and simplify:\n\\[u Z'' + Z' + \\left( u - \\frac{n(n+1) + \\frac{1}{4}}{u} \\right) Z = 0\\]\nNow multiply through by \\(u\\):\n\\[u^2 Z'' + u Z' + \\left( u^2 - \\left(n(n+1) + \\tfrac{1}{4} \\right) \\right) Z = 0\\]\nStep 4: \\(~\\) Identify the equation\nThis is Bessel’s equation of order \\(\\nu\\), where:\n\\[\\nu^2 = n(n+1) + \\tfrac{1}{4}\n\\;\\Rightarrow\\;\n\\nu = \\sqrt{n(n+1) + \\tfrac{1}{4}} = n + \\tfrac{1}{2}\\]\n(because \\(n\\) is an integer, and this square root simplifies nicely)\nSo we now recognize:\n\\[u^2 Z'' + u Z' + (u^2 - (n + \\tfrac{1}{2})^2) Z = 0\\]\nFinal Answer\nThe general solution is:\n\\[R(x) = \\frac{1}{\\sqrt{\\alpha x}} \\left[ A J_{n + 1/2} (\\alpha x) + B Y_{n + 1/2} (\\alpha x) \\right]\\]\n\\(~\\)\n4. \\(~\\) Use the indicated change of variable to find the general solution of the given differential equation on the interval \\((0, \\infty)\\)\n\\[ x^2 y'' +\\left( \\alpha^2 x^2 - \\nu^2 +\\frac{1}{4} \\right)y = 0, \\;\\;y= \\sqrt{x} \\,u(x)\\]\nSolution\nStep 1: \\(~\\) Compute derivatives of \\(y = \\sqrt{x} u(x)\\)\n\nFirst derivative:\n\n\\[y' = \\frac{1}{2} x^{-1/2} u(x) + x^{1/2} u'(x)\\]\n\nSecond derivative:\n\n\\[y'' = -\\frac{1}{4} x^{-3/2} u(x) + x^{-1/2} u'(x) + x^{1/2} u''(x)\\]\nStep 2: \\(~\\) Plug into the original equation\nNow the full equation becomes:\n\\[\\left( -\\frac{1}{4} x^{1/2} u + x^{3/2} u' + x^{5/2} u'' \\right)\n    +   \\left( \\alpha^2 x^2 - \\nu^2 + \\frac{1}{4} \\right) x^{1/2} u = 0\\]\nGroup like terms and factor out \\(x^{1/2}\\):\n\\[x^{1/2} \\left( x^2 u'' + x u' + (\\alpha^2 x^2 - \\nu^2) u \\right) = 0\\]\nSince \\(x^{1/2} &gt; 0\\) on \\((0, \\infty)\\), we divide both sides:\n\\[x^2 u'' + x u' + (\\alpha^2 x^2 - \\nu^2) u = 0\\]\nThis is a parameterized Bessel differential equation\nFinal Answer\nThe general solution is:\n\\[y(x) = \\sqrt{x} \\left[ A J_\\nu(\\alpha x) + B Y_\\nu(\\alpha x) \\right]\\]\n\\(~\\)\n5. \\(~\\) When \\(n=0\\), Legendre’s differential equation has the polynomial solution \\(y=P_0(x)=1\\). Use reduction of order to find a second Legendre function satisfying the DE on the interval \\((-1,1)\\)\nSolution\nStep 1: \\(~\\)Use reduction of order\nLet:\n\\[y_2(x) = v(x) y_1(x) = v(x) \\cdot 1 = v(x)\\]\nSo we’re seeking a second solution \\(y_2(x) = v(x)\\), where \\(y_1(x) = 1\\) is a known solution\nStep 2: \\(~\\) Plug into the equation\nStart from the reduced equation:\n\\[(1 - x^2) y'' - 2x y' = 0\\]\nSince \\(y = v(x)\\), we compute:\n\\[y' = v', \\quad y'' = v''\\]\nSo the equation becomes:\n\\[(1 - x^2) v'' - 2x v' = 0\\]\nThis is a first-order ODE in \\(v'\\) if we let \\(w = v'\\). Then:\n\\[(1 - x^2) w' - 2x w = 0\n\\quad \\text{(a linear first-order equation in \\( w \\))}\\]\nStep 3: \\(~\\) Solve for \\(w\\)\nWe rewrite the equation as:\n\\[w' - \\frac{2x}{1 - x^2} w = 0\\]\nThis is a linear first-order ODE. Use integrating factor:\n\\[\\mu(x) = \\exp\\left( -\\int \\frac{2x}{1 - x^2} dx \\right)\\]\nLet \\(u = 1 - x^2 \\Rightarrow du = -2x dx\\)\nThen:\n\\[\\int \\frac{2x}{1 - x^2} dx = -\\int \\frac{1}{u} du = -\\ln|1 - x^2|\\]\nSo:\n\\[\\mu(x) = e^{\\ln|1 - x^2|} = |1 - x^2|\\]\nbut on \\((-1, 1)\\), \\(1 - x^2 &gt; 0\\) so:,\n\\[\\mu(x) = 1 - x^2\\]$$\nNow solve:\n\\[\\frac{d}{dx}(w \\cdot \\mu(x)) = 0\n\\Rightarrow w(1 - x^2) = C\n\\Rightarrow w = \\frac{C}{1 - x^2}\\]\nRecall \\(w = v'\\), so:\n\\[v’ = \\frac{C}{1 - x^2}\n\\Rightarrow v(x) = C \\int \\frac{1}{1 - x^2} dx\\]\nSo:\n\\[v(x) = \\frac{C}{2} \\ln\\left| \\frac{1 + x}{1 - x} \\right| + D\\]\nStep 4: \\(~\\) General solution\nWe already had \\(y_1(x) = 1\\), and now we have:\n\\[y_2(x) = v(x) = \\frac{1}{2} \\ln\\left( \\frac{1 + x}{1 - x} \\right)\n\\quad \\text{(valid on } (-1,1) \\text{ where the log is real)}\\]\n\\(~\\)\n6. \\(~\\) When \\(n=1\\), Legendre’s differential equation has the polynomial solution \\(y=P_1(x)=x\\). Use reduction of order to find a second Legendre function satisfying the DE on the interval \\((-1,1)\\)\nSolution\nStep 1: \\(~\\) Use reduction of order\nLet:\n\\[y_2(x) = v(x) \\cdot y_1(x) = v(x) \\cdot x\\]\nPlug into the differential equation:\n\\[(1 - x^2)(v''x + 2v') - 2x(v'x + v) + 2(vx) = 0\\]\nExpand and simplify:\n\\[x(1 - x^2)v'' + (2 - 4x^2)v' = 0\\]\nStep 2: \\(~\\) Let \\(w = v'\\), then solve\nLet \\(w = v'\\), so \\(w' = v''\\). Then:\n\\[x(1 - x^2) w' + (2 - 4x^2) w = 0\\]\nWe rewrite as:\n\\[w' + \\frac{2 - 4x^2}{x(1 - x^2)} w = 0\\]\nStep 3: \\(~\\) Solve a first-order linear ODE\nIntegrating factor:\n\\[\\scriptsize\\mu(x) = \\exp\\left( \\int \\frac{2 - 4x^2}{x(1 - x^2)} dx \\right) = \\exp \\left( \\int \\left( \\frac{2}{x} - \\frac{1}{1 - x} + \\frac{1}{1 + x} \\right) dx \\right)\n= x^2 \\cdot \\frac{1}{1 - x} \\cdot (1 + x)\n\\]\nNow solve the equation:\n\\[\\frac{d}{dx} \\left( w \\cdot \\mu(x) \\right) = 0 \\Rightarrow w \\cdot \\mu(x) = C \\Rightarrow w = \\frac{C}{\\mu(x)} = C \\cdot \\frac{1 - x}{x^2(1 + x)}\\]\nNow integrate to find \\(v\\):\n\\[\\begin{aligned}\nv &= \\int w \\, dx = C \\int \\frac{1 - x}{x^2(1 + x)} dx\\\\\n  &= C \\int \\left( \\frac{1}{x^2} - \\frac{2}{x} + \\frac{2}{1 + x} \\right) dx \\\\\n  &= C \\left( -\\frac{1}{x} - 2\\ln|x| + 2\\ln|1 + x| \\right) + D\n\\end{aligned}\\]\nFinal Answer:\nThe second linearly independent solution for \\(n = 1\\) on \\((-1, 1)\\) is:\n\\[\ny_2(x) = -1 - 2x \\ln|x| + 2x \\ln|1 + x|\n\\quad \\text{(up to constant multiples)}\\]\n\\(~\\)\n7. \\(~\\) Find the general solution of the differential equation on the interval \\((0, \\infty)\\)\n\\[ x y'' +y=0 \\]\nHint) \\(~\\)Use the substitution \\(\\displaystyle y(x)=\\sqrt{x}u(x)\\), \\(~2\\sqrt{x}=z\\)\nSolution\nBy using \\(\\displaystyle y(x)=\\sqrt{x}u(x)~\\) and \\(~2\\sqrt{x}=z\\), the differential equation becomes\n\\[\\begin{aligned}\ny' &=\\frac{1}{2\\sqrt{x}}u +\\sqrt{x}u'\\\\\ny''&=-\\frac{1}{4x\\sqrt{x}} u +\\frac{1}{\\sqrt{x}} u' +\\sqrt{x}u''\\\\\n&\\Downarrow \\\\\nx\\sqrt{x} u'' &+\\sqrt{x} u' +\\left( \\sqrt{x} - \\frac{1}{4\\sqrt{x}} \\right )u = 0 \\\\\nu' &=\\frac{du}{dz}\\frac{dz}{dx}=\\frac{1}{\\sqrt{x}} \\frac{du}{dz} \\\\\nu'' &=\\frac{d}{dz}\\left( \\frac{du}{dx}\\right) \\frac{dz}{dx} =\\frac{1}{x}\\frac{d^2u}{dz^2}-\\frac{1}{2x\\sqrt{x}}\\frac{du}{dz}\\\\\n&\\Downarrow \\\\\nz^2 \\frac{d^2u}{dz^2} + z \\frac{du}{dz}&+ ( z^2-1)u = 0\n\\end{aligned}\\]\nThe general solution of the above differential equation on the interval \\((0, \\infty)\\) is\n\\[ u(z)=c_1 J_1(z) + c_2 Y_1 (z) \\]\nThus the general solution on the interval \\((0, \\infty)\\) is\n\\[ y(x)=c_1 \\sqrt{x} J_1 (2\\sqrt{x}) + c_2 \\sqrt{x} Y_1(2\\sqrt{x}) \\]\n\\(~\\)\n8. \\(~\\) The equation\n\\[ y'' -2xy' +2\\alpha y=0 \\]\nwhere \\(\\alpha\\) is a constant, is called the Hermite equation.\n\\((a)~\\) Find two linearly independent solutions for \\(-\\infty &lt; x &lt;\\infty\\)\n\\((b)~\\) Show that there is a polynomial solution of degree \\(n\\), in case \\(\\alpha=n\\) is a non-negative integer\nSolution\n\\((a)\\)\n\\[\\begin{aligned}\ny'' &-2xy' +2\\alpha y=0 \\\\\n&\\Downarrow \\;\\;y=\\sum_{k=0}^\\infty c_nk x^k, \\;\\;x=0 \\text{ is an ordinary point}\\\\\n\\sum_{k=2}^\\infty &c_k k(k-1)x^{k-2} -2\\sum_{k=1}^\\infty c_k k x^{k} +2\\alpha\\sum_{k=0}^\\infty c_k x^k =0 \\\\\n2c_2 &+ 2\\alpha c_0 +\\sum_{k=1}^\\infty\n\\left[(k+2)(k+1) c_{k+2} -2(k -\\alpha)c_k \\right] x^k = 0\\\\\n&\\Downarrow \\\\\nc_2&=-\\alpha c_0, \\;\\; c_{k+2} =-\\frac{2(\\alpha -k)}{(k+2)(k+1)} c_k \\\\\n&\\Downarrow \\\\\nc_{2k} &= (-2)^k \\frac{\\left(\\alpha - (2k-2)\\right) \\left(\\alpha - (2k-4)\\right) \\cdots \\left(\\alpha - 2\\right) \\alpha}{(2k)!} c_0 = \\hat{c}_{2k} c_0 \\\\\nc_{2k+1} &= (-2)^k \\frac{\\left(\\alpha - (2k-1)\\right) \\left(\\alpha - (2k-3)\\right) \\cdots \\left(\\alpha - 1\\right)}{(2k+1)!} c_1 =\\hat{c}_{2k+1} c_1 \\\\\n&\\Downarrow \\\\\ny &= c_0 \\sum_{k=1}^\\infty \\hat{c}_{2k} x^{2k} +c_1 \\sum_{k=1}^\\infty \\hat{c}_{2k+1} x^{2k+1} = c_0 y_0(x) + c_1 y_1(x)\n\\end{aligned}\\]\n\\((b)\\) \\(~\\)If \\(n\\) is an even integer, the first series terminates, whereas \\(y_1(x)\\) is an infinite series. For example, if \\(n=4\\), then\n\\[ y_0(x) = c_0 \\left[ 1 - 2\\frac{4}{2!}x^2 +2^2 \\frac{(4 - 2) 4}{4!} x^4 \\right] =c_0 \\left[1 -4x^2 +\\frac{4}{3}x^4 \\right] \\]\nSimilarly, when \\(n\\) is an odd integer, the series for \\(y_1(x)\\) terminates with \\(x^n\\). For exampe, if \\(n=5\\), then\n\\[ y_1(x) = c_1 \\left[ x - 2\\frac{(5 -1)}{3!}x^3 +4 \\frac{(5 - 3)(5-1)}{5!} x^5 \\right] =c_1 \\left[x -\\frac{4}{3}x^3 +\\frac{4}{15}x^5 \\right] \\]\nThat is, when \\(n\\) is a nonnegative integer, we obtain an \\(n\\)-th degree polynomial solution of Hermite equation\n\\(~\\)\n9. \\(~\\) Find the general solution of the differential equation on the interval \\((0, \\infty)\\)\n\\[ x^2 y'' +2x y'+\\left(\\alpha^2x^2 -n(n+1)\\right)y=0, \\;\\; n \\text{ is an integer}\\]\nHint) \\(~\\) Use the substitution \\(\\displaystyle y(x)=\\frac{1}{\\sqrt{x}}z(x)\\)\nSolution\nBy using \\(\\displaystyle y(x)=\\frac{1}{\\sqrt{x}}z(x)\\), the differential equation becomes\n\\[ x^2 z'' +x z' +\\left[\\alpha^2x^2 -\\left( n + \\frac{1}{2} \\right)^2 \\right]z=0\\]\nThe general solution of the above differential equation on the interval \\((0, \\infty)\\) is\n\\[ z(x)=c_1 J_{n+\\frac{1}{2}}(\\alpha x) + c_2 Y_{n+\\frac{1}{2}}(\\alpha x) \\]\nThus the general solution on the interval \\((0, \\infty)\\) is\n\\[ y(x)=c_1 \\frac{1}{\\sqrt{x}}J_{n+\\frac{1}{2}}(\\alpha x) + c_2 \\frac{1}{\\sqrt{x}}Y_{n+\\frac{1}{2}}(\\alpha x) \\]\n\\(~\\)",
    "crumbs": [
      "**First Semester**",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Series Solutions of Linear Differential Equations</span>"
    ]
  },
  {
    "objectID": "ch_08_Matrices.html",
    "href": "ch_08_Matrices.html",
    "title": "6  Matrices",
    "section": "",
    "text": "6.1 Prerequisites",
    "crumbs": [
      "**First Semester**",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Matrices</span>"
    ]
  },
  {
    "objectID": "ch_08_Matrices.html#prerequisites",
    "href": "ch_08_Matrices.html#prerequisites",
    "title": "6  Matrices",
    "section": "",
    "text": "Vector Spaces (Section 9.6)\nGram-Schmidt Orthogonalization Process (Section 9.7)",
    "crumbs": [
      "**First Semester**",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Matrices</span>"
    ]
  },
  {
    "objectID": "ch_08_Matrices.html#sec-8-1",
    "href": "ch_08_Matrices.html#sec-8-1",
    "title": "6  Matrices",
    "section": "6.2 Matrix Algebra",
    "text": "6.2 Matrix Algebra\n\nA matrix is any rectangular array of numbers or functions\n\\[\\begin{pmatrix}\n  a_{11} & a_{12} & \\cdots & a_{1n}\\\\\n  a_{21} & a_{22} & \\ddots & a_{2n} \\\\\n  \\vdots & \\ddots & \\ddots & \\vdots \\\\\n  a_{m1} & a_{m2} & \\cdots & a_{mn}\n\\end{pmatrix}\\]\n\nThe numbers or functions in the array are entries or elements\nAn \\(n \\times n\\) matrix is a square matrix of order \\(n\\)\n\nColumn and row vectors are \\(n \\times 1\\) and \\(1 \\times n\\) matrices\n\\[\n\\begin{pmatrix}\n  a_1\\\\\n  a_2\\\\\n  \\vdots\\\\\n  a_n\n\\end{pmatrix},\\;\n\\begin{pmatrix}\n  a_1 & a_2 & \\cdots & a_n\n\\end{pmatrix}  \n\\]\nEquality of Matrices\n\\[\n\\begin{aligned}\n\\mathbf{A} = \\left(a_{ij}\\right)_{m \\times n} \\; \\text{ and } \\;&\\mathbf{B} = \\left(b_{ij}\\right)_{m \\times n} \\;\\text{ are equal } \\\\[5pt]\n\\text{ if } a_{ij}=b_{ij} \\;&\\text{ for each } i \\text{ and } j\n\\end{aligned}\\]\nMatrix Addition\n\\[\\mathbf{A} +\\mathbf{B} = \\left(a_{ij} +b_{ij}\\right)_{m \\times n}\\]\nScalar Multiplication\n\\[k\\mathbf{A} = \\left(ka_{ij}\\right)_{m \\times n}\\]\nProperties of Matrix Addition and Scalar Multiplication\nSuppose \\(\\mathbf{A}\\), \\(\\mathbf{B}\\), and \\(\\mathbf{C}\\) are \\(m \\times n\\) matrices and \\(k_1\\) and \\(k_2\\) are scalars. Then\n\\[\n\\begin{aligned}\n  \\mathbf{A} +\\mathbf{B} &= \\mathbf{B} +\\mathbf{A} \\\\\n  \\mathbf{A} +\\left(\\mathbf{B} +\\mathbf{C}\\right) &= \\left(\\mathbf{A} +\\mathbf{B}\\right) +\\mathbf{C} \\\\\n  \\left(k_1 k_2\\right)\\mathbf{A} &= k_1 \\left(k_2\\mathbf{A}\\right) \\\\\n  k_1\\left(\\mathbf{A} +\\mathbf{B}\\right)&=k_1\\mathbf{A} +k_1\\mathbf{B} \\\\\n  \\left(k_1 +k_2\\right)\\mathbf{A} &= k_1 \\mathbf{A} +k_2\\mathbf{A}  \n\\end{aligned}\\]\nMatrix multiplication\n\\[\\mathbf{A}\\mathbf{B}=\\left(\\sum_{k=1}^p a_{ik} b_{kj}\\right)_{m \\times n}\\]\nwhere \\(\\mathbf{A}\\) is an \\(m \\times p\\) matrix, \\(~\\mathbf{B}\\) is a \\(p \\times n\\) matrix, and \\(~\\mathbf{A}\\mathbf{B}\\) is the \\(m \\times n\\) matrix\n\nIn general, \\(~\\mathbf{A}\\mathbf{B}\\neq\\mathbf{B}\\mathbf{A}\\)\nAssociative Law: \\(~\\mathbf{A}\\left(\\mathbf{B}\\mathbf{C}\\right)=\\left(\\mathbf{A}\\mathbf{B}\\right)\\mathbf{C}\\)\nDistributive Law: \\(~\\mathbf{A}\\left(\\mathbf{B}+\\mathbf{C}\\right)=\\mathbf{A}\\mathbf{B} +\\mathbf{A}\\mathbf{C}\\)\n\nTranspose of a Matrix\n\n\\[\\mathbf{A}^T =\n   {\\scriptsize\\begin{pmatrix}\n     a_{11} & a_{21} & \\cdots & a_{m1}\\\\\n     a_{12} & a_{22} & \\ddots & a_{m2} \\\\\n     \\vdots & \\ddots & \\ddots & \\vdots \\\\\n     a_{1n} & a_{2n} & \\cdots & a_{mn}\n   \\end{pmatrix}}\\]\n\nProperties of Transpose\n\\[\n\\begin{aligned}\n   \\left(\\mathbf{A}^T\\right)^T &= \\mathbf{A} \\\\\n   \\left(\\mathbf{A} +\\mathbf{B}\\right)^T\n     &=\\mathbf{A}^T +\\mathbf{B}^T \\\\         \n   \\left(\\mathbf{A}\\mathbf{B}\\right)^T\n     &=\\mathbf{B}^T\\mathbf{A}^T \\\\\n   \\left(k\\mathbf{A}\\right)^T\n    &=k\\mathbf{A}^T\n\\end{aligned}\\]\nSpecial Matrices\n\nIn a zero matrix, \\(~\\)all entries are zeros\nIn a triangular matrix, \\(~\\)all entries above or below the main diagonal are zeros (lower triangular or upper triangular)\nIn a diagonal matrix, \\(~\\)all entries not on the main diagonal are zeros\nA scalar matrix is a diagonal one where all entries on the main diagonal are equal. \\(~\\)If those entries are \\(1\\)’s, it is an identity matrix, \\(\\mathbf{I}\\) (or \\(~\\mathbf{I}_n\\) when there is a need to emphasize the order of the matrix)\nAn \\(n \\times n\\) matrix \\(\\mathbf{A}\\) is symmetric \\(~\\)if \\(\\mathbf{A}^T=\\mathbf{A}\\)\n\n\n\\(~\\)\nExample \\(\\,\\) If \\(~\\mathbf{A}=\n   \\begin{pmatrix}\n     \\phantom{-}2 & -3 \\\\\n     -5 & \\phantom{-}4\n   \\end{pmatrix}\\) and \\(~\\mathbf{B}=\n    \\begin{pmatrix}\n       -1 & 6 \\\\\n       \\phantom{-}3 & 2\n    \\end{pmatrix}\\),\nfind \\(~\\)(a) \\(\\mathbf{AB}\\), \\(~\\)(b) \\(\\mathbf{BA}\\), \\(~\\)(c) \\(\\mathbf{A}^2\\), \\(~\\)(d) \\(\\mathbf{B}^2\\)\n\\(~\\)\nExample \\(\\,\\) Show that if \\(\\mathbf{A}\\) is an \\(m\\times n\\) matrix, \\(~\\)then \\(\\mathbf{AA}^T\\) is symmetric\n\\(~\\)\nExample \\(\\,\\) In matrix theory, \\(~\\)many of the familiar properties of the real number system are not valid. \\(~\\)If \\(a\\) and \\(b\\) are real numbers, then \\(ab=0\\;\\) implies that \\(a=0\\) or \\(b=0\\). \\(~\\)Find two matrices such that \\(\\mathbf{AB}=\\mathbf{0}~\\) but \\(\\mathbf{A}\\neq\\mathbf{0}\\) and \\(\\mathbf{B}\\neq \\mathbf{0}\\)\n\\(~\\)\nExample \\(\\,\\) Let \\(\\mathbf{A}\\) and \\(\\mathbf{B}\\) be \\(n\\times n\\) matrices. Explain why, in general, the given formula is not valid\n\\[(\\mathbf{A} + \\mathbf{B})^2 = \\mathbf{A}^2 + 2\\mathbf{A}\\mathbf{B} +\\mathbf{B}^2\\]\n\\(~\\)\nExample \\(\\,\\) Find the resulting vector \\(\\mathbf{b}\\) if the given vector \\(\\mathbf{a} = \\langle 1, 1 \\rangle\\) is rotated through the indicated angle \\(\\theta=\\pi/2\\)\n\\(~\\)\nExample \\(\\,\\) Verify that the quadratic form \\(ax^2 +bxy +cy^2\\) is the same as\n\\[\\begin{pmatrix}\n      x & y\n    \\end{pmatrix}\n    \\begin{pmatrix}\n      a & \\frac{1}{2}b \\\\\n     \\frac{1}{2}b & c\n    \\end{pmatrix}\n    \\begin{pmatrix}\n      x \\\\ y\n\\end{pmatrix}\\]\n\\(~\\)",
    "crumbs": [
      "**First Semester**",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Matrices</span>"
    ]
  },
  {
    "objectID": "ch_08_Matrices.html#sec-8-2",
    "href": "ch_08_Matrices.html#sec-8-2",
    "title": "6  Matrices",
    "section": "6.3 Systems of Linear Algebraic Equations",
    "text": "6.3 Systems of Linear Algebraic Equations\n\nGeneral Form\nA system of \\(m\\) linear equations in \\(n\\) unknowns has the general form\n\\[\n\\begin{aligned}\n  a_{11} x_1 +a_{12} x_2 + \\cdots +a_{1n} x_n\n    & = b_1\\\\\n  a_{21} x_1 +a_{22} x_2 + \\cdots +a_{2n} x_n\n    & = b_2\\\\\n   &\\;\\, \\vdots \\\\\n   a_{m1} x_1 +a_{m2} x_2 + \\cdots +a_{mn} x_n\n    & = b_m\n\\end{aligned}\\]\nThe coefficients of the unknowns can be abbreviated as \\(a_{ij}\\). The numbers \\(b_1, b_2, \\cdots, b_m\\) are called the constants of the system. If all the constants are zero, the system is said to be homogeneous, otherwise it is nonhomogeneous \\[\n\\begin{aligned}\n  \\mathbf{A}\n  &=\n  \\begin{pmatrix}\n    a_{11} & a_{12} & \\cdots & a_{1n} \\\\\n    a_{21} & a_{22} & \\cdots & a_{2n} \\\\\n    \\vdots &   & \\ddots & \\\\\n    a_{m1} & a_{m2} & \\cdots & a_{mn} \\\\\n  \\end{pmatrix}, \\;\\;\n  \\mathbf{x} =\n  \\begin{pmatrix}\n     x_1  \\\\ x_2 \\\\ \\vdots \\\\ x_n\n  \\end{pmatrix}, \\;\\;\n  \\mathbf{b} =\n  \\begin{pmatrix}\n     b_1 \\\\ b_2 \\\\ \\vdots \\\\ b_m\n  \\end{pmatrix}\n\\end{aligned}\\]\n\n\\[\\mathbf{A} \\mathbf{x} = \\mathbf{b}\\]\n\nA linear system of equations is said to be consistent if it has at least one solution and inconsistent if it has no solutions. If a linear system is consistent, \\(~\\)it has either\n\na unique solution (that is, precisely one solution), or\ninfinitely many solutions\n\n\n\\(~\\)\n\n\n\n\n\n\nAugmented Matrix\n\\[\n\\left(\\begin{array}{cccc|c}\n  a_{11} & a_{12} & \\cdots & a_{1n} & b_1\\\\\n  a_{21} & a_{22} & \\ddots & a_{2n} & b_2\\\\\n  \\vdots & \\ddots & \\ddots & \\vdots & \\vdots\\\\\n  a_{m1} & a_{m2} & \\cdots & a_{mn} & b_m\n\\end{array}\\right)\\]\nA system can be solved with elementary operations (row reduction for matrices) on an augmented matrix\n\n\n\n\nElementaryOperations\nMeaning\n\n\n\n\n\\(R_{ij}\\)\nInterchange rows \\(i\\) and \\(j\\)\n\n\n\\(cR_{i}\\)\nMultiply the row \\(i\\) by the nonzero constant \\(c\\)\n\n\n\\(cR_{i}+R_{j}\\)\nMultiply the row \\(i\\) by \\(c\\) and add to the row \\(j\\)\n\n\n\n\\(~\\)\n\nIn the Gaussian elimination, \\(~\\) we row-reduce the augmented matrix until we arrive at a row-equivalent augmented matrix in row-echelon form\n\nThe first nonzero entry in a nonzero row is a \\(1\\)\nIn consecutive nonzero rows, \\(~\\)the first entry \\(1\\) in the lower row appears to the right of the \\(1\\) in the higher row\nRows consisting of all zeros are at the bottom of the matrix\n\n\n\\(~\\)\nExample \\(\\,\\) Solve\n\\[\\begin{pmatrix}\n    2 & 6 & \\phantom{-}1\\\\\n    1 & 2 & -1\\\\\n    5 & 7 & -4\n  \\end{pmatrix}\n  \\begin{pmatrix}\n    x_1\\\\\n    x_2\\\\\n    x_3\n  \\end{pmatrix}=\n  \\begin{pmatrix}\n    \\phantom{-}7\\\\\n    -1\\\\\n    \\phantom{-}9\n  \\end{pmatrix}\\]\n\\(~\\)\nUsing row operations on the augmented matrix, \\(~\\) we obtain\n\\[{\\scriptsize\n  \\left(\\begin{array}{rrr|r}\n    2 & 6 &  1 & 7\\\\\n    1 & 2 & -1 & -1\\\\\n    5 & 7 & -4 & 9\n  \\end{array}\\right)\n  \\overset{ R_{12}}{\\Longrightarrow}}\n  {\\scriptsize\n  \\left(\\begin{array}{rrr|r}\n    1 & 2 & -1 & -1\\\\   \n    2 & 6 &  1 & 7\\\\\n    5 & 7 & -4 & 9\n  \\end{array}\\right)\n  \\overset{\\begin{matrix} -2R_1 +R_2 \\\\ -5R_1 +R_3 \\end{matrix}}{\\Longrightarrow}\n  \\left(\\begin{array}{rrr|r}\n    1 & 2 & -1 & -1\\\\   \n    0 & 2 &  3 & 9\\\\\n    0 & -3 & 1 & 14\n  \\end{array}\\right)}\\]\n\\[{\\scriptsize\n  \\overset{\\frac{1}{2}R_2}{\\Longrightarrow}\n  \\left(\\begin{array}{rrr|r}\n    1 & 2 & -1 & -1\\\\   \n    0 & 1 &  \\frac{3}{2} & \\frac{9}{2} \\\\\n    0 & -3 & 1 & 14\n  \\end{array}\\right)\n  \\overset{ 3R_2 +R_3}{\\Longrightarrow}\n  \\left(\\begin{array}{rrr|r}\n    1 & 2 & -1 & -1\\\\   \n    0 & 1 &  \\frac{3}{2} & \\frac{9}{2} \\\\\n    0 & 0 & \\;\\frac{11}{2} & \\;\\frac{55}{2}\n  \\end{array}\\right)}\\] \\[{\\scriptsize\\overset{\\frac{2}{11}R_3}{\\Longrightarrow}\n  \\left(\\begin{array}{rrr|r}\n    1 & 2 & -1 & -1\\\\   \n    0 & 1 &  \\frac{3}{2} & \\frac{9}{2} \\\\\n    0 & 0 & 1 & 5\n  \\end{array}\\right)}\\]\nThe last matrix is in row-echelon form. \\(~\\)We can make the last matrix above to be in reduced row-echelon form\n\\[{\\scriptsize\n  \\left(\\begin{array}{rrr|r}\n    1 & 2 & -1 & -1\\\\   \n    0 & 1 &  \\frac{3}{2} & \\frac{9}{2} \\\\\n    0 & 0 & 1 & 5\n  \\end{array}\\right)  \n  \\overset{ -2R_2 +R_1}{\\Longrightarrow}\n  \\left(\\begin{array}{rrr|r}\n    1 & 0 & -4 & -10\\\\   \n    0 & 1 &  \\frac{3}{2} & \\frac{9}{2} \\\\\n    0 & 0 & 1 & 5\n  \\end{array}\\right)   \n  \\overset{\\begin{matrix}\n           -4R_3 +R_1 \\\\\n          -\\frac{3}{2}R_3 +R_2\n           \\end{matrix}}{\\Longrightarrow}\n  \\left(\\begin{array}{rrr|r}\n    1 & 0 & 0 & 10\\\\   \n    0 & 1 & 0 & -3 \\\\\n    0 & 0 & 1 & 5\n  \\end{array}\\right)}\\]\nWe see that the solution is \\(x_1=10\\), \\(~x_2=-3\\), \\(~x_3=5\\)\n\\(~\\)\nExample \\(\\,\\) Solve\n\\[\n  \\left(\\begin{array}{rrr}\n    1 & 3 & -2\\\\\n    4 & 1 & 3\\\\\n    2 & -5 & 7\n  \\end{array}\\right)\n  \\begin{pmatrix}\n    x_1\\\\\n    x_2\\\\\n    x_3\n  \\end{pmatrix}=\n\\left(\\begin{array}{r}\n    -7\\\\\n     5\\\\\n    19\n  \\end{array}\\right)\\]\n\\(~\\)\nUsing row operations on the augmented matrix, we obtain\n\\[{\\scriptsize\n  \\left(\\begin{array}{rrr|r}\n    1 & 3 & -2 & -7\\\\\n    4 & 1 &  3 & 5\\\\\n    2 & -5 & 7 & 19\n  \\end{array}\\right)\n  \\overset{\\begin{matrix}\n           -4R_1 +R_2 \\\\\n           -2R_1 +R_3\n           \\end{matrix}}\n  {\\Longrightarrow}\n  \\left(\\begin{array}{rrr|r}\n    1 & 3 & -2 & -7\\\\\n    0 & -11 & 11 & 33\\\\\n    0 & -11 & 11 & 33\n  \\end{array}\\right)\n  \\overset{\\begin{matrix}\n           -R_2 +R_3 \\\\\n           -\\frac{1}{11}R_2\n           \\end{matrix}}\n  {\\Longrightarrow}\n  \\left(\\begin{array}{rr|r|r}\n    1 & 3 & -2 & -7\\\\\n    0 & 1 & -1 & -3\\\\ \\hline\n    0 & 0 & 0 & {\\color{Red}0 }\n  \\end{array}\\right)}\\]\nIn this case, \\(~\\)the last matrix implies that the original system of three equations is really equivalent to two equations\n\\[{\n  \\overset{-3R_2 +R_1}\n  {\\Longrightarrow}\n  \\left(\\begin{array}{rr|r|r}\n    1 & 0 & 1 & 2\\\\\n    0 & 1 & -1 & -3\\\\\n    \\hline\n    0 & 0 & 0 & 0\n  \\end{array}\\right)}\\]\nIf we let \\(x_3=t\\), \\(x_1=-t +2\\) and \\(x_2=t -3\\), \\(~\\)then we see that the system has infinitely many solutions\n\\(~\\)\nExample \\(\\,\\) Solve\n\\[\n  \\left(\\begin{array}{rr}\n    1 & 1 \\\\\n    4 & -1 \\\\\n    2 & -3\n  \\end{array}\\right)\n  \\begin{pmatrix}\n    x_1\\\\\n    x_2\n  \\end{pmatrix}=\n\\left(\\begin{array}{r}\n    1\\\\\n   -6\\\\\n    8\n  \\end{array}\\right)\\]\n\\(~\\)\nUsing row operations on the augmented matrix, \\(~\\)we obtain\n\\[{\\scriptsize\n  \\left(\\begin{array}{rr|r}\n    1 & 1 & 1\\\\\n    4 & -1 & -6\\\\\n    2 & -3 & 8\n  \\end{array}\\right)\n  \\overset{\\begin{matrix}\n           -4R_1 +R_2 \\\\\n           -2R_1 +R_3\n           \\end{matrix}}\n  {\\Longrightarrow}\n  \\left(\\begin{array}{rr|r}\n    1 & 1 & 1\\\\\n    0 & -5 & -10\\\\\n    0 & -5 & 6\n  \\end{array}\\right)\n  \\overset{\\begin{matrix}\n           -R_2 +R_3 \\\\\n           -\\frac{1}{5}R_2\n           \\end{matrix}}\n  {\\Longrightarrow}\n  \\left(\\begin{array}{rr|r}\n    1 & 1 & 1\\\\\n    0 & 1 & 2\\\\ \\hline\n    0 & 0 & {\\color{Red}{16}}\n  \\end{array}\\right)}\\]\nThe system has no solution\n\\(~\\)\n\nA homogeneous system of linear equations is always consistent. The solution consisting of all zeros is called the trivial solution. A homogeneous system either possesses only the trivial solution or possesses the trivial solution along with infinitely many nontrivial solutions\nA homogeneous system possesses nontrivial solutions if the number \\(m\\) of equations is less than the number \\(n\\) of unknowns \\((m&lt;n)\\)\n\n\\(~\\)\nExample \\(\\,\\) Find the positive integers \\(x_1\\), \\(x_2\\), \\(x_3\\), and \\(x_4\\) so that\n\\[x_1 \\mathrm{C_2H_6} +x_2 \\mathrm{O_2} \\rightarrow x_3 \\mathrm{CO_2} +x_4 \\mathrm{H_2O}\\]\nBecause the number of atoms of each element must be the same on each side of the last equation, \\(~\\)we have:\n\n\n\nAtom\n\n\n\n\n\n\\(\\mathrm{C}\\)\n\\(2x_1=x_3\\)\n\n\n\\(\\mathrm{H}\\)\n\\(6x_1=2x_4\\)\n\n\n\\(\\mathrm{O}\\)\n\\(2x_2=2x_3 +x_4\\)\n\n\n\n\\[{\\scriptsize\n  \\left(\\begin{array}{rrrr|r}\n    2 & 0 & -1 &  0 & 0\\\\   \n    6 & 0 &  0 & -2 & 0\\\\\n    0 & 2 & -2 & -1 & 0\n  \\end{array}\\right)\n  \\overset{\\begin{matrix}\n             R_{12} \\\\\n             R_{23}\n           \\end{matrix}}{\\Longrightarrow}\n  \\left(\\begin{array}{rrrr|r}\n    6 & 0 &  0 & -2 & 0\\\\\n    0 & 2 & -2 & -1 & 0\\\\\n    2 & 0 & -1 &  0 & 0  \n  \\end{array}\\right)\n  \\overset{\\;\\,\\text{ row } \\\\ \\text{operations}}{\\Longrightarrow}\n  \\left(\\begin{array}{rrr|r|r}\n    1 & 0 & 0 & -\\frac{1}{3} & 0\\\\   \n    0 & 1 & 0 & -\\frac{7}{6} & 0\\\\\n    0 & 0 & 1 & -\\frac{2}{3} & 0\n  \\end{array}\\right)}\\]\nThen when we let \\(x_4=t\\), \\(~x_1=\\frac{1}{3}t\\), \\(x_2=\\frac{7}{6}t\\), \\(x_3=\\frac{2}{3}t\\). \\(\\,\\) If we pick \\(t=6\\), \\(\\,x_1=2\\), \\(\\,x_2=7\\), \\(\\,x_3=4\\), \\(\\,x_4=6\\)\n\\(~\\)\nExample \\(\\,\\) Use either Gaussian elimination or Gauss-Jordan elimination to solve the given system or show that no solution exists\n\\[\n\\begin{aligned}\n   x_1 - x_2 &= 11\\\\\n   4x_1 +3x_2 &=-5\n\\end{aligned}\\]\n\\[\n\\begin{aligned}\n   9x_1 +3x_2 &= -5\\\\\n   2x_1 +x_2 &= -1\n\\end{aligned}\\]\n\\[\n\\begin{aligned}\n    x_1 -x_2 -x_3 &= -3\\\\\n    2x_1 +3x_2 +5x_3 &= 7\\\\\n    x_1 -2x_2 +3x_3 &=-11\n\\end{aligned}\\]\n\\[\n\\begin{aligned}\n    x_1 +x_2 +x_3 &= 0\\\\\n    x_1 +x_2 +3x_3 &=0\n\\end{aligned}\\]\n\\[\n\\begin{aligned}\n    &x_1 -x_2 -x_3 = 8\\\\\n    &x_1 -x_2 +x_3 = 3\\\\\n    -&x_1 +x_2 +x_3 = 4\n\\end{aligned}\\]\n\\(~\\)\nExample \\(\\,\\) Balance the given chemical equation:\n\\[\\mathrm{C}_5\\mathrm{H}_8 +\\mathrm{O}_2 \\rightarrow \\mathrm{CO}_2 + \\mathrm{H}_2\\mathrm{O}\\]\n\\[\\mathrm{Cu} + \\mathrm{HNO}_3 \\rightarrow \\mathrm{Cu(NO}_3\\mathrm{)}_2 + \\mathrm{H}_2\\mathrm{O} +\\mathrm{NO}\\]\n\\(~\\)\nExample \\(\\,\\) Compute the given product for an arbitrary \\(~3 \\times 3\\) matrix \\(\\mathbf{A}\\)\n\\[\\begin{pmatrix}\n0 & 1 & 0\\\\\n1 & 0 & 0\\\\\n0 & 0 & 1\n\\end{pmatrix}\n\\begin{pmatrix}\n1 & 0 & 0\\\\\n0 & 1 & 0\\\\\n0 & c & 1\n\\end{pmatrix} \\mathbf{A}\\]\n\\(~\\)",
    "crumbs": [
      "**First Semester**",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Matrices</span>"
    ]
  },
  {
    "objectID": "ch_08_Matrices.html#sec-8-3",
    "href": "ch_08_Matrices.html#sec-8-3",
    "title": "6  Matrices",
    "section": "6.4 Rank of a Matrix",
    "text": "6.4 Rank of a Matrix\n\nThe rank of an \\(m \\times n\\) matrix \\(\\mathbf{A}\\), \\(~\\mathrm{rank}(\\mathbf{A})\\), \\(\\,\\) is the maximum number of linearly independent row vectors. If a matrix \\(\\mathbf{A}\\) is now equivalent to a row-echelon form \\(\\mathbf{B}\\), \\(~\\)then\n\nthe row space of \\(\\mathbf{A}\\) = the row space of \\(\\mathbf{B}\\)\nthe nonezero rows of \\(\\mathbf{B}\\) form a basis for the row space of \\(\\mathbf{A}\\), \\(~\\)and\n\\(\\mathrm{rank}(\\mathbf{A})\\) = the number of nonzero rows in \\(\\mathbf{B}\\)\n\nConsistency of \\(~\\mathbf{A}\\mathbf{x}=\\mathbf{b}\\)\n\nA linear system of equations \\(\\mathbf{A}\\mathbf{x}=\\mathbf{b}\\) is consistent if and only if \\(~\\mathrm{rank}(\\mathbf{A})=\\mathrm{rank}(\\mathbf{A}|\\mathbf{b})\\)\nSuppose a linear system \\(\\mathbf{A}\\mathbf{x}=\\mathbf{b}\\) with \\(m\\) equations and \\(n\\) unknowns is consistent. \\(~\\)If \\(\\mathrm{rank}(\\mathbf{A})=r\\leq n\\), then the solution of the system contains \\(n -r\\) parameters. This means that we have the unique solution when \\(r=n\\)\n\n\n\\[{\\scriptsize\n  \\left(\\begin{array}{cccc|c}\n    a_{11} & a_{12} & \\cdots & a_{1n} & b_1\\\\\n    a_{21} & a_{22} & \\ddots & a_{2n} & b_2\\\\\n    \\vdots & \\ddots & \\ddots & \\vdots & \\vdots\\\\\n    a_{m1} & a_{m2} & \\cdots & a_{mn} & b_m\n  \\end{array}\\right)\n  \\overset{\\text{row operations}}{\\Longrightarrow}} \\\\\n  {\\scriptsize\n  \\left(\\begin{array}{cccc|ccc|c}\n    1      & a_{12}' & \\cdots & a_{1{\\color{red}r}}'& a_{1r+1}' & \\cdots   & a_{1n}' & b_1' \\\\\n    0      & 1       & \\ddots & \\vdots & a_{2r+1}' & \\ddots   & a_{2n}' & b_2' \\\\\n    \\vdots & \\ddots  & \\ddots & \\vdots & \\vdots    & \\ddots   & \\vdots  & \\vdots \\\\    \n    0      & \\cdots  & 0      & 1      & a_{{\\color{red}r}r+1}' & \\cdots   & a_{rn}' & b_r' \\\\ \\hline     \n    0      & 0       & 0      & 0      & 0         & 0        & 0       & {\\color{red} 0} \\\\\n    \\vdots & \\vdots  & \\vdots & \\vdots & \\vdots    & \\vdots   & \\vdots  & {\\color{red} \\vdots} \\\\    \n    0      & 0       & 0      & 0      & 0         & 0        & 0       & {\\color{red} 0}    \n  \\end{array}\\right)}\\]\n\n\n\n\n\n\\(~\\)\nExample \\(\\,\\) Find the rank of the given matrix\n\\[{\\scriptsize\\begin{pmatrix}\n     \\phantom{-}2 & \\phantom{-}1 & \\phantom{-}3 \\\\\n     \\phantom{-}6 & \\phantom{-}3 & \\phantom{-}9 \\\\\n     -1 & -\\frac{1}{2} & -\\frac{3}{2}\n    \\end{pmatrix}}\\]\n\\[{\\scriptsize\\begin{pmatrix}\n     1 & 1 & 1\\\\\n     1 & 0 & 4\\\\\n     1 & 4 & 1\n   \\end{pmatrix}}\\]\n\\[{\\scriptsize\\begin{pmatrix}\n    0 & 2 & 4 & 2 & 2\\\\\n    4 & 1 & 0 & 5 & 1\\\\\n    2 & 1 & \\frac{2}{3} & 3 & \\frac{1}{3} \\\\\n    6 & 6 & 6 & 12 & 0\n   \\end{pmatrix}}\\]\n\\(~\\)\nExample \\(\\,\\) Determine whether the given set of vectors is linearly dependent or linearly independent\n\\[\\mathbf{u}_1 = \\langle 1, 2, 3 \\rangle, \\;\\mathbf{u}_2 = \\langle 1, 0, 1 \\rangle, \\; \\mathbf{u}_3 = \\langle 1, -1, 5 \\rangle\\]\n\\[\\mathbf{u}_1 = \\langle 1, -1, 3, -1 \\rangle, \\;\\mathbf{u}_2 = \\langle 1, -1, 4, 2 \\rangle, \\; \\mathbf{u}_3 = \\langle 1, -1, 5, 7 \\rangle\\]\n\\(~\\)\nExample \\(\\,\\) Suppose the system \\(\\mathbf{Ax}=\\mathbf{b}~\\) is consistent and \\(~\\mathbf{A}\\) is a \\(5\\times 8~\\) matrix and \\(~\\mathrm{rank}\\,\\mathbf{A}=3.\\) \\(~\\)How many parameters does the solution of the system have?\n\\(~\\)\nExample \\(\\,\\) Let \\(\\mathbf{A}\\) be a nonzero \\(4 \\times 6~\\) matrix\n1. \\(~\\) What is the maximum rank that \\(~\\mathbf{A}\\) can have?\n\\(~\\)\n2. \\(~\\) If \\(\\mathrm{rank}(\\mathbf{A}|\\mathbf{b})=2\\), \\(~\\)then for what value(s) of \\(\\mathrm{rank}(\\mathbf{A})\\) is the system \\(~\\mathbf{Ax}=\\mathbf{b}\\), \\(~\\mathbf{b}\\neq \\mathbf{0}\\), inconsistent? Consistent?\n\\(~\\)\n3. \\(~\\) If \\(\\mathrm{rank}(\\mathbf{A})=3\\), \\(~\\)then how many parameters does the solution of the system \\(~\\mathbf{Ax}=\\mathbf{0}~\\) have?\n\\(~\\)\nExample \\(\\,\\) Let \\(\\mathbf{v}_1\\), \\(\\mathbf{v}_2\\), and \\(~\\mathbf{v}_3~\\) be the first, second, and third column vectors, respectively, of the matrix\n\\[\\mathbf{A} =\n\\begin{pmatrix}\n\\phantom{-}2 & 1 & 7\\\\\n\\phantom{-}1 & 0 & 2\\\\\n-1 & 5 & 13\n\\end{pmatrix}\\]\nWhat can we conclude about \\(\\mathrm{rank} (\\mathbf{A})\\) from the observation \\(2\\mathbf{v}_1 +3\\mathbf{v}_2 -\\mathbf{v}_3=\\mathbf{0}\\)?\n\\(~\\)",
    "crumbs": [
      "**First Semester**",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Matrices</span>"
    ]
  },
  {
    "objectID": "ch_08_Matrices.html#sec-8-4",
    "href": "ch_08_Matrices.html#sec-8-4",
    "title": "6  Matrices",
    "section": "6.5 Determinants",
    "text": "6.5 Determinants\n\nDeterminant of a \\(2 \\times 2\\) Matrix\n\n\\[\\mathrm{det}(\\mathbf{A})={\\scriptsize\n   \\begin{vmatrix}\n     a_{11} & a_{12}\\\\\n     a_{21} & a_{22}\n   \\end{vmatrix}=a_{11}a_{22}-a_{12}a_{21}}\\]\n\nDeterminant of a \\(3 \\times 3\\) Matrix\n\n\\[\n   \\mathrm{det}(\\mathbf{A})={\\scriptsize\n   \\begin{vmatrix}\n     a_{11} & a_{12} & a_{13}\\\\\n     a_{21} & a_{22} & a_{23}\\\\\n     a_{31} & a_{32} & a_{33}\n   \\end{vmatrix}}\\\\{\\scriptsize=\n   a_{11}(-1)^{1+1}\n   \\begin{vmatrix}\n     a_{22} & a_{23}\\\\\n     a_{32} & a_{33}\n   \\end{vmatrix} +\n   a_{12}(-1)^{1+2}\n   \\begin{vmatrix}\n     a_{21} & a_{23}\\\\\n     a_{31} & a_{33}\n   \\end{vmatrix} +\n   a_{13}(-1)^{1+3}\n   \\begin{vmatrix}\n     a_{21} & a_{22}\\\\\n     a_{31} & a_{32}\n   \\end{vmatrix}}\\]\n\nDeterminant of a \\(n\\times n\\) Matrix\n\\[\\mathrm{det}\\,\\mathbf{A} = \\sum (-1)^h a_{1l_1} a_{2l_2} \\cdots a_{nl_n}\\]\nwhere the summation is over all permutations \\(l_1,\\) \\(l_2,\\) \\(\\cdots,\\) \\(l_n\\) of \\(1,\\) \\(2,\\) \\(\\cdots,\\) \\(n\\) and the sign accords with the parity of the permutation\nCofactor and Minor\nThe cofactor of \\(\\,a_{ij}\\) is the determinant\n\\[C_{ij}=(-1)^{i +j} M_{ij}\\]\nwhere \\(M_{ij}\\) is the determinant of the submatrix obtained by deleting the \\(i\\)-th row and the \\(j\\)-th column of \\(\\mathbf{A}\\). The determinant \\(M_{ij}\\) is called a minor determinant\nCofactor Expansion of a Determinant: Laplace Development\nLet \\(\\mathbf{A}=\\left(a_{ij}\\right)_{n \\times n}\\) be an \\(n \\times n\\) matrix. \\(~\\)For each \\(1 \\leq i \\leq n\\), \\(~\\) the cofactor expansion of \\(\\mathrm{det}(\\mathbf{A})\\) along the \\(i\\)-th row is\n\\[{\\mathrm{det}(\\mathbf{A})=\\sum_{k=1}^na_{ik}C_{ik}}\\]\nFor each \\(1 \\leq j \\leq n\\), \\(~\\)the cofactor expansion of \\(\\mathrm{det}(\\mathbf{A})\\) along the \\(j\\)-th column is\n\\[{\\mathrm{det}(\\mathbf{A})=\\sum_{k=1}^na_{kj}C_{kj}}\\]\n\n\\(~\\)\nExample \\(\\,\\) Suppose\n\\[\\mathbf{A} = \\begin{pmatrix}\n    \\phantom{-}2 & \\phantom{-}3 & 4\\\\\n    \\phantom{-}1 & -1 & 2\\\\\n     -2 & \\phantom{-}3 & 5\n   \\end{pmatrix}\\]\nEvaluate the indicated minor determinant or cofactor\n1. \\(~M_{12}~\\) 2. \\(~M_{32}~\\) 3. \\(~C_{13}~\\) 4. \\(~C_{22}\\)\n\\(~\\)\nExample \\(\\,\\) Evaluate the determinant of the matrix\n\\[\\begin{pmatrix}\n\\phantom{-}3 & 5 \\\\\n-1 & 4\n\\end{pmatrix}\\]\n\\[\\begin{pmatrix}\n1 - \\lambda & 3 \\\\\n2 & 2 - \\lambda\n\\end{pmatrix}\\]\n\\(~\\)\nExample \\(\\,\\) Evaluate the determinant of the given matrix by cofactor expansion\n\\[\\begin{pmatrix}\n-2 & -1 & 4\\\\\n-3 & \\phantom{-}6 & 1\\\\\n-3 & \\phantom{-}4 & 8\n\\end{pmatrix}\\]\n\\[\\begin{pmatrix}\n1 & 1 & 1\\\\\nx & y & z\\\\\n2 & 3 & 4\n\\end{pmatrix}\\]\n\\(~\\)",
    "crumbs": [
      "**First Semester**",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Matrices</span>"
    ]
  },
  {
    "objectID": "ch_08_Matrices.html#sec-8-5",
    "href": "ch_08_Matrices.html#sec-8-5",
    "title": "6  Matrices",
    "section": "6.6 Properties of Determinants",
    "text": "6.6 Properties of Determinants\n\nIf \\(\\mathbf{A}^T\\) is the transpose of the \\(n \\times n\\) matrix \\(\\mathbf{A}\\), \\(~\\)then \\(\\mathrm{det}(\\mathbf{A}^T)=\\mathrm{det}(\\mathbf{A})\\)\n\\[{\\scriptsize\\begin{vmatrix}\na_{11} & a_{12} & a_{13}\\\\\na_{21} & a_{22} & a_{23}\\\\\na_{31} & a_{32} & a_{33}\n\\end{vmatrix} =\n\\begin{vmatrix}\na_{11} & a_{21} & a_{31}\\\\\na_{12} & a_{22} & a_{32}\\\\\na_{13} & a_{23} & a_{33}\n\\end{vmatrix}}\\]\nIf any two rows (columns) of an \\(n \\times n\\) matrix \\(\\mathbf{A}\\) are the same, \\(~\\)then \\(\\mathrm{det}(\\mathbf{A})=0\\)\n\\[{\\scriptsize\\begin{vmatrix}\na_{11} & a_{12} & a_{13}\\\\\na_{11} & a_{12} & a_{13}\\\\\na_{31} & a_{32} & a_{33}\n\\end{vmatrix} = 0}\\]\nIf all the entries in a row (column) of an \\(n \\times n\\) matrix \\(\\mathbf{A}\\) are zero, \\(~\\)then \\(\\mathrm{det}(\\mathbf{A})=0\\)\n\\[{\\scriptsize\\begin{vmatrix}\na_{11} & a_{12} & a_{13}\\\\\n0 & 0 & 0\\\\\na_{31} & a_{32} & a_{33}\n\\end{vmatrix} = 0}\\]\nIf \\(\\mathbf{B}\\) is the matrix obtained by interchanging any two rows (columns) of an \\(n \\times n\\) matrix \\(\\mathbf{A}\\), \\(~\\)then \\(\\mathrm{det}(\\mathbf{B})=-\\mathrm{det}(\\mathbf{A})\\)\n\\[{\\scriptsize\\begin{vmatrix}\na_{21} & a_{22} & a_{23}\\\\\na_{11} & a_{12} & a_{13}\\\\\na_{31} & a_{32} & a_{33}\n\\end{vmatrix} = -\n\\begin{vmatrix}\na_{11} & a_{12} & a_{13}\\\\\na_{21} & a_{22} & a_{23}\\\\\na_{31} & a_{32} & a_{33}\n\\end{vmatrix}}\\]\nIf \\(\\mathbf{B}\\) is the matrix obtained by multiplying a row (column) by a nonzero real number \\(k\\), \\(~\\) then \\(\\mathrm{det}(\\mathbf{B})=k\\,\\mathrm{det}(\\mathbf{A})\\)\n\\[{\\scriptsize\\begin{vmatrix}\na_{11} & a_{12} & a_{13}\\\\\nka_{21} & ka_{22} & ka_{23}\\\\\na_{31} & a_{32} & a_{33}\n\\end{vmatrix} = k\n\\begin{vmatrix}\na_{11} & a_{12} & a_{13}\\\\\na_{21} & a_{22} & a_{23}\\\\\na_{31} & a_{32} & a_{33}\n\\end{vmatrix}}\\]\nIf \\(\\mathbf{A}\\) and \\(\\mathbf{B}\\) are both \\(n \\times n\\) matrices, \\(~\\)then \\(\\mathrm{det}(\\mathbf{AB})=\\mathrm{det}(\\mathbf{A})\\cdot \\mathrm{det}(\\mathbf{B})\\)\n\\[{\\scriptsize\\begin{vmatrix}\n\\begin{pmatrix}\na_{11} & a_{12} & a_{13}\\\\\na_{21} & a_{22} & a_{23}\\\\\na_{31} & a_{32} & a_{33}\n\\end{pmatrix}\n\\begin{pmatrix}\nb_{11} & b_{12} & b_{13}\\\\\nb_{21} & b_{22} & b_{23}\\\\\nb_{31} & b_{32} & b_{33}\n\\end{pmatrix}\n\\end{vmatrix}\n=\n\\begin{vmatrix}\na_{11} & a_{12} & a_{13}\\\\\na_{21} & a_{22} & a_{23}\\\\\na_{31} & a_{32} & a_{33}\n\\end{vmatrix}\n\\begin{vmatrix}\nb_{11} & b_{12} & b_{13}\\\\\nb_{21} & b_{22} & b_{23}\\\\\nb_{31} & b_{32} & b_{33}\n\\end{vmatrix}}\\]\nSuppose \\(\\mathbf{B}\\) is the matrix obtained from an \\(n \\times n\\) matrix \\(\\mathbf{A}\\) by multiplying a row(column) by a nonzero \\(k\\) and adding the result to another row(column). \\(~\\) Then \\(\\mathrm{det}(\\mathbf{B})=\\mathrm{det}(\\mathbf{A})\\)\n\\[{\\scriptsize\\begin{vmatrix}\na_{11} & a_{12} & a_{13}\\\\\na_{21} & a_{22} & a_{23}\\\\\nka_{21} +a_{31} & ka_{22}+a_{32} & ka_{23} +a_{33}\n\\end{vmatrix}\n=\n\\begin{vmatrix}\na_{11} & a_{12} & a_{13}\\\\\na_{21} & a_{22} & a_{23}\\\\\na_{31} & a_{32} & a_{33}\n\\end{vmatrix}}\\]\nIf \\(\\mathbf{A}\\) is an \\(n \\times n\\) triangular matrix, \\(~\\)then \\(\\mathrm{det}(\\mathbf{A})=a_{11} a_{22}\\cdots a_{nn}\\)\n\\[{\\scriptsize\\begin{vmatrix}\na_{11} & 0 & 0\\\\\na_{21} & a_{22} & 0\\\\\na_{31} & a_{32} & a_{33}\n\\end{vmatrix} = a_{11} a_{22} a_{33}}\\]\n\nAlien Cofactors\n\nSuppose \\(\\mathbf{A}\\) is an \\(n \\times n\\) matrix. If \\(a_{i1}, a_{i2}, \\cdots, a_{in}\\) are the entries in the \\(i\\)-th row and \\(C_{p1}, C_{p2}, \\cdots, C_{pn}\\,\\) are the cofactors of the entries in the \\(p\\)-th row, \\(~\\)then\n\\[\\sum_{k=1}^n a_{ik}C_{pk}=0\\;\\;\\text{for}\\; i \\neq p\\]\nIf \\(a_{1j}, a_{2j}, \\cdots, a_{nj}\\) are the entries in the \\(j\\)-th column and \\(C_{1p},\\) \\(C_{2p},\\) \\(\\cdots,\\) \\(C_{np}\\) are the cofactors of the entries in the \\(p\\)-th column, \\(~\\)then\n\\[\\sum_{k=1}^n a_{kj}C_{kp}=0\\;\\;\\text{for}\\; j \\neq p\\]\n\n\\(~\\)\nExample \\(\\,\\) State the appropriate theorem(s) in this section that justifies the given equality\n\\[\\begin{vmatrix}\n1 & 2\\\\\n3 & 4\n\\end{vmatrix} = -\n\\begin{vmatrix}\n3 & 4\\\\\n1 & 2\n\\end{vmatrix}\\]\n\\[\\begin{vmatrix}\n-5 & \\phantom{-}6\\\\\n\\phantom{-}2 & -8\n\\end{vmatrix} =\n\\begin{vmatrix}\n\\phantom{-}1 & \\phantom{-}6\\\\\n-6 & -8\n\\end{vmatrix}\\]\n\\[{\\begin{vmatrix}\n1 & 2 & \\phantom{--}3\\\\\n4 & 2 & \\phantom{-}18\\\\\n5 & 9 & -12\n\\end{vmatrix} = 6\n\\begin{vmatrix}\n1 & 2 & \\phantom{-}1\\\\\n2 & 1 & \\phantom{-}3\\\\\n5 & 9 & -4\n\\end{vmatrix}}\\]\n\\(~\\)\nExample \\(\\,\\) Evaluate the determinant of the given matrix using the result\n\\[\\begin{vmatrix}\na_1 & a_2 & a_3\\\\\nb_1 & b_2 & b_3\\\\\nc_1 & c_2 & c_3\n\\end{vmatrix} = 5\\]\n1. \\(~\\) \\({\\scriptsize \\mathbf{A} =\n\\begin{pmatrix}\na_3 & a_2 & a_1\\\\\nb_3 & b_2 & b_1\\\\\nc_3 & c_2 & c_1\n\\end{pmatrix}}\\)\n2. \\(~\\) \\({\\scriptsize\\mathbf{A} =\n\\begin{pmatrix}\n2a_1 & a_2 & a_3\\\\\n6b_1 & 3b_2 & 3b_3\\\\\n2c_1 & c_2 & c_3\n\\end{pmatrix}}\\)\n3. \\(~\\) \\({\\scriptsize\\mathbf{A} =\n\\begin{pmatrix}\n4a_1 -2a_3 & a_2 & a_3\\\\\n4b_1 -2b_3 & b_2 & b_3\\\\\n2c_1 -c_3 & \\frac{1}{2}c_2 & \\frac{1}{2}c_3\n\\end{pmatrix}}\\)\n\\(~\\)\nExample \\(\\,\\) Consider the matrix\n\\[{\\mathbf{A} =\n\\begin{pmatrix}\n1 & 1 & 1\\\\\nx & y & z\\\\\ny+z & x+z & x+y\n\\end{pmatrix}}\\]\nWithout expanding, \\(~\\)show that \\(\\mathrm{det}\\, \\mathbf{A}=0\\)\n\\(~\\)\nExample \\(\\,\\) Evaluate\n\\[{\\begin{vmatrix}\n1 & 1 & 1 & 1\\\\\na & b & c & d\\\\\na^2 & b^2 & c^2 & d^2\\\\\na^3 & b^3 & c^3 & d^3\n\\end{vmatrix}}\\]\n\\(~\\)",
    "crumbs": [
      "**First Semester**",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Matrices</span>"
    ]
  },
  {
    "objectID": "ch_08_Matrices.html#sec-8-6",
    "href": "ch_08_Matrices.html#sec-8-6",
    "title": "6  Matrices",
    "section": "6.7 Inverse of a Matrix",
    "text": "6.7 Inverse of a Matrix\n\nIf \\(\\mathbf{A}\\) is an \\(n \\times n\\) matrix and there exists an \\(n \\times n\\) matrix \\(\\mathbf{B}\\) such that\n\\[\\mathbf{A}\\mathbf{B}=\\mathbf{B}\\mathbf{A}=\\mathbf{I}\\]\nthen \\(\\mathbf{A}\\) is said to be nonsingular or invertible and \\(\\mathbf{B}\\) is the inverse of \\(\\mathbf{A}\\)\nAn \\(n \\times n\\) matrix that has no inverse is called singular. If \\(\\mathbf{A}\\) is nonsingular, \\(~\\)its inverse is denoted by \\(\\mathbf{B}=\\mathbf{A}^{-1}\\)\nProperties of the Inverse\nLet \\(\\mathbf{A}\\) and \\(\\mathbf{B}~\\) be nonsingular matrices. Then\n\\[\n\\begin{aligned}\n  \\left(\\mathbf{A}^{-1}\\right)^{-1}&=\\mathbf{A} \\\\\n  \\left(\\mathbf{A}\\mathbf{B}\\right)^{-1}&=\\mathbf{B}^{-1}\\mathbf{A}^{-1} \\\\\n  \\left(\\mathbf{A}^T\\right)^{-1}&=\\left(\\mathbf{A}^{-1}\\right)^T\n\\end{aligned}\\]\nAdjoint Matrix\n\\[\\mathrm{adj}(\\mathbf{A})=\n\\begin{pmatrix}\n   C_{11} & C_{12} & \\cdots & C_{1n}\\\\\n   C_{21} & C_{22} & \\cdots & C_{2n}\\\\\n   \\vdots &        &        & \\vdots\\\\\n   C_{n1} & C_{n2} & \\cdots & C_{nn}\n\\end{pmatrix}^T\\]\nFinding the Inverse\nLet \\(\\mathbf{A}\\) be an \\(n \\times n\\) matrix. \\(~\\)If \\(\\mathrm{det}(\\mathbf{A})\\neq 0\\) (nonsingular), then\n\\[\\mathbf{A}^{-1}=\\frac{\\mathrm{adj}(\\mathbf{A})}{\\mathrm{det}(\\mathbf{A})}\\]\nor\n\n\n\n\n\nUsing the Inverse to Solve Systems\nThe coefficient matrix \\(\\mathbf{A}\\) is \\(n \\times n\\). In particular, \\(~\\)if \\(\\mathbf{A}\\) is nonsingular, \\(~\\)the system \\(\\mathbf{A}\\mathbf{x}=\\mathbf{b}\\) can be solved by\n\\[\\mathbf{x}=\\mathbf{A}^{-1}\\mathbf{b}\\]\nA homogeneous system of \\(n\\) linear equations in \\(~n\\) unknowns \\(\\mathbf{A}\\mathbf{x}=\\mathbf{0}\\) \\(~\\)has\n\nonly the trivial solution if and only if \\(\\mathbf{A}\\) is nonsingular\na nontrivial solution if and only if \\(\\mathbf{A}\\) is singular\n\n\n\\(~\\)\nExample \\(\\,\\) Verify that the matrix \\(\\mathbf{B}\\) is the inverse of the matrix \\(\\mathbf{A}\\)\n\\[\\mathbf{A}=\\begin{pmatrix}\n1 & \\frac{1}{2} \\\\\n2 & \\frac{3}{2}\n\\end{pmatrix}, \\;\\;\n\\mathbf{B}=\\begin{pmatrix}\n\\phantom{-}3 & -1 \\\\\n-4 & \\phantom{-}2\n\\end{pmatrix}\\]\n\\(~\\)\nExample \\(\\,\\) Determine whether the given matrix is singular or nonsingular. \\(\\,\\)If it is nonsingular, find the inverse using \\(\\mathbf{A}^{-1}=\\frac{\\mathrm{adj}\\,\\mathbf{A}}{\\mathrm{det}\\,\\mathbf{A}}\\)\n\\[\\begin{pmatrix}\n3&  0& \\phantom{-}0\\\\\n0&  6& \\phantom{-}0\\\\\n0&  0& -2\n\\end{pmatrix}, \\;\\;\\;\n\\begin{pmatrix}\n0 & -1 & \\phantom{-}1 & 4\\\\\n3 & \\phantom{-}2 & -2 & 1\\\\\n0 & \\phantom{-}4 & \\phantom{-}0 & 1\\\\\n1 & \\phantom{-}0 & -1 & 1\n\\end{pmatrix}, \\;\\;\\;\n\\begin{pmatrix}\n1 & \\phantom{-}0 & 0 & 0\\\\\n0 & \\phantom{-}1 & 0 & 0\\\\\n0 & -2 & 1 & 0\\\\\n0 & -3 & 0 & 1\n\\end{pmatrix}\\]\n\\(~\\)\nExample \\(\\,\\) Find the inverse of the given matrix or show that no inverse exists\n\\[\\begin{pmatrix}\n1 & 2 & 3\\\\\n4 & 5 & 6\\\\\n7 & 8 & 9\n\\end{pmatrix},\\;\\;\n\\begin{pmatrix}\n\\phantom{-}4 & \\phantom{-}2 & 3 \\\\\n\\phantom{-}2 & \\phantom{-}1 & 0\\\\\n-1 & -2 & 0\n\\end{pmatrix}\\]\n\\(~\\)\nExample \\(\\,\\) If \\(\\mathbf{A}\\) is nonsingular, then \\(\\left( \\mathbf{A}^T\\right)^{-1}=\\left( \\mathbf{A}^{-1}\\right)^T\\), \\(~\\)verify this for\n\\[\\mathbf{A}=\\begin{pmatrix}\n1 & 4\\\\\n2 & 10\n\\end{pmatrix}\\]\n\\(~\\)\nExample \\(\\,\\) Find the inverse of the rotation matrix\n\\[\\mathbf{M}=\\begin{pmatrix}\n\\cos\\theta & -\\sin\\theta\\\\\n\\sin\\theta & \\phantom{-}\\cos\\theta\n\\end{pmatrix}\\]\nWhat does \\(\\mathbf{A}=\\mathbf{M}^{-1}\\mathbf{B}\\) represents?\n\\(~\\)\nExample \\(\\,\\) A nonsingular matrix \\(\\mathbf{A}\\) is said to be orthogonal if \\(\\mathbf{A}^{-1}=\\mathbf{A}^T\\)\n1. \\(~\\) Verify tha the rotation matrix is orthogonal\n2. \\(~\\) Verify that \\[{\\scriptsize\\mathbf{A}=\\frac{1}{\\sqrt{6}}\\begin{pmatrix}\n\\sqrt{2} & \\phantom{-}0 & -2 \\\\\n\\sqrt{2} & \\phantom{-}\\sqrt{3} & \\phantom{-}1\\\\\n\\sqrt{2} &  -\\sqrt{3} & \\phantom{-}1\n\\end{pmatrix}}~\\] is an orthogonal matrix\n\\(~\\)\nExample \\(\\,\\) Answer the questions based on the supposition\n\nShow that if \\(\\mathbf{A}\\) is an orthogonal matrix, \\(~\\)then \\(\\mathrm{det}\\,\\mathbf{A}=\\pm 1\\)\nSuppose \\(\\mathbf{A}\\) and \\(\\mathbf{B}~\\) are nonsingular \\(~n\\times n\\) matrices. Then show that \\(\\mathbf{AB}~\\) is nonsingular\nSuppose \\(\\mathbf{A}\\) and \\(\\mathbf{B}~\\) are \\(~n\\times n\\) matrices and that either \\(\\mathbf{A}\\) or \\(\\mathbf{B}~\\) is singular. \\(~\\)Then show that \\(\\mathbf{AB}~\\) is singular\nSuppose \\(\\mathbf{A}~\\) is a nonsingular matrix. \\(~\\)Then show that \\(\\displaystyle\\mathrm{det}\\,\\mathbf{A}^{-1}=\\frac{1}{\\mathrm{det}\\,\\mathbf{A}}\\)\nSuppose \\(\\mathbf{A}^2=\\mathbf{A}\\). \\(~\\)Then show that either \\(\\mathbf{A}=\\mathbf{I}~\\) or \\(~\\mathbf{A}~\\) is singular\nSuppose \\(\\mathbf{A}\\) and \\(\\mathbf{B}~\\) are \\(~n\\times n\\) matrices. \\(~\\mathbf{A}~\\) is nonsingular, \\(~\\) and \\(\\mathbf{AB}=\\mathbf{0}\\). \\(~\\)Then show that \\(\\mathbf{B}=\\mathbf{0}\\)\nSuppose \\(\\mathbf{A}\\) and \\(\\mathbf{B}~\\) are \\(~n\\times n\\) matrices. \\(~\\mathbf{A}~\\) is nonsingular, \\(~\\) and \\(\\mathbf{AB}=\\mathbf{AC}\\). \\(~\\)Then show that \\(\\mathbf{B}=\\mathbf{C}\\)\nSuppose \\(\\mathbf{A}\\) and \\(\\mathbf{B}~\\) are nonsingular \\(~n\\times n\\) matrices. Is \\(~\\mathbf{A} +\\mathbf{B}~\\) necessarily nonsingular?\nSuppose \\(\\mathbf{A}~\\) is a nonsingular matrix. \\(~\\)Then show that \\(\\mathbf{A}^T~\\) is nonsingular\nSuppose \\(\\mathbf{A}\\) and \\(\\mathbf{B}~\\) are \\(~n\\times n~\\) nonzero matrices and \\(\\mathbf{AB}=\\mathbf{0}\\). \\(~\\)Then show that both \\(\\mathbf{A}~\\) and \\(~\\mathbf{B~}\\) are singular",
    "crumbs": [
      "**First Semester**",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Matrices</span>"
    ]
  },
  {
    "objectID": "ch_08_Matrices.html#sec-8-7",
    "href": "ch_08_Matrices.html#sec-8-7",
    "title": "6  Matrices",
    "section": "6.8 Cramer’s Rule",
    "text": "6.8 Cramer’s Rule\nIf \\(\\mathrm{det}(\\mathbf{A}) \\neq 0\\), \\(~\\)the solution of the system is given by\n\\[x_k=\\frac{\\mathrm{det}(\\mathbf{A}_k)}{\\mathrm{det}(\\mathbf{A})}, \\;\\;{\\scriptsize k=1, 2, \\cdots, n}\\]\nwhere\n\\[{\\scriptsize\\mathbf{A}_k=\n  \\begin{pmatrix}\n    a_{11} & \\cdots & a_{1k-1} & {\\color{red} {b_1}}    & a_{1k+1} & \\cdots & a_{1n}\\\\\n    a_{21} & \\cdots & a_{2k-1} & {\\color{red} {b_2}}    & a_{2k+1} & \\cdots & a_{2n} \\\\\n    \\vdots &        & \\vdots   & {\\color{red} {\\vdots}} & \\vdots   &        & \\vdots\\\\\n    a_{n1} & \\cdots & a_{nk-1} & {\\color{red} {b_n}}    & a_{nk+1} & \\cdots & a_{nn}\n  \\end{pmatrix}}\\]\n\\(~\\)\nExample \\(\\,\\) Solve the given system of equations by Cramer’s rule\n\\[\n  \\begin{aligned}\n   -3x_1 + x_2 &= 3 \\\\\n    2x_1 -4x_2 &= -6 \\\\ \\\\\n    0.1x_1 -0.4x_2 &= 0.13 \\\\\n    x_1 -x_2 &= 0.4 \\\\ \\\\\n    2x + y &=1\\\\\n    3x +2y &=-2\n  \\end{aligned}\\]\n\\(~\\)\nExample \\(\\,\\) Consider the system\n\\[\n  \\begin{aligned}\n    x_1 + x_2 &= 1 \\\\\n    x_1 +\\epsilon x_2 &= 2\n  \\end{aligned}\\]\nWhen \\(\\epsilon\\) is close to \\(1\\), \\(~\\)the lines that make up the system are almost parallel\n1. Use Cramer’s rule to show that a solution of the system is\n\\[{\\scriptsize x_1 = 1 -\\frac{1}{\\epsilon -1}, \\;\\;x_2 = \\frac{1}{\\epsilon - 1}}\\]\n2. The system is said to be ill-conditioned since small changes in the input data(for example, the coefficients) causes a significant or large change in the output or solution. \\(~\\)Verify this by finding the solution of the system for \\(\\epsilon=1.01\\) and then for \\(\\epsilon=0.99\\)\n\\(~\\)",
    "crumbs": [
      "**First Semester**",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Matrices</span>"
    ]
  },
  {
    "objectID": "ch_08_Matrices.html#sec-8-8",
    "href": "ch_08_Matrices.html#sec-8-8",
    "title": "6  Matrices",
    "section": "6.9 The Eigenvalue Problem",
    "text": "6.9 The Eigenvalue Problem\n\nLet \\(\\mathbf{A}\\) be an \\(n \\times n~\\) matrix. \\(~\\)A number \\(\\lambda\\) is said to be an eigenvalue of \\(\\mathbf{A}\\) if there exists a nonzero solution vector \\(\\mathbf{k}\\) of the linear system\n\\[\\mathbf{A}\\mathbf{k}=\\lambda\\mathbf{k}\\]\nand the solution vector \\(\\mathbf{k}\\) is said to be an eigenvector corresponding to the eigenvalue \\(\\lambda\\)\nThe problem of solving \\(~\\mathbf{A}\\mathbf{k}=\\lambda\\mathbf{k}~\\) for nonzero vectors \\(\\mathbf{k}\\) is called to be the eigenvalue problem for \\(\\mathbf{A}\\)\nWe must solve the characteristic equation \\(~\\mathrm{det}(\\mathbf{A} -\\lambda\\mathbf{I})=0~\\) to find an eigenvalue \\(\\lambda\\)\nTo find an eigenvector \\(\\mathbf{k}\\) corresponding to an eigenvalue \\(\\lambda\\), \\(~\\)we solve \\(~(\\mathbf{A} -\\lambda\\mathbf{I})\\mathbf{k}=\\mathbf{0}~\\) by applying Gauss elimination to \\(~(\\mathbf{A} -\\lambda\\mathbf{I}|\\mathbf{0})\\)\n\n\\(~\\)\nExample \\(\\,\\) Find the eigenvalues and eigenvectors of\n\\[\\mathbf{A}=\n  \\left(\\begin{array}{rrr}\n    1 & 2 &  1\\\\\n    6 &-1 &  0\\\\\n   -1 &-2 & -1\n  \\end{array}\\right)   \n  \\]\nTo find the eigenvalues, \\(~\\) we solve\n\\[{\\scriptsize\n  \\mathrm{det} (\\mathbf{A} -\\lambda\\mathbf{I}) =\n  \\begin{vmatrix}\n    1 -\\lambda & \\;\\;\\,2 & \\;\\;\\,1\\\\\n    6 & -1 -\\lambda & \\;\\;\\,0\\\\\n    -1\\;\\;\\, & -2 & -1 -\\lambda\n  \\end{vmatrix}=0}\\]\nIt follows that the characteristic equation is\n\\[~-\\lambda^3 -\\lambda^2 +12\\lambda=-\\lambda(\\lambda+4)(\\lambda-3)=0\\]\nHence the eigenvalues are \\(~\\lambda_1=-4\\), \\(\\,\\lambda_2=0\\), \\(\\,\\lambda_3=3\\)\nFor \\(\\lambda_1=-4\\), \\(~\\)we have\n\\[\n  (\\mathbf{A} +4\\mathbf{I}|\\mathbf{0}) = {\\scriptsize\n  \\left(\\begin{array}{rrr|r}\n    5 & 2 &  1 & 0\\\\\n    6 & 3 &  0 & 0\\\\\n   -1 &-2 &  3 & 0\n  \\end{array}\\right)\n  \\overset{\\;\\text{row operations}\\;}{\\Longrightarrow}\n  \\left(\\begin{array}{rrr|r}\n    1 & 0 & 1 & 0\\\\\n    0 & 1 &-2 & 0\\\\ \\hline\n    0 & 0 & 0 & 0\n  \\end{array}\\right)}\\]\nThus \\(k_1=-k_3\\), \\(\\text{ }k_2=2k_3\\). Choosing \\(\\text{ }k_3=1\\) gives the eigenvector\n\\[\\mathbf{k}_1=\n  \\left(\\begin{array}{r}\n    -1\\\\\n     2\\\\\n     1\n  \\end{array}\\right)\\]\nFor \\(\\lambda_2=0\\), \\(~\\)we have\n\\[\n  (\\mathbf{A} -0\\mathbf{I}|\\mathbf{0}) = {\\scriptsize\n  \\left(\\begin{array}{rrr|r}\n    1 & 2 &  1 & 0\\\\\n    6 &-1 &  0 & 0\\\\\n   -1 &-2 & -1 & 0\n  \\end{array}\\right)\n  \\overset{\\text{row operations}}{\\Longrightarrow}\n  \\left(\\begin{array}{rrr|r}\n    1 & 0 & \\frac{1}{13} & 0\\\\\n    0 & 1 & \\frac{6}{13} & 0\\\\ \\hline\n    0 & 0 & 0\\; & 0\n  \\end{array}\\right)}\\]\nThus \\(k_1=-\\frac{1}{13}k_3\\), \\(\\text{ }k_2=-\\frac{6}{13}k_3\\). \\(~\\)Choosing \\(~k_3=1\\) gives the eigenvector\n\\[\\mathbf{k}_2=\n  \\left(\\begin{array}{r}\n    -\\frac{1}{13}\\\\\n    -\\frac{6}{13}\\\\\n    1\\;\n  \\end{array}\\right)\\]\nFor \\(\\lambda_3=3\\), \\(~\\) we have\n\\[\n  (\\mathbf{A} -3\\mathbf{I}|\\mathbf{0}) = {\\scriptsize\n  \\left(\\begin{array}{rrr|r}\n   -2 & 2 &  1 & 0\\\\\n    6 &-4 &  0 & 0\\\\\n   -1 &-2 & -4 & 0\n  \\end{array}\\right)\n  \\overset{\\text{row operations}}{\\Longrightarrow}\n  \\left(\\begin{array}{rrr|r}\n    1 & 0 & 1 & 0\\\\\n    0 & 1 & \\frac{3}{2} & 0\\\\ \\hline\n    0 & 0 & 0 & 0\n  \\end{array}\\right)}\\]\nThus \\(k_1=-k_3\\), \\(\\text{ }k_2=-\\frac{3}{2}k_3\\). \\(~\\)Choosing \\(\\text{ }k_3=1\\) gives the eigenvector\n\\[\\mathbf{k}_3=\n  \\left(\\begin{array}{r}\n   -1\\\\\n   -\\frac{3}{2}\\\\\n    1\n  \\end{array}\\right)\\]\n\nimport sympy\nsympy.init_printing()\n\nA = sympy.Matrix([[1, 2, 1], [6, -1, 0], [-1, -2, -1]])\n\nA.eigenvects()\n\n\\(\\displaystyle \\left[ \\left( -4, \\  1, \\  \\left[ \\left[\\begin{matrix}-1\\\\2\\\\1\\end{matrix}\\right]\\right]\\right), \\  \\left( 0, \\  1, \\  \\left[ \\left[\\begin{matrix}- \\frac{1}{13}\\\\- \\frac{6}{13}\\\\1\\end{matrix}\\right]\\right]\\right), \\  \\left( 3, \\  1, \\  \\left[ \\left[\\begin{matrix}-1\\\\- \\frac{3}{2}\\\\1\\end{matrix}\\right]\\right]\\right)\\right]\\)\n\n\n\\(~\\)\nExample \\(\\,\\) Find the eigenvalues and eigenvectors of\n\\[\\mathbf{A}=\n  \\left(\\begin{array}{rrr}\n    9 & 1 & 1\\\\\n    1 & 9 & 1\\\\\n    1 & 1 & 9\n  \\end{array}\\right)\\]\nThe characteristic equation\n\\[{\\scriptsize\n  \\mathrm{det} (\\mathbf{A} -\\lambda\\mathbf{I}) =\n  \\begin{vmatrix}\n    9 -\\lambda & 1 & 1\\\\\n    1 & 9 -\\lambda & 1\\\\\n    1 & 1 & 9 -\\lambda\n  \\end{vmatrix}=-(\\lambda-11)(\\lambda-8)^2=0}\\]\nshows that \\(\\lambda_1=11\\) and that \\(\\lambda_2=\\lambda_3=8\\) is an eigenvalue of multiplicity 2\nFor \\(\\lambda_1=11\\), \\(~\\)we have\n\\[{\\scriptsize\n  (\\mathbf{A} -11\\mathbf{I}|\\mathbf{0}) =\n  \\left(\\begin{array}{rrr|r}\n   -2 & 1 &  1 & 0\\\\\n    1 &-2 &  1 & 0\\\\\n    1 & 1 & -2 & 0\n  \\end{array}\\right)\n  \\overset{\\text{row operations}}{\\Longrightarrow}\n  \\left(\\begin{array}{rrr|r}\n    1 & 0 & -1 & 0\\\\\n    0 & 1 & -1 & 0\\\\ \\hline\n    0 & 0 & 0 & 0\n  \\end{array}\\right)}\\]\nThus \\(k_1=k_3\\), \\(k_2=k_3\\). \\(~\\)Choosing \\(k_3=1\\) gives the eigenvector\n\\[\\mathbf{k}_1=\n  \\left(\\begin{array}{r}\n    1\\\\\n    1\\\\\n    1\n  \\end{array}\\right)\\]\nFor \\(\\lambda_2=8\\), \\(~\\)we have\n\\[{\\scriptsize\n  (\\mathbf{A} -8\\mathbf{I}|\\mathbf{0}) =\n  \\left(\\begin{array}{rrr|r}\n    1 & 1 &  1 & 0\\\\\n    1 & 1 &  1 & 0\\\\\n    1 & 1 &  1 & 0\n  \\end{array}\\right)\n  \\overset{\\text{row operations}}{\\Longrightarrow}\n  \\left(\\begin{array}{rrr|r}\n    1 & 1 & 1 & 0\\\\ \\hline\n    0 & 0 & 0 & 0\\\\\n    0 & 0 & 0 & 0\n  \\end{array}\\right)}\\]\nHere \\(k_1 +k_2 +k_3=0\\), \\(~\\)we are free to select two of the variables arbitrarily\nChoosing, \\(\\,\\) on the one hand, \\(~k_2=1\\), \\(\\,k_3=0\\), and on the other, \\(~k_2=0\\), \\(\\,k_3=1\\), \\(\\,\\) we obtain two linearly independent eigenvectors\n\\[\\mathbf{k}_2=\n  \\left(\\begin{array}{r}\n   -1\\\\\n    1\\\\\n    0\n  \\end{array}\\right) \\text{ and }\n  \\mathbf{k}_3=\n  \\left(\\begin{array}{r}\n   -1\\\\\n    0\\\\\n    1\n  \\end{array}\\right)\\]\ncorresponding to a single eigenvalue\nIf instead we choose \\(k_2=1\\), \\(k_3=1\\) and then \\(k_2=1\\), \\(k_3=-1\\), \\(~\\)we obtain, respectively, two entirely different but orthogonal eigenvectors\n\\[\\mathbf{k}_2=\n  \\left(\\begin{array}{r}\n   -2\\\\\n    1\\\\\n    1\n  \\end{array}\\right) \\text{ and }\n  \\mathbf{k}_3=\n  \\left(\\begin{array}{r}\n    0\\\\\n    1\\\\\n   -1\n  \\end{array}\\right)\\]\n\nA = sympy.Matrix([[9, 1, 1], [1, 9, 1], [1, 1, 9]])\nA.eigenvects()\n\n\\(\\displaystyle \\left[ \\left( 8, \\  2, \\  \\left[ \\left[\\begin{matrix}-1\\\\1\\\\0\\end{matrix}\\right], \\  \\left[\\begin{matrix}-1\\\\0\\\\1\\end{matrix}\\right]\\right]\\right), \\  \\left( 11, \\  1, \\  \\left[ \\left[\\begin{matrix}1\\\\1\\\\1\\end{matrix}\\right]\\right]\\right)\\right]\\)\n\n\n\\(~\\)\nExample \\(\\,\\) Find the eigenvalues and eigenvectors of\n\\[\\mathbf{A}=\n   \\left(\\begin{array}{rr}\n     3 & 4 \\\\\n    -1 & 7\n   \\end{array}\\right)\\]\nFrom the characteristic equation\n\\[\\scriptsize\n   \\mathrm{det}(\\mathbf{A} -\\lambda\\mathbf{I})=\n   \\left|\\begin{array}{cc}\n     3-\\lambda & 4 \\\\\n    -1 & 7 -\\lambda\n   \\end{array}\\right|\n   =(\\lambda -5)^2=0\\]\nwe see \\(\\lambda_1=\\lambda_2=5\\) is an eigenvalue of algebraic multiplicity 2\nTo find the eigenvector(s) corresponding to \\(\\lambda_1=5\\), \\(~\\)we resort to the system \\((\\mathbf{A} -5\\mathbf{I}|\\mathbf{0})\\)\n\\[{\\scriptsize\n  (\\mathbf{A} -5\\mathbf{I}|\\mathbf{0}) =\n  \\left(\\begin{array}{rr|r}\n   -2 & 4 & 0\\\\\n   -1 & 2 & 0\n  \\end{array}\\right)\n  \\overset{\\text{row operations}}{\\Longrightarrow}\n  \\left(\\begin{array}{rr|r}\n    1 &-2 & 0\\\\ \\hline\n    0 & 0 & 0\n  \\end{array}\\right)}\\]\nThus \\(k_1=2k_2\\). \\(~\\)If we choose \\(k_2=1\\), \\(\\,\\) we find the single eigenvector\n\\[\\mathbf{k}_1=\\begin{pmatrix}\n     2 \\\\ 1\n   \\end{pmatrix}\\]\nWe define the geometric multiplicity of an eigenvalue to be the number of linearly independent eigenvectors for the eigenvalue\nWhen the geometric multiplicity of an eigenvalue is less than the algebraic multiplicity, \\(~\\) we say the matrix is defective. \\(~\\) In the case of defective matrices, \\(~\\) we must search for additional system\n\\[{\\scriptsize\n  (\\mathbf{A} -5\\mathbf{I}|\\mathbf{k}_1) =\n  \\left(\\begin{array}{rr|r}\n   -2 & 4 & 2\\\\\n   -1 & 2 & 1\n  \\end{array}\\right)\n  \\overset{\\text{row operations}}{\\Longrightarrow}\n  \\left(\\begin{array}{rr|r}\n    1 &-2 & -1\\\\ \\hline\n    0 & 0 & 0\n  \\end{array}\\right) }\\]\nThus \\(k_1-2k_2=-1\\). \\(~\\)If we choose \\(k_2=0\\), \\(~\\)we find the generalized eigenvector\n\\[\\mathbf{k}_2=\\begin{pmatrix}\n     -1 \\\\ \\;\\;0\n   \\end{pmatrix}\\]\n\nA = sympy.Matrix([[3, 4], [-1, 7]])\nA.eigenvects()\n\n\\(\\displaystyle \\left[ \\left( 5, \\  2, \\  \\left[ \\left[\\begin{matrix}2\\\\1\\end{matrix}\\right]\\right]\\right)\\right]\\)\n\n\n\nlambda1 = A.eigenvects()[0][0]\nk1 = A.eigenvects()[0][2][0]\nA1 = A -lambda1 *sympy.eye(2)\nA1.pinv_solve(k1)\n\n\\(\\displaystyle \\left[\\begin{matrix}\\frac{4 w_{0 0}}{5} + \\frac{2 w_{1 0}}{5} - \\frac{1}{5}\\\\\\frac{2 w_{0 0}}{5} + \\frac{w_{1 0}}{5} + \\frac{2}{5}\\end{matrix}\\right]\\)\n\n\n\\(~\\)\n\nLet \\(\\mathbf{A}\\) be a square matrix with real entries. If \\(\\lambda=\\alpha +i\\beta\\), \\(~\\beta \\neq 0\\), \\(~\\)is a complex eigenvalue of \\(\\mathbf{A}\\),\n\\[\\mathbf{A}\\bar{\\mathbf{k}}=\\bar{\\lambda}\\bar{\\mathbf{k}}\\]\n\\(\\lambda=0~\\) is an eigenvalue of \\(~\\mathbf{A}\\) if and only if \\(~\\mathbf{A}\\) is singular\nIf \\(~\\lambda~\\) is an eigenvalue of nonsingular \\(~\\mathbf{A}\\) with eigenvector \\(~\\mathbf{k}\\), \\(~1/\\lambda\\) is an eigenvalue of \\(~\\mathbf{A}^{-1}\\) with the same eigenvector \\(~\\mathbf{k}\\)\nThe eigenvalues of an upper triangular, \\(~\\)lower triangular, \\(~\\)and diagonal matrix are the main diagonal entries\n\n\\(~\\)\nExample \\(\\,\\) Find the eigenvalues and eigenvectors of the given matrix. \\(~\\)State whether the matrix is singular or nonsingular\n\\[\\begin{pmatrix}\n-1& 2\\\\\n-7& 8\n\\end{pmatrix}, \\;\\;\n\\begin{pmatrix}\n4 & \\phantom{-}8 \\\\\n0 & -5\n\\end{pmatrix}, \\;\\;\n\\begin{pmatrix}\n0 & 0 & -1 \\\\\n1 & 0 & \\phantom{-}0 \\\\\n1 & 1 & -1\n\\end{pmatrix}\\]\n\\(~\\)\nExample \\(\\,\\) Find the eigenvalues and eigenvectors of the given nonsingular matrix \\(\\mathbf{A}\\). \\(~\\)Then without finding \\(\\mathbf{A}^{-1}\\), \\(~\\)find its eigenvalues and corresponding eigenvectors\n\\[\\begin{pmatrix}\n5& 1\\\\\n1& 5\n\\end{pmatrix}, \\;\\;\n\\begin{pmatrix}\n4 & 2 & -1 \\\\\n0 & 3 & -2 \\\\\n0 & 0 & \\phantom{-}5\n\\end{pmatrix}\\]\n\\(~\\)\nExample \\(\\,\\) True or False: \\(~\\) If \\(\\lambda\\) is an eigenvalue of an \\(n \\times n~\\) matrix \\(\\mathbf{A}\\), \\(~\\) then the matrix \\(\\mathbf{A}-\\lambda\\mathbf{I}~\\) is singular. Justify your answer\n\\(~\\)\nExample \\(\\,\\) Suppose \\(\\lambda\\) is an eigenvalue with corresponding eigenvector \\(~\\mathbf{k}~\\) of an \\(n\\times n~\\) matrix \\(\\mathbf{A}\\)\n1. If \\(\\mathbf{A}^2=\\mathbf{AA}\\), \\(~\\) then show that \\(\\mathbf{A}^2\\mathbf{k}=\\lambda^2\\mathbf{k}\\). \\(~\\) Explain the meaning of the last equation\n2. Verify the result obtained in part 1 for the matrix\n\\[\\mathbf{A}=\\begin{pmatrix}\n2 & 3\\\\\n5 & 4\n\\end{pmatrix}\\]\n3. Generalize the result in part 1\n\\(~\\)\nExample \\(\\,\\) Let \\(\\mathbf{A}\\) and \\(\\mathbf{B}\\) be \\(n \\times n~\\) matrices. The matrix \\(\\mathbf{B}\\) is said to be similar to the matrix \\(\\mathbf{A}~\\) if there exists a nonsingular matrix \\(\\mathbf{S}\\) such that \\(\\mathbf{B}=\\mathbf{S}^{-1}\\mathbf{AS}\\). \\(~\\)If \\(\\mathbf{B}\\) is similar to \\(\\mathbf{A}\\), \\(~\\)then show that \\(\\mathbf{A}\\) is similar to \\(\\mathbf{B}\\)\n\\(~\\)\nExample \\(\\,\\) Suppose \\(\\mathbf{A}\\) and \\(\\mathbf{B}\\) are similar matrices. Show that \\(\\mathbf{A}\\) and \\(\\mathbf{B}\\) have the same eigenvalues\n\\(~\\)",
    "crumbs": [
      "**First Semester**",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Matrices</span>"
    ]
  },
  {
    "objectID": "ch_08_Matrices.html#sec-8-9",
    "href": "ch_08_Matrices.html#sec-8-9",
    "title": "6  Matrices",
    "section": "6.10 Powers of Matrices",
    "text": "6.10 Powers of Matrices\n\nCayley-Hamilton Theorem\nIf \\((-1)^n \\lambda^n +c_{n-1}\\lambda^{n-1} + \\cdots +c_1 \\lambda +c_0 = 0~\\) is the characteristic equation of \\(n \\times n\\) matrix \\(\\mathbf{A}\\), \\(~\\)then\n\\[(-1)^n \\mathbf{A}^n +c_{n-1}\\mathbf{A}^{n-1} + \\cdots +c_1 \\mathbf{A} +c_0 \\mathbf{I} = \\mathbf{0}\\]\nAnd we can write\n\\[\\mathbf{A}^m = a_{n-1}\\mathbf{A}^{n-1} +a_{n-2}\\mathbf{A}^{n-2} + \\cdots +a_1 \\mathbf{A} +a_0 \\mathbf{I} \\]\nand the equation for the eigenvalues\n\\[\\lambda^m = a_{n-1}\\lambda^{n-1} +a_{n-2}\\lambda^{n-2} + \\cdots +a_1 \\lambda +a_0\\]\nhold for the same constants\n\n\\(~\\)\nExample \\(\\,\\) Verify that the given matrix satisfies its own characteristic equation\n\\[~\\mathbf{A}=\\begin{pmatrix}\n1 & -2\\\\\n4 & \\phantom{-}5\n\\end{pmatrix}\\]\n\\(~\\)\nExample \\(\\,\\) Compute \\(\\mathbf{A}^m\\)\n\\[\\mathbf{A}=\\begin{pmatrix}\n8 & 5\\\\\n4 & 0\n\\end{pmatrix}; \\;\\;m=5,\\;\\;\n\\mathbf{A}=\\begin{pmatrix}\n1 & 1 & 1\\\\\n0 & 1 & 2\\\\\n0 & 1 & 0\n\\end{pmatrix}; \\;\\;m=10\\]\n\\(~\\)\nExample \\(\\,\\) Show that the given matrix has an eigenvalue \\(\\lambda_1\\) of multiplicity two. \\(~\\) As a consequence, \\(~\\)the equation \\(\\lambda^m=c_0+c_1\\lambda\\) does not yield enough independent equations to form a system for determining the coefficients \\(c_i\\). \\(~\\)Use the derivative (with respect to \\(\\lambda\\)) of this equation evaluated at \\(\\lambda_1\\) as the extra needed equation to form a system. \\(~\\)Compute \\(\\mathbf{A}^m\\) and use this result to compute the indicated power of the matrix \\(\\mathbf{A}\\)\n\\[\\mathbf{A}=\\begin{pmatrix}\n\\phantom{-}7 & 3\\\\\n-3 & 1\n\\end{pmatrix}; \\;\\;m=6\\]\n\\(~\\)\nExample \\(\\,\\) \\(\\lambda=0~\\) is an eigenvalue of each matrix. \\(~\\)In this case, \\(~\\) show that the coefficient \\(c_0\\) in the characteristic equation\n\\[(-1)^n\\mathbf{A}^n + c_{n-1}\\mathbf{A}^{n-1}+\\cdots+c_1\\mathbf{A}+c_0\\mathbf{I}=\\mathbf{0}\\]\nis \\(0\\). \\(~\\)Compute \\(\\mathbf{A}^m\\) in each case. \\(~\\)In part (a), \\(~\\)explain why we do not have to solve any system for the coefficients \\(c_1\\) in determining \\(\\mathbf{A}^m\\)\n\\[(a)\\;\\;\\mathbf{A}=\\begin{pmatrix}\n1 & 1\\\\\n3 & 3\n\\end{pmatrix},\\;\\;(b)\\;\\;\n\\mathbf{A}=\\begin{pmatrix}\n2 & 1 & \\phantom{-}1\\\\\n1 & 0 & -2\\\\\n1 & 1 & \\phantom{-}3\n\\end{pmatrix}\\]\n\\(~\\)\nExample \\(\\,\\) A non-zero \\(n \\times n\\) matrix \\(\\mathbf{A}~\\) is said to be nilpotent of index \\(m\\) \\(~\\)if \\(m\\) is the smallest positive integer for which \\(\\mathbf{A}^m=\\mathbf{0}\\).\n1. \\(~\\)Explain why any nilpotent matrix \\(~\\mathbf{A}\\) is singular\n2. \\(~\\)Show that all the eigenvalues of a nilpotent matrix \\(~\\mathbf{A}\\) are \\(0\\)",
    "crumbs": [
      "**First Semester**",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Matrices</span>"
    ]
  },
  {
    "objectID": "ch_08_Matrices.html#sec-8-10",
    "href": "ch_08_Matrices.html#sec-8-10",
    "title": "6  Matrices",
    "section": "6.11 Orthogonal Matrices",
    "text": "6.11 Orthogonal Matrices\n\nLet \\(\\mathbf{A}\\) be a symmetric matrix (\\(\\mathbf{A}=\\mathbf{A}^T\\)) with real entries. Then the eigenvalues of \\(\\mathbf{A}\\) are real\nLet \\(\\mathbf{A}\\) be a symmetric matrix. Then eigenvectors corresponding to distinct(different) eigenvalues are orthogonal\nAn \\(n \\times n\\) matrix \\(\\mathbf{A}\\) is orthogonal (\\(\\mathbf{A}^{-1}=\\mathbf{A}^T\\)) \\(~\\)if and only if its columns \\(\\mathbf{x}_1,\\) \\(\\mathbf{x}_2,\\) \\(\\cdots,\\) \\(\\mathbf{x}_n\\) form an orthonormal set\n\\[\\mathbf{x}_i \\cdot \\mathbf{x}_j=0, \\;i \\neq j \\; \\text{and} \\;\\mathbf{x}_i \\cdot \\mathbf{x}_i=1\\]\nIt may not be possible to find \\(n\\) linearly independent eigenvectors for an \\(n \\times n\\) matrix \\(\\mathbf{A}\\) when some of eigenvalues are repeated (defective matrix)\nBut a symmetric matrix is an exception. \\(~\\)It can be proved that a set of \\(n\\) linearly independent eigenvectors can always be found for an \\(n \\times n\\) symmetric matrix \\(\\mathbf{A}\\) even there is some repetition of the eigenvalues\nHowever, \\(~\\)this does not mean that all eigenvectors are mutually orthogonal for an \\(n \\times n\\) symmetric matrix \\(\\mathbf{A}\\). The set of eigenvectors corresponding to distinct eigenvalues are orthogonal; \\(~\\)but different eigenvectors corresponding to a repeated eigenvalue may not be orthogonal\nBut it is always possible to find or construct a set of \\(n\\) mutually orthogonal eigenvectors by using Gram-Schmidt orthogonalization. See Section 9.7\n\n\\(~\\)\nExample \\(\\,\\) Construct an orthogonal matrix from the eigenvectors of\n\\[\\mathbf{A}=\n  \\left(\\begin{array}{rrr}\n    7 & 4 & -4\\\\\n    4 &-8 & -1\\\\\n   -4 &-1 & -8\n  \\end{array}\\right)\\]\n\nA = sympy.Matrix([[7, 4, -4], [4, -8, -1], [-4, -1, -8]])\nA.eigenvects()\n\n\\(\\displaystyle \\left[ \\left( -9, \\  2, \\  \\left[ \\left[\\begin{matrix}- \\frac{1}{4}\\\\1\\\\0\\end{matrix}\\right], \\  \\left[\\begin{matrix}\\frac{1}{4}\\\\0\\\\1\\end{matrix}\\right]\\right]\\right), \\  \\left( 9, \\  1, \\  \\left[ \\left[\\begin{matrix}-4\\\\-1\\\\1\\end{matrix}\\right]\\right]\\right)\\right]\\)\n\n\n\nv1 = A.eigenvects()[0][2][0].T\nv2 = A.eigenvects()[0][2][1].T\nv3 = A.eigenvects()[1][2][0].T\n\nB = sympy.Matrix(sympy.GramSchmidt([v1, v2, v3], orthonormal=True)).T; B\n\n\\(\\displaystyle \\left[\\begin{matrix}- \\frac{\\sqrt{17}}{17} & \\frac{2 \\sqrt{34}}{51} & - \\frac{2 \\sqrt{2}}{3}\\\\\\frac{4 \\sqrt{17}}{17} & \\frac{\\sqrt{34}}{102} & - \\frac{\\sqrt{2}}{6}\\\\0 & \\frac{\\sqrt{34}}{6} & \\frac{\\sqrt{2}}{6}\\end{matrix}\\right]\\)\n\n\n\nB.T *B\n\n\\(\\displaystyle \\left[\\begin{matrix}1 & 0 & 0\\\\0 & 1 & 0\\\\0 & 0 & 1\\end{matrix}\\right]\\)\n\n\n\\(~\\)\nExample \\(\\,\\) Determine whether the given matrix is orthogonal\n\\[\\begin{pmatrix}\n0 & 1 & 0\\\\\n1 & 0 & 0\\\\\n0 & 0 & 1\n\\end{pmatrix}, \\;\\;\n\\begin{pmatrix}\n\\phantom{-}0 & 0 & 1\\\\\n-\\frac{12}{13}& \\frac{5}{13} & 0\\\\\n\\phantom{-}\\frac{5}{13}&  \\frac{12}{13}& 0\n\\end{pmatrix}\\]\n\\(~\\)\nExample \\(\\,\\) (a)\\(~\\) Verify that the indicated column vectors are eigenvectors of the given symmetric matrix and \\(~\\)(b)\\(~\\) identify the corresponding eigenvalues. \\(~\\)(c)\\(~\\) Use Gram-Schmidt process to construct an orthogonal matrix \\(\\mathbf{P}\\) from the eigenvectors\n\\[\\mathbf{A}=\n\\begin{pmatrix}\n0 & 2 & 2\\\\\n2 & 0 & 2\\\\\n2 & 2 & 0\n\\end{pmatrix}; \\;\\; \\mathbf{k}_1=\\begin{pmatrix}\n\\phantom{-}1\\\\ -1\\\\ \\phantom{-}0\n\\end{pmatrix}, \\;\\; \\mathbf{k}_2=\\begin{pmatrix}\n\\phantom{-}1\\\\ \\phantom{-}0\\\\ -1\n\\end{pmatrix}, \\;\\; \\mathbf{k}_3 = \\begin{pmatrix}\n1 \\\\ 1 \\\\ 1\n\\end{pmatrix}\\]\n\\(~\\)\nExample \\(\\,\\) Answer the questions based on the supposition\n\nSuppose \\(\\mathbf{A}\\) and \\(\\mathbf{B}~\\) are \\(~n\\times n\\) orthogonal matrices. Then show that \\(\\mathbf{AB}~\\) is orthogonal\nSuppose \\(\\mathbf{A}\\) is an orthogonal matrix. \\(~\\)Is \\(\\mathbf{A}^2~\\) is orthogonal\nSuppose \\(\\mathbf{A}~\\) is an orthogonal matrix. \\(~\\)Then show that \\(\\mathbf{A}^{-1}\\) is orthogonal\nSuppose \\(\\mathbf{A}~\\) is an orthogonal matrix such that \\(~\\mathbf{A}^2=\\mathbf{I}~\\). \\(~\\)Then show that \\(\\mathbf{A}^T=\\mathbf{A}\\)\nShow that the rotation matrix is orthogonal\n\n\\(~\\)",
    "crumbs": [
      "**First Semester**",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Matrices</span>"
    ]
  },
  {
    "objectID": "ch_08_Matrices.html#sec-8-11",
    "href": "ch_08_Matrices.html#sec-8-11",
    "title": "6  Matrices",
    "section": "6.12 Approximation of Eigenvalues",
    "text": "6.12 Approximation of Eigenvalues\n\nLet \\(\\lambda_1\\), \\(\\lambda_2\\), \\(\\cdots\\), \\(\\lambda_k\\), \\(\\cdots\\), \\(\\lambda_n\\) denote the eigenvalues of an \\(n \\times n\\) matrix \\(\\mathbf{A}\\). The eigenvalue \\(\\lambda_k\\) is said to be the dominant eigenvalue of \\(\\mathbf{A}\\) if \\(|\\lambda_k| &gt; |\\lambda_i|\\), \\(~i=1,2,\\cdots,n\\), \\(~\\)but \\(\\text{ }i \\neq k\\)\nAn eigenvector corresponding to \\(\\lambda_k\\) is called the dominant eigenvector of \\(\\mathbf{A}\\)\nPower Method\nLet us assume that the eigenvalues of \\(\\mathbf{A}\\) are such that \\(|\\lambda_1| &gt; |\\lambda_2| \\geq |\\lambda_3| \\geq \\cdots \\geq |\\lambda_n|\\) and that the corresponding \\(n\\) eigenvectors \\(\\mathbf{k}_1\\), \\(\\mathbf{k}_2\\), \\(\\cdots\\), \\(\\mathbf{k}_n\\) are linearly independent. \\(~\\)Because of this last assumption, \\(~n\\) eigenvectors can serve as a basis for \\(\\mathbb{R}^n\\). \\(~\\)For any nonzero \\(n \\times 1\\) vector \\(\\mathbf{x}_0\\),\n\\[ \\mathbf{x}_0 =c_1 \\mathbf{k}_1 +c_2 \\mathbf{k}_2 +c_3 \\mathbf{k}_3 +\\cdots +c_n \\mathbf{k}_n \\]\nWe shall also assume \\(\\mathbf{x}_0\\) is chosen so that \\(c_1 \\neq 0\\). \\(\\text{ }\\) We do the following procedure\n\\[\\scriptsize\n\\begin{aligned}\n    \\mathbf{A}\\mathbf{x}_0 & = c_1 \\mathbf{A}\\mathbf{k}_1 +c_2 \\mathbf{A}\\mathbf{k}_2\n       +c_3 \\mathbf{A}\\mathbf{k}_3 +\\cdots +c_n \\mathbf{A}\\mathbf{k}_n\\\\\n   &\\;\\big\\Downarrow \\;\\;\\mathbf{x}_i=\\mathbf{A}\\mathbf{x}_{i -1}, \\;\\mathbf{A}\\mathbf{k}_j=\\lambda_j \\mathbf{k}_j\\\\\n    \\mathbf{x}_1 & = c_1 \\lambda_1\\mathbf{k}_1 +c_2 \\lambda_2\\mathbf{k}_2\n       +c_3 \\lambda_3\\mathbf{k}_3 +\\cdots +c_n \\lambda_n\\mathbf{k}_n\\\\\n   &\\Downarrow \\\\\n   \\mathbf{A}\\mathbf{x}_1 & = c_1 \\lambda_1\\mathbf{A}\\mathbf{k}_1 +c_2 \\lambda_2\\mathbf{A}\\mathbf{k}_2\n       +c_3 \\lambda_3\\mathbf{A}\\mathbf{k}_3 +\\cdots +c_n \\lambda_n\\mathbf{A}\\mathbf{k}_n\\\\\n   &\\Downarrow \\\\\n   \\mathbf{x}_2 & = c_1 \\lambda_1^2\\mathbf{k}_1 +c_2 \\lambda_2^2\\mathbf{k}_2\n       +c_3 \\lambda_3^2\\mathbf{k}_3 +\\cdots +c_n \\lambda_n^2\\mathbf{k}_n\\\\\n   &\\Downarrow \\\\\n   \\mathbf{x}_m & = c_1 \\lambda_1^m\\mathbf{k}_1 +c_2 \\lambda_2^m\\mathbf{k}_2\n       +c_3 \\lambda_3^m\\mathbf{k}_3 +\\cdots +c_n \\lambda_n^m\\mathbf{k}_n\\\\\n   & = \\lambda_1^m \\left[c_1 \\mathbf{k}_1 +c_2 \\left(\\frac{\\lambda_2}{\\lambda_1}\\right)^m\\mathbf{k}_2\n       +c_3 \\left(\\frac{\\lambda_3}{\\lambda_1}\\right)^m\\mathbf{k}_3 +\\cdots +c_n \\left(\\frac{\\lambda_n}{\\lambda_1}\\right)^m\\mathbf{k}_n \\right]\\\\        & \\;\\big\\Downarrow \\;\\;m \\rightarrow \\infty\\\\\n    \\mathbf{x}_m &\\simeq \\lambda_1^m c_1 \\mathbf{k}_1   \n\\end{aligned}\\]\nFor large values of \\(m\\) and under all the assumptions that were made, \\(~\\)the \\(n \\times 1\\) vector \\(\\mathbf{x}_m\\) is an approximation to a dominant eigenvector associated with the dominant eigenvalue \\(\\lambda_1\\)\nIf \\(\\mathbf{x}_m\\) is an approximation to a dominant eigenvector, \\(~\\)then the dominant eigenvalue \\(\\lambda_1\\) can be approximated by the Rayleigh quotient\n\\[\n\\lambda_1=\\frac{\\mathbf{A}\\mathbf{x}_m \\cdot \\mathbf{x}_m}{\\mathbf{x}_m \\cdot \\mathbf{x}_m}\n\\]\nIteration often results in vectors whose entries become very large. \\(~\\)Large numbers can cause a problem in computation. One way around this difficulty is to use a scaled-down normalized vector\n\\[\n\\mathbf{x}_m \\leftarrow\n  \\frac{\\mathbf{x}_m}{\\left \\| \\mathbf{x}_m \\right \\|}\\]\n\n\nimport numpy as np\nimport pprint\n\ndef power_method(A, n_iteration):\n    # Ideally choose a random vector\n    # To decrease the chance that \n    #   our vector is orthogonal to the eigenvector\n    k = np.random.rand(A.shape[1])\n\n    for _ in range(n_iteration):\n        # calculate the matrix-by-vector product Ak\n        k_1 = np.dot(A, k)\n\n        # calculate the norm\n        k_1_norm = np.linalg.norm(k_1)\n\n        # re normalize the vector\n        k = k_1 /k_1_norm\n\n    return k\n\ndef Rayleigh_quotient(A, k):\n    # calculate Rayleigh quotient\n    lambda_ = np.dot(np.dot(A, k), k) /np.dot(k, k)\n    \n    return lambda_\n\nA = np.array([[4, 2], [3, -1]])\nk_1 = power_method(A, 10)\nlambda_1 = Rayleigh_quotient(A, k_1)\n\nprint('A =')\npprint.pprint(A)\nprint('\\nDominant Eigenvector:', np.around(k_1, 3))\nprint('Dominant Eigenvalue:', np.around(lambda_1, 3))\n\nA =\narray([[ 4,  2],\n       [ 3, -1]])\n\nDominant Eigenvector: [0.894 0.447]\nDominant Eigenvalue: 5.0\n\n\n\nMethod of Deflation\n\nAfter we have found the dominant eigenvalue \\(\\lambda_1\\) of a matrix \\(\\mathbf{A}\\), \\(\\,\\)it may still be necessary to find nondominant eigenvalues. \\(\\,\\)We will limit the discussion to the case where \\(\\mathbf{A}\\) is a symmetric matrix\nSuppose \\(\\lambda_1\\) and \\(\\mathbf{k}_1\\) are, respectively, the dominant eigenvalue and a corresponding normalized eigenvector of a symmetric matrix \\(\\mathbf{A}\\). \\(\\,\\)Furthermore, \\(\\,\\)suppose the eigenvalues of \\(\\mathbf{A}\\) are such that\n\\[ |\\lambda_1| &gt; |\\lambda_2| &gt; |\\lambda_3| &gt; \\cdots &gt; |\\lambda_n| \\]\nIn this case, the matrix\n\\[ \\mathbf{A}_1 =\\mathbf{A} -\\lambda_1\\mathbf{k}_1\\mathbf{k}_1^T \\]\nhas eigenvalues \\(0\\), \\(|\\lambda_2|\\), \\(|\\lambda_3|\\), \\(\\cdots\\), \\(|\\lambda_n|\\) and that eigenvectors of \\(\\mathbf{A}_1\\) are also eigenvectors of \\(\\mathbf{A}\\). \\(~\\) Note that \\(\\lambda_2\\) is now the dominant eigenvalue of \\(\\mathbf{A}_1\\)\n\n\n\ndef deflation_method(A, n_iteration):\n    \n    n = A.shape[1]\n\n    K = np.zeros((n, n))\n    Lambda = np.zeros(n)\n    \n    k0 = np.zeros(n)\n    L0 = 0\n    for i in range(n):\n        A = A -L0 *np.outer(k0, k0)\n        k0 = power_method(A, n_iteration)\n        L0 = Rayleigh_quotient(A, k0)\n        K[i, :] = k0\n        Lambda[i] = L0\n        \n    return K, Lambda      \n\n# symmetric matrix\nA = np.array([[1, 2, -1], [2, 1, 1], [-1, 1, 0]])\n\nK, Lambda = deflation_method(A, 20)  \n\nprint('A =')\npprint.pprint(A)\n    \nprint('\\nk_1 =', np.around(K[0, :], 3))\nprint('k_2 =', np.around(K[1, :], 3))\nprint('k_3 =', np.around(K[2, :], 3))\n\nprint('\\nlambda_1 =', np.around(Lambda[0], 3))\nprint('lambda_2 =', np.around(Lambda[1], 3))\nprint('lambda_3 =', np.around(Lambda[2], 3))\n\nA =\narray([[ 1,  2, -1],\n       [ 2,  1,  1],\n       [-1,  1,  0]])\n\nk_1 = [0.707 0.707 0.   ]\nk_2 = [-0.578  0.577 -0.577]\nk_3 = [-0.408  0.408  0.816]\n\nlambda_1 = 3.0\nlambda_2 = -2.0\nlambda_3 = 1.0\n\n\n\nInverse Power Method\nIf we want to find the smallest eigenvalue instead of the largest one, then we perform power iteration for \\(\\mathbf{A}^{−1}\\) (since the eigenvalues of \\(\\mathbf{A}^{-1}\\) are the reciprocals of the eigenvalues of \\(\\mathbf{A}\\)). Of course, \\(~\\)we do not want to compute \\(\\mathbf{A}^{-1}\\)\n\n\ndef inverse_power_method(A, n_iteration):\n    # Ideally choose a random vector\n    # To decrease the chance that \n    #   our vector is orthogonal to the eigenvector\n    k = np.random.rand(A.shape[1])\n\n    for _ in range(n_iteration):\n        # calculate the matrix-by-vector product Ak\n        w = np.linalg.solve(A, k)\n\n        # calculate the norm\n        w_norm = np.linalg.norm(w)\n\n        # re normalize the vector\n        k = w /w_norm\n\n    return k\n\nA = np.array([[4, 2], [3, -1]])\nk_2 = inverse_power_method(A, 10)\nlambda_2 = Rayleigh_quotient(A, k_2)\n\nprint('A =')\npprint.pprint(A)\nprint('\\nEigenvector =', np.around(k_2, 3))\nprint('Eigenvalue of Least Magnitude =', np.around(lambda_2, 3))\n\nA =\narray([[ 4,  2],\n       [ 3, -1]])\n\nEigenvector = [-0.316  0.949]\nEigenvalue of Least Magnitude = -2.0",
    "crumbs": [
      "**First Semester**",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Matrices</span>"
    ]
  },
  {
    "objectID": "ch_08_Matrices.html#sec-8-12",
    "href": "ch_08_Matrices.html#sec-8-12",
    "title": "6  Matrices",
    "section": "6.13 Diagonalization",
    "text": "6.13 Diagonalization\n\nFor an \\(n \\times n\\) matrix \\(\\mathbf{A}\\), \\(~\\)can we find an \\(n \\times n\\,\\) nonsingular matrix \\(\\mathbf{P}\\) such that \\(\\mathbf{P}^{-1}\\mathbf{A}\\mathbf{P}=\\mathbf{D}\\) is a diagonal matrix?\nAn \\(n \\times n\\) matrix \\(\\mathbf{A}\\) is diagonalizable if and only if \\(\\mathbf{A}\\) has \\(n\\) linearly independent eigenvectors\nLet \\(\\mathbf{k}_1\\), \\(\\mathbf{k}_2\\), \\(\\cdots\\), \\(\\mathbf{k}_n\\) be linearly independent eigenvectors corresponding to eigenvalues \\(\\lambda_1\\), \\(\\lambda_2\\), \\(\\cdots\\), \\(\\lambda_n\\). \\(~\\)Next form the matrix \\(~\\mathbf{P}~\\) with column vectors \\(~\\mathbf{k}_1\\), \\(\\mathbf{k}_2\\), \\(\\cdots\\), \\(\\mathbf{k}_n\\)\n \\[{\n\\mathbf{P}= \\begin{pmatrix}\n  \\mathbf{k}_1 & \\mathbf{k}_2 & \\cdots & \\mathbf{k}_n\n\\end{pmatrix}}\\]\nWe can wrtie the product \\(\\mathbf{A}\\mathbf{P}\\) as\n\\[\n\\begin{aligned}\n\\mathbf{A}\\mathbf{P}\n    &=\n    \\begin{pmatrix}\n      \\mathbf{A}\\mathbf{k}_1 & \\mathbf{A}\\mathbf{k}_2 & \\cdots & \\mathbf{A}\\mathbf{k}_n\n    \\end{pmatrix} =\n    \\begin{pmatrix}\n      \\lambda_1\\mathbf{k}_1 & \\lambda_2\\mathbf{k}_2 & \\cdots & \\lambda_n\\mathbf{k}_n\n    \\end{pmatrix} \\\\\n    &=\\begin{pmatrix}\n         \\mathbf{k}_1 & \\mathbf{k}_2 & \\cdots & \\mathbf{k}_n\n      \\end{pmatrix}\n      \\begin{pmatrix}\n         \\lambda_1 &  &  & \\\\\n                   & \\lambda_2 &  & \\\\\n                   &  & \\ddots & \\\\\n                   &  &  & \\lambda_n\n      \\end{pmatrix}=\\mathbf{P}\\mathbf{D}\n\\end{aligned}\\]\nMultiplying by \\(\\mathbf{P}^{-1}\\) on the left then gives \\(\\mathbf{P}^{-1}\\mathbf{A}\\mathbf{P}=\\mathbf{D}\\)\n\n\n\n\n\n\n\nIf an \\(n \\times n\\) matrix \\(\\mathbf{A}\\) has \\(n\\) distinct eigenvalues, \\(\\,\\)it is diagonalizable\nAn \\(n \\times n\\) matrix \\(\\mathbf{A}\\) can be orthogonally diagonalized if and only if \\(\\mathbf{A}\\) is symmetric\n\n\\(~\\)\nExample \\(\\,\\) Diagonalize\n\\[\\mathbf{A}=\n  \\left(\\begin{array}{rrr}\n    9 & 1 & 1\\\\\n    1 & 9 & 1\\\\\n    1 & 1 & 9\n  \\end{array}\\right)\\]\n\nThe eigenvalues and corresponding orthogonal eigenvectors are \\(\\lambda_1=11\\), \\(\\lambda_2=\\lambda_3=8\\)\n\\[\\mathbf{k}_1=\n\\left(\\begin{array}{r}\n  1\\\\\n  1\\\\\n  1\n\\end{array}\\right),  \n\\mathbf{k}_2=\n\\left(\\begin{array}{r}\n-2\\\\\n  1\\\\\n  1\n\\end{array}\\right) \\text{ and }\n\\mathbf{k}_3=\n\\left(\\begin{array}{r}\n  0\\\\\n  1\\\\\n-1\n\\end{array}\\right)\\]\nMultiplying these vectors, in turn, by the reciprocals of the norms \\(\\left \\| \\mathbf{k}_1 \\right \\|=\\sqrt{3}\\), \\(\\,\\left \\| \\mathbf{k}_2 \\right \\|=\\sqrt{6}\\,\\) and \\(\\,\\left \\| \\mathbf{k}_3 \\right \\|=\\sqrt{2}\\), \\(\\,\\)we obtain an orthonormal set\n\\[{\\scriptsize\n\\mathbf{k}_1=\n\\left(\\begin{array}{r}\n  \\frac{1}{\\sqrt{3}}\\\\\n  \\frac{1}{\\sqrt{3}}\\\\\n  \\frac{1}{\\sqrt{3}}\n\\end{array}\\right), \\;\n\\mathbf{k}_2=\n\\left(\\begin{array}{r}\n  -\\frac{2}{\\sqrt{6}}\\\\\n   \\frac{1}{\\sqrt{6}}\\\\\n   \\frac{1}{\\sqrt{6}}\n\\end{array}\\right) \\;{\\text{ and }} \\;\n\\mathbf{k}_3=\n\\left(\\begin{array}{r}\n  0\\phantom{\\;\\,}\\\\\n  \\frac{1}{\\sqrt{2}}\\\\\n-\\frac{1}{\\sqrt{2}}\n\\end{array}\\right)}\\]\nWe then use these vectors as columns to construct an orthogonal matrix that diagonalizes \\(\\mathbf{A}\\)\n\\[{\\mathbf{P}=\n\\left(\\begin{array}{rrr}\n  \\frac{1}{\\sqrt{3}} & -\\frac{2}{\\sqrt{6}} &  0\\phantom{\\;\\,}\\\\\n  \\frac{1}{\\sqrt{3}} &  \\frac{1}{\\sqrt{6}} &  \\frac{1}{\\sqrt{2}}\\\\\n  \\frac{1}{\\sqrt{3}} &  \\frac{1}{\\sqrt{6}} & -\\frac{1}{\\sqrt{2}}\n\\end{array}\\right)}\\]\nThis transforms \\(\\mathbf{A}\\) to \\(\\mathbf{D}\\)\n\\[\\mathbf{P}^{-1}\\mathbf{A}\\mathbf{P}=\\mathbf{P}^{T}\\mathbf{A}\\mathbf{P}=\n\\begin{pmatrix}\n  11 & 0 & 0\\\\\n   0 & 8 & 0\\\\\n   0 & 0 & 8\n\\end{pmatrix}=\\mathbf{D}\\]\nThe entries in \\(\\mathbf{D}\\) are the eigenvalues of \\(\\mathbf{A}\\) and the order in which these numbers appear on the diagonal corresponds to the order in which the eigenvectors are used as columns in the matrix \\(\\mathbf{P}\\)\n\n\\(~\\)\nExample - Quadratic Forms: \\(\\,\\) Identify the conic section whose equation \\(\\,2x^2 +4xy -y^2 =1\\)\n\nWe can write the given equation as\n\\[\n\\begin{pmatrix}\n  x & y\n\\end{pmatrix}\n\\left(\\begin{array}{rr}\n  2 & 2\\\\\n  2 &-1\n\\end{array}\\right)\n\\begin{pmatrix}\n  x \\\\\n  y\n\\end{pmatrix}\n=1 \\;\\text{ or }\\; \\mathbf{x}^T\\mathbf{A}\\mathbf{x}=1\n\\]\nThe eigenvalues and corresponding eigenvectors of \\(\\mathbf{A}\\) are found to be\n\\[\\lambda_1=-2, \\,\\lambda_2=3, \\,\\mathbf{k}_1=\\left(\\begin{array}{r} 1\\\\ -2 \\end{array}\\right),\n\\,\\mathbf{k}_2=\\left(\\begin{array}{r} 2\\\\ 1 \\end{array}\\right)\\]\n\n\\(~\\)\n\nfrom sympy import symbols, Eq, plot_implicit\n\nx, y = symbols('x y')\n\np1 = plot_implicit(Eq(2*x**2 +4*x*y -y**2, 1), \n                   (x, -3, 3), (y, -3, 3),\n                   aspect_ratio=(1, 1), size=(5, 5))\n\n\n\n\n\n\n\n\n\nObserve that \\(\\mathbf{k}_1\\) and \\(\\mathbf{k}_2\\) are orthogonal. \\(~\\)Moreover, \\(~\\left \\| \\mathbf{k}_1 \\right \\| =\\left \\| \\mathbf{k}_2 \\right \\| =\\sqrt{5}\\), \\(~\\) and so the vectors\n\\[{\\mathbf{k}_1=\\left(\\begin{array}{r} \\frac{1}{\\sqrt{5}}\\\\ -\\frac{2}{\\sqrt{5}} \\end{array}\\right) \\;\\text{and}\n\\; \\mathbf{k}_2=\\left(\\begin{array}{r} \\frac{2}{\\sqrt{5}}\\\\ \\frac{1}{\\sqrt{5}} \\end{array}\\right)}\\]\nare orthogonal. \\(\\,\\)Hence, \\(\\,\\)the matrix\n\\[{\\mathbf{P}=\n   \\left(\\begin{array}{rr} \\frac{1}{\\sqrt{5}} & \\frac{2}{\\sqrt{5}}\\\\\n  -\\frac{2}{\\sqrt{5}} & \\frac{1}{\\sqrt{5}}\n  \\end{array}\\right)}\\]\nis orthogonal\nIf we define the change of variables \\(~\\mathbf{x}=\\mathbf{P}\\bar{\\mathbf{x}}\\text{ }\\) where \\(\\scriptsize \\bar{\\mathbf{x}}=\\begin{pmatrix} \\bar{x} \\\\ \\bar{y} \\end{pmatrix}\\), \\(\\,\\)then the quadratic form can be written\n\\[\n\\begin{aligned}\n    \\mathbf{x}^T\\mathbf{A}\\mathbf{x}\n   & =\n    \\bar{\\mathbf{x}}^T\\mathbf{P}^T\\mathbf{A}\\mathbf{P}\\bar{\\mathbf{x}}\n    =\\bar{\\mathbf{x}}^T\\mathbf{D}\\bar{\\mathbf{x}}\\\\\n   & =\n   \\begin{pmatrix}\n     \\bar{x} & \\bar{y}\n   \\end{pmatrix}\n   \\left(\\begin{array}{rr}\n     -2 & 0\\\\\n      0 & 3\n   \\end{array}\\right)\n   \\begin{pmatrix}\n     \\bar{x} \\\\\n     \\bar{y}\n   \\end{pmatrix}=1 \\\\\n   & \\text{ or} \\\\\n   -2\\bar{x}^2 &+3\\bar{y}^2 =1\n\\end{aligned}\\]\nThis last equation is recognized as the standard form of a hyperbola\n\n\\(~\\)\n\nbar_x, bar_y = symbols('bar_x bar_y')\n\np1 = plot_implicit(Eq(-bar_x**2 +3*bar_y**2, 1), \n                   (bar_x, -3, 3), (bar_y, -3, 3), \n                   aspect_ratio=(1, 1), size=(5, 5),\n                   xlabel=r'$\\bar{x}$', ylabel=r'$\\bar{y}$')\n\n\n\n\n\n\n\n\n\\(~\\)\nExample \\(\\,\\) Determine whether the given matrix \\(\\mathbf{A}\\) is diagonalizable. \\(~\\) If so, find the matrix \\(\\mathbf{P}\\) that diagonalizes \\(\\mathbf{A}\\) and the disgonal matrix \\(\\mathbf{D}\\) such that \\(\\mathbf{D}=\\mathbf{P}^{-1}\\mathbf{AP}\\)\n\\[\\begin{pmatrix}\n-9 & 13\\\\\n-2 & 6\n\\end{pmatrix}, \\;\\; \\begin{pmatrix}\n1 & -1 & 1 \\\\\n0 & \\phantom{-}1 & 0\\\\\n1 &  -1&  1\n\\end{pmatrix}, \\;\\;\\begin{pmatrix}\n1 & \\phantom{-}2 & 0\\\\\n2 & -1 & 0\\\\\n0 & \\phantom{-}0 & 1\n\\end{pmatrix}\\]\n\\(~\\)\nExample \\(\\,\\) The given matrix \\(\\mathbf{A}\\) is symmetric. \\(~\\) Find an orthogonal matrix \\(\\mathbf{P}\\) that diagonalizes \\(\\mathbf{A}\\) and the diagonal matrix \\(\\mathbf{D }\\) such that \\(\\mathbf{D}=\\mathbf{P}^T\\mathbf{AP}\\)\n\\[\\begin{pmatrix}\n1 & 1\\\\\n1 & 1\n\\end{pmatrix}, \\;\\; \\begin{pmatrix}\n0 & 1 & 0 \\\\\n1 & 0 & 0\\\\\n0 & 0 &  1\n\\end{pmatrix}, \\;\\;\\begin{pmatrix}\n1 & 0 & 7\\\\\n0 & 1 & 0\\\\\n7 & 0 & 1\n\\end{pmatrix}\\]\n\\(~\\)\nExample \\(\\,\\) Identify the given conic section\n\\[\\phantom{-}5x^2 -2xy +5y^2 =24\\]\n\\[-3x^2 +8xy +3y^2 =20\\]\n\\(~\\)\nExample \\(\\,\\) Find \\(3 \\times 3~\\) symmetric matrix that has eigenvalues \\(~\\lambda_1=1\\), \\(~\\lambda_2=3\\), and \\(~\\lambda_3=5\\) and corresponding eigenvectors\n\\[\\mathbf{k}_1 = \\begin{pmatrix}\n  \\phantom{-}1 \\\\ -1 \\\\ 1\n\\end{pmatrix}, \\;\\;\n\\mathbf{k}_2 = \\begin{pmatrix}\n  \\phantom{-}1 \\\\ \\phantom{-}0 \\\\ -1\n\\end{pmatrix}, \\;\\;\n\\mathbf{k}_3 = \\begin{pmatrix}\n  1 \\\\ 2 \\\\ 1\n\\end{pmatrix}\\]\n\\(~\\)\nExample \\(\\,\\) If \\(\\mathbf{A}\\) is an \\(n\\times n~\\) diagonalizable matrix, then \\(\\mathbf{D}=\\mathbf{P}^{-1}\\mathbf{AP}\\), \\(~\\) where \\(\\mathbf{D}\\) is a diagonal matrix. \\(~\\) Show that if \\(m\\) is a positive integer, then \\(\\mathbf{A}^m=\\mathbf{PD}^m\\mathbf{P}^{-1}\\)\n\\(~\\)\nExample \\(\\,\\) Find the indicated power of the given matrix\n\\[\n\\begin{aligned}\n  \\mathbf{A} &= \\begin{pmatrix}\n     1 & 1\\\\\n     2 & 0\n  \\end{pmatrix},\\;\\;\\mathbf{A}^5\\\\\n  \\mathbf{A} &= \\begin{pmatrix}\n     6 & -10\\\\\n     3 & -5\n  \\end{pmatrix},\\;\\;\\mathbf{A}^{10}\n\\end{aligned}\\]\n\\(~\\)\nExample \\(\\,\\) Suppose \\(\\mathbf{A}\\) is a nonsingular diagonalizable matrix. Then show that \\(\\mathbf{A}^{-1}\\) is diagonalizable\n\\(~\\)\nExample \\(\\,\\) Suppose \\(\\mathbf{A}\\) is a diagonalizable matrix. \\(~\\)Is the matrix \\(\\mathbf{P}\\) unique?\n\\(~\\)",
    "crumbs": [
      "**First Semester**",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Matrices</span>"
    ]
  },
  {
    "objectID": "ch_08_Matrices.html#sec-8-13",
    "href": "ch_08_Matrices.html#sec-8-13",
    "title": "6  Matrices",
    "section": "6.14 LU Factorization",
    "text": "6.14 LU Factorization\n\nLet \\(\\mathbf{A}\\) be a square matrix. \\(~\\)An LU factorization refers to the factorization of \\(\\mathbf{A}\\) into two factors – a lower triangular matrix \\(\\mathbf{L}\\) and an upper triangular matrix \\(\\mathbf{U}\\):\n\\[\\mathbf{A}=\\mathbf{L}\\mathbf{U}\\]\nWithout a proper ordering or permutations in the matrix, \\(~\\)the factorization may fail to materialize. \\(\\,\\)This is a procedural problem. \\(\\,\\)It can be removed by simply reordering the rows of \\(\\mathbf{A}\\). \\(\\,\\)It turns out that a proper permutation in rows (or columns) is sufficient for LU factorization. \\(\\,\\)LU factorization with partial pivoting (LUP) refers often to LU factorization with row permutations only\n\\[\\mathbf{P}\\mathbf{A}=\\mathbf{L}\\mathbf{U}\\]\nwhere \\(\\mathbf{P}\\) is a permutation matrix, which, when left-multiplied to \\(\\mathbf{A}\\), reorders the rows of \\(\\mathbf{A}\\). \\(\\,\\)It turns out that all square matrices can be factorized in this form\nIf \\(\\mathbf{A}\\) is invertible, then it admits an LU factorization if and only if all its leading principal minors are nonzero. \\(\\,\\)If \\(\\mathbf{A}\\) is a singular matrix of rank \\(k\\), \\(\\,\\)then it admits an LU factorization if the first \\(k\\) leading principal minors are nonzero\nLU decomposition is basically a modified form of Gaussian elimination. \\(~\\)We transform the matrix \\(\\mathbf{A}\\) into an upper triangular matrix \\(\\mathbf{U}\\) by eliminating the entries below the main diagonal. \\(\\,\\)The Doolittle algorithm does the column-by-column elimination, starting from the left, by multiplying \\(\\mathbf{A}\\) to the left with atomic lower triangular matrices. It results in a unit lower triangular matrix and an upper triangular matrix\n\n\\(~\\)\n\nDoolittle Algorithm\n\nWe define\n\\[\\mathbf{A}^{(0)}=\\mathbf{A}\\]\nWe eliminate the matrix elements below the main diagonal in the \\(k\\)-th column of \\(\\mathbf{A}^{(k -1)}\\) by adding to the \\(i\\)-th row of this matrix the \\(k\\)-th row multiplied by\n\\[l_{i,k}=\\frac{a_{i,k}^{(k-1)}}{a_{k,k}^{(k-1)}} \\;\\text{ for }\\; i=k+1, \\cdots, n\\]\nThis can be done by multiplying \\(\\mathbf{A}^{(k -1)}\\) to the left with the lower triangular matrix\n\\[{\\scriptsize\n\\mathbf{L}_k=\n\\begin{pmatrix}\n     1      & 0      &           & \\cdots &        & 0      \\\\\n     0      & \\ddots & \\ddots    &        &        &        \\\\\n            &        & 1         &        &        &        \\\\\n    \\vdots  &        &-l_{k+1,k} &        &        & \\vdots \\\\\n            &        & \\vdots    &        & \\ddots & 0      \\\\\n     0      &        &-l_{n,k}   &        & 0      & 1\n\\end{pmatrix}}\\]\nWe set\n\\[{\\mathbf{A}^{(k)}=\\mathbf{L}_k\\mathbf{A}^{(k-1)}, \\;k=1,\\cdots,n -1}\\]\nAfter \\(n -1\\) steps, \\(~\\)we eliminated all the matrix elements below the main diagonal, \\(\\,\\)so we obtain an upper triangular matrix \\(\\mathbf{A}^{(n -1)}\\). \\(\\,\\)We find the decomposition\n\\[\\scriptsize\n\\begin{aligned}\n\\mathbf{A}\n&= \\mathbf{L}_1^{-1}\\mathbf{L}_1\\mathbf{A}^{(0)}\n  =\\mathbf{L}_1^{-1}\\mathbf{A}^{(1)}\n  = \\mathbf{L}_1^{-1}\\mathbf{L}_2^{-1}\n    \\mathbf{L}_2\\mathbf{A}^{(1)}\n  =\\mathbf{L}_1^{-1}\\mathbf{L}_2^{-1}\n   \\mathbf{A}^{(2)}\\\\\n  &\\;\\;\\vdots \\\\\n  &=\\mathbf{L}_1^{-1}\\cdots\\mathbf{L}_{n -1}^{-1}\n    \\mathbf{A}^{(n -1)}\n\\end{aligned}\\]\nDenote the upper triangular matrix \\(\\mathbf{A}^{(n -1)}\\) by \\(\\mathbf{U}\\), \\(\\,\\) and \\(\\mathbf{L}=\\mathbf{L}_1^{-1}\\cdots\\mathbf{L}_{n -1}^{-1}\\)\nBecause the inverse of a lower triangular matrix \\(\\mathbf{L}_k\\) is again a lower triangular matrix, and the multiplication of two lower triangular matrices is again a lower triangular matrix, it follows that \\(\\mathbf{L}\\) is a lower triangular matrix:\n\\[{\\scriptsize\n\\mathbf{L}=\n\\begin{pmatrix}\n     1      & 0      &           & \\cdots &          & 0      \\\\\n     l_{2,1}& \\ddots & \\ddots    &        &          &        \\\\\n            &        & 1         &        &          &        \\\\\n    \\vdots  &        & l_{k+1,k} &        &          & \\vdots \\\\\n            &        & \\vdots    &        & 1        & 0      \\\\\n     l_{n,1}& \\cdots & l_{n,k}   & \\cdots & l_{n,n-1}& 1\n\\end{pmatrix}}\\]\nWe obtain \\(\\mathbf{A}=\\mathbf{L}\\mathbf{U}\\)\n\n\nNOTE \\(\\,\\)It is clear that in order for this algorithm to work, one needs to have \\(a_{k,k}^{(k-1)}\\) at each step (see the definition of \\(l_{i,k}\\)). If this assumption fails at some point, one needs to interchange \\(k\\)-th row with another row below it before continuing. This is why an LU decomposition in general looks like \\(\\mathbf{P}\\mathbf{A}=\\mathbf{L}\\mathbf{U}\\)\n\\(~\\)\n\ndef lu_factor(A):\n    \"\"\"\n        LU factorization with partial pivoting\n    \n        PA = LU    \n        P(permutation), \n        L(unit Lower triangular) and \n        U(upper triangular) \n    \n        Return P, L, U\n    \"\"\"\n    n = A.shape[0]    \n    U = A.copy()\n    P = np.identity(n)\n    L = np.identity(n)\n\n    for k in range(n -1):\n\n        # Partial Pivoting\n        max_row_index = np.argmax(abs(U[k:n,k])) +k\n        P[[k,max_row_index]] = P[[max_row_index,k]]\n        U[[k,max_row_index]] = U[[max_row_index,k]]\n\n        # LU\n        L[k+1:,k] = U[k+1:,k] /U[k,k]\n        U[k+1:,k] = 0.0\n        U[k+1:,k+1:] -= np.tensordot(L[k+1:,k],\n                           U[k,k+1:], axes=0)\n\n    return P, L, U\n\nA = np.array([[7, 3, -1, 2], [3, 8, 1, -4], [-1, 1, 4, -1], \n             [2, -4, -1, 6]], dtype='float64')\nP, L, U = lu_factor(A)\n\nprint('A ='); pprint.pprint(A)\nprint('\\nP ='); pprint.pprint(P)\nprint('\\nL ='); pprint.pprint(L)\nprint('\\nU ='); pprint.pprint(U)\n\nA =\narray([[ 7.,  3., -1.,  2.],\n       [ 3.,  8.,  1., -4.],\n       [-1.,  1.,  4., -1.],\n       [ 2., -4., -1.,  6.]])\n\nP =\narray([[1., 0., 0., 0.],\n       [0., 1., 0., 0.],\n       [0., 0., 1., 0.],\n       [0., 0., 0., 1.]])\n\nL =\narray([[ 1.        ,  0.        ,  0.        ,  0.        ],\n       [ 0.42857143,  1.        ,  0.        ,  0.        ],\n       [-0.14285714,  0.21276596,  1.        ,  0.        ],\n       [ 0.28571429, -0.72340426,  0.08982036,  1.        ]])\n\nU =\narray([[ 7.        ,  3.        , -1.        ,  2.        ],\n       [ 0.        ,  6.71428571,  1.42857143, -4.85714286],\n       [ 0.        ,  0.        ,  3.55319149,  0.31914894],\n       [ 0.        ,  0.        ,  0.        ,  1.88622754]])\n\n\n\\(~\\)\n\nfrom scipy.linalg import lu\n\nA = np.array([[7, 3, -1, 2], [3, 8, 1, -4], [-1, 1, 4, -1],\n             [2, -4, -1, 6]], dtype='float64')\nP, L, U = lu(A)\n\nprint('P =')\npprint.pprint(P)\nprint('\\nL =')\npprint.pprint(L)\nprint('\\nU =')\npprint.pprint(U)\n\nP =\narray([[1., 0., 0., 0.],\n       [0., 1., 0., 0.],\n       [0., 0., 1., 0.],\n       [0., 0., 0., 1.]])\n\nL =\narray([[ 1.        ,  0.        ,  0.        ,  0.        ],\n       [ 0.42857143,  1.        ,  0.        ,  0.        ],\n       [-0.14285714,  0.21276596,  1.        ,  0.        ],\n       [ 0.28571429, -0.72340426,  0.08982036,  1.        ]])\n\nU =\narray([[ 7.        ,  3.        , -1.        ,  2.        ],\n       [ 0.        ,  6.71428571,  1.42857143, -4.85714286],\n       [ 0.        ,  0.        ,  3.55319149,  0.31914894],\n       [ 0.        ,  0.        ,  0.        ,  1.88622754]])\n\n\n\nA_ = sympy.Matrix([[7, 3, -1, 2], [3, 8, 1, -4], [-1, 1, 4, -1],\n             [2, -4, -1, 6]])\nL_, U_, _ = A_.LUdecomposition()\nL_, U_\n\n\\(\\displaystyle \\left( \\left[\\begin{matrix}1 & 0 & 0 & 0\\\\\\frac{3}{7} & 1 & 0 & 0\\\\- \\frac{1}{7} & \\frac{10}{47} & 1 & 0\\\\\\frac{2}{7} & - \\frac{34}{47} & \\frac{15}{167} & 1\\end{matrix}\\right], \\  \\left[\\begin{matrix}7 & 3 & -1 & 2\\\\0 & \\frac{47}{7} & \\frac{10}{7} & - \\frac{34}{7}\\\\0 & 0 & \\frac{167}{47} & \\frac{15}{47}\\\\0 & 0 & 0 & \\frac{315}{167}\\end{matrix}\\right]\\right)\\)\n\n\n\nSolving Linear Equations\nGiven a system of linear equations in matrix form\n\\[\\mathbf{A}\\mathbf{x}=\\mathbf{b}\\]\nSuppose we have already obtained the LUP decomposition of \\(\\mathbf{A}\\) such that \\(\\mathbf{P}\\mathbf{A}=\\mathbf{L}\\mathbf{U}\\), \\(\\,\\)so\n\\[\\mathbf{L}\\mathbf{U}\\mathbf{x}=\\mathbf{P}\\mathbf{b}\\]\nIn this case the solution is done in two logical steps:\n\\(~\\)\n\nFirst, \\(\\,\\)we solve the equation \\(\\mathbf{L}\\mathbf{y}=\\mathbf{P}\\mathbf{b}\\) for \\(\\mathbf{y}\\)\n\nSecond, \\(\\,\\)we solve the equation \\(\\mathbf{U}\\mathbf{x}=\\mathbf{y}\\) for \\(\\mathbf{x}\\)\nNote that in both cases we are dealing with triangular matrices \\(\\mathbf{L}\\) and \\(\\mathbf{U}\\), which can be solved directly by forward and backward substitution without using the Gaussian elimination process (however we do need this process or equivalent to compute the LU decomposition itself)\nThe cost of solving a system of linear equations is approximately \\(\\frac{2}{3}n^{3}\\) floating-point operations\nThe above procedure can be repeatedly applied to solve the equation multiple times for different \\(\\mathbf{b}\\). \\(\\,\\)In this case it is faster (and more convenient) to do an LU decomposition of the matrix \\(\\mathbf{A}\\) once and then solve the triangular matrices for the different \\(\\mathbf{b}\\), rather than using Gaussian elimination each time\nThe matrices \\(\\mathbf{L}\\) and \\(\\mathbf{U}\\) could be thought to have encoded the Gaussian elimination process\n\n\n\\(~\\)\n\ndef ufsub(L, b):\n    \"\"\" Unit row oriented forward substitution \"\"\"\n    y = b.copy()\n    for i in range(1, L.shape[0]):\n        y[i] -= np.dot(L[i,:i], y[:i])\n    return y\n\ndef bsub(U, y):\n    \"\"\" Row oriented backward substitution \"\"\"\n    x = y.copy()\n    x[-1] /= U[-1,-1]\n    for i in range(U.shape[0] -2, -1, -1): \n        x[i] -= np.dot(U[i,i+1:], x[i+1:])\n        x[i] /= U[i,i]\n    return x    \n                    \nA = np.array([[7, 3, -1, 2], [3, 8, 1, -4], \n              [-1, 1, 4, -1], [2, -4, -1, 6]], dtype='float64')\nb = np.array([1, 2, 3, 4], dtype='float64')\n\nP, L, U = lu_factor(A)\nPb = np.matmul(P, b)\n\ny = ufsub(L, Pb)\nx = bsub(U, y)\n\nprint('A ='); pprint.pprint(A)\nprint('\\nb =', end=' '); pprint.pprint(b)\nprint('\\nx =', end=' '); pprint.pprint(x)\n\nA =\narray([[ 7.,  3., -1.,  2.],\n       [ 3.,  8.,  1., -4.],\n       [-1.,  1.,  4., -1.],\n       [ 2., -4., -1.,  6.]])\n\nb = array([1., 2., 3., 4.])\n\nx = array([-1.27619048,  1.87619048,  0.57142857,  2.43809524])\n\n\n\nA_ = sympy.Matrix([[7, 3, -1, 2], [3, 8, 1, -4], \n              [-1, 1, 4, -1], [2, -4, -1, 6]])\nb_ = sympy.Matrix([1, 2, 3, 4])\n\nA_.LUsolve(b_)\n\n\\(\\displaystyle \\left[\\begin{matrix}- \\frac{134}{105}\\\\\\frac{197}{105}\\\\\\frac{4}{7}\\\\\\frac{256}{105}\\end{matrix}\\right]\\)\n\n\n\\(~\\)\n\nInverting a Matrix\nIn matrix inversion, instead of vector \\(\\mathbf{b}\\), \\(\\,\\)we have matrix \\(\\mathbf{I}_n\\) so that we are trying to find a matrix \\(\\mathbf{X}\\)\n\\[\\mathbf{L}\\mathbf{U}\\mathbf{X}=\\mathbf{I}_n\\]\nWe can use the same algorithm presented earlier to solve for each column of matrix \\(\\mathbf{X}\\)\n\n\nA_.inv(method='LU')\n\n\\(\\displaystyle \\left[\\begin{matrix}\\frac{122}{315} & - \\frac{101}{315} & \\frac{2}{21} & - \\frac{103}{315}\\\\- \\frac{101}{315} & \\frac{143}{315} & - \\frac{2}{21} & \\frac{124}{315}\\\\\\frac{2}{21} & - \\frac{2}{21} & \\frac{2}{7} & - \\frac{1}{21}\\\\- \\frac{103}{315} & \\frac{124}{315} & - \\frac{1}{21} & \\frac{167}{315}\\end{matrix}\\right]\\)\n\n\n\nComputing the Determinant\nGiven the LUP decomposition \\(\\mathbf{A}=\\mathbf{P}^{-1}\\mathbf{L}\\mathbf{U}\\) of a square matrix \\(\\mathbf{A}\\), \\(\\,\\)the determinant of \\(\\mathbf{A}\\) can be computed straightforwardly as\n\\[\\mathrm{det}\\,\\mathbf{A}=\\mathrm{det}\\,\\mathbf{P}^{-1}\\,\\mathrm{det}\\,\\mathbf{L}\\,\n\\mathrm{det}\\,\\mathbf{U}=(-1)^s \\prod_{i=1}^n l_{ii}\\prod_{i=1}^n u_{ii}\\]\nwhere \\(s\\) is the number of row exchanges in the permutation matrix\n\n\\(~\\)\nExample \\(\\,\\) Find the LU factorization of the given matrix\n\\[\n  \\begin{pmatrix}\n    2 & -2\\\\\n    1 & \\phantom{-}2\n  \\end{pmatrix}, \\;\\;\n  \\begin{pmatrix}\n    -1 & 4 \\\\\n    \\phantom{-}2 & 2\n  \\end{pmatrix}, \\;\\;\n  \\begin{pmatrix}\n   \\phantom{-}4 & -2 & 1\\\\\n   -4 & 1 & 2\\\\\n   12 & 1 & 3\n  \\end{pmatrix}\\]\n\\(~\\)\nExample \\(\\,\\) Use the LU factorization to solve the given linear system of equations\n\\[\\begin{pmatrix} 2 & -2 \\\\ 1 & \\phantom{-}2\\end{pmatrix}\n  \\begin{pmatrix} x_1 \\\\ x_2 \\end{pmatrix} =\n  \\begin{pmatrix} \\phantom{-}1 \\\\-2 \\end{pmatrix}\\\\\\]\n\\[\\begin{pmatrix}\n    \\phantom{-}4 & -2 & 1 \\\\ -4 & \\phantom{-}1 & 2\\\\12 & \\phantom{-}1 & 3\n  \\end{pmatrix}\n  \\begin{pmatrix} x_1 \\\\ x_2 \\\\x_3 \\end{pmatrix} =\n  \\begin{pmatrix} 7 \\\\ 7\\\\28 \\end{pmatrix}\\]",
    "crumbs": [
      "**First Semester**",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Matrices</span>"
    ]
  },
  {
    "objectID": "ch_08_Matrices.html#sec-8-14",
    "href": "ch_08_Matrices.html#sec-8-14",
    "title": "6  Matrices",
    "section": "6.15 Applications",
    "text": "6.15 Applications\n\n6.15.1 Cryptography\nCryptography is the study of making secret writings or codes. \\(~\\) We will consider a system of encoding and decoding messages that requires both the sender and the receiver to know:\n\nA specified rule of correspondence between a set of symbols and a set of integers; and\nA specified nonsingular matrix \\(\\mathbf{A}\\)\n\n\\(~\\)\nExample \\(\\,\\) A correspondence between the twenty-seven integers and the letters of the alphabet and a blank space is given by\n\\[\\tiny\n\\left(\\begin{array}{rrrrrrrrrrrrrrrr}\n  0 & 1 & 2 & 3 & 4 & 5 & 6 & 7 & 8 & 9 &10 &11 &12 &13 &14 &15  \\\\\n\\text{space} & j & k & l & n & m & s & t & u & w & x & g & h & i & o & p     \n\\end{array}\\right. \\\\ \\\\\n\\tiny\\left.\\begin{array}{rrrrrrrrrrrr}\n   &16 &17 &18 &19 &20 &21 &22 &23 &24 &25 &26 \\\\\n   &q & r & v & y & z & a & b & c & d & e & f   \n  \\end{array}\\right)\\]\nThe numerical equivalent of the message DR JOHN IS A DOUBLE SPY is\n\\[\\tiny\\left.\\begin{array}{rrrrrrrrrrrrrrrr}\n    24 & 17 & 0 & 1 & 14 & 12 & 4 & 0 & 13 & 6 & 0 & 21 & 0 & 24 & 14 \\\\\n    8 & 22 & 3 & 25 & 0 & 6 & 15 & 19\n  \\end{array}\\right.\\]\n\nThe sender will encode the message by means of the nonsingular matrix \\(\\mathbf{A}\\) and the receiver will decode the encoded message by means of the matrix \\(\\mathbf{A}^{-1}\\). \\(~\\) We choose to write the numerical message as the \\(3 \\times 8\\) matrix\n\\[{\\scriptsize\n\\mathbf{M}=\n  \\left(\\begin{array}{rrrrrrrr}\n    24 & 17 & 0 & 1 & 14 & 12 & 4 & 0 \\\\\n    13 & 6 & 0 & 21 & 0 & 24 & 14 & 8 \\\\\n    22 & 3 & 25 & 0 & 6 & 15 & 19 & 0\n  \\end{array}\\right)}\\]\nNote that the last entry \\(m_{38}\\) has been simply padded with a space (the number \\(0\\))\nA \\(3 \\times 8\\) matrix allows us to encode the message by means of a \\(3 \\times 3\\) matrix. The encoding matrix \\(\\mathbf{A}\\) is constructed, so that\n\n\\(\\mathbf{A}\\) is nonsingular\n\\(\\mathbf{A}\\) has only integer entries, and\n\\(\\mathbf{A}^{-1}\\) has only integer entries\n\nTo accomplish the last criterion, \\(~\\) we need only select the integer entries of \\(\\mathbf{A}\\) in such a manner that \\(\\mathrm{det}\\,\\mathbf{A}=\\pm 1\\). \\(~\\) We choose\n\\[{\\scriptsize\n\\mathbf{A}=\n  \\left(\\begin{array}{rrr}\n      -1 & 0 &-1 \\\\\n      2 & 3 & 4 \\\\\n      2 & 4 & 5\n   \\end{array}\\right)}\\]\nand\n\\[{\\scriptsize\n\\mathbf{A}^{-1}=\n  \\left(\\begin{array}{rrr}\n      1 & 4 &-3 \\\\\n      2 & 3 &-2 \\\\\n     -2 &-4 & 3\n   \\end{array}\\right)}\\]\nYou should verify that \\(\\mathrm{det}\\,\\mathbf{A}=-1\\)\nThe original message is encoded as following:\n\\[{\\tiny\\mathbf{B}=\\mathbf{A}\\mathbf{M}=\n  \\left(\\begin{array}{rrrrrrrr}\n   -46 & -20 & -25 & -1 & -20 & -27 & -23 & 0 \\\\\n   175 &  64 & 100 & 65 & 52 & 156 & 126 & 24 \\\\\n   210 &  73 & 125 & 86 & 58 & 195 & 159 & 32\n  \\end{array}\\right)}\\]\nTo send the encoded message as letters of the alphabet rather than as numbers, we rewrite \\(\\mathbf{B}\\) as \\(\\mathbf{B}'\\) using integers modulo 27:\n\\[{\\tiny\\mathbf{B'}=\n  \\left(\\begin{array}{rrrrrrrr}\n   8 & 7 & 2 & 26 & 7 & 0 & 4 & 0 \\\\\n   13 &  10 & 19 & 11 & 25 & 21 & 18 & 24 \\\\\n   21 &  19 & 17 & 5 & 4 & 6 & 24 & 5\n  \\end{array}\\right)}\\]\nThe encoded message to be sent in letters is\n\nUTKFT N IXYGEAVDAYRMNSDM\n\n\n\nimport numpy as np\n\n#-- Data for Encoding and Decoding------------------------------------\n\ncp = { 0:' ',  1:'j',  2:'k',  3:'l',  4:'n',  5:'m',  6:'s',  7:'t',    \n       8:'u',  9:'w', 10:'x', 11:'g', 12:'h', 13:'i', 14:'o', 15:'p',  \n      16:'q', 17:'r', 18:'v', 19:'y', 20:'z', 21:'a', 22:'b', 23:'c', \n      24:'d', 25:'e', 26:'f' }\n\nA = np.array([[-1, 0, -1], [2, 3, 4], [2, 4, 5]])\n\ninv_cp =  {v: k for k, v in cp.items()}\ninv_A = np.rint(np.linalg.inv(A)).astype('int32')\n\nprint('inv_A = ')\nprint(inv_A)\nprint('det_A =', np.linalg.det(A))\n\ninv_A = \n[[ 1  4 -3]\n [ 2  3 -2]\n [-2 -4  3]]\ndet_A = -1.0\n\n\n\ndef convert_to_numeric(message):   \n    return np.array([inv_cp[m] for m in message])\n\ndef convert_to_letter(numeric_message):\n    return ''.join([cp[m] for m in numeric_message])\n\ndef encoding_message(message):  \n    \n    numeric_message = convert_to_numeric(message)\n    \n    n_app = (3 -len(numeric_message) % 3) % 3\n    M = np.append(numeric_message, [0]*n_app).reshape(3, -1)\n    B = (A @ M) % 27\n\n    numeric_message_encoded = B.flatten()\n    \n    return convert_to_letter(numeric_message_encoded)\n\n\nmessage = 'dr john is a double spy'\nmessage_encoded = encoding_message(message)\n\nprint('Message =', message.upper())\nprint('Encoded message =', message_encoded.upper())\n\nMessage = DR JOHN IS A DOUBLE SPY\nEncoded message = UTKFT N IXYGEAVDAYRMNSDM\n\n\n\nYou should try to imagine the difficulty of decoding the encoded message without prior knowledge. \\(\\,\\)Using the original correspondence and \\(\\mathbf{A}\\), \\(\\,\\)the decoding is the straightforward computation\n\\[\\mathbf{M}=\\mathbf{A}^{-1}\\mathbf{B}'\\]\n\n\ndef decoding_message(message_encoded):\n\n    B_ = convert_to_numeric(message_encoded).reshape(3, -1)\n    M_ = (inv_A @ B_) % 27\n    numeric_message_decoded = M_.flatten()\n\n    return convert_to_letter(numeric_message_decoded)\n\n\nmessage_decoded = decoding_message(message_encoded)\n\nprint('Decoding message =', message_decoded.upper())\n\nDecoding message = DR JOHN IS A DOUBLE SPY \n\n\n\n\n6.15.2 An Error-Correcting Code\nWe are going to examine briefly the concept of digital communication between a satellite and a computer. As a result, we will deal only with matrices whose entries are binary digits, namely \\(0\\)s and \\(1\\)s\n\nWhen addng and multipying such matrices, \\(\\,\\)we will use arithmetic modulo 2. This arithmetic is defined by the addition and multiplication tables\n\n\n\n\\(~\\)\n\n\n\n\n+\n0\n1\n\n\n\n\n0\n0\n1\n\n\n1\n1\n 0 \n\n\n\n\n\\(~\\)\n\n\n\n\nx\n0\n1\n\n\n\n\n0\n0\n0\n\n\n1\n0\n1\n\n\n\n\n\\(~\\)\n\n\n\nIn digital communication the messages or words are binary \\(n\\)-tuples. \\(\\,\\)An \\(n\\)-bit word is also said to be a binary string of length \\(n\\). \\(\\,\\)By encoding a message, \\(\\,\\)we mean a process whereby we transform a word \\(\\mathbf{W}\\) of length \\(n\\) into another word \\(\\mathbf{C}\\) of length \\(n +m\\) by augmenting \\(\\mathbf{W}\\) with \\(m\\) additional bits, \\(~\\)called parity check bits. \\(\\,\\)An encoding/decoding scheme is called a code\nThe Hamming (7, 4) code is an encoding/decoding scheme that can detect the presence of a single error in a received message and can tell which bit must be corrected. In \\((7, 4)\\) code the encoding process consists of transforming a 4-bit word\n\\[\\mathbf{W}=\\begin{bmatrix} w_1 & w_2 & w_3 & w_4 \\end{bmatrix}\\]\ninto a 7-bit code word\n\\[\\mathbf{C}=\\begin{bmatrix} \\color{green}{c_1} & \\color{green}{c_2} & w_1 & \\color{green}{c_3} & w_2 & w_3 & w_4 \\end{bmatrix}\\]\nwhere \\(c_1\\), \\(c_2\\), and \\(c_3\\) denote the parity check bits and are defined in terms of the information bits \\(w_1\\), \\(w_2\\), \\(w_3\\), and \\(w_4\\)\n\\[\n\\begin{aligned}\n  c_1 & = w_1 +w_2 +w_4\\\\\n  c_2 & = w_1 +w_3 +w_4 \\\\\n  c_3 & = w_2 +w_3 +w_4\n\\end{aligned} \\tag{C1}\\label{eq:C1}\\]\nWe first observe that in modulo 2 arithmatic there are no negative numbers; \\(\\,\\)the additive inverse is \\(1\\), not \\(-1\\). \\(\\,\\)With this in mind, \\(\\,\\)we can write the system \\(\\eqref{eq:C1}\\) in the equivalent form\n\\[\n\\begin{aligned}\n   c_3 & + w_2 +w_3 +w_4 = 0\\\\\n   c_2 & + w_1 +w_3 +w_4 = 0 \\\\        \n   c_1 & + w_1 +w_2 +w_4 = 0\n\\end{aligned} \\tag{C2}\\label{eq:C2}\\]\nThese are called parity check equations\nAs a matrix product, \\(\\eqref{eq:C2}\\) can be written\n\\[\\mathbf{H}\\mathbf{C}^T=\\mathbf{0}\\]\nwhere\n\\[\\mathbf{H}=\n  \\begin{pmatrix}\n      0 & 0 & 0 & 1 & 1 & 1 & 1\\\\\n      0 & 1 & 1 & 0 & 0 & 1 & 1\\\\\n      1 & 0 & 1 & 0 & 1 & 0 & 1\n  \\end{pmatrix}\\]\nis called the parity check matrix. \\(~\\)A closer inspection of \\(\\mathbf{H}\\) shows a surprising fact: The columns of \\(\\mathbf{H}\\), left to right, are the numbers \\(1\\) through \\(7\\) written in binary\nLet \\(\\mathbf{R}\\) be a \\(1 \\times 7\\) matrix representing the received message. The product\n\\[\\mathbf{S}=\\mathbf{H}\\mathbf{R}^T\\]\nis called the syndome of \\(\\mathbf{R}\\). \\(\\,\\)If \\(\\mathbf{S}=\\mathbf{0}\\), \\(\\,\\)it is assumed that the transmission is correct and that \\(\\mathbf{R}\\) is the same as the original encoded message \\(\\mathbf{C}\\). \\(\\,\\)The decoding of the message is accomplished by simply dropping the three check bits in \\(\\mathbf{R}\\)\nLet\n\\[{\\mathbf{E}=\\begin{bmatrix} e_1 & e_2 & e_3 & e_4 & e_5 & e_6 & e_7 \\end{bmatrix}}\\]\nbe a single-error noise word added to \\(\\mathbf{C}\\) during its transmission\nIf noise changes the \\(i\\)-th bit, \\(\\,e_i=1\\). \\(\\,\\)The received message is then \\(\\mathbf{R}=\\mathbf{C}+\\mathbf{E}\\). \\(\\,\\)We see that\n\\[\n\\begin{aligned}\n   \\mathbf{S}&=\\mathbf{H}\\mathbf{R}^T\n      =\\mathbf{H}(\\mathbf{C}^T +\\mathbf{E}^T)=\\mathbf{H}\\mathbf{E}^T\\\\[5pt]\n      &=\\scriptsize e_1 \\begin{pmatrix} 0\\\\ 0\\\\ 1\\end{pmatrix}\n        +e_2 \\begin{pmatrix} 0\\\\ 1\\\\ 0\\end{pmatrix}\n        +e_3 \\begin{pmatrix} 0\\\\ 1\\\\ 1\\end{pmatrix}\n        +e_4 \\begin{pmatrix} 1\\\\ 0\\\\ 0\\end{pmatrix}\n        +e_5 \\begin{pmatrix} 1\\\\ 0\\\\ 1\\end{pmatrix}\n        +e_6 \\begin{pmatrix} 1\\\\ 1\\\\ 0\\end{pmatrix}\n        +e_7 \\begin{pmatrix} 1\\\\ 1\\\\ 1\\end{pmatrix}           \n  \\end{aligned}\\]\nHence, if \\(\\mathbf{S}\\neq\\mathbf{0}\\), \\(\\,\\)then \\(\\mathbf{S}\\) must be one of the columns of \\(\\mathbf{H}\\). \\(\\,\\)If \\(\\mathbf{R}\\) contains a single error, \\(\\,\\)we see that the syndrome itself indicates which bit is in error\n\n\\(~\\)\n\n\n6.15.3 Method of Least Squares\n\n\n\n\n\nWhen performing experiments we often tabulate data in the form of ordered pairs \\((x_1, y_1),\\) \\((x_2, y_2),\\) \\(\\cdots,\\) \\((x_n, y_n),\\) with each \\(x_i\\) distinct. Given the data, it is then often desirable to predict \\(y\\) from \\(x\\) by finding a mathematical model, that is, a function \\(f(x)\\) that approximates or fits the data\n\nWe shall confine our attention to the problem of finding a linear polynomial \\(f(x)=ax +b\\) that best fits the data \\((x_i, y_i)\\), \\(i=1,\\cdots, n\\). The procedure for finding this linear function is known as the method of least squares\nOne way to determine how well the linear function \\(f(x)=ax +b~\\) fits the data is to measure the vertical distances between the data points \\(y_i\\) and the graphs \\(f(x_i)\\)\n\\[e_i=|y_i -f(x_i)|, \\;\\;i=1,\\cdots, n\\]\nAn actual approach is to find a linear function \\(f\\) so that the sum of the squares of all the \\(e_i\\) values is a minimum\n\\[\\min_{a, \\,b} E=\\min_{a, \\,b} \\sum_{i=1}^n \\left[y_i -ax_i -b\\right]^2\\]\nThen to find the minimum value of \\(E\\), \\(\\,\\)we set the first partial derivatives with respect to \\(a\\) and \\(b\\) to zero:\n\\[\\frac{\\partial E}{\\partial a}=0 \\;\\text{ and }\\; \\frac{\\partial E}{\\partial b}=0\\]\nThe last two conditions yield, in turn,\n\\[\n\\begin{aligned}\n      -2 \\sum_{i=1}^n x_i [y_i -a x_i -b] &= 0\\\\\n      -2 \\sum_{i=1}^n [y_i -a x_i -b] &= 0\n\\end{aligned}\\]\nExpanding the sums and rearranging, \\(~\\)we find the above system is the same as\n\\[{\\scriptsize\n  \\begin{pmatrix}\n   \\displaystyle\\sum_{i=1}^n x_i^2 & \\displaystyle\\sum_{i=1}^n x_i\\\\\n   \\displaystyle\\sum_{i=1}^n x_i   & n\n  \\end{pmatrix}\n  \\begin{pmatrix}\n    a \\\\ b\n  \\end{pmatrix}=\n  \\begin{pmatrix}\n   \\displaystyle \\sum_{i=1}^n x_i y_i\\\\ \\displaystyle \\sum_{i=1}^n y_i \\;\\;\\;\n  \\end{pmatrix}}\n\\]\nand, in terms of matrices, is equivalent to\n\\[\\mathbf{A}^T\\mathbf{A}\\mathbf{x}=\\mathbf{A}^T\\mathbf{b}\\]\nwhere\n\\[{\\scriptsize\n  \\mathbf{A}=\n  \\begin{pmatrix}\n   x_1 & 1\\\\\n   x_2 & 1\\\\\n   \\vdots & \\vdots\\\\\n   x_n & 1\n  \\end{pmatrix},\n  \\;\\mathbf{b}=\n  \\begin{pmatrix}\n   y_1\\\\\n   y_2\\\\\n   \\vdots\\\\\n   y_n\n  \\end{pmatrix},\n  \\;\\mathbf{x}=\n  \\begin{pmatrix}\n   a \\\\ b\n  \\end{pmatrix}} \\tag{C3}\\label{eq:C3}\n\\]\nUnless the data points all lie on the same vertical line, the matrix \\(\\mathbf{A}^T\\mathbf{A}\\) is nonsingular. Thus \\(\\eqref{eq:C3}\\) has the unique solution\n\\[\n  \\mathbf{x} =\\left(\\mathbf{A}^T \\mathbf{A} \\right)^T \\mathbf{A}^T \\mathbf{b}\n\\]\n\n\\(~\\)\nExample \\(\\,\\) Nonlinear Least Squares: \\(\\text{ } f(x) = 2.5e^{-1.3x}\\)\n\nfrom scipy.optimize import curve_fit\nimport matplotlib.pyplot as plt\n\nnp.random.seed(1970)\n\ndef func(x, a, b):\n    return a *np.exp(-b*x)\n\n# Define the data to be fit with some noise\nxdata = np.linspace(0, 4, 50)\nydata = func(xdata, 2.5, 1.3) +0.1 *np.random.normal(size=xdata.size)\nplt.plot(xdata, ydata, 'ro', label='data')\n\n# Fit for the parameters a and b of the function func:\npopt, pcov = curve_fit(func, xdata, ydata)\nplt.plot(xdata, func(xdata, *popt), \n         'b-', label='fit: a=%5.3f, b=%5.3f' % tuple(popt))\nplt.xlabel('x'), plt.ylabel('y'), plt.legend()\nplt.show()\n\n\n\n\n\n\n\n\n\\(~\\)\n\n\n6.15.4 Discrete Compartmental Models\n\nStrontium 90 is deposited into pastureland by rainfall. To study how this material is cycled through the ecosystem, \\(~\\)we divide the system into the compartments:\n\n\n\n\n\nSuppose that \\(\\Delta t=1\\) month and the transfer coefficients shown in the figure are measured in fraction/month\nSuppose that rainfall has deposited the strotium 90 into the compartments so that\n\\[{\\mathbf{x}_0=\n\\begin{pmatrix}\n  20 &\n  60 &\n  15 &\n  20\n\\end{pmatrix}^T}\\]\nUnits might be grams per hectare. Compute the states of the ecosystem over the next 12 months\nFrom the data in figure, \\(\\,\\)we see that transfer matrix \\(\\mathbf{T}\\) is\n\\[\\scriptsize\n  \\mathbf{T}=\n  \\begin{pmatrix}\n   0.85 & 0.01 & 0   & 0 \\\\\n   0.05 & 0.98 & 0.2 & 0 \\\\\n   0.1  & 0    & 0.8 & 0 \\\\\n   0    & 0.01 & 0   & 1\n  \\end{pmatrix}  \n\\]\nWe must compute \\(\\mathbf{x}_1\\), \\(\\mathbf{x}_2\\), \\(\\cdots\\), \\(\\mathbf{x}_{12}\\) by using the recursion formula \\(~\\mathbf{x}_{n+1}=\\mathbf{T}\\mathbf{x}_n\\)\n\n\\(~\\)\n\nT = np.array([[0.85, 0.01, 0.00, 0.00], \n              [0.05, 0.98, 0.20, 0.00],\n              [0.10, 0.00, 0.80, 0.00], \n              [0.00, 0.01, 0.00, 1.00]])\nx = np.array([20, 60, 15, 20]).T\n\nprint('-' *47)\nprint('Month     Grasses   Soil      Dead_OM   Streams')\nprint('-' *47)\nfor month in range(13):   \n    print(f'{month:5d} {x[0]:9.2f} {x[1]:9.2f} {x[2]:9.2f} {x[3]:9.2f}')  \n    x = T @ x \n\n-----------------------------------------------\nMonth     Grasses   Soil      Dead_OM   Streams\n-----------------------------------------------\n    0     20.00     60.00     15.00     20.00\n    1     17.60     62.80     14.00     20.60\n    2     15.59     65.22     12.96     21.23\n    3     13.90     67.29     11.93     21.88\n    4     12.49     69.03     10.93     22.55\n    5     11.31     70.46      9.99     23.24\n    6     10.32     71.61      9.13     23.95\n    7      9.48     72.52      8.33     24.66\n    8      8.79     73.21      7.61     25.39\n    9      8.20     73.71      6.97     26.12\n   10      7.71     74.04      6.40     26.86\n   11      7.29     74.22      5.89     27.60\n   12      6.94     74.28      5.44     28.34",
    "crumbs": [
      "**First Semester**",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Matrices</span>"
    ]
  },
  {
    "objectID": "ch_08_Matrices.html#worked-exercises",
    "href": "ch_08_Matrices.html#worked-exercises",
    "title": "6  Matrices",
    "section": "Worked Exercises",
    "text": "Worked Exercises\n1. \\(\\phantom{1}\\) Use the inverse of the matrix \\(\\mathbf{A}\\) to solve the system \\(\\mathbf{Ax}=\\mathbf{b}_i\\), where\n\\[\\displaystyle \\mathbf{A}=\\begin{pmatrix}\n1 & 2 & 3 \\\\\n2 & 3 & 0 \\\\\n0 & 1 & 2 \\\\\n\\end{pmatrix}\\]\nand the vectors \\(\\mathbf{b}_i\\), \\(i=1,2\\) are given by (a) \\(~\\mathbf{b}_1=\\begin{pmatrix}\n1\\\\\n1\\\\\n1\n\\end{pmatrix}\\), (b) \\(~\\mathbf{b}_2=\\begin{pmatrix}\n-2\\\\\n\\phantom{-}1\\\\\n\\phantom{-}3\n\\end{pmatrix}\\)\nSolution\n\\[\\color{blue}{\\mathbf{A}^{-1} = \\frac{1}{\\det(\\mathbf{A})} \\cdot \\text{adj}(\\mathbf{A}) = \\frac{1}{4} \\begin{pmatrix}\n\\phantom{-}6 & -1 & -9 \\\\\n-4 & \\phantom{-}2 & \\phantom{-}6 \\\\\n\\phantom{-}2 & -1 & -1 \\\\\n\\end{pmatrix}}\\]\nFor \\(\\mathbf{b}_1 = \\begin{pmatrix} 1 \\\\ 1 \\\\ 1 \\end{pmatrix}\\)\n\\[\\mathbf{x}_1 = \\mathbf{A}^{-1} \\mathbf{b}_1\n= \\frac{1}{4}\n\\begin{pmatrix}\n  \\phantom{-}6 & -1 & -9 \\\\\n  -4 & \\phantom{-}2 & \\phantom{-}6 \\\\\n  \\phantom{-}2 & -1 & -1 \\\\\n\\end{pmatrix}\n\\begin{pmatrix} 1 \\\\ 1 \\\\ 1 \\end{pmatrix} =\n\\color{blue}{\\begin{pmatrix} -1 \\\\ \\phantom{-}1 \\\\ \\phantom{-}0 \\end{pmatrix}}\\]\nFor \\(\\mathbf{b}_2 = \\begin{pmatrix} -2 \\\\ \\phantom{-}1 \\\\ \\phantom{-}3 \\end{pmatrix}\\)\n\\[ \\mathbf{x}_2 = \\frac{1}{4} \\begin{pmatrix}\n\\phantom{-}6 & -1 & -9 \\\\\n-4 & \\phantom{-}2 & \\phantom{-}6 \\\\\n\\phantom{-}2 & -1 & -1 \\\\\n\\end{pmatrix}\n\\begin{pmatrix} -2 \\\\ \\phantom{-}1 \\\\ \\phantom{-}3 \\end{pmatrix} = \\color{blue}{\\begin{pmatrix} -10 \\\\ \\phantom{-}7 \\\\ -2 \\end{pmatrix}}\\]\n\\(~\\)\n2. \\(\\phantom{1}\\) Use the given LU-factorization\n\\[\\mathbf{A} = \\begin{pmatrix}\n1 & 1 & 1 \\\\\n1 & 2 & 2\\\\\n1&  2 & 3 \\\\\n\\end{pmatrix} =\n\\begin{pmatrix}\n1 & 0 & 0 \\\\\n1 & 1 & 0 \\\\\n1 & 1 & 1 \\\\\n\\end{pmatrix}\n\\begin{pmatrix}\n1 & 1 & 1 \\\\\n0 & 1& 1 \\\\\n0 & 0 & 1 \\\\\n\\end{pmatrix}\\]\nto solve the linear system \\(\\mathbf{Ax}=\\mathbf{b_i}\\) for the given column matrix \\(\\mathbf{b}_i\\), \\(i=1,2,3,4\\)\n\\((a)\\) \\(~\\mathbf{b}_1=\\begin{pmatrix}\n2\\\\\n4\\\\\n1\n\\end{pmatrix}\\), \\((b)\\) \\(~\\mathbf{b}_2=\\begin{pmatrix}\n-4\\\\\n\\phantom{-}7\\\\\n10\n\\end{pmatrix}\\), \\((c)\\) \\(~\\mathbf{b}_3=\\begin{pmatrix}\n\\phantom{-}\\frac{1}{2}\\\\\n\\phantom{-}\\frac{3}{4}\\\\\n-\\frac{1}{2}\n\\end{pmatrix}\\), \\((d)\\) \\(~\\mathbf{b}_4=\\begin{pmatrix}\n\\phantom{-}30\\\\\n-42\\\\\n-18\n\\end{pmatrix}\\)\nSolution\nTo solve \\(\\mathbf{A}\\mathbf{x} = \\mathbf{b}\\), we use the factorization:\n\\[\\mathbf{A}\\mathbf{x} = \\mathbf{L}\\mathbf{U}\\mathbf{x} = \\mathbf{b}\\]\n\nStep 1: Solve \\(\\mathbf{L}\\mathbf{y} = \\mathbf{b}\\) (forward substitution) \nStep 2: Solve \\(\\mathbf{U}\\mathbf{x} = \\mathbf{y}\\) (backward substitution) \n\nWe will apply this for each given \\(\\mathbf{b}_i\\)\n(a) \\(~\\mathbf{b}_1 = \\begin{pmatrix} 2 \\\\ 4 \\\\ 1 \\end{pmatrix}\\)\nStep 1: \\(~\\)Solve \\(\\mathbf{L}\\mathbf{y} = \\mathbf{b}_1\\)\n\\[\\begin{pmatrix}\n1 & 0 & 0 \\\\\n1 & 1 & 0 \\\\\n1 & 1 & 1\n\\end{pmatrix}\n\\begin{pmatrix}\ny_1 \\\\ y_2 \\\\ y_3\n\\end{pmatrix} =\n\\begin{pmatrix}\n2 \\\\ 4 \\\\ 1\n\\end{pmatrix}\\]\n\n\\(y_1 = 2\\)\n\\(y_1 + y_2 = 4 \\Rightarrow y_2 = 2\\)\n\\(y_1 + y_2 + y_3 = 1 \\Rightarrow  y_3 = -3\\)\n\nSo:\n\\[\\mathbf{y} = \\begin{pmatrix} \\phantom{-}2 \\\\ \\phantom{-}2 \\\\ -3 \\end{pmatrix}\\]\nStep 2: \\(~\\)Solve \\(\\mathbf{U}\\mathbf{x} = \\mathbf{y}\\)\n\\[\\begin{pmatrix}\n1 & 1 & 1 \\\\\n0 & 1 & 1 \\\\\n0 & 0 & 1\n\\end{pmatrix}\n\\begin{pmatrix}\nx_1 \\\\ x_2 \\\\ x_3\n\\end{pmatrix} =\n\\begin{pmatrix}\n\\phantom{-}2 \\\\ \\phantom{-}2 \\\\ -3\n\\end{pmatrix}\\]\nBack substitution:\n\n\\(x_3 = -3\\)\n\\(x_2 + x_3 = 2 \\Rightarrow x_2 = 5\\)\n\\(x_1 + x_2 + x_3 = 2 \\Rightarrow  x_1 = 0\\)\n\n\\[\\color{red}{\\mathbf{x}_1 = \\begin{pmatrix} \\phantom{-}0 \\\\ \\phantom{-}5 \\\\ -3 \\end{pmatrix}}\\]\n(b) \\(~\\mathbf{b}_2=\\begin{pmatrix}\n-4\\\\\n\\phantom{-}7\\\\\n10\n\\end{pmatrix}\\)\nStep 1: \\(~\\mathbf{L}\\mathbf{y} = \\mathbf{b}_2\\)\n\n\\(y_1 = -4\\)\n\\(y_1 + y_2 = 7 \\Rightarrow y_2 = 11\\)\n\\(y_1 + y_2 + y_3 = 10 \\Rightarrow  y_3 = 3\\)\n\n\\[\\mathbf{y} = \\begin{pmatrix} -4 \\\\ \\phantom{-}11 \\\\ \\phantom{-}3 \\end{pmatrix}\\]\nStep 2: \\(~\\mathbf{U}\\mathbf{x} = \\mathbf{y}\\)\n\n\\(x_3 = 3\\)\n\\(x_2 + x_3 = 11 \\Rightarrow x_2 = 8\\)\n\\(x_1 + x_2 + x_3 = -4 \\Rightarrow x_1 = -15\\)\n\n\\[\\color{red}{\\mathbf{x}_2 = \\begin{pmatrix} -15 \\\\ \\phantom{-}8 \\\\ \\phantom{-}3 \\end{pmatrix}}\n\\]\n(c) \\(\\mathbf{b}_3 = \\begin{pmatrix} \\phantom{-}\\frac{1}{2} \\\\ \\phantom{-}\\frac{3}{4} \\\\ -\\frac{1}{2} \\end{pmatrix}\\)\nStep 1:\n\n\\(y_1 = \\frac{1}{2}\\)\n\\(y_1 + y_2 = \\frac{3}{4} \\Rightarrow y_2 = \\frac{1}{4}\\)\n\\(y_1 + y_2 + y_3 = -\\frac{1}{2} \\Rightarrow  y_3 = -\\frac{5}{4}\\)\n\n\\[\\mathbf{y} = \\begin{pmatrix} \\phantom{-}\\frac{1}{2} \\\\ \\phantom{-}\\frac{1}{4} \\\\ -\\frac{5}{4} \\end{pmatrix}\\]\nStep 2:\n\n\\(x_3 = -\\frac{5}{4}\\)\n\\(x_2 + x_3 = \\frac{1}{4} \\Rightarrow x_2 = \\frac{3}{2}\\)\n\\(x_1 + x_2 + x_3 = \\frac{1}{2} \\Rightarrow  x_1 = \\frac{1}{4}\\)\n\n\\[\\color{red}{\\mathbf{x}_3 = \\begin{pmatrix} \\phantom{-}\\frac{1}{4} \\\\ \\phantom{-}\\frac{3}{2} \\\\ -\\frac{5}{4} \\end{pmatrix}}\\]\n(d) \\(\\mathbf{b}_4 = \\begin{pmatrix} \\phantom{-}30 \\\\ -42 \\\\ -18 \\end{pmatrix}\\)\nStep 1:\n\n\\(y_1 = 30\\)\n\\(y_1 + y_2 = -42 \\Rightarrow y_2 = -72\\)\n\\(y_1 + y_2 + y_3 = -18 \\Rightarrow y_3 = 24\\)\n\n\\[\\mathbf{y} = \\begin{pmatrix} \\phantom{-}30 \\\\ -72 \\\\ \\phantom{-}24 \\end{pmatrix}\\]\nStep 2:\n\n\\(x_3 = 24\\)\n\\(x_2 + x_3 = -72 \\Rightarrow x_2 = -96\\)\n\\(x_1 + x_2 + x_3 = 30 \\Rightarrow  x_1 = 102\\)\n\n\\[\\color{red}{\\mathbf{x}_4 = \\begin{pmatrix} \\phantom{-}102 \\\\ -96 \\\\ \\phantom{-}24 \\end{pmatrix}}\n\\]\n\\(~\\)\n3. \\(~\\)Consider the matrix\n\\[\\mathbf{A}=\\begin{pmatrix}\n3 & 0 & 1\\\\\n0 & 1 & 0\\\\\n1 & 0 & 1\n\\end{pmatrix}\\]\n\nFind an orthogonal matrix \\(\\mathbf{P}\\) that diagonalizes \\(\\mathbf{A}\\) and the diagonal matrix \\(\\mathbf{D}\\)\nCalculate the eigenvalues and eigenvectors of the matrix\n\n\\[3\\mathbf{A}^2 +4\\mathbf{A}^{-1} -7\\mathbf{I}\\]\nSolution (a)\nCompute \\(\\det(\\mathbf{A} - \\lambda \\mathbf{I})\\):\n\\[\\mathbf{A} - \\lambda \\mathbf{I} =\n\\begin{pmatrix}\n3 - \\lambda & 0 & 1 \\\\\n0 & 1 - \\lambda & 0 \\\\\n1 & 0 & 1 - \\lambda\n\\end{pmatrix}\\]\nUse cofactor expansion along the 2nd row:\n\\[\\det(\\mathbf{A} - \\lambda \\mathbf{I}) = (1 -\\lambda)(\\lambda^2 -4\\lambda +2) = 0\\]\nSo the eigenvalues are:\n\\[\\color{blue}{\\lambda_1 = 1,\\quad \\lambda_2 = 2 + \\sqrt{2},\\quad \\lambda_3 = 2 - \\sqrt{2}}\\]\nLet’s find eigenvectors for each eigenvalue. We’ll normalize them at the end.\n\nFor \\(\\lambda = 1\\),\nSolve \\((\\mathbf{A} - \\mathbf{I}) \\mathbf{v}_1 = \\mathbf{0}\\):\n\\[ \\mathbf{A} - \\mathbf{I} = \\begin{pmatrix}\n  2 & 0 & 1 \\\\\n  0 & 0 & 0 \\\\\n  1 & 0 & 0\n  \\end{pmatrix} \\; \\Rightarrow \\;\n  \\color{blue}{\\mathbf{v}_1 = \\begin{pmatrix} 0 \\\\ 1 \\\\ 0 \\end{pmatrix}}\\]\nFor \\(\\lambda = 2 + \\sqrt{2}\\),\nSolve \\((\\mathbf{A} - \\lambda \\mathbf{I}) \\mathbf{v}_2 = \\mathbf{0}\\):\nThis yields:\n\\[\\color{blue}{\\mathbf{v}_2 = \\begin{pmatrix}\n  1 \\\\\n  0 \\\\\n  -1 + \\sqrt{2}\n  \\end{pmatrix}}\\]\nNormalize:\n\\[ \\|\\mathbf{v}_2\\| = \\sqrt{1^2 + (-1 + \\sqrt{2})^2} = \\sqrt{4 - 2\\sqrt{2}}\\]\nLet’s denote the normalized vector as \\(\\color{blue}{\\mathbf{u}_2 = \\mathbf{v}_2 /\\| \\mathbf{v_2}\\|}\\)\nFor \\(\\lambda = 2 - \\sqrt{2}\\),\nSimilarly:\n\\[\\color{blue}{\\mathbf{v}_3 = \\begin{pmatrix}\n  1 \\\\\n  0 \\\\\n  -1 - \\sqrt{2}\n  \\end{pmatrix}}\\]\n\\[ \\|\\mathbf{v}_3\\| = \\sqrt{1^2 + (1 + \\sqrt{2})^2} = \\sqrt{4 + 2\\sqrt{2}}\\]\nNormalize to get \\(\\color{blue}{\\mathbf{u}_3 = \\mathbf{v}_3 /\\| \\mathbf{v_3}\\|}\\)\n\nLet:\n\\[ \\color{blue}{\\mathbf{P} = \\begin{pmatrix} \\mathbf{v}_1 & \\mathbf{u}_2 & \\mathbf{u}_3 \\end{pmatrix}, \\quad\n\\mathbf{D} = \\begin{pmatrix}\n1 & 0 & 0 \\\\\n0 & 2 + \\sqrt{2} & 0 \\\\\n0 & 0 & 2 - \\sqrt{2}\n\\end{pmatrix}}\\]\nSolution (b)\nLet’s use the fact that if \\(\\mathbf{v}\\) is an eigenvector of \\(\\mathbf{A}\\) with eigenvalue \\(\\lambda\\), then:\n\\[\n\\begin{aligned}\n&\\mathbf{A}^2 \\mathbf{v} = \\lambda^2 \\mathbf{v} \\\\\n&\\mathbf{A}^{-1} \\mathbf{v} = \\lambda^{-1} \\mathbf{v} \\\\\n&\\mathbf{I} \\mathbf{v} = \\mathbf{v}\n\\end{aligned}\\]\nSo:\n\\[\\color{blue}{\\left( 3\\mathbf{A}^2 + 4\\mathbf{A}^{-1} - 7\\mathbf{I} \\right) \\mathbf{v} = \\left( 3\\lambda^2 + 4\\lambda^{-1} - 7 \\right) \\mathbf{v}}\\]\nSo the eigenvalues of this new matrix are:\n\\[ \\mu_i = 3\\lambda_i^2 + \\frac{4}{\\lambda_i} - 7\\]\nThis matrix shares the same set of eigenvectors with matrix \\(\\mathbf{A}\\)\nNow compute for each \\(\\lambda_i\\):\n\\[ \\begin{aligned}\n\\color{blue}{\\mu_1} &= 3(1)^2 + \\frac{4}{1} - 7 = 3 + 4 - 7 = 0 \\\\\n\\color{blue}{\\mu_2} &= 3(6 + 4\\sqrt{2}) + 4 \\cdot \\frac{2 - \\sqrt{2}}{2} - 7 = 15 + 10\\sqrt{2} \\\\\n\\color{blue}{\\mu_3} &= 3(6 - 4\\sqrt{2}) + 4 \\cdot \\frac{2 + \\sqrt{2}}{2} - 7 = 15 - 10\\sqrt{2}\n\\end{aligned}\\]\n\\(~\\)\n4. \\(\\phantom{1}\\) Simplify the following equation of conic section in two-dimensional space, and give the corresponding coordinate tranformation:\n\\[16 x^2 + 24xy +9y^2 -3x +4y =0\\]\nand plot the transformed conic section\nSolution\nWe diagonalized the quadratic part:\n\\[\\color{red}{\\mathbf{A} = \\begin{pmatrix} 16 & 12 \\\\ 12 & 9 \\end{pmatrix}}\\]\nThe eigenvalues of this matrix give the coefficients of the conic in the rotated \\((u,v)\\) coordinate system, and the eigenvectors provide the rotation matrix\nEigenvalues:\n\\[\\color{red}{\\lambda_1 = 0, \\quad \\lambda_2 = 25}\\]\nThese are the new coefficients of the quadratic terms in the rotated coordinate system. That is, the conic becomes:\n\\[\\color{red}{0 \\cdot u^2 + 25v^2 + \\text{(linear terms in } u, v) = 0}\\]\nEigenvectors (columns of rotation matrix):\n\\[\\color{red}{\\mathbf{P} = \\begin{pmatrix}\n\\phantom{-}\\frac{3}{5} & -\\frac{4}{5} \\\\\n-\\frac{4}{5} &  -\\frac{3}{5}\n\\end{pmatrix}}\\]\nThis gives the coordinate transformation from \\((u, v)\\) to \\((x, y)\\):\n\\[\\color{red}{\\mathbf{x} = \\mathbf{P}\\mathbf{y}}, \\quad \\mathbf{x}=\\begin{pmatrix} x \\\\ y\\end{pmatrix},\n\\quad \\mathbf{y}=\\begin{pmatrix} u \\\\ v\\end{pmatrix}\\]\nThe linear terms come from substituting \\(x = \\frac{3}{5}u - \\frac{4}{5}v\\), \\(y = -\\frac{4}{5}u - \\frac{3}{5}v\\) into the original equation’s linear part:\n\\[-3x + 4y = -3(\\frac{3}{5}u - \\frac{4}{5}v) + 4(-\\frac{4}{5}u - \\frac{3}{5}v)= -5u\\]\nSo, the full rotated equation becomes:\n\\[\\color{red}{5v^2 - u = 0}\\]\nThis is a parabola that opens along the \\(u\\)-axis with its vertex at the origin in the \\((u, v)\\) coordinate system\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Define v values\nv_vals = np.linspace(-2, 2, 400)\n\n# Compute corresponding u values from u = 5 *v^2\nu_vals = 5 *v_vals**2\n\n# Plot the parabola in (u, v) coordinates\nplt.figure(figsize=(8, 6))\n\nplt.plot(u_vals, v_vals, label=r'$5v^2 -u = 0$', color='purple')\nplt.gca().set_aspect('equal')\n\nplt.title(\"Conic Section in Rotated Coordinates (u, v)\")\nplt.xlabel(\"u\")\nplt.ylabel(\"v\")\nplt.grid(True)\nplt.legend()\nplt.show()\n\n\n\n\n\n\n\n\n\n# Step 1: Define the (u, v) parabola\nv_vals = np.linspace(-2, 2, 400)\nu_vals = 5 *v_vals**2  # From u = 5v^2\n\n# Step 2: Rotation matrix (from previous eigenvectors)\nP = np.array([[ 0.6, -0.8],\n              [-0.8, -0.6]])\n\n# Step 3: Stack u and v into coordinate array\nUV = np.vstack((u_vals, v_vals))  # Shape (2, N)\n\n# Step 4: Transform back to original (x, y) \n# using inverse rotation: [x, y]^T = P @ [u, v]^T\nXY = P @ UV\nx_vals, y_vals = XY[0, :], XY[1, :]\n\n# Step 5: Plot the result in original (x, y) coordinates\nplt.figure(figsize=(8, 6))\n\nplt.plot(x_vals, y_vals, label=\"Conic in (x, y)\", color='darkgreen')\nplt.gca().set_aspect('equal')\n\nplt.title(\"Original Conic in (x, y) Coordinates (Back-Transformed)\")\nplt.xlabel(\"x\")\nplt.ylabel(\"y\")\nplt.grid(True)\nplt.legend()\nplt.show()\n\n\n\n\n\n\n\n\n5. \\(~\\) Find the inverse of the given matrix or show that no inverse exists\n\\[\\begin{pmatrix}\n\\phantom {-} 1 & \\phantom {-}2 & \\phantom {-}3 & \\phantom {-}1\\\\\n-1& \\phantom {-}0 & \\phantom {-}2 &\\phantom {-}1 \\\\\n\\phantom {-}2&  \\phantom {-}1&  -3& \\phantom {-}0\\\\\n\\phantom {-}1& \\phantom {-}1 & \\phantom {-}2 & \\phantom {-}1\n\\end{pmatrix}\\]\nSolution\n\\[\\begin{aligned}\n\\mathbf{A} &= \\begin{pmatrix}\n\\phantom {-} 1 & \\phantom {-}2 & \\phantom {-}3 & \\phantom {-}1\\\\\n-1& \\phantom {-}0 & \\phantom {-}2 &\\phantom {-}1 \\\\\n\\phantom {-}2&  \\phantom {-}1&  -3& \\phantom {-}0\\\\\n\\phantom {-}1& \\phantom {-}1 & \\phantom {-}2 & \\phantom {-}1\n\\end{pmatrix} \\\\\n&\\Downarrow \\\\\n\\left ( \\mathbf{A} \\;| \\;\\mathbf{I} \\right ) \\;\\;\n&\\overset{\\text{row operations}}{\\longrightarrow}  \\left ( \\mathbf{I} \\;|\\; \\mathbf{A}^{-1} \\right ) \\\\\n&\\Downarrow \\\\\n\\mathbf{A}^{-1} &=\n\\begin{pmatrix}\n  -\\frac{1}{2} & -\\frac{2}{3} & -\\frac{1}{6} & \\phantom {-}\\frac{7}{6}\\\\\n  \\phantom {-}1& \\phantom {-}\\frac{1}{3} &  \\phantom {-}\\frac{1}{3}  &-\\frac{4}{3}  \\\\\n\\phantom {-}0& -\\frac{1}{3}&  -\\frac{1}{3}&  \\phantom {-}\\frac{1}{3}\\\\\n  -\\frac{1}{2}& \\phantom {-}1 & \\phantom {-}\\frac{1}{2} & \\phantom {-}\\frac{1}{2}\n\\end{pmatrix}\n\\end{aligned}\\]\n\\(~\\)\n6. \\(~\\) Use Cramer’s rule to determine the solution of the system\n\\[\\begin{matrix}\n(2-k) x_1&  +  & kx_2 & = &4 \\\\\nk x_1 & + & (3-k) x_2  & = &3\n\\end{matrix}\\]\nFor what value(s) of \\(k\\) is the system inconsistent?\nSolution\n\\[\\begin{aligned}\n\\mathbf{A } &=\\begin{pmatrix}\n2-k & k \\\\\nk & 3-k\n\\end{pmatrix},\\;\\;\\mathbf{b}=\\begin{pmatrix}\n4\\\\3\n\\end{pmatrix} \\\\\n&\\Downarrow \\\\\n\\left |  \\mathbf{A } \\right |&=\\begin{vmatrix}\n2-k & k \\\\\nk & 3-k\n\\end{vmatrix}=-5k+6 \\\\\n\\left |  \\mathbf{A }_1 \\right |&=\\begin{vmatrix}\n4 & k \\\\\n3 & 3-k\n\\end{vmatrix}=-7k+12, \\;\\; \\left |  \\mathbf{A}_2 \\right |=\\begin{vmatrix}\n2-k & 4 \\\\\nk & 3\n\\end{vmatrix}=-7k+6 \\\\\n&\\Downarrow \\\\\nx_1 &=\\frac{12-7k}{6-5k}, \\;\\;x_2=\\frac{6-7k}{6-5k} \\;\\;\\;\\;\\text{when}\\;\\;k\\neq\\frac{6}{5}\n\\end{aligned}\\]\n\\(~\\)\n7. \\(~\\) Simplify the following equations of quadric surfaces in three-dimensional space, and give the corresponding coordinate tranformations:\n\\[x^2 -2y^2 +z^2 +6xy -2yz +8x +2y -12z -3 = 0\\]\nSolution\n\\[\\begin{aligned}\n\\mathbf{A }=\\begin{pmatrix}\n1& \\phantom{-}3 &\\phantom{-}0 \\\\\n3& -2 &-1 \\\\\n0& -1 & \\phantom{-}1\n\\end{pmatrix}&, \\;   \\mathbf{x}=\\begin{pmatrix}\nx\\\\y \\\\z\n\\end{pmatrix} \\\\\n&\\Downarrow \\\\\n\\mathbf{x}^T \\mathbf{A}\\mathbf{x} +&\\begin{pmatrix}\n8 & 2 & -12\n\\end{pmatrix}\\mathbf{x}=3 \\\\\n&\\Downarrow \\\\\n|\\mathbf{A} -\\lambda \\mathbf{I} | = 0 \\;\\; \\Rightarrow \\;\\; & \\lambda_1=-4, \\;\\lambda_2=1, \\;\\lambda_3=3 \\\\\n\\lambda_1=-4 \\;\\;&\\rightarrow \\;\\; \\mathbf{k}_1 =\\frac{1}{\\sqrt{35}}  \\begin{pmatrix}\n-3\\\\\\phantom{-}5\\\\\\phantom{-}1 \\end{pmatrix}\\\\\n\\lambda_2=1 \\;\\;&\\rightarrow \\;\\; \\mathbf{k}_2 =\\frac{1}{\\sqrt{10}}  \\begin{pmatrix}\n1\\\\0\\\\3 \\end{pmatrix}\\\\\n\\lambda_3=3 \\;\\;&\\rightarrow \\;\\; \\mathbf{k}_3 =\\frac{1}{\\sqrt{14}}  \\begin{pmatrix}\n-3\\\\-2\\\\\\phantom{-}1 \\end{pmatrix}\\\\\n&\\Downarrow \\\\\n\\mathbf{P} =\\begin{pmatrix}\n\\mathbf{k}_1 &  \\mathbf{k}_2 &  \\mathbf{k}_3\n\\end{pmatrix} \\;\\; &\\Rightarrow \\;\\; \\mathbf{x} = \\mathbf{P}\\mathbf{y}\\\\\n&\\Downarrow \\\\\n  \\mathbf{y}^T  \\mathbf{P}^T \\mathbf{A} \\mathbf{P} \\mathbf{y} & +\n  \\begin{pmatrix}\n8 & 2 & -12\n\\end{pmatrix} \\mathbf{P} \\mathbf{y} =3\\\\\n\\end{aligned}\\]\n\\[\\begin{aligned}\n&\\Downarrow \\\\\n\\mathbf{y}^T\n\\begin{pmatrix}\n-4 &  \\phantom{-}0  &  \\phantom{-}0\\\\\n\\phantom{-}0 &  \\phantom{-}1 &  \\phantom{-}0\\\\\n  \\phantom{-}0 &  \\phantom{-}0 &  \\phantom{-}3\n\\end{pmatrix}  \\mathbf{y} &-\n\\begin{pmatrix}\n\\frac{26}{\\sqrt{35}} & \\frac{28}{\\sqrt{10}} & \\frac{40}{\\sqrt{14}}\n\\end{pmatrix}  \\mathbf{y}= 3\\\\\n&\\Downarrow \\\\\n{\\scriptsize -4\\left( y_1 + \\frac{13}{4\\sqrt{35}} \\right )^2}\n&{\\scriptsize +\\left( y_2 - \\frac{14}{\\sqrt{10}} \\right )^2 +3 \\left( y_3 - \\frac{20}{3\\sqrt{14}} \\right )^2 =\\frac{2597}{84}} \\\\\n&\\Downarrow \\\\\n-\\frac{(y_1 +0.5494)^2}{2.7801^2} &+ \\frac{(y_2 -4.4272)^2}{5.5603^2} + \\frac{(y_3 -1.7817)^2}{3.2102^2} \\approx 1 \\\\\n&\\Downarrow \\\\\n\\mathbf{z} &=  \\mathbf{y} +\\begin{pmatrix}\n\\phantom{-}0.5494 \\\\ -4.4272 \\\\ -1.7817\n\\end{pmatrix} \\\\\n&\\Downarrow \\\\\n-\\frac{z_1^2}{2.7801^2} &+ \\frac{z_2^2}{5.5603^2} + \\frac{z_3^2}{3.2102^2} \\approx 1 \\\\\n&\\Downarrow \\\\\n\\mathbf{x} &= \\mathbf{P}\\left[\\mathbf{z} -\\begin{pmatrix}\n\\phantom{-}0.5494 \\\\ -4.4272 \\\\ -1.7817\n\\end{pmatrix} \\right]\n\\end{aligned}\\]\n\\(~\\)\n8. \\(~\\) Simplify the following equations of quadric surfaces in three-dimensional space, and give the corresponding coordinate tranformations:\n\\[5x^2 + 6y^2 +7z^2 -4xy +4yz -10x +8y +14z -6=0\\]\nSolution\n\\[\\begin{aligned}\n\\mathbf{A }=\\begin{pmatrix}\n\\phantom{-}5 &-2 &\\phantom{-}0 \\\\\n-2& \\phantom{-}6  &\\phantom{-}2 \\\\\n  \\phantom{-}0& \\phantom{-}2  & \\phantom{-}7\n\\end{pmatrix}&, \\;   \\mathbf{x}=\\begin{pmatrix}\nx\\\\y \\\\z\n\\end{pmatrix} \\\\\n&\\Downarrow \\\\\n\\mathbf{x}^T \\mathbf{A}\\mathbf{x} +&\\begin{pmatrix}\n-10 & 8 & 14\n\\end{pmatrix}\\mathbf{x}=6 \\\\\n&\\Downarrow \\\\\n|\\mathbf{A} -\\lambda \\mathbf{I} | = 0 \\;\\; \\Rightarrow \\;\\; & \\lambda_1=3, \\;\\lambda_2=6, \\;\\lambda_3=9 \\\\\n\\lambda_1=3 \\;\\;&\\rightarrow \\;\\; \\mathbf{k}_1 =\\frac{1}{3}  \\begin{pmatrix}\n\\phantom{-}2\\\\\\phantom{-}2\\\\-1 \\end{pmatrix}\\\\\n\\lambda_2=6 \\;\\;&\\rightarrow \\;\\; \\mathbf{k}_2 =\\frac{1}{3}  \\begin{pmatrix}\n\\phantom{-}2\\\\-1\\\\\\phantom{-}2 \\end{pmatrix}\\\\\n\\lambda_3=9 \\;\\;&\\rightarrow \\;\\; \\mathbf{k}_3 =\\frac{1}{3}  \\begin{pmatrix}\n-1 \\\\ \\phantom{-}2 \\\\ \\phantom{-}2 \\end{pmatrix}\\\\\n&\\Downarrow \\\\\n\\mathbf{P} =\\begin{pmatrix}\n\\mathbf{k}_1 &  \\mathbf{k}_2 &  \\mathbf{k}_3\n\\end{pmatrix} \\;\\; &\\Rightarrow \\;\\; \\mathbf{x} = \\mathbf{P}\\mathbf{y}\n\\end{aligned}\\]\n\\[\n\\begin{aligned}\n&\\Downarrow \\\\\n\\mathbf{y}^T\n\\begin{pmatrix}\n3 & 0  & 0\\\\\n0 & 6 & 0\\\\\n0 & 0 & 9\n\\end{pmatrix}  \\mathbf{y} &+\n\\begin{pmatrix}\n6 &  0 & 18\n\\end{pmatrix} \\mathbf{y} = 6\\\\\n&\\Downarrow \\\\\n3(y_1^2 +2y_1 +1) +6y_2^2 &+9(y_3^2 +2y_3+1) =18 \\\\\n&\\Downarrow \\\\\n\\frac{(y_1 +1)^2}{6} +\\frac{y_2^2}{3} &+\\frac{(y_3+1)^2}{2} = 1 \\\\\n&\\Downarrow \\\\\n\\mathbf{z} &=  \\mathbf{y} +\\begin{pmatrix}\n\\phantom{-} 1 \\\\ \\phantom{-} 0 \\\\ \\phantom{-} 1\n\\end{pmatrix} \\\\\n&\\Downarrow \\\\\n\\frac{z_1^2}{6} &+ \\frac{z_2^2}{3} + \\frac{z_3^2}{2} = 1 \\\\\n&\\Downarrow \\\\\n\\mathbf{x} &= \\mathbf{P}\\left[\\mathbf{z} -\\begin{pmatrix}\n\\phantom{-} 1 \\\\ \\phantom{-} 0 \\\\ \\phantom{-}1\n\\end{pmatrix} \\right]\n\\end{aligned}\n\\]",
    "crumbs": [
      "**First Semester**",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Matrices</span>"
    ]
  },
  {
    "objectID": "ch_09_Vector_Calculus.html",
    "href": "ch_09_Vector_Calculus.html",
    "title": "10  Vector Calculus",
    "section": "",
    "text": "10.1 Vector Functions",
    "crumbs": [
      "**Second Semester**",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Vector Calculus</span>"
    ]
  },
  {
    "objectID": "ch_09_Vector_Calculus.html#sec-9-1",
    "href": "ch_09_Vector_Calculus.html#sec-9-1",
    "title": "10  Vector Calculus",
    "section": "",
    "text": "A parametric curve in space is a set of ordered triples \\((x,y,z),\\) \\(~\\)where\n\\[x=f(t), \\;y=g(t), \\;z=h(t)\\]\nare continuous on an interval: \\(~a \\leq t \\leq b\\)\nThe following\n\\[\\mathbf{r}(t) =\\left\\langle f(t), \\,g(t), \\,h(t) \\right\\rangle = f(t)\\,\\mathbf{i} +g(t)\\,\\mathbf{j} +h(t)\\,\\mathbf{k}\\]\nare vector-valued functions\n\n\n\n\n\n\n\nLimits\n\n\\(\\displaystyle\\lim_{t \\to a} \\mathbf{r}(t)=\\left\\langle \\lim_{t \\to a} f(t), \\;\\lim_{t \\to a} g(t), \\;\\lim_{t \\to a} h(t) \\right\\rangle\\)\nIf \\(~\\)\\(\\displaystyle\\lim_{t\\to a} \\mathbf{r}_1(t)=\\mathbf{L}_1\\) and \\(~\\)\\(\\displaystyle\\lim_{t\\to a} \\mathbf{r}_2(t)=\\mathbf{L}_2\\), \\(~\\)then\n\\(\\displaystyle\\lim_{t\\to a} c\\mathbf{r}_1(t)=c\\mathbf{L}_1\\)\n\\(\\displaystyle\\lim_{t\\to a} \\left[\\mathbf{r}_1(t) +\\mathbf{r}_2(t) \\right] =\\mathbf{L}_1 +\\mathbf{L}_2\\)\n\\(\\displaystyle\\lim_{t\\to a} \\left[\\mathbf{r}_1(t) \\cdot \\mathbf{r}_2(t) \\right] =\\mathbf{L}_1 \\cdot \\mathbf{L}_2\\)\n\nContinuity\n\nA vector function \\(\\mathbf{r}\\) is said to be continuous at \\(t=a\\) \\(\\,\\)if\n\\(\\displaystyle\\,\\mathbf{r}(a) \\;\\text{ is defined, }\\phantom{\\frac{1}{1}}\\)\n\\(\\displaystyle\\lim_{t\\to a} \\mathbf{r}(t)\\) exists, and\n\\(\\displaystyle\\lim_{t\\to a} \\mathbf{r}(t) = \\mathbf{r}(a)\\)\n\nDerivatives\n\nThe derivative of a vector function \\(\\mathbf{r}\\) is\n\\(\\displaystyle\\phantom{xx}\\mathbf{r}'(t) = \\lim_{\\Delta t\\to 0} \\frac{1}{\\Delta t} \\left[ \\mathbf{r}(t +\\Delta t) -\\mathbf{r}(t) \\right]\\)\nfor all \\(\\,t\\,\\) for which the limit exists\nIf \\(~\\mathbf{r}(t) = \\langle f(t), g(t), h(t) \\rangle,\\) \\(\\,\\)where \\(~f\\), \\(g\\), and \\(h\\,\\) are differentiable, then\n\\(\\phantom{xx}\\mathbf{r}'(t) = \\left\\langle f'(t), g'(t), h'(t) \\right\\rangle\\)\nWhen \\(\\mathbf{r}\\) have continuous first derivative and \\(\\mathbf{r}'(t)\\neq\\mathbf{0}~\\) for all \\(~t~\\) in the open interval \\((a,b),\\) \\(\\,\\)then \\(\\mathbf{r}\\) is said to be a smooth function\n\n\n\n\n\nIn the case of the second derivative, \\(\\,\\)we have\n\\(\\phantom{xx}\n\\mathbf{r}''(t) = \\left\\langle f''(t), g''(t), h''(t) \\right\\rangle\n= f''(t)\\mathbf{i} +g''(t)\\mathbf{j} +h''(t)\\mathbf{k}\\)\nChain rule\nIf \\(\\mathbf{r}\\) is a differentiable vector function and \\(s=u(t)\\) is a differentiable scalar function, then the derivative of \\(\\mathbf{r}(s)\\) with respect to \\(t\\) is\n\\(\\displaystyle\\phantom{xx}\\frac{d\\mathbf{r}}{dt} =\\frac{d\\mathbf{r}}{ds} \\frac{ds}{dt} =\\mathbf{r}'(s) u'(t)\\)\nRules of Differentiation\nLet \\(\\mathbf{r}_1(t)\\) and \\(\\mathbf{r}_2(t)\\) be differentiable vector functions and \\(u(t)\\) a differentiable scalar function\n\n\\(\\displaystyle \\frac{d}{dt} \\left[ \\mathbf{r}_1(t) +\\mathbf{r}_2(t) \\right] = \\mathbf{r}_1'(t) +\\mathbf{r}_2'(t)\\)\n\\(\\displaystyle \\frac{d}{dt} \\left[ u(t)\\mathbf{r}_1(t) \\right] = u(t)\\mathbf{r}_1'(t) +u'(t)\\mathbf{r}_1(t)\\)\n\\(\\displaystyle \\frac{d}{dt} \\left[ \\mathbf{r}_1(t) \\cdot \\mathbf{r}_2(t) \\right] = \\mathbf{r}_1(t)\\cdot\\mathbf{r}_2'(t) +\\mathbf{r}_1'(t)\\cdot\\mathbf{r}_2(t)\\)\n\\(\\displaystyle \\frac{d}{dt} \\left[ \\mathbf{r}_1(t) \\times \\mathbf{r}_2(t) \\right] = \\mathbf{r}_1(t)\\times\\mathbf{r}_2'(t) +\\mathbf{r}_1'(t)\\times\\mathbf{r}_2(t)\\)\n\n\nIntegrals\n\\(\\phantom{xx}\\displaystyle\\int \\mathbf{r}(t) \\,dt = \\left[ \\int f(t) \\,dt \\right]\\mathbf{i} +\\left[ \\int g(t) \\,dt \\right]\\mathbf{j} +\\left[ \\int h(t) \\,dt \\right]\\mathbf{k}\\)\nLength of a Space Curve\nIf \\(~\\mathbf{r}(t) = f(t)\\mathbf{i} +g(t)\\mathbf{j} +h(t)\\mathbf{k}\\,\\) is a smooth function, \\(\\,\\)then it can be shown that the length of the smooth curve traced by \\(\\,\\mathbf{r}\\,\\) is given by\n\\(\\phantom{xx}\\displaystyle s = \\int_a^b \\sqrt{\\left[ f'(t) \\right]^2 +\\left[ g'(t) \\right]^2 +\\left[ h'(t) \\right]^2}\\,dt =\\int_a^b \\left\\| \\mathbf{r}'(t) \\right\\|\\,dt\\)",
    "crumbs": [
      "**Second Semester**",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Vector Calculus</span>"
    ]
  },
  {
    "objectID": "ch_09_Vector_Calculus.html#sec-9-2",
    "href": "ch_09_Vector_Calculus.html#sec-9-2",
    "title": "10  Vector Calculus",
    "section": "10.2 Motion on a Curve",
    "text": "10.2 Motion on a Curve\n\nPosition\nSuppose a body moves along a curve \\(C\\) so that its position at time \\(t\\) is given by the vector function\n\\(\\phantom{xx}\\mathbf{r}(t) = f(t)\\mathbf{i} +g(t)\\mathbf{j} +h(t)\\mathbf{k}\\)\nVelocity and Acceleration\nIf \\(\\,f\\), \\(g\\), and \\(h\\) have second derivatives, \\(\\,\\) then the vectors\n\\(\\phantom{xx}\\begin{aligned}\n  \\mathbf{v}(t) &= \\mathbf{r}'(t) = f'(t)\\mathbf{i} +g'(t)\\mathbf{j} +h'(t)\\mathbf{k} \\\\\n  \\mathbf{a}(t) &= \\mathbf{r}''(t) = f''(t)\\mathbf{i} +g''(t)\\mathbf{j} +h''(t)\\mathbf{k}\n\\end{aligned}\\)\nare called the velocity and acceleration of the particle, respectively\n\nThe scalar function \\(\\left\\| \\mathbf{v}(t)\\right\\|\\) is the speed of the particle\nThe speed is related to arc length \\(s\\) by \\(s'(t) = \\left\\| \\mathbf{v}(t) \\right\\|\\)\nIf a particle moves with a constant speed \\(c\\), \\(\\,\\) then its acceleration vector is perpendicular to the velocity vector \\(\\mathbf{v}\\)\nTo see this, note that \\(\\mathbf{v}\\cdot\\mathbf{v}=c^2\\). \\(~\\) We differentiate both sides with respect to \\(t\\) and obtain\n\\[{\\frac{d}{dt}(\\mathbf{v} \\cdot \\mathbf{v}) = \\mathbf{v}\\cdot\\frac{d\\mathbf{v}}{dt} +\\frac{d\\mathbf{v}}{dt} \\cdot \\mathbf{v}\n=2\\mathbf{v}\\cdot\\frac{d\\mathbf{v}}{dt}=0}\\]\nThus, \\(\\displaystyle\\,\\frac{d\\mathbf{v}}{dt} \\cdot \\mathbf{v}=0\\) \\(\\text{ }\\)or\\(\\text{ }\\) \\(\\mathbf{a}(t) \\cdot \\mathbf{v}(t)=0~\\,\\) for all \\(\\,t\\)\n\nCentripetal Acceleration\nFor circular motion in the plane, described by \\(\\mathbf{r}(t) = r_0 \\cos\\omega t \\,\\mathbf{i} +r_0\\sin\\omega t \\,\\mathbf{j},\\) \\(\\,\\)it is evident that\n\\[\\mathbf{r}''=-\\omega^2\\mathbf{r}\\]\n\nThis means that the acceleration vector \\(\\mathbf{a}(t)=\\mathbf{r}''(t)\\) points in the direction opposite to that of the position vector \\(\\mathbf{r}(t)\\). \\(\\,\\) We then say \\(~\\mathbf{a}(t)\\) is centripetal acceleration",
    "crumbs": [
      "**Second Semester**",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Vector Calculus</span>"
    ]
  },
  {
    "objectID": "ch_09_Vector_Calculus.html#sec-9-3",
    "href": "ch_09_Vector_Calculus.html#sec-9-3",
    "title": "10  Vector Calculus",
    "section": "10.3 Curvature and Components of Acceleration",
    "text": "10.3 Curvature and Components of Acceleration\n\nWe know that \\(\\mathbf{r}'(t)\\) is a tangent vector to the curve \\(C\\), and consequently\n\\[\\mathbf{T}(t) = \\frac{\\mathbf{r}'(t)}{\\left\\| \\mathbf{r}'(t) \\right\\|}\\]\nis a unit tangent\n\n\n\n\n\n\n\\[\n\\begin{aligned}\n  \\frac{d\\mathbf{r}}{dt} &=\\frac{d\\mathbf{r}}{ds}\n    \\frac{ds}{dt}\\;\\text{ and so }\\\\[5pt]\n  \\frac{d\\mathbf{r}}{ds} =\n   \\frac{\\displaystyle\\frac{d\\mathbf{r}}{dt}} {\\displaystyle\\frac{ds}{dt}}\n   &=\\frac{\\mathbf{r}'(t)}{\\left\\| \\mathbf{r}'(t) \\right\\|}\n   = \\mathbf{T}(t)\n\\end{aligned}\\]\n\nLet \\(\\,\\mathbf{r}(t)\\,\\) be a vector function defining a smooth curve \\(C\\). \\(\\,\\)If \\(s\\) is the arc length parameter and \\(\\mathbf{T}\\) is the unit tangent vector, \\(\\,\\)then the curvature of \\(C\\) at a point is\n\\[\\kappa=\\left\\|\\frac{d\\mathbf{T}}{ds}\\right\\|\\]\nUsing the chain rule, \\(\\,\\)we can write\n\\[\\frac{d\\mathbf{T}}{dt}=\\frac{d\\mathbf{T}}{ds}\\frac{ds}{dt} \\;\\;\\Rightarrow\\;\\; \\frac{d\\mathbf{T}}{ds}=\\frac{\\displaystyle\\frac{d\\mathbf{T}}{\\displaystyle dt}}{\\frac{\\displaystyle ds}{\\displaystyle dt}}\\]\nIn other words, \\(\\,\\)curvature is given by \\(\\,\\kappa(t)=\\frac{\\displaystyle\\left\\|\\mathbf{T}'(t)\\right\\|}{\\displaystyle\\left\\|\\mathbf{r}'(t)\\right\\|}\\)\nTangential and Normal Components of Acceleration\n\nThe velocity of the particle on \\(C\\) is \\(~\\mathbf{v}(t)=\\mathbf{r}'(t)\\), \\(\\,\\)whereas its speed is \\(\\displaystyle\\frac{ds}{dt}=v=\\| \\mathbf{v}(t) \\|\\). \\(\\,\\) Thus, \\[\\mathbf{v}(t)=v\\mathbf{T}\\]\nDifferentiating this last expression with respect to \\(t\\) gives acceleration\n\\[\\mathbf{a}(t)=v\\frac{d\\mathbf{T}}{dt}+\\frac{dv}{dt}\\mathbf{T}\\]\nIt follows from the differentiation of \\(\\mathbf{T}\\cdot\\mathbf{T}=1\\,\\) that \\(\\displaystyle\\,\\mathbf{T}\\cdot \\frac{d\\mathbf{T}}{dt}=0\\). \\(\\,\\)If \\(\\displaystyle\\,\\left\\|\\frac{d\\mathbf{T}}{dt}\\right\\| \\neq 0\\), \\(\\,\\)the vector\n\\[\\mathbf{N}(t) = \\frac{\\frac{\\displaystyle d\\mathbf{T}}{\\displaystyle dt}}{\\left\\| \\frac{\\displaystyle d\\mathbf{T}}{\\displaystyle dt}\\right\\|}\\]\nis a unit normal to the curve \\(C\\). \\(\\,\\)The vector \\(\\mathbf{N}\\) is also called the principal normal\nSince curvature is \\(\\displaystyle\\,\\kappa = \\frac{\\left\\|\\frac{d\\mathbf{T}}{dt} \\right\\|}{v}\\), \\(\\,\\) it follows that \\(\\displaystyle\\frac{d\\mathbf{T}}{dt}=\\kappa v \\mathbf{N}\\). \\(\\,\\)Thus\n\\[\\mathbf{a}(t)=\\kappa v^2 \\mathbf{N} +\\frac{dv}{dt}\\mathbf{T} =a_N \\mathbf{N} +a_T\\mathbf{T}\\]\n\n\n\n\n\n\n\n\nBinormal: \\(\\,\\mathbf{B}(t)=\\mathbf{T}(t) \\times \\mathbf{N}(t)\\)\n\n\n\n\n\n\n\nFormulas for \\(a_T\\), \\(a_N\\), and Curvature\n\\[\n\\begin{aligned}\n  \\mathbf{v}\\cdot\\mathbf{a}\n    &= a_N(v\\mathbf{T}\\cdot\\mathbf{N})\n     +a_T(v\\mathbf{T}\\cdot\\mathbf{T})=a_T v\\\\[5pt]\n    &\\Rightarrow \\; a_T =\\frac{dv}{dt}\n     =\\frac{\\mathbf{v}\\cdot\\mathbf{a}}{\\| \\mathbf{v} \\|}\n     =\\frac{\\mathbf{r}'(t) \\cdot \\mathbf{r}''(t)}{\\| \\mathbf{r}'(t) \\|} \\\\ \\\\\n   \\mathbf{v}\\times\\mathbf{a}\n    &= a_N(v\\mathbf{T}\\times\\mathbf{N})\n     +a_T(v\\mathbf{T}\\times\\mathbf{T})\n      =a_N v\\mathbf{B}\\\\[5pt]\n   &\\Rightarrow \\; a_N =\\kappa v^2\n      =\\frac{\\|\\mathbf{v}\\times\\mathbf{a}\\|}{\\| \\mathbf{v} \\|}\n      =\\frac{\\| \\mathbf{r}'(t) \\times \\mathbf{r}''(t) \\|}{\\| \\mathbf{r}'(t) \\|} \\\\ \\\\\n\\kappa(t)&=\\frac{\\|\n   \\mathbf{r}'(t)\\times\\mathbf{r}''(t) \\|}{\\|\n     \\mathbf{r}'(t) \\|^3}\n\\end{aligned}\\]\nRadius of Curvature\n\\[\\rho=\\frac{1}{\\kappa}\\]\n\n\n\n\n\n\n\\(~\\)",
    "crumbs": [
      "**Second Semester**",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Vector Calculus</span>"
    ]
  },
  {
    "objectID": "ch_09_Vector_Calculus.html#sec-9-4",
    "href": "ch_09_Vector_Calculus.html#sec-9-4",
    "title": "10  Vector Calculus",
    "section": "10.4 Partial Derivatives",
    "text": "10.4 Partial Derivatives\n\nFunctions of Two Variables\n\n\n\n\n\n\n\nLevel Curves\n\n\n\n\n\n\n\nFunctions of Three Variables, Level Surfaces\n\\[w=F(x,y,z) \\;\\Rightarrow \\; c=F(x,y,z)\\]\nex) \\(~\\displaystyle w=\\frac{x^2+y^2}{z}\\)\n\n\n\n\n\n\n\nPartial Derivatives\n\\(\\phantom{xx}\n\\begin{aligned}\n  \\frac{\\partial z}{\\partial x}\n    &=\\lim_{\\Delta x \\to 0} \\frac{f(x+\\Delta x, y) -f(x,y)}{\\Delta x} = f_x \\\\\n  \\frac{\\partial z}{\\partial y}\n    &=\\lim_{\\Delta y \\to 0} \\frac{f(x, y+\\Delta y) -f(x,y)}{\\Delta y} = f_y \\\\\n  \\frac{\\partial^2 z}{\\partial x^2}\n    &=\\frac{\\partial}{\\partial x}\\left( \\frac{\\partial z}{\\partial x} \\right) = f_{xx} \\\\\n  \\frac{\\partial^2 z}{\\partial x \\partial y}\n    &=\\frac{\\partial}{\\partial x} \\left( \\frac{\\partial z}{\\partial y} \\right) = f_{xy}\n\\end{aligned}\\)\nChain Rule\nIf \\(z=f(u,v)\\) is differentiable and \\(u=g(x,y)\\) and \\(v=h(x,y)\\) have continuous first partial derivatives, \\(\\,\\)then\n\\[\n\\begin{aligned}\n  \\frac{\\partial z}{\\partial x}&= \\frac{\\partial z}{\\partial u}\\frac{\\partial u}{\\partial x}\n+\\frac{\\partial z}{\\partial v}\\frac{\\partial v}{\\partial x}\\\\\n  \\frac{\\partial z}{\\partial y}&= \\frac{\\partial z}{\\partial u}\\frac{\\partial u}{\\partial y}\n+\\frac{\\partial z}{\\partial v}\\frac{\\partial v}{\\partial y}\n\\end{aligned}\\]\nIf \\(z=f(u,v)\\) is differentiable and \\(u=g(t)\\,\\) and \\(v=h(t)\\) are differentiable functions of a single variable \\(t\\), \\(\\,\\)the ordinary derivative \\(\\displaystyle\\frac{dz}{dt}\\) is\n\\[\\frac{dz}{dt}=\\frac{\\partial z}{\\partial u}\\frac{du}{dt}+\\frac{\\partial z}{\\partial v}\\frac{dv}{dt}\\]\n\n\\(~\\)",
    "crumbs": [
      "**Second Semester**",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Vector Calculus</span>"
    ]
  },
  {
    "objectID": "ch_09_Vector_Calculus.html#sec-9-5",
    "href": "ch_09_Vector_Calculus.html#sec-9-5",
    "title": "10  Vector Calculus",
    "section": "10.5 Directional Derivatives",
    "text": "10.5 Directional Derivatives\n\nVector Differential Operator\n\\(\\phantom{xx}\n\\begin{aligned}\n   \\nabla\n   &= \\mathbf{i} \\frac{\\partial }{\\partial x} +\\mathbf{j}\\frac{\\partial }{\\partial y} \\\\\n   \\nabla\n   &= \\mathbf{i}\\frac{\\partial }{\\partial x} +\\mathbf{j}\\frac{\\partial }{\\partial y} +\\mathbf{k}\\frac{\\partial }{\\partial z}\n\\end{aligned}\\)\nGradient of a Function\n\\(\\phantom{xx}\n\\begin{aligned}\n  \\nabla f\n  &= \\mathbf{i} \\frac{\\partial f}{\\partial x} +\\mathbf{j}\\frac{\\partial f}{\\partial y} \\\\\n  \\nabla F\n  &= \\mathbf{i}\\frac{\\partial F}{\\partial x} +\\mathbf{j}\\frac{\\partial F}{\\partial y} +\\mathbf{k}\\frac{\\partial F}{\\partial z}\n\\end{aligned}\\)\n\n\\(~\\)\n\n\n\n\n\n\nGeneralization of Partial Differentiation\nThe directional derivative of \\(z=f(x,y)\\,\\) in the direction of a unit vector \\(\\mathbf{u}\\) \\(\\,\\)is\n\\[\n\\begin{aligned}\nD_\\mathbf{u} f(x,y) \\;\n   &= { \\scriptsize \\lim_{h\\to 0}\n    \\frac{f(x+\\Delta x, \\,y+\\Delta y) -f(x,y)}{h} }\\\\\n   &= { \\scriptsize \\lim_{h\\to 0}\n    \\frac{f(x+h\\cos\\theta, \\,y+h\\sin\\theta) -f(x,y)}{h} }\n     = \\nabla f(x,y)\\cdot \\mathbf{u}\n  \\end{aligned}\\]\n\n\n\n\n\n\n\nPartial Proof \\(\\,\\) Let \\(x\\), \\(y\\), and \\(\\theta\\) \\(\\,\\)be fixed so that\n\\[g(t)=f(x+t\\cos\\theta,\\,y+t\\sin\\theta)\\]\nis a function of one variable. \\(\\,\\)We wish to compare the value of \\(g'(0)\\), \\(\\,\\)which is found by two different methods\n\nFirst, \\(\\,\\)by the definition of a derivative,\n\n\\[{ g'(0)= {\\scriptsize \\lim_{h\\to 0} \\frac{g(0+h)-g(0)}{h}}\n= {\\scriptsize \\lim_{h\\to 0}\\frac{f(x+h\\cos\\theta, \\,y+h\\sin\\theta)-f(x,y)}{h} } }\\]\n\nSecond, \\(\\,\\)by the chain rule,\n\\[\n\\begin{aligned}\n  g'(t) &= {\\scriptsize f_1(x+t\\cos\\theta,\\,y+t\\sin\\theta)\\frac{d}{dt}(x+t\\cos\\theta) } \\\\\n      &\\qquad\n      {\\scriptsize +f_2(x+t\\cos\\theta,\\,y+t\\sin\\theta)\\frac{d}{dt}(x+t\\sin\\theta) } \\\\\n      &= {\\scriptsize f_1(x+t\\cos\\theta,\\,y+t\\sin\\theta)\\cos\\theta\n        +f_2(x+t\\cos\\theta,\\,y+t\\sin\\theta)\\sin\\theta } \\\\  \n      &\\big\\Downarrow\\;\\; t\\to 0 \\\\         \ng'(0) &= {\\scriptsize f_x(x,y)\\cos\\theta+f_y(x,y)\\sin\\theta } \\\\\n&={\\scriptsize \\left[\\,f_x(x,y)\\,\\mathbf{i}+f_y(x,y)\\,\\mathbf{j} \\,\\right]\\cdot \\left(\\cos\\theta\\,\\mathbf{i}+\\sin\\theta\\,\\mathbf{j}\\right) } \\\\[3pt]\n  &=\\nabla f(x,y)\\cdot \\mathbf{u}\n\\end{aligned}\\]\n\nFor a function \\(w=F(x,y,z)\\), \\(~\\)the directional derivative is defined by\n\\[\\begin{aligned}\nD_\\mathbf{u} F(x,y,z) &= {\\scriptsize \\lim_{h\\to 0} \\frac{F(x+h\\cos\\alpha, \\,y+h\\cos\\beta, \\,z +h\\cos\\gamma) -F(x,y,z)}{h} } \\\\\n&=\\nabla F(x,y,z)\\cdot \\mathbf{u}\n\\end{aligned}\\]\nMaximum Value of the Directional Derivative\n\\[\n\\begin{aligned}\n  D_\\mathbf{u}\\,f\n    &= \\| \\nabla f\\| \\| \\mathbf{u} \\| \\cos\\phi =\\| \\nabla f\\| \\cos\\phi \\\\[5pt]\n    &\\,\\Rightarrow\\, \\color{red}{-\\| \\nabla f\\| \\leq D_\\mathbf{u} \\,f \\leq \\| \\nabla f \\|}\n\\end{aligned}\\]\nThe gradient vector \\(\\nabla f\\) points in the direction in which \\(f\\) increases most rapidly, \\(\\,\\)whereas \\(-\\nabla f\\) points in the direction of the most decrease of \\(\\,f\\)\n\n\\(~\\)",
    "crumbs": [
      "**Second Semester**",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Vector Calculus</span>"
    ]
  },
  {
    "objectID": "ch_09_Vector_Calculus.html#sec-9-6",
    "href": "ch_09_Vector_Calculus.html#sec-9-6",
    "title": "10  Vector Calculus",
    "section": "10.6 Tangent Planes and Normal Lines",
    "text": "10.6 Tangent Planes and Normal Lines\n\n\\(\\nabla f\\) is orthogonal to the level curve at \\(P\\)\n\n\n\n\n\n\n\nThe derivative of \\(\\,f\\left(x(t), \\,y(t)\\right)=c\\,\\) with respect to \\(\\,t\\,\\) is\n\\[{\\frac{\\partial f}{\\partial x}\\frac{dx}{dt}\n      +\\frac{\\partial f}{\\partial y}\\frac{dy}{dt}=0\n    ={\\scriptsize \\left( \\frac{\\partial f}{\\partial x}\\mathbf{i}\n      +\\frac{\\partial f}{\\partial y} \\mathbf{j} \\right)\n      \\cdot \\left( \\frac{dx}{dt}\\mathbf{i} +\\frac{dy}{dt} \\mathbf{j} \\right) }\n      =\\nabla f \\cdot \\mathbf{r}'}\\]\nThe derivative of \\(\\,F\\left(x(t),y(t),z(t)\\right)=c\\,\\) implies that\n\\[\n\\begin{aligned}\n\\frac{\\partial F}{\\partial x}&\\frac{dx}{dt}\n   +\\frac{\\partial F}{\\partial y}\\frac{dy}{dt}\n   +\\frac{\\partial F}{\\partial z}\\frac{dz}{dt}=0 \\\\\n  &={\\scriptsize \\left( \\frac{\\partial F}{\\partial x}\\mathbf{i}\n   +\\frac{\\partial F}{\\partial y} \\mathbf{j}\n   +\\frac{\\partial F}{\\partial z} \\mathbf{k} \\right)\n   \\cdot\n  \\left( \\frac{dx}{dt}\\mathbf{i}\n         +\\frac{dy}{dt} \\mathbf{j}\n          +\\frac{dz}{dt} \\mathbf{k} \\right) }\n  =\\nabla F \\cdot \\mathbf{r}'\n\\end{aligned}\\]\n\\(\\nabla F\\) is normal to the level surface at \\(P\\)\n\n\n\n\n\n\n\nLet \\(\\,P(x_0,y_0,z_0)\\) be a point on the graph of \\(F(x,y,z)=c\\). \\(\\,\\)The tangent plane at \\(P\\) is\n\\[\\begin{aligned}\n  F_x&(x_0,y_0,z_0)(x-x_0) \\\\\n  & +F_y(x_0,y_0,z_0)(y-y_0) \\\\\n  &\\qquad +F_z(x_0,y_0,z_0)(z-z_0)=0\n  \\end{aligned}\\]\nThe line containing \\(P(x_0,y_0,z_0)\\) \\(\\,\\)that is parallel to \\(\\nabla F(x_0,y_0,z_0)\\) \\(\\,\\)is called the normal line to the surface at \\(P\\)\n\n\\(~\\)",
    "crumbs": [
      "**Second Semester**",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Vector Calculus</span>"
    ]
  },
  {
    "objectID": "ch_09_Vector_Calculus.html#sec-9-7",
    "href": "ch_09_Vector_Calculus.html#sec-9-7",
    "title": "10  Vector Calculus",
    "section": "10.7 Gradient, Curl and Divergence",
    "text": "10.7 Gradient, Curl and Divergence\n\nVector Fields - Vector functions of two or three variables\n\\[\n\\begin{aligned}\n\\mathbf{f}(x,y)\n   &= P(x,y) \\mathbf{i} +Q(x,y)\\mathbf{j}\\\\\n\\mathbf{f}(x,y,z)\n   &= P(x,y,z) \\mathbf{i}\n    +Q(x,y,z)\\mathbf{j} +R(x,y,z)\\mathbf{k}\n\\end{aligned}\\]\n\n\n\n\n\n\n\nThe del operator combined with a scalar function \\(\\,\\phi(x,y,z)\\) \\(\\,\\)produces a vector field\n\\[\\mathbf{f}(x,y,z)=\\nabla \\phi\n= \\mathrm{grad}\\, f   \n=\\frac{\\partial \\phi}{\\partial x}\\mathbf{i}\n+\\frac{\\partial \\phi}{\\partial y}\\mathbf{j}\n  +\\frac{\\partial \\phi}{\\partial z}\\mathbf{k}\n  \\;\\Rightarrow\\; \\delta_{ij} \\mathbf{e}_i \\frac{\\partial \\phi}{\\partial x_j}\\]\ncalled the gradient of \\(\\phi\\), \\(\\,\\)where \\(\\,\\delta_{ij}\\) \\(\\,\\)is Kronecker Delta\nThe curl of a vector field \\(~\\mathbf{f}=P\\,\\mathbf{i} +Q\\,\\mathbf{j} +R\\,\\mathbf{k}\\,\\) is the vector field\n\\[\\nabla \\times \\mathbf{f}=\n\\mathrm{curl}\\, \\mathbf{f}=\n  \\begin{vmatrix}\n     \\mathbf{i} & \\mathbf{j} & \\mathbf{k}\\\\\n     \\frac{\\partial }{\\partial x} & \\frac{\\partial }{\\partial y} & \\frac{\\partial }{\\partial z} \\\\\n     P & Q & R\n   \\end{vmatrix}\n\\; \\Rightarrow \\; \\varepsilon_{ijk} \\mathbf{e}_i \\frac{\\partial}{\\partial x_j} f_k\\]\nwhere \\(\\,\\varepsilon_{ijk}\\) is Levi-Civita Symbol\nThe divergence of a vector field \\(~\\mathbf{f}=P\\,\\mathbf{i} +Q\\,\\mathbf{j} + R\\,\\mathbf{k}~\\) is the scalar function\n\\[\\nabla \\cdot \\mathbf{f}=\n\\mathrm{div} \\,\\mathbf{f}=\n\\frac{\\partial P}{\\partial x}\n+\\frac{\\partial Q}{\\partial y}\n+\\frac{\\partial R}{\\partial z}\n\\;\\Rightarrow\\;\\delta_{ij}\n\\frac{\\partial}{\\partial x_i} f_j\\]\nTwo important properties and plus alpha\n \\(\\phantom{xx}\n\\begin{aligned}\n  \\nabla\\times\\nabla f\n   &= \\mathrm{curl}(\\mathrm{grad}\\, f) = \\mathbf{0}\\\\\n   &\\Rightarrow \\; \\varepsilon_{ijk} \\mathbf{e}_i\n    \\frac{\\partial}{\\partial x_j}\n    \\frac{\\partial f}{\\partial x_k} = 0\n     \\,\\mathbf{e}_i=\\mathbf{0}\n\\end{aligned}\\) \n \\(\\phantom{xx}\n\\begin{aligned}\n\\nabla \\cdot(\\nabla\\times\\mathbf{f})\n  &= \\mathrm{div}(\\mathrm{curl} \\,\\mathbf{f}) =0 \\\\\n   &\\Rightarrow \\; \\delta_{ij}\n   \\frac{\\partial}{\\partial x_i} \\varepsilon_{jlm}\n   \\frac{\\partial}{\\partial x_l} f_m =\n   \\varepsilon_{ilm}\n   \\frac{\\partial^2}{\\partial x_i \\partial x_l} f_m=0\n\\end{aligned}\\) \n \\(\\displaystyle \\phantom{xx}\n\\nabla \\cdot \\nabla f\n  =\\nabla^2 f \\;\n  \\Rightarrow \\; \\delta_{ij}\n  \\frac{\\partial}{\\partial x_i}\n  \\frac{\\partial f}{\\partial x_j}\n  =\\frac{\\partial^2 f}{\\partial x_i^2}\\) \n \\(\\displaystyle \\phantom{xx}\n\\nabla \\cdot \\nabla \\mathbf{f}\n=\\nabla^2 \\mathbf{f} \\;\n\\Rightarrow \\; \\delta_{ij}\n  \\frac{\\partial}{\\partial x_i}\n   \\mathbf{e}_m \\frac{\\partial f_m}{\\partial x_j}\n=\\mathbf{e}_m\n   \\frac{\\partial^2 f_m}{\\partial x_i^2}\\) \n \\(\\displaystyle \\phantom{xx}\n\\begin{aligned}\n\\nabla \\times (\\nabla \\times \\mathbf{f})\n&= \\nabla (\\nabla \\cdot \\mathbf{f}) -\\nabla^2 \\mathbf{f} \\\\\n&\\Rightarrow \\;\n\\varepsilon_{ijk} \\mathbf{e}_i\n  \\frac{\\partial}{\\partial x_j}\n   \\varepsilon_{klm}\n   \\frac{\\partial}{\\partial x_l}f_m \\\\\n&\\qquad\n=(\\delta_{il}\\delta_{jm}-\\delta_{im}\\delta_{jl})\n\\mathbf{e}_i \\frac{\\partial}{\\partial x_j}\n  \\frac{\\partial}{\\partial x_l}f_m \\\\\n&\\qquad\n=\\mathbf{e}_i \\frac{\\partial}{\\partial x_i}\n\\frac{\\partial f_j}{\\partial x_j}\n-\\mathbf{e}_i \\frac{\\partial^2 f_i}{\\partial x_j^2}\n\\end{aligned}\\) \nProduct Rules with \\(\\nabla\\)\n\\(\\phantom{xx}\n\\begin{aligned}\n\\nabla (fg) &= f \\nabla g + g \\nabla f \\\\\n\\nabla (\\mathbf{u} \\cdot \\mathbf{v})\n   &= \\mathbf{u} \\times (\\nabla \\times \\mathbf{v})\n     +\\mathbf{v} \\times (\\nabla \\times \\mathbf{u}) \\\\\n     &+(\\mathbf{u} \\cdot \\nabla) \\mathbf{v}\n     +(\\mathbf{v} \\cdot \\nabla) \\mathbf{u}\n   = \\mathbf{v} \\cdot \\nabla \\mathbf{u}\n     +\\mathbf{u} \\cdot \\nabla \\mathbf{v} \\\\\n\\nabla \\cdot (f \\mathbf{u})\n   &= f (\\nabla \\cdot \\mathbf{u})\n     +\\mathbf{u} \\cdot \\nabla f \\\\\n\\nabla \\cdot (\\mathbf{u} \\times \\mathbf{v})\n    &= \\mathbf{v} \\cdot (\\nabla \\times \\mathbf{u})\n     -\\mathbf{u} \\cdot (\\nabla \\times \\mathbf{v}) \\\\\n  \\nabla \\times (f \\mathbf{u})\n    &= \\nabla f \\times \\mathbf{u}\n     +f (\\nabla \\times \\mathbf{u}) \\\\\n  \\nabla \\times (\\mathbf{u} \\times \\mathbf{v})\n    &= \\mathbf{u} (\\nabla \\cdot \\mathbf{v})\n     -\\mathbf{v} (\\nabla \\cdot \\mathbf{u})\n     +(\\mathbf{v} \\cdot \\nabla) \\mathbf{u}\n     -(\\mathbf{u} \\cdot \\nabla) \\mathbf{v}\n\\end{aligned}\\)\nGeneralized Curvlinear Coorindates: \\(\\,\\) \\(\\mathbf{q}_i\\), \\(\\,q_i\\), \\(\\,h_i\\)\n\\[\n\\begin{aligned}\n\\nabla f \\;\n&= {\\scriptsize \\mathbf{q}_1 \\frac{1}{h_1}\n    \\frac{\\partial f}{\\partial q_1}\n  + \\mathbf{q}_2 \\frac{1}{h_2}\n    \\frac{\\partial f}{\\partial q_2}\n  + \\mathbf{q}_3 \\frac{1}{h_3}\n    \\frac{\\partial f}{\\partial q_3} } \\\\ \\\\\n\\nabla \\cdot \\mathbf{u} \\;\n&= {\\scriptsize \\frac{1}{h_1 h_2 h_3}\n    \\left[ \\frac{\\partial }{\\partial q_1}\n      \\left( u_1h_2h_3 \\right)\n  + \\frac{\\partial }{\\partial q_2}\n      \\left( u_2h_1h_3 \\right)  \n  + \\frac{\\partial }{\\partial q_3}\n      \\left( u_3h_1h_2 \\right)\\right] } \\\\ \\\\\n\\nabla^2 f\\;\n& = {\\scriptsize \\frac{1}{h_1 h_2 h_3}\n  \\left[ \\frac{\\partial }{\\partial q_1}\n    \\left( \\frac{h_2 h_3}{h_1}\n      \\frac{\\partial f}{\\partial q_1} \\right)\n    + \\frac{\\partial }{\\partial q_2}\n    \\left( \\frac{h_1 h_3}{h_2}\n      \\frac{\\partial f}{\\partial q_2}  \\right)  \n    + \\frac{\\partial }{\\partial q_3}\n      \\left( \\frac{h_1 h_2}{h_3}\n      \\frac{\\partial f}{\\partial q_3} \\right)\\right] }\\\\ \\\\\n\\nabla \\times \\mathbf{u} \\;\n  &={\\scriptsize \\frac{1}{h_1 h_2 h_3}\n  \\begin{vmatrix}\n   h_1 \\mathbf{q}_1 & h_2 \\mathbf{q}_2 & h_3 \\mathbf{q}_3\\\\\n   \\displaystyle\\frac{\\partial }{\\partial q_1} & \\displaystyle\\frac{\\partial }{\\partial q_2} & \\displaystyle\\frac{\\partial }{\\partial q_3}  \\\\\n   h_1 u_1 & h_2 u_2 & h_3 u_3\n\\end{vmatrix}}\n\\end{aligned}\\]\n\nCylindrical Coordinates\n\\(\\phantom{xx} q_1=r, \\;q_2=\\theta, \\; q_3=z\\)\n\\(\\phantom{xx} h_1=h_r=1, \\;\\;h_2=h_\\theta=r, \\;\\;h_3=h_z=1\\)\nSpherical Coordinates\n\\(\\phantom{xx} q_1=\\rho, \\;q_2=\\phi, \\; q_3=\\theta\\)\n\\(\\phantom{xx} h_1=h_\\rho=1, \\;\\;h_2=h_\\phi=\\rho, \\;\\;h_3=h_\\phi=\\rho \\sin\\phi\\)\n\nPhysical Interpretations\n\n\n\n\n\n\n\n\n\n\n\n\\(~\\)",
    "crumbs": [
      "**Second Semester**",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Vector Calculus</span>"
    ]
  },
  {
    "objectID": "ch_09_Vector_Calculus.html#sec-9-8",
    "href": "ch_09_Vector_Calculus.html#sec-9-8",
    "title": "10  Vector Calculus",
    "section": "10.8 Line Integrals",
    "text": "10.8 Line Integrals\n\nIntegration of a function defined along a curve\n\n\n\n\n\n\n\nDefinite Integral\n\\[\\lim_{\\|P\\|\\to 0} \\sum_{k=1}^n f(x_k^*)\\Delta x_k = \\int_a^b f(x)\\,dx\\]\n\n\n\n\n\n\n\nLine Integrals in the Plane\n\n\n\n\n\n\n\\[\n\\begin{aligned}\n    \\lim_{\\| P \\|\\to 0} \\sum_{k=1}^n G(x_k^*, y_k^*)\\,\\Delta x_k &= \\int_C G(x,y)\\,dx\\\\\n    \\lim_{\\| P \\|\\to 0} \\sum_{k=1}^n G(x_k^*, y_k^*)\\,\\Delta y_k &=\\int_C G(x,y)\\,dy\\\\\n    \\lim_{\\| P \\|\\to 0} \\sum_{k=1}^n G(x_k^*, y_k^*)\\,\\Delta s_k &= \\int_C G(x,y)\\,ds\n\\end{aligned}\\]\n\nMethod of Evaluation\n\nCurve Defined Parametrically, \\(\\;x=f(t)\\), \\(\\;y=g(t)\\), \\(\\;a\\leq t \\leq b\\)\n\\[\n\\begin{aligned}\n  \\int_C G(x,y)\\,dx\n   &=\\int_a^b G(\\,f(t), g(t)) \\,f'(t)\\,dt \\\\\n  \\int_C G(x,y)\\,dy\n   &=\\int_a^b G(\\,f(t), g(t)) \\,g'(t)\\,dt \\\\\n  \\int_C G(x,y)\\,ds\n   &=\\int_a^b G(\\,f(t), g(t)) \\,\\sqrt{[f'(t)]^2\n   +[g'(t)]^2}\\,dt\n\\end{aligned}\\]\nCurve Defined by an Explicit Function, \\(\\;y=f(x)\\), \\(\\;a\\leq x \\leq b\\)\n\\[\n\\begin{aligned}\n  \\int_C G(x,y)\\,dx\n   &=\\int_a^b G(x, f(x)) \\,dx \\\\\n  \\int_C G(x,y)\\,dy\n   &=\\int_a^b G(x, f(x)) \\,f'(x)\\,dx \\\\\n  \\int_C G(x,y)\\,ds\n   &=\\int_a^b G(x, f(x)) \\,\\sqrt{1\n    +[f'(x)]^2}\\,dx\n\\end{aligned}\\]\n\nNotation\n\\[{\\displaystyle\\int_C P\\,dx +\\int_C Q\\,dy \\,\\Rightarrow \\, \\int_C P\\,dx +Q \\,dy}\\]\n\\[{\\displaystyle\\int_{-C} P\\,dx +Q\\,dy = -\\int_C P\\,dx +Q\\,dy}\\]\n\n\n\n\n\n\n\nA line integral along a closed curve \\(C\\)\n\\[\\oint_C P \\,dx +Q \\,dy\\]\nLine Integrals in Space\n\\[\n\\begin{aligned}\n\\int_C &G(x,y,z)\\,ds = \\\\ &\\int_a^b G(\\,f(t), g(t),h(t)) \\,\\sqrt{[f'(t)]^2 +[g'(t)]^2 +[h'(t)]^2}\\,dt\n\\end{aligned}\\]\nSuppose the vector-valued function\n\\[\\mathbf{f}(x,y,z)=P(x,y,z)\\mathbf{i} +Q(x,y,z)\\mathbf{j} +R(x,y,z)\\mathbf{k}\\]\nis defined along a curve \\(C\\) and\n\\[d\\mathbf{r}=dx\\mathbf{i} +dy\\mathbf{j} +dz\\mathbf{k}\\,\\]\nis the displacement vector of points on \\(C\\), \\(\\,\\)then\n\\[\\int_C \\mathbf{f}\\cdot \\,d\\mathbf{r} =\\int_C P(x,y,z) \\, dx +Q(x,y,z) \\, dy +R(x,y,z) \\, dz\\]\nWork and Circulation \\[W =\\int_C \\mathbf{f} \\cdot d\\mathbf{r} = \\int_C \\mathbf{f}\\cdot \\mathbf{T} \\,ds \\] \\[\\mathrm{Circulation}= \\oint_C \\mathbf{f} \\cdot d\\mathbf{r} = \\oint_C \\mathbf{f}\\cdot \\mathbf{T} \\,ds \\]\n\n\n\n\n\n\n\\(~\\)",
    "crumbs": [
      "**Second Semester**",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Vector Calculus</span>"
    ]
  },
  {
    "objectID": "ch_09_Vector_Calculus.html#sec-9-9",
    "href": "ch_09_Vector_Calculus.html#sec-9-9",
    "title": "10  Vector Calculus",
    "section": "10.9 Independence of the Path",
    "text": "10.9 Independence of the Path\n\nThe value of a line integral\n\\[\\int_C \\mathbf{F}\\cdot \\,d\\mathbf{r}\\]\ndepends on the path of integration\nStated another way, \\(\\,\\)if \\(C_1\\) and \\(C_2\\) are two different paths between the same points \\(A\\) and \\(B\\), \\(\\,\\)then we expect that \\[\\int_{C_1} \\mathbf{F}\\cdot \\,d\\mathbf{r} \\neq \\int_{C_2} \\mathbf{F}\\cdot \\,d\\mathbf{r}\\]\nPath Independence\nA line integral \\(\\displaystyle\\int_C \\mathbf{f}\\cdot d\\mathbf{r}~\\) is independence of the path if\n\\[\\int_{C_1} \\mathbf{f}\\cdot d\\mathbf{r}=\\int_{C_2}\\mathbf{f}\\cdot d\\mathbf{r}\\]\nfor any two paths \\(C_1\\) and \\(C_2\\) between \\(A\\) and \\(B\\)\nA vector function \\(\\mathbf{f}\\) in 2- or 3-space is said to be conservative if \\(\\,\\mathbf{f}\\) can be written as the gradient of a scalar function \\(\\phi\\). \\(\\,\\)The function \\(\\phi\\) is called a potential function for \\(\\,\\mathbf{f}\\)\nIn other words, \\(\\,\\mathbf{f}\\) is conservative if there exists a function \\(\\phi\\) such that \\(\\,\\mathbf{f}=\\nabla \\phi\\). \\(\\,\\)A conservative vector field is also called a gradient vector field\nFundamental Theorem\nIf \\(~\\mathbf{f}(x,y,z)~\\) is a conservative vector field in \\(R\\) and \\(\\phi\\) is a potential function for \\(\\,\\mathbf{f}\\), \\(\\,\\)then\n\\[\\int_C \\mathbf{f}\\cdot d\\mathbf{r}=\\int_C \\nabla\\phi \\cdot d\\mathbf{r}=\\phi(B) -\\phi(A)\\]\nwhere \\(A=(x(a), y(a), z(a))~\\) and \\(\\,B=(x(b), y(b), z(b))\\)\n\n\n\n\n\nEquivalent Concepts: \\(\\,\\)In an open connected region \\(R\\), \\(\\displaystyle\\int_C \\mathbf{f}\\cdot d\\mathbf{r}\\) is independent of the path \\(C\\)\n\nif and only if the vector field \\(\\mathbf{f}\\) is conservative in \\(R\\)\nif and only if \\(\\displaystyle\\oint_{C'} \\mathbf{f}\\cdot d\\mathbf{r}=0\\,\\) for every closed path \\(\\,C'\\) in \\(R\\)\n\nTest for a Conservative Field:\n\nSuppose\n\\[\\mathbf{f}(x,y,z)=P(x,y,z)\\mathbf{i} +Q(x,y,z)\\mathbf{j} +R(x,y,z)\\mathbf{k}\\]\nis a conservative vector field in an open region of 3-space, and that \\(P\\), \\(Q\\), and \\(R\\) are continuous and have continuous first partial derivatives in that region\nThen \\(\\,\\mathbf{f}=\\nabla \\phi\\,\\) and \\(\\,\\nabla\\times\\mathbf{f}=\\nabla\\times\\nabla\\phi=\\mathbf{0}\\), \\(\\,\\)that is\n\\[{\\scriptsize \\nabla\\times\\mathbf{f}=\\left( \\frac{\\partial R}{\\partial y} -\\frac{\\partial Q}{\\partial z}\\right)\\mathbf{i}\n+\\left( \\frac{\\partial P}{\\partial z} -\\frac{\\partial R}{\\partial x}\\right)\\mathbf{j}\n+\\left( \\frac{\\partial Q}{\\partial x} -\\frac{\\partial P}{\\partial y}\\right)\\mathbf{k}=\\mathbf{0}}\\;\\;\\]\n\\[~\\;\\Downarrow\\]\n\\[\\;\\;{\\frac{\\partial P}{\\partial y} =\\frac{\\partial Q}{\\partial x}, \\;\\frac{\\partial P}{\\partial z} =\\frac{\\partial R}{\\partial x},\\;\n\\frac{\\partial Q}{\\partial z} =\\frac{\\partial R}{\\partial y}}\\]\nfor all \\(\\,(x,y,z)\\) in that region\n\n\n\\(~\\)",
    "crumbs": [
      "**Second Semester**",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Vector Calculus</span>"
    ]
  },
  {
    "objectID": "ch_09_Vector_Calculus.html#sec-9-10",
    "href": "ch_09_Vector_Calculus.html#sec-9-10",
    "title": "10  Vector Calculus",
    "section": "10.10 Double Integrals",
    "text": "10.10 Double Integrals\n\nThe Double Integral\nLet \\(\\,f\\) be a function of two variables defined on a closed region \\(R\\). Then the double integral of \\(\\,f\\) over \\(R\\) \\(\\,\\)is given by\n\\[\\lim_{\\|P\\|\\to 0} \\sum_{k=1}^n f\\left(x_k^*, y_k^*\\right) \\Delta A_k = \\iint_R f(x,y)\\,dA\\]\nArea and Volume\n\\[ A=\\iint_R dA \\]\n\\[ V=\\iint_R f(x,y) \\,dA \\]\n\n\n\n\n\n\n\nProperties of Double Integrals\n\\[ \\iint_R kf(x,y) \\,dA = k \\iint_R f(x,y) \\,dA \\]\n\\[ \\iint_R f(x,y) \\pm g(x,y)  \\,dA = \\iint_R f(x,y) \\,dA \\pm \\iint_R g(x,y) \\,dA \\]\n\\[ \\iint_R f(x,y) \\,dA = \\iint_{R_1} f(x,y) \\,dA + \\iint_{R_2} f(x,y) \\,dA \\]\n\n\n\n\n\nEvaluation of Double Integrals\n\n\n\n\n\n\nIf \\(R\\) is of Type I, \\(\\,\\)then\n\\[{\\iint_R f(x,y) \\,dA =\\int_a^b\\int_{g_1(x)}^{g_2(x)} f(x,y) \\,dydx}\\]\nIf \\(R\\) is of Type II, \\(\\,\\)then\n\\[{\\iint_R f(x,y) \\,dA =\\int_c^d\\int_{h_1(y)}^{h_2(y)} f(x,y) \\,dxdy}\\]\n\nCenter of Mass and Moments of Inertia\n\\[ \\bar{x} = \\frac{\\displaystyle\\iint_R x\\rho(x,y)\\,dA}{M} \\, \\;\\textrm{ and } \\; \\bar{y}=\\frac{\\displaystyle\\iint_R y\\rho(x,y) \\,dA}{M}\\]\n\\[ I_y=\\iint_R x^2\\rho(x,y) \\, dA\\;\\textrm{ and } \\; I_x=\\iint_R y^2\\rho(x,y) \\,dA \\]\n\n\\(~\\)",
    "crumbs": [
      "**Second Semester**",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Vector Calculus</span>"
    ]
  },
  {
    "objectID": "ch_09_Vector_Calculus.html#sec-9-11",
    "href": "ch_09_Vector_Calculus.html#sec-9-11",
    "title": "10  Vector Calculus",
    "section": "10.11 Double Integrals in Polar Coordinates",
    "text": "10.11 Double Integrals in Polar Coordinates\n\nPolar Rectangles\n\\[\n\\begin{aligned}\n  \\Delta A_k &=\\frac{1}{2}(r_{k+1}^2 -r_k^2)\\Delta \\theta_k \\\\\n  &=\\frac{1}{2}(r_{k+1} +r_k)(r_{k+1} -r_k) \\Delta\\theta_k \\\\[5pt]\n  &=r_k^*\\Delta r_k \\Delta\\theta_k\n\\end{aligned}\\]\n\n\n\n\n\n\n\nDouble Integrals in Polar Coordinates\n\\[\n\\begin{aligned}\n\\lim_{\\| P \\|\\to 0} \\sum_{k=1}^n & f\\left( r_k^*, \\theta_k^* \\right) r_k^* \\Delta r_k \\Delta\\theta_k \\\\\n  & = \\iint_R f(r,\\theta) \\,dA\\\\\n  & = \\int_\\alpha^\\beta\n    \\int_{g_1(\\theta)}^{g_2(\\theta)} f(r,\\theta)\\,r \\,dr \\,d\\theta \\\\\n  & =\\int_a^b \\int_{h_1(r)}^{h_2(r)} f(r,\\theta)\\,r \\,d\\theta \\,dr\n\\end{aligned}\\]\n\n\n\n\n\n\n\\(~\\)",
    "crumbs": [
      "**Second Semester**",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Vector Calculus</span>"
    ]
  },
  {
    "objectID": "ch_09_Vector_Calculus.html#sec-9-12",
    "href": "ch_09_Vector_Calculus.html#sec-9-12",
    "title": "10  Vector Calculus",
    "section": "10.12 Green’s Theorem",
    "text": "10.12 Green’s Theorem\n\nSuppose that \\(C\\) is a piecewise-smooth simple closed curve bounding a simply connected region \\(R\\)\nIf \\(\\,P\\), \\(Q\\), \\(\\displaystyle\\frac{\\partial P}{\\partial y}\\), and \\(\\displaystyle\\frac{\\partial Q}{\\partial x}\\) are continuous on \\(R\\), \\(\\,\\)then\n\\[{\\oint_C P \\,dx +Q \\,dy = \\iint_R \\left( \\frac{\\partial Q}{\\partial x} -\\frac{\\partial P}{\\partial y} \\right)\\, dA}\\]\n\n\\(~\\)\n\n\n\n\n\n\nPartial Proof\n\\[\n\\begin{aligned}\n   -\\iint_R \\frac{\\partial P}{\\partial y}\\, dA &= -\\int_a^b \\int_{g_1(x)}^{g_2(x)} \\frac{\\partial P}{\\partial y} \\,dy dx \\\\\n   &=-\\int_a^b [P(x,g_2(x)) -P(x,g_1(x))] \\,dx\\\\\n   &=\\int_a^b P(x,g_1(x))\\,dx +\\int_b^a P(x,g_2(x)) \\,dx \\\\\n   &= \\oint_C P(x,y) \\,dx\n\\end{aligned}\\]\n\\(~\\)\n\n\n\n\n\nRegion with Holes\n\\[\n\\begin{aligned}\n  \\iint_R &\\left( \\frac{\\partial Q}{\\partial x} -\\frac{\\partial P}{\\partial y} \\right)\\, dA \\\\\n  &=\\iint_{R_1} \\left( \\frac{\\partial Q}{\\partial x} -\\frac{\\partial P}{\\partial y} \\right)\\, dA\n   +\\iint_{R_2} \\left( \\frac{\\partial Q}{\\partial x} -\\frac{\\partial P}{\\partial y} \\right)\\, dA\\\\\n  &=\\oint_{C_1} P \\,dx +Q \\,dy +\\oint_{C_2} P \\,dx +Q \\,dy =\\oint_C P \\,dx +Q \\,dy\n\\end{aligned}\\]\n\n\n\n\n\n\n\\(~\\)\nExample \\(\\,\\) Evaluate \\(\\displaystyle\\oint_{C} \\frac{-y}{x^2 +y^2}\\, dx + \\frac{x}{x^2 + y^2}\\,dy\\), \\(\\,\\) where \\(C=C_1 \\cup C_2\\) is the boundary of the shaded region \\(R\\)\n\n\n\n\n\n\\(~\\)\nExample \\(\\,\\) Evaluate \\(\\displaystyle\\oint_{C_1} \\frac{-y}{x^2 +y^2}\\, dx + \\frac{x}{x^2 + y^2}\\,dy\\)\n\\(~\\)",
    "crumbs": [
      "**Second Semester**",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Vector Calculus</span>"
    ]
  },
  {
    "objectID": "ch_09_Vector_Calculus.html#sec-9-13",
    "href": "ch_09_Vector_Calculus.html#sec-9-13",
    "title": "10  Vector Calculus",
    "section": "10.13 Surface Integrals",
    "text": "10.13 Surface Integrals\n\nSurface Area\n\n\n\n\n\n\\[\n\\begin{aligned}\n  \\mathbf{u} &= \\Delta x_k \\mathbf{i} +f_x(x_k,y_k)\\Delta x_k \\mathbf{k}\\\\\n  \\mathbf{v} &= \\Delta y_k \\mathbf{j} +f_y(x_k,y_k)\\Delta y_k \\mathbf{k}\\\\\n  & \\Downarrow \\\\\n  \\mathbf{u} \\times \\mathbf{v} &={\\scriptsize\\left[-f_x(x_k,y_k)\\mathbf{i} -f_y(x_k,y_k)\\mathbf{j}+\\mathbf{k}\\right] \\Delta x_k \\Delta y_k}\\\\\n  & \\Downarrow \\\\\n  \\Delta S_k &= {\\sqrt{1 +\\left[\\,f_x(x_k,y_k)\\right]^2 +\\left[\\,f_y(x_k,y_k)\\right]^2}\\Delta A_k }\n\\end{aligned}\\] Then the area of the surface over \\(R\\) is given by \\[ { S=\\iint_R \\sqrt{1+\\left[\\,f_x(x,y)\\right]^2 +\\left[\\,f_y(x,y)\\right]^2}\\,dA} \\]\nSurface Integral\nLet \\(\\,G\\,\\) be a function of three variables defined over a region of space containing the surface \\(S\\). \\(\\,\\)Then the surface integral of \\(G\\) over \\(S\\) is given by\n\\[\n\\begin{aligned}\n   \\lim_{\\|P\\|\\to 0}&\\sum_{k=1}^n G(x_k^*,y_k^*,z_k^*)\\,\\Delta S_k = \\iint_S G(x,y,z)\\,dS\\\\\n   &= \\iint_R G(x,y,f(x,y)) \\,\\sqrt{1+\\left[\\,f_x(x,y)\\right]^2 +\\left[\\,f_y(x,y)\\right]^2}\\,dA\n\\end{aligned}\\]\nIf a smooth surface \\(S\\) \\(\\,\\)is \\(g(x,y,z)=0,\\) \\(\\,\\)a unit normal is\n\\[\\mathbf{n}=\\frac{\\nabla g}{\\|\\nabla g\\|}\\]\n\n\n\n\n\nIntegrals of Vector Fields\nThe total volume of a fluid passing through \\(\\,S\\) \\(\\,\\)is called the flux of \\(\\mathbf{v}\\) through \\(S\\) and is given by\n\n\\[ \\mathrm{flux} =\\iint_S (\\mathbf{v}\\cdot\\mathbf{n})\\,dS \\]\n\n\n\n\n\n\\(~\\)",
    "crumbs": [
      "**Second Semester**",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Vector Calculus</span>"
    ]
  },
  {
    "objectID": "ch_09_Vector_Calculus.html#sec-9-14",
    "href": "ch_09_Vector_Calculus.html#sec-9-14",
    "title": "10  Vector Calculus",
    "section": "10.14 Stokes’ Theorem",
    "text": "10.14 Stokes’ Theorem\n\nVector Form of Green’s Theorem\nIf \\(\\,\\mathbf{f}(x,y)=P(x,y)\\mathbf{i} +Q(x,y)\\mathbf{j} \\,\\) is a two-dimensional vector field, \\(\\text{ }\\)then\n\\[\\mathrm{curl}\\, \\mathbf{f} =\\nabla \\times \\mathbf{f}\n=\\left|\\begin{matrix} \\mathbf{i} & \\mathbf{j} & \\mathbf{k}\\\\\n    \\displaystyle\\frac{\\partial}{\\partial x}\n    & \\displaystyle\\frac{\\partial}{\\partial y}\n    & \\displaystyle\\frac{\\partial}{\\partial z}\\\\ P & Q & 0 \\end{matrix}\\right|\n=\\left( \\frac{\\partial Q}{\\partial x} -\\frac{\\partial P}{\\partial y} \\right )\\mathbf{k}\\]\nGreen’s theorem can be written in vector notation as\n\\[\\oint_C \\mathbf{f}\\cdot \\,d\\mathbf{r} =\\oint_C \\mathbf{f}\\cdot\\mathbf{T}\\,ds\n=\\iint_R (\\nabla \\times \\mathbf{f}) \\cdot \\mathbf{k}\\,dA\\]\nStokes’ Theorem - Green’s Theorem in 3-Space\n\nLet \\(S\\) be a piecewise-smooth orientable surface bounded by a piecewise-smooth simple closed curve \\(C\\)\nLet \\(~\\mathbf{f}(x,y,z)=P(x,y,z)\\mathbf{i} +Q(x,y,z)\\mathbf{j} +R(x,y,z)\\mathbf{k} \\,\\) be a vector field for which \\(P\\), \\(Q\\), and \\(R\\) are continuous and have continuous first partial derivatives in a region of 3-space containing \\(S\\). \\(~\\)If \\(C\\) is traversed in the positive direction, then\n\n\\[\\oint_C \\mathbf{f}\\cdot \\,d\\mathbf{r} =\\oint_C \\mathbf{f}\\cdot\\mathbf{T}\\,ds = \\iint_R (\\nabla \\times\\mathbf{f}) \\cdot \\mathbf{n}\\,dS\\]\n\n\n\n\n\n\n\nPhysical Interpretation of Curl\n\\[\n\\begin{aligned}\n  \\oint_{C_r} \\mathbf{f} \\cdot d\\mathbf{r}\n   = \\iint_{S_r} \\left[\\nabla\\times\\mathbf{f}(P_0)\\right] \\cdot \\mathbf{n}(P_0) &\\, dS\\\\\n   = \\left[\\nabla\\times\\mathbf{f}(P_0)\\right]\\, \\cdot \\mathbf{n}(P_0)\\,\\iint_{S_r}\\,dS &\n    =\\left[\\nabla\\times\\mathbf{f}(P_0)\\right] \\cdot \\mathbf{n}(P_0)\\,S_r\\\\\n  \\qquad\\qquad\\qquad\\qquad &\\Downarrow\\\\\n  \\left[\\nabla\\times\\mathbf{f}(P_0)\\right] \\cdot \\mathbf{n}(P_0)\n  = \\lim_{r\\to 0} \\frac{1}{S_r} &\\oint_{C_r} \\mathbf{f} \\cdot\n  d\\mathbf{r} \\approx \\frac{1}{S_r}\\oint_{C_r} \\mathbf{f} \\cdot\n  d\\mathbf{r}\n\\end{aligned}\\]\n\n\n\n\n\n\n\\(~\\)",
    "crumbs": [
      "**Second Semester**",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Vector Calculus</span>"
    ]
  },
  {
    "objectID": "ch_09_Vector_Calculus.html#sec-9-15",
    "href": "ch_09_Vector_Calculus.html#sec-9-15",
    "title": "10  Vector Calculus",
    "section": "10.15 Triple Integrals",
    "text": "10.15 Triple Integrals\n\nLet \\(F\\) be a function of three variables defined over a closed region \\(D\\) of space. Then the triple integral of \\(F\\) over \\(D\\) is given by\n\\[{\\lim_{\\| P\\|\\to 0} \\sum_{k=1}^n F(x_k^*,y_k^*,z_k^*)\\,\\Delta V_k = \\iiint_D F(x,y,z)\\,dV}\\]\n\n\n\n\n\n\n\nEvaluation by Iterated Integrals\n\\[\n\\begin{aligned}\n    \\iiint_D F(x,y,z)\\,dV &= \\iint_R \\left[ \\int_{f_1(x,y)}^{f_2(x,y)} F(x,y,z)\\,dz \\right]\\,dA\\\\\n   &= \\int_a^b \\int_{g_1(x)}^{g_2(x)} \\int_{f_1(x,y)}^{f_2(x,y)} F(x,y,z)\\,dz\\,dy\\,dx\n\\end{aligned}\\]\n\n\n\n\n\n\n\\(~\\)\nExample \\(\\,\\) Find the volume of the solid in the first octant bounded by the graphs of \\(z=1-y^2\\), \\(y=2x\\), and \\(x=3\\)\n\n\n\n\n\n\\(~\\)\n\nCylindrical Coordinates\n\\[\\begin{aligned}\n  x=r\\cos\\theta, \\;\\;&y=r\\sin\\theta, \\;\\;z=z \\\\\n  &\\Downarrow \\\\\n  r^2 = x^2 +y^2, \\;\\; &\\tan\\theta=\\frac{y}{x},\\;\\; z=z\n\\end{aligned}\\]\n\n\n\n\n\nTriple Integrals in Cylindrical Coordinates\n\\[\n\\begin{aligned}\n   \\iiint_D F(r,\\theta,z)\\,dV &= \\iint_R \\left[ \\int_{f_1(r,\\theta)}^{f_2(r,\\theta)}\n      F(r,\\theta,z)\\,dz \\right]\\,dA \\\\\n   &= \\int_\\alpha^\\beta \\int_{g_1(\\theta)}^{g_2(\\theta)}\n     \\int_{f_1(r,\\theta)}^{f_2(r,\\theta)} F(r,\\theta,z)\\,r\\,dz\\,dr\\,d\\theta\n\\end{aligned}\\]\n\n\n\n\n\nSpherical Coordinates\n\\[\n\\begin{aligned}\nx = \\rho\\sin\\phi\\cos\\theta, \\;\\;\n&y=\\rho\\sin\\phi\\sin\\theta, \\;\\;\nz=\\rho\\cos\\phi \\\\\n   &\\Downarrow \\\\\n\\rho^2 =x^2 +y^2 +z^2, \\;\\; &\\tan\\theta=\\frac{y}{x},\n\\;\\;\\cos\\phi=\\frac{z}{\\sqrt{x^2 +y^2 +z^2}}\n\\end{aligned}\\]\n\n\n\n\n\n\n\nTriple Integrals in Spherical Coordinates\n\n\\[\n  \\begin{aligned}\n      \\iiint_D &F(\\rho,\\theta,\\phi)\\,dV  \\\\\n      &= \\int_\\alpha^\\beta \\int_{g_1(\\theta)}^{g_2(\\theta)}\n      \\int_{f_1(\\phi,\\theta)}^{f_2(\\phi,\\theta)} F(\\rho,\\theta,\\phi)\\,\n      \\rho^2 \\sin\\phi\\, d\\rho\\,d\\phi\\,d\\theta\n  \\end{aligned}\\]",
    "crumbs": [
      "**Second Semester**",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Vector Calculus</span>"
    ]
  },
  {
    "objectID": "ch_09_Vector_Calculus.html#sec-9-16",
    "href": "ch_09_Vector_Calculus.html#sec-9-16",
    "title": "10  Vector Calculus",
    "section": "10.16 Divergence Theorem",
    "text": "10.16 Divergence Theorem\n\nAnother Vector Form of Green’s Theorem\n\\[\n\\begin{aligned}\n\\oint_C (\\mathbf{f}\\cdot\\mathbf{n})&\\,ds\n   = \\oint_C -Qdx +Pdy \\\\\n   &= \\iint_R \\left[ \\frac{\\partial P}{\\partial x}\n     -\\left( -\\frac{\\partial Q}{\\partial y} \\right) \\right]\\,dA\n   = \\iint_R \\left[ \\frac{\\partial P}{\\partial x}\n     +\\frac{\\partial Q}{\\partial y} \\right]\\,dA \\\\\n   &= \\iint_R \\nabla\\cdot\\mathbf{f}\\,dA\n\\end{aligned}\\]\nwhere \\(\\displaystyle\\,\\mathbf{n}=\\frac{dy}{ds}\\mathbf{i} -\\frac{dx}{ds}\\mathbf{j}\\)\nDivergence Theorem\n\nLet \\(D\\,\\) be a closed and bounded region in 3-space with a piecewise-smooth boundary \\(S\\) that is oriented outward\nLet \\(\\;\\mathbf{f}(x,y,z)=P(x,y,z)\\,\\mathbf{i}+Q(x,y,z)\\,\\mathbf{j} +R(x,y,z)\\,\\mathbf{k}~\\) be a vector field for which \\(P\\), \\(Q,\\) and \\(R\\) are continuous first partial derivatives in a region of 3-space containing \\(D\\). \\(\\,\\) Then\n\n \\[\\iint_S (\\mathbf{f}\\cdot \\mathbf{n})\\,dS =\\iiint_D \\nabla\\cdot\\mathbf{f}\\,dV\\] \nPartial Proof\n\n\n\n\n\n\\[{\\scriptsize \\displaystyle\\iiint_D \\frac{\\partial R}{\\partial z}\\,dV\n=\\iint_{R'} \\left[ \\int_{f_1(x,y)}^{f_2(x,y)} \\frac{\\partial R}{\\partial z}\\,dz \\right]\\,dA\n=\\iint_{R'} \\left[ R(x,y,f_2(x,y)) -R(x,y,f_1(x,y)) \\right] \\,dA }\\]\n\\[{\\scriptsize \\displaystyle\\iint_S R(\\mathbf{k}\\cdot\\mathbf{n}) \\,dS=\\iint_{S_1} R(\\mathbf{k}\\cdot\\mathbf{n})\\,dS\n+\\iint_{S_2} R(\\mathbf{k}\\cdot\\mathbf{n})\\,dS +\\iint_{S_3} R(\\mathbf{k}\\cdot\\mathbf{n}) \\,dS }\\]\n\nOn \\(S_1\\): \\(\\,\\) Since the outward normal points downward, \\(g(x,y,z)=f_1(x,y)-z=0\\). Thus\n\\[\n\\begin{aligned}\n  {\\scriptsize \\mathbf{n}}\n  &{\\scriptsize \\;= \\frac{ \\frac{\\partial f_1}{\\partial x} \\mathbf{i}\n  +\\frac{\\partial f_1}{\\partial y} \\mathbf{j} -\\mathbf{k} }{ \\sqrt{1+\\left(\\frac{\\partial f_1}{\\partial x}\\right)^2\n  +\\left(\\frac{\\partial f_1}{\\partial y}\\right)^2}} \\;\\;\n\\Rightarrow \\;\\;\n\\mathbf{k}\\cdot\\mathbf{n}\n=\\frac{-1}{\\sqrt{1+\\left(\\frac{\\partial f_1}{\\partial x}\\right)^2\n+\\left(\\frac{\\partial f_1}{\\partial y}\\right)^2}} } \\\\\n&\\Downarrow \\\\\n\\iint_{S_1} &R(\\mathbf{k}\\cdot\\mathbf{n})\\,dS = -\\iint_{R'} R(x,y,f_1(x,y))\\,dA\n\\end{aligned}\\]\nOn \\(S_2\\): \\(\\,\\) Since the outward normal points upward, \\(g(x,y,z)=z -f_2(x,y)=0\\). Thus\n\\[\n\\begin{aligned}\n{\\scriptsize \\mathbf{n} }& {\\scriptsize \\;= \\frac{ -\\frac{\\partial f_2}{\\partial x} \\mathbf{i}\n-\\frac{\\partial f_2}{\\partial y} \\mathbf{j} +\\mathbf{k} }{ \\sqrt{1+\\left(\\frac{\\partial f_2}{\\partial x}\\right)^2\n+\\left(\\frac{\\partial f_2}{\\partial y}\\right)^2}} \\;\\;\n\\Rightarrow \\;\\;\n\\mathbf{k}\\cdot\\mathbf{n}\n=\\frac{1}{\\sqrt{1+\\left(\\frac{\\partial f_2}{\\partial x}\\right)^2\n+\\left(\\frac{\\partial f_2}{\\partial y}\\right)^2}} }\\\\\n&\\Downarrow \\\\\n\\iint_{S_2} &R(\\mathbf{k}\\cdot\\mathbf{n})\\,dS = \\iint_{R'} R(x,y,f_2(x,y))\\,dA\n\\end{aligned}\\]\nOn \\(S_3\\): \\(\\,\\) \\(\\mathbf{k}\\cdot\\mathbf{n}=0\\)\n\n\\[\\quad \\iint_{S_3} R(\\mathbf{k}\\cdot\\mathbf{n})\\,dS = 0\\]\nPhysical Interpretation of Divergence\n\\[\n\\begin{aligned}\n  \\iint_{S_r} (\\mathbf{f} \\cdot \\mathbf{n})\\,dS\n    &= \\iiint_{D_r} \\nabla\\cdot\\mathbf{f}\\,dV \\\\\n    \\approx\n  \\iiint_{D_r} &\\nabla\\cdot\\mathbf{f}(P_0)\\,dV\n  = \\nabla\\cdot\\mathbf{f}(P_0) \\iiint_{D_r}\\,dV\n  = \\nabla\\cdot\\mathbf{f}(P_0) D_r\\\\\n   &\\Downarrow\\\\\n  \\nabla\\cdot\\mathbf{f}(P_0) &= \\lim_{r\\to 0} \\frac{1}{D_r}\\iint_{S_r}\n   (\\mathbf{f} \\cdot \\mathbf{n})\\,dS\n\\end{aligned}\\]\n\n\n\n\n\nContinuity Equation\n\\[\n\\begin{aligned}\n\\frac{dm}{dt}\n  &= \\frac{d}{dt}\\iiint_D \\rho(x,y,z,t)\\,dV\n   = \\iiint_D \\frac{\\partial \\rho}{\\partial t}\\,dV \\\\\n  &= -\\iint_S (\\rho\\mathbf{v}\\cdot\\mathbf{n})\\,dS\n   =-\\iiint_D \\nabla\\cdot(\\rho\\mathbf{v})\\,dV\\\\\n  &\\Downarrow \\\\\n   \\iiint_D \\left[ \\frac{\\partial \\rho}{\\partial t} \\right.\n  &+\\left. \\phantom{\\frac{}{}}\\nabla\\cdot(\\rho\\mathbf{v}) \\right]\\,dV=0 \\\\\n  &\\Downarrow \\\\\n  \\frac{\\partial \\rho}{\\partial t} &\n   +\\nabla\\cdot(\\rho\\mathbf{v})=0\n\\end{aligned}\\]\nEuler’s Equation of Motion\nAllowing for the presence of a gravitational body force per unit mass \\(\\mathbf{g}\\), \\(\\,\\)the basic equations of an ideal fluid are\n\\[\\frac{D \\mathbf{v}}{D t} = -\\frac{1}{\\rho} \\nabla p + \\mathbf{g}\\]\n\\[\\nabla \\cdot \\mathbf{v} = 0\\]\nFurthermore, it can be helpful to use \\(\\mathbf{g}=-\\nabla \\chi\\) \\(\\,\\)and the identity\n\\[ \\left( \\mathbf{v} \\cdot \\nabla \\right) \\mathbf{v}\n  = \\left( \\nabla \\times \\mathbf{v} \\right) \\times \\mathbf{v} + \\nabla \\left( \\frac{1}{2} \\mathbf{v}^2 \\right)\\]\nto cast the momentum equation into the form\n\\[ \\frac{\\partial \\mathbf{v}}{\\partial t} + \\left( \\nabla \\times \\mathbf{v} \\right) \\times \\mathbf{v} = -\\nabla \\left( \\frac{p}{\\rho} +\\frac{1}{2} \\mathbf{v}^2 + \\chi \\right) =- \\nabla H\\]\n\n\\(~\\)",
    "crumbs": [
      "**Second Semester**",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Vector Calculus</span>"
    ]
  },
  {
    "objectID": "ch_09_Vector_Calculus.html#sec-9-17",
    "href": "ch_09_Vector_Calculus.html#sec-9-17",
    "title": "10  Vector Calculus",
    "section": "10.17 Change of Variables in Multiple Integrals",
    "text": "10.17 Change of Variables in Multiple Integrals\n\nTo change the Variable in a Definite Integral\n\\[\\int_a^b f(x)\\,dx =\\int_c^d f(g(u))\\, g'(u)\\,du\\]\nDouble Integrals\n\\[\\begin{aligned}\n\\iint_R & F(x,y)\\,dA \\\\\n&= \\iint_R F(f(u,v), g(u,v))\\, \\left| \\frac{\\partial(x,y)}{\\partial(u,v)} \\right|\\,dA'\n\\end{aligned}\\]\n\\[\\left| \\frac{\\partial(x,y)}{\\partial(u,v)} \\right| =\n  \\left|\\begin{matrix}\n  \\frac{\\partial x}{\\partial u} & \\frac{\\partial x}{\\partial v}\\\\\n  \\frac{\\partial y}{\\partial u} & \\frac{\\partial y}{\\partial v}\n  \\end{matrix}\\right| = h_1 h_2 dq_1 dq_2\\]\nTriple Integrals\n\\[\\begin{aligned}\n  \\iiint_D & F(x,y,z)\\,dV \\\\\n  &= \\iiint_D F\\left(f(u,v,w), g(u,v,w), h(u,v,w)\\right)\\, \\left| \\frac{\\partial(x,y,z)}{\\partial(u,v,w)} \\right|\\,dV'\n  \\end{aligned}\\]\n\\[\\left| \\frac{\\partial (x,y,z)}{\\partial (u,v,w)} \\right| =\n  \\left|\\begin{matrix}\n  \\frac{\\partial x}{\\partial u} & \\frac{\\partial x}{\\partial v} & \\frac{\\partial x}{\\partial w}\\\\\n  \\frac{\\partial y}{\\partial u} & \\frac{\\partial y}{\\partial v} & \\frac{\\partial y}{\\partial w}\\\\\n  \\frac{\\partial z}{\\partial u} & \\frac{\\partial z}{\\partial v} & \\frac{\\partial z}{\\partial w}\n\\end{matrix}\\right| = h_1 h_2 h_3 dq_1 dq_2 dq_3\\]\n\n\\(~\\)",
    "crumbs": [
      "**Second Semester**",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Vector Calculus</span>"
    ]
  },
  {
    "objectID": "ch_09_Vector_Calculus.html#worked-exercises",
    "href": "ch_09_Vector_Calculus.html#worked-exercises",
    "title": "10  Vector Calculus",
    "section": "Worked Exercises",
    "text": "Worked Exercises\n1. \\(~\\) Find an equation of the plane containing the lines \\(x=t\\), \\(~y=4t\\), \\(~z=-2t\\), and \\(~x=1+t\\), \\(~y=1+4t\\), \\(~z=3-2t\\)\nSolution\nStep 1: \\(~\\) Write the parametric forms of both lines\nFirst line:\n\\[\nL_1: \\begin{cases}\nx = t \\\\\ny = 4t \\\\\nz = -2t\n\\end{cases}\\\\\n\\;\\Rightarrow\\;\n\\begin{aligned}\n&\\text{Point: } P_1 = (0, 0, 0)\\\\ &\\text{Direction vector: } \\mathbf{d}_1 = \\langle 1, 4, -2 \\rangle\n\\end{aligned}\\]\nSecond line:\n\\[\nL_2: \\begin{cases}\nx = 1 + t \\\\\ny = 1 + 4t \\\\\nz = 3 - 2t\n\\end{cases}\n\\;\\Rightarrow\\;\n\\begin{aligned}\n&\\text{Point: } P_2 = (1, 1, 3)\\\\\n&\\text{Direction vector: } \\mathbf{d}_2 = \\langle 1, 4, -2 \\rangle\n\\end{aligned}\\]\nStep 2: \\(~\\) Observe relationship between the lines\n\nBoth lines share the same direction vector: \\(\\mathbf{d}_1 = \\mathbf{d}_2 = \\langle 1, 4, -2 \\rangle\\)\nBut they pass through different points\n\nSo the lines are parallel and distinct, and therefore they lie in a common plane\nStep 3: \\(~\\) Find two vectors in the plane\nThe plane is determined by:\n\nDirection vector of the lines: \\[\\mathbf{v}_1 = \\langle 1, 4, -2 \\rangle\\]\nVector connecting a point on one line to a point on the other line:\n\n\\[\\mathbf{v}_2 = P_2 - P_1 = \\langle 1, 1, 3\\rangle - \\langle0, 0, 0\\rangle = \\langle 1, 1, 3 \\rangle\\]\nStep 4: \\(~\\) Find a normal vector to the plane\nTake the cross product:\n\\[\\mathbf{n} = \\mathbf{v}_1 \\times \\mathbf{v}_2 =\n\\begin{vmatrix}\n\\mathbf{i} & \\mathbf{j} & \\phantom{-}\\mathbf{k} \\\\\n1 & 4 & -2 \\\\\n1 & 1 & \\phantom{-}3\n\\end{vmatrix}\\]\nCompute:\n\\[\\mathbf{n} = {\\scriptsize\\mathbf{i}(4 \\cdot 3 - (-2)\\cdot 1) - \\mathbf{j}(1 \\cdot 3 - (-2)\\cdot 1) + \\mathbf{k}(1 \\cdot 1 - 4 \\cdot 1)}\n\\;\\Rightarrow\\;\n\\mathbf{n} = \\langle 14, -5, -3 \\rangle\\]\nStep 5: \\(~\\) Use the point-normal form of a plane\nWith point \\(P_1 = (0,0,0)\\) and normal vector \\(\\langle 14, -5, -3 \\rangle\\), the equation is:\n\\[14(x - 0) - 5(y - 0) - 3(z - 0) = 0\n\\Rightarrow 14x - 5y - 3z = 0\\]\n\\(~\\)\n2. \\(~\\) Find an equation of the plane containing \\((1, 7, -1)\\) that is perpendicular to the line of intersection of \\(-x +y -8z=4~\\) and \\(~3x -y +2z=0\\)\nSolution\nStep 1: \\(~\\) Find direction vector of the line of intersection\nThe line of intersection of two planes lies in both planes, and its direction vector is perpendicular to both normals of the planes\nSo, compute the cross product of the normal vectors of the two planes\n\nNormal to \\(\\pi_1\\): \\(~\\mathbf{n}_1 = \\langle -1, 1, -8 \\rangle\\)\nNormal to \\(\\pi_2\\): \\(~\\mathbf{n}_2 = \\langle 3, -1, 2 \\rangle\\)\n\nThen:\n\\[\n\\mathbf{d} = \\mathbf{n}_1 \\times \\mathbf{n}_2 =\n\\begin{vmatrix}\n\\phantom{-}\\mathbf{i} & \\phantom{-}\\mathbf{j} & \\phantom{-}\\mathbf{k} \\\\\n-1 & \\phantom{-}1 & -8 \\\\\n\\phantom{-}3 & -1 & \\phantom{-}2\n\\end{vmatrix}\\]\nCompute the determinant:\n\\[\\begin{aligned}\n\\mathbf{d} &= {\\scriptsize\\mathbf{i}(1\\cdot2 - (-8)\\cdot(-1)) - \\mathbf{j}((-1)\\cdot2 - (-8)\\cdot3) + \\mathbf{k}((-1)\\cdot(-1) - 1\\cdot3)} \\\\\n&= \\langle -6, -22, -2 \\rangle\n\\end{aligned}\\]\nThis is the direction vector of the line of intersection.\nStep 2: \\(~\\) Use the fact that the desired plane is perpendicular to this line\nSo the direction vector \\(~\\mathbf{d} = \\langle -6, -22, -2 \\rangle\\) is the normal vector of the desired plane\nWe now have:\n\nPoint on the plane: \\((1, 7, -1)\\)\nNormal vector: \\(~\\mathbf{n} = \\langle -6, -22, -2 \\rangle\\)\n\nStep 3: \\(~\\) Use point-normal form of a plane\n\\[-6(x - 1) -22(y - 7) -2(z + 1) = 0\\]\nExpand:\n\\[-6x + 6 -22y + 154 -2z -2 = 0\n\\Rightarrow -6x - 22y - 2z + 158 = 0\\]\nMultiply both sides by -1 to make the leading coefficient positive:\n\\[6x + 22y + 2z = 158\\]\n\\(~\\)\n3. \\(~\\) Use Green’s theorem to evaluate the given double integral by a line integral\n\\[\\iint_R x^2 \\,dA\\]\nwhere \\(R~\\) is the region bounded by the ellipse \\(\\displaystyle\\frac{x^2}{9} +\\frac{y^2}{4}=1\\)\nSolution\nStep 1: \\(~\\) Green’s Theorem\nGreen’s Theorem relates a line integral over a positively oriented, simple closed curve \\(C\\) to a double integral over the region \\(R\\) it encloses:\n\\[\\iint_R \\left( \\frac{\\partial Q}{\\partial x} - \\frac{\\partial P}{\\partial y} \\right) dA = \\oint_C P\\,dx + Q\\,dy\\]\nWe need to rewrite \\(\\displaystyle \\iint_R x^2 \\, dA\\) as the right-hand side of Green’s Theorem.\nStep 2: \\(~\\) Choose \\(P(x, y)\\) and \\(Q(x, y)\\)\nWe want:\n\\[\\frac{\\partial Q}{\\partial x} - \\frac{\\partial P}{\\partial y} = x^2\\]\nLet’s choose:\n\n\\(\\displaystyle P = 0 \\Rightarrow \\frac{\\partial P}{\\partial y} = 0\\)\n\\(\\displaystyle Q = \\frac{1}{3} x^3 \\Rightarrow \\frac{\\partial Q}{\\partial x} = x^2\\)\n\nSo:\n\\[\\frac{\\partial Q}{\\partial x} - \\frac{\\partial P}{\\partial y} = x^2\\]\nStep 3: \\(~\\) Set up the line integral\nThen by Green’s Theorem:\n\\[\\iint_R x^2 \\, dA = \\oint_C P\\,dx + Q\\,dy = \\oint_C \\frac{1}{3} x^3 \\, dy\\]\nwhere \\(C\\) is the positively oriented boundary of the ellipse.\nStep 4: \\(~\\) Parametrize the ellipse\nGiven:\n\\[\\frac{x^2}{9} + \\frac{y^2}{4} = 1\\]\nUse the parametric equations:\n\\[x = 3\\cos\\theta, \\quad y = 2\\sin\\theta, \\quad 0 \\leq \\theta \\leq 2\\pi\\]\nCompute:\n\n\\(x^3 = (3\\cos\\theta)^3 = 27\\cos^3\\theta\\)\n\\(\\displaystyle dy = \\frac{dy}{d\\theta} \\, d\\theta = 2\\cos\\theta \\, d\\theta\\)\n\nNow plug into the line integral:\n\\[\\oint_C \\frac{1}{3} x^3 \\, dy\n= \\int_0^{2\\pi} \\frac{1}{3} \\cdot 27 \\cos^3\\theta \\cdot 2\\cos\\theta \\, d\\theta\n= \\int_0^{2\\pi} 18\\cos^4\\theta \\, d\\theta\\]\nStep 5: \\(~\\) Evaluate the integral\nUse the identity:\n\\[\\cos^4\\theta = \\frac{3}{8} + \\frac{1}{2} \\cos(2\\theta) + \\frac{1}{8} \\cos(4\\theta)\\]\nSo:\n\\[\\int_0^{2\\pi} 18 \\cos^4\\theta \\, d\\theta\n= 18 \\int_0^{2\\pi} \\left( \\frac{3}{8} + \\frac{1}{2} \\cos(2\\theta) + \\frac{1}{8} \\cos(4\\theta) \\right) d\\theta\\]\nNow integrate:\n\n\\(\\displaystyle\\int_0^{2\\pi} \\frac{3}{8} \\, d\\theta = \\frac{3}{8} \\cdot 2\\pi = \\frac{3\\pi}{4}\\)\n\\(\\displaystyle\\int_0^{2\\pi} \\cos(2\\theta) \\, d\\theta = 0\\)\n\\(\\displaystyle\\int_0^{2\\pi} \\cos(4\\theta) \\, d\\theta = 0\\)\n\nSo:\n\\[\\iint_R x^2\\,dA = 18 \\cdot \\frac{3\\pi}{4} = \\frac{27\\pi}{2}\\]\n\\(~\\)\n4. \\(~\\) Find the work done by the given force \\(~\\mathbf{f}=(x-y)\\mathbf{i} +(x+y)\\mathbf{j}~\\) around the closed curve in the following\n\n\n\n\n\nSolution\nThe image shows a region bounded between two quarter circles:\n\nOuter curve: \\(x^2 + y^2 = 4\\) (radius 2)\nInner curve: \\(x^2 + y^2 = 1\\) (radius 1)\n\nThe region is the quarter annulus in the first quadrant, swept from the positive \\(x\\)-axis to the positive \\(y\\)-axis, and the curve is oriented counterclockwise, forming a positively oriented closed curve \\(C\\)\nStep 1: \\(~\\) Use Green’s Theorem\nWe are to compute:\n\\[\\oint_C \\mathbf{f} \\cdot d\\mathbf{r} = \\iint_R \\left( \\frac{\\partial N}{\\partial x} - \\frac{\\partial M}{\\partial y} \\right)\\,dA\\]\nWhere:\n\n\\(M = x - y \\Rightarrow \\frac{\\partial M}{\\partial y} = -1\\)\n\\(N = x + y \\Rightarrow \\frac{\\partial N}{\\partial x} = 1\\)\n\nSo:\n\\[\\frac{\\partial N}{\\partial x} - \\frac{\\partial M}{\\partial y} = 1 - (-1) = 2\\]\nStep 2: \\(~\\) Area of the Region \\(R\\)\nThe region \\(R\\) is a quarter annulus between radius 1 and 2, in the first quadrant.\nArea of an annulus between \\(r = 1\\) and \\(r = 2\\) is:\n\\[\\text{Full area} = \\pi(2^2) - \\pi(1^2) = 4\\pi - \\pi = 3\\pi \\]\nBut we only have \\(1/4\\) of it:\n\\[\\text{Area}(R) = \\frac{1}{4} \\cdot 3\\pi = \\frac{3\\pi}{4}\\]\nStep 3: \\(~\\) Apply Green’s Theorem\n\\[\\oint_C \\mathbf{f} \\cdot d\\mathbf{r} = \\iint_R 2\\, dA = 2 \\cdot \\frac{3\\pi}{4} = \\frac{3\\pi}{2}\\]\n\\(~\\)\n5. \\(~\\) Evaluate the integral\n\\[\\iint_R \\frac{1}{\\sqrt{(x-y)^2 +2(x+y) +1}} \\,dA\\]\nwhere \\(R\\) is the region bounded by the graphs of \\(y=x\\), \\(x=2\\), and \\(y=0\\) by means of the change of variables \\(x=u +uv\\), \\(y=v +uv\\)\nSolution\nStep 1: \\(~\\) Understand the transformation\nWe compute:\n\n\\(x - y = (u + uv) - (v + uv) = u - v\\)\n\\(x + y = (u + uv) + (v + uv) = u + v + 2uv\\)\n\nThen,\n\\[(x - y)^2 + 2(x + y) + 1 = (u - v)^2 + 2(u + v + 2uv) + 1 = (u +v +1)^2\\]\nStep 2: \\(~\\) Jacobian of the transformation\nWe need the Jacobian determinant \\(J = \\left| \\frac{\\partial(x,y)}{\\partial(u,v)} \\right|\\). Compute partial derivatives:\n\\[x = u + uv \\Rightarrow\n\\begin{cases}\n\\frac{\\partial x}{\\partial u} = 1 + v, \\\\\n\\frac{\\partial x}{\\partial v} = u,\n\\end{cases}\n\\quad\ny = v + uv \\Rightarrow\n\\begin{cases}\n\\frac{\\partial y}{\\partial u} = v, \\\\\n\\frac{\\partial y}{\\partial v} = 1 + u.\n\\end{cases}\\]\nSo the Jacobian matrix is:\n\\[J = \\begin{vmatrix}\n1 + v & u \\\\\nv & 1 + u\n\\end{vmatrix} = (1 + v)(1 + u) - uv = 1 + u + v\\]\nStep 3: \\(~\\) Find the region \\(S\\) in the \\(uv\\)-plane\nWe translate the region \\(R\\) into the \\(uv\\)-coordinates\nFrom the substitution:\n\\[x = u + uv = u(1 + v), \\quad y = v + uv = v(1 + u)\\]\nWe now rewrite the boundaries:\n\n\\(y = 0 \\Rightarrow v(1 + u) = 0 \\Rightarrow v = 0\\) (since \\(1 + u \\ne 0\\))\n\\(y = x \\Rightarrow v(1 + u) = u(1 + v) \\Rightarrow v = u\\)\n\\(x = 2 \\Rightarrow u(1 + v) = 2 ⇒ u = \\frac{2}{1 + v}\\)\n\nSo the region in the \\(uv\\)-plane is:\n\nlower bound: \\(v = 0\\)\nupper bound: \\(v = u\\)\nand \\(u \\le \\frac{2}{1 + v}\\)\n\nLet’s fix bounds:\nSince \\(u = \\frac{2}{1 + v}\\), and also \\(v = u\\), we substitute:\n\\[v = \\frac{2}{1 + v} \\Rightarrow v(1 + v) = 2 \\Rightarrow (v - 1)(v + 2) = 0\\]\nSo \\(v = 1\\) is the upper limit (since \\(v &gt; 0\\)), and the region \\(S\\) is bounded by:\n\n\\(0 \\le v \\le 1\\)\n\\(v \\le u \\le \\frac{2}{1 + v}\\)\n\nStep 4: \\(~\\) Change of variables in the integral\nWe rewrite the integral in terms of \\(u\\), \\(v\\):\n\\[\\iint_S \\frac{1}{\\sqrt{(u + v +1)^2}} \\cdot |J| \\, du\\, dv\\]\nSo the integrand becomes simply 1. Then the integral becomes the area of the region \\(S\\):\n\\[\\iint_S 1 \\, du\\, dv = \\int_{v=0}^{1} \\int_{u=v}^{\\frac{2}{1 + v}} 1 \\, du\\, dv = \\int_0^1 \\left( \\frac{2}{1 + v} - v \\right) dv =2 \\ln 2 - \\frac{1}{2}\\]\n\\(~\\)\n6. \\(~\\) Evaluate the integral\n\\[ \\iint_R (x^2 +y^2) \\sqrt[3]{x^2 -y^2} \\,dA \\]\nwhere \\(R\\) is the region bounded by the graphs of \\(x=0\\), \\(x=1\\), \\(y=0\\), and \\(y=1\\) by means of the change of variables \\(u=2xy\\), \\(v=x^2 -y^2\\)\nSolution\nStep 1: \\(~\\) Compute the Jacobian\nWe need the Jacobian determinant \\(J = \\left| \\frac{\\partial(x, y)}{\\partial(u, v)} \\right|\\), but we are given \\(u = 2xy\\), \\(v = x^2 - y^2\\), so we compute \\(\\frac{\\partial(u, v)}{\\partial(x, y)}\\) and then take the reciprocal for the change of variables\nCompute partial derivatives:\n\n\\(\\displaystyle u = 2xy \\;\\Rightarrow \\frac{\\partial u}{\\partial x} = 2y, \\; \\frac{\\partial u}{\\partial y} = 2x\\)\n\\(\\displaystyle v = x^2 - y^2 \\;\\Rightarrow \\frac{\\partial v}{\\partial x} = 2x, \\; \\frac{\\partial v}{\\partial y} = -2y\\)\n\nSo the Jacobian matrix is:\n\\[\\frac{\\partial(u,v)}{\\partial(x,y)} =\n\\begin{pmatrix}\n2y & \\phantom{-}2x \\\\\n2x & -2y\n\\end{pmatrix}\\]\nTherefore,\n\\[\\left| \\frac{\\partial(x,y)}{\\partial(u,v)} \\right| = \\frac{1}{|J|} = \\frac{1}{| -4(x^2 + y^2) |} = \\frac{1}{4(x^2 + y^2)}\\]\nStep 2: \\(~\\) Substituting the integrand and Jacobian\nThe integrand is \\((x^2 + y^2)\\sqrt[3]{x^2 - y^2}\\), \\(~\\) and from the transformation:\n\n\\(v = x^2 - y^2 \\Rightarrow \\sqrt[3]{x^2 - y^2} = \\sqrt[3]{v}\\)\n\\(x^2 + y^2\\) appears in both the integrand and the Jacobian, so:\n\n\\[(x^2 + y^2) \\cdot \\sqrt[3]{x^2 - y^2} \\cdot \\left| \\frac{\\partial(x,y)}{\\partial(u,v)} \\right| = (x^2 + y^2) \\cdot \\sqrt[3]{v} \\cdot \\frac{1}{4(x^2 + y^2)} = \\frac{1}{4} \\sqrt[3]{v}\\]\nSo the transformed integral becomes:\n\\[\\iint_S \\frac{1}{4} \\sqrt[3]{v} \\, du\\, dv\\]\nStep 3: \\(~\\) Find the region \\(S\\) in \\(uv\\)-coordinates\nWe are given the transformation:\n\\[u = 2xy, \\quad v = x^2 - y^2\\]\nand the original region \\(R\\) is the unit square:\n\\[0 \\le x \\le 1,\\quad 0 \\le y \\le 1\\]\nWe compute the image of the four corners of the square under the transformation:\n\n\n\n\n\n\n\n\n\n\\((x, y)\\)\n\\(u = 2xy\\)\n\\(v = x^2 - y^2\\)\n\\((u, v)\\)\n\n\n\n\n\\((0, 0)\\)\n\\(0\\)\n\\(\\phantom{-}0\\)\n\\((0, \\phantom{-}0)\\)\n\n\n\\((1, 0)\\)\n\\(0\\)\n\\(\\phantom{-}1\\)\n\\((0, \\phantom{-}1)\\)\n\n\n\\((0, 1)\\)\n\\(0\\)\n\\(-1\\)\n\\((0, -1)\\)\n\n\n\\((1, 1)\\)\n\\(2\\)\n\\(\\phantom{-}0\\)\n\\((2, \\phantom{-}0)\\)\n\n\n\nWe examine how each edge of the square transforms under the change of variables:\n\nEdge \\(x = 0\\): \\(~u = 0, v = -y^2 \\in [-1, 0] \\Rightarrow\\; {\\scriptsize\\text{Vertical segment on the } u = 0}\\)\nEdge \\(y = 0\\): \\(~u = 0, v = x^2 \\in [0, 1] \\Rightarrow\\; {\\scriptsize\\text{Another vertical segment on the } u = 0}\\)\nEdge \\(x = 1\\): \\(~u = 2y \\in [0, 2], v = 1 - y^2 = 1 - \\frac{u^2}{4} \\Rightarrow\\; {\\scriptsize\\text{Upper parabola}}\\)\nEdge \\(y = 1\\): \\(~u = 2x \\in [0, 2], v = x^2 - 1 = \\frac{u^2}{4} - 1 \\Rightarrow\\; {\\scriptsize\\text{Lower parabola}}\\)\n\nSo, the transformed region is:\n\\[S = \\left\\{ (u, v) \\ \\middle| \\ 0 \\le u \\le 2,\\quad \\frac{u^2}{4} - 1 \\le v \\le 1 - \\frac{u^2}{4} \\right\\}\\]\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Create u values\nu = np.linspace(0, 2, 400)\n\n# Define the upper and lower boundaries of v\nv_upper = 1 -(u**2)/4\nv_lower = (u**2)/4 -1\n\n# Plot the region S\nplt.figure(figsize=(6, 6))\nplt.plot((0, 0, 0, 2), (0, 1, -1, 0), 'ko')\nplt.plot(u, v_upper, 'r', label=r'$v = 1 -\\frac{u^2}{4}$')\nplt.plot((0, 0), (0, 1), 'r', lw=3)\nplt.plot(u, v_lower, 'b', label=r'$v = \\frac{u^2}{4} -1$')\nplt.plot((0, 0), (0, -1), 'b', lw=3)\nplt.fill_between(u, v_lower, v_upper, color='lightgray', alpha=0.6)\n\n# Formatting\nplt.title(\"Region S in the uv-plane\")\nplt.xlabel(\"u\")\nplt.ylabel(\"v\")\nplt.legend()\nplt.grid(True)\nplt.xlim(0, 2)\nplt.ylim(-1.1, 1.1)\nplt.gca().set_aspect('equal')\n\nplt.show()\n\n\n\n\n\n\n\n\nStep 4: \\(~\\) Evaluate the integral\nSince the integrand \\(v^{1/3}\\) is an odd function, and the bounds of \\(u\\) are symmetric in \\(v\\), the integral is:\n\\[\\iint_S \\frac{1}{4} \\sqrt[3]{v} \\, du\\, dv = 0\\]\n\\(~\\)\n7. \\(~\\) Find the tangential and normal components of acceleration for the path:\n\\[ \\mathbf{x}(t) = \\left(2t, e^{2t} \\right)\\]\nSolution\nStep 1: \\(~\\) Compute the velocity vector\n\\[\\mathbf{v}(t) = \\frac{d\\mathbf{x}}{dt} = \\left(2, 2e^{2t} \\right)\\]\nStep 2: \\(~\\) Compute the acceleration vector\n\\[\\mathbf{a}(t) = \\frac{d\\mathbf{v}}{dt} = \\left(0, 4e^{2t} \\right)\\]\nStep 3: \\(~\\) Compute the speed\n\\[|\\mathbf{v}(t)| = \\sqrt{2^2 + (2e^{2t})^2} = \\sqrt{4 + 4e^{4t}} = 2\\sqrt{1 + e^{4t}}\\]\nStep 4: \\(~\\) Tangential component of acceleration\nThe tangential component of acceleration is given by the derivative of speed:\n\\[a_T = \\frac{d}{dt}|\\mathbf{v}(t)| = \\frac{d}{dt} \\left(2\\sqrt{1 + e^{4t}} \\right)\n= \\frac{2 \\cdot \\frac{1}{2} \\cdot 4e^{4t}}{\\sqrt{1 + e^{4t}}}\n= \\frac{4e^{4t}}{\\sqrt{1 + e^{4t}}}\\]\nStep 5: \\(~\\) Normal component of acceleration\nThe normal component of acceleration is given by:\n\\[a_N = \\sqrt{|\\mathbf{a}(t)|^2 - a_T^2}\\]\nWe compute:\n\n\\(|\\mathbf{a}(t)| = \\sqrt{0^2 + (4e^{2t})^2} = 4e^{2t}\\)\nSo \\(|\\mathbf{a}(t)|^2 = 16e^{4t}\\)\nAnd \\(a_T^2 = \\left(\\frac{4e^{4t}}{\\sqrt{1 + e^{4t}}}\\right)^2 = \\frac{16e^{8t}}{1 + e^{4t}}\\)\n\nNow plug into the normal acceleration formula:\n\\[\n\\begin{aligned}\na_N &= \\sqrt{16e^{4t} - \\frac{16e^{8t}}{1 + e^{4t}}} \\\\\n&= \\sqrt{ \\frac{16e^{4t}(1 + e^{4t}) - 16e^{8t}}{1 + e^{4t}} } \\\\\n&= \\sqrt{ \\frac{16e^{4t}}{1 + e^{4t}} } \\\\\n&= \\frac{4e^{2t}}{\\sqrt{1 + e^{4t}}}\n\\end{aligned}\n\\]\n\\(~\\)\n8. \\(~\\) Find the tangential and normal components of acceleration for the path:\n\\[ \\mathbf{x}(t) = \\left(t, t, t^2 \\right) \\]\nSolution\nStep 1: \\(~\\) Compute velocity and acceleration\n\\[\n\\begin{aligned}\n\\mathbf{v}(t) &= \\frac{d\\mathbf{x}}{dt} = (1, 1, 2t)\\\\\n\\mathbf{a}(t) &= \\frac{d\\mathbf{v}}{dt} = (0, 0, 2)\n\\end{aligned}\\]\nStep 2: \\(~\\) Compute speed\n\\[|\\mathbf{v}(t)| = \\sqrt{1^2 + 1^2 + (2t)^2} = \\sqrt{2 + 4t^2}\\]\nStep 3: \\(~\\) Tangential component of acceleration\nThe tangential component is:\n\\[a_T = \\frac{d}{dt} |\\mathbf{v}(t)| = \\frac{d}{dt} \\sqrt{2 + 4t^2}\n= \\frac{1}{2\\sqrt{2 + 4t^2}} \\cdot 8t = \\frac{4t}{\\sqrt{2 + 4t^2}}\\]\nStep 4: \\(~\\) Normal component of acceleration\nThe normal component is given by:\n\\[a_N = \\sqrt{|\\mathbf{a}(t)|^2 - a_T^2}\\]\nWe have:\n\n\\(|\\mathbf{a}(t)| = \\sqrt{0^2 + 0^2 + 2^2} = 2\\)\n\\(|\\mathbf{a}(t)|^2 = 4\\)\n\\(a_T^2 = \\left( \\frac{4t}{\\sqrt{2 + 4t^2}} \\right)^2 = \\frac{16t^2}{2 + 4t^2}\\)\n\nNow compute:\n\\[\n\\begin{aligned}\na_N &= \\sqrt{4 - \\frac{16t^2}{2 + 4t^2}} = \\sqrt{ \\frac{4(2 + 4t^2) - 16t^2}{2 + 4t^2} } \\\\\n&= \\sqrt{ \\frac{8 + 16t^2 - 16t^2}{2 + 4t^2} } = \\sqrt{ \\frac{8}{2 + 4t^2} }\\\\\n&= \\frac{2\\sqrt{2}}{\\sqrt{2 + 4t^2}}\n\\end{aligned}\\]\n\\(~\\)\n9. \\(~\\) Let \\(\\mathbf{r} = x \\mathbf{i} + y \\mathbf{j} + z \\mathbf{k}\\) and \\(r\\) denote \\(\\| \\mathbf{r} \\|\\). Evaluate the following:\n\\[\\nabla \\cdot (r^n \\mathbf{r})\\]\nSolution\nStep 1: \\(~\\) Use product rule for divergence\nLet’s use the identity:\n\\[\\nabla \\cdot (f \\mathbf{v}) = (\\nabla f) \\cdot \\mathbf{v} + f (\\nabla \\cdot \\mathbf{v})\\]\nSet:\n\n\\(f = r^n\\)\n\\(\\mathbf{v} = \\mathbf{r}\\)\n\nThen:\n\\[\\nabla \\cdot (r^n \\mathbf{r}) = (\\nabla r^n) \\cdot \\mathbf{r} + r^n (\\nabla \\cdot \\mathbf{r})\\]\nStep 2: \\(~\\) Compute the terms\n\nGradient of \\(r^n\\):\n\nWe use the chain rule:\n\\[\\nabla r^n = n r^{n-1} \\nabla r\\]\nand since \\(r = \\sqrt{x^2 + y^2 + z^2}\\), \\(\\,\\) we know:\n\\[\\nabla r = \\left( \\frac{x}{r}, \\frac{y}{r}, \\frac{z}{r} \\right) = \\frac{\\mathbf{r}}{r}\\]\nSo:\n\\[\\nabla r^n = n r^{n-1} \\cdot \\frac{\\mathbf{r}}{r} = n r^{n-2} \\mathbf{r}\\]\nTherefore,\n\\[(\\nabla r^n) \\cdot \\mathbf{r} = n r^{n-2} \\mathbf{r} \\cdot \\mathbf{r} = n r^{n-2} r^2 = n r^n\\]\n\nDivergence of \\(\\mathbf{r}\\):\n\n\\[\\nabla \\cdot \\mathbf{r} = \\frac{\\partial x}{\\partial x} + \\frac{\\partial y}{\\partial y} + \\frac{\\partial z}{\\partial z} = 1 + 1 + 1 = 3\\]\nStep 3: \\(~\\) Combine everything\n\\[\\nabla \\cdot (r^n \\mathbf{r}) = n r^n + 3 r^n = (n + 3) r^n\\]\n\\(~\\)\n10. \\(~\\) Let \\(\\mathbf{r} = x \\mathbf{i} + y \\mathbf{j} + z \\mathbf{k}\\) and \\(r\\) denote \\(\\| \\mathbf{r} \\|\\). Evaluate the following:\n\\[\\nabla \\times (r^n \\mathbf{r})\\]\nSolution\nStep 1: \\(~\\) Use Vector Identity\nWe apply the identity:\n\\[\\nabla \\times (f \\mathbf{v}) = (\\nabla f) \\times \\mathbf{v} + f (\\nabla \\times \\mathbf{v})\\]\nLet:\n\n\\(f = r^n\\)\n\\(\\mathbf{v} = \\mathbf{r}\\)\n\nSo:\n\\[\\nabla \\times (r^n \\mathbf{r}) = (\\nabla r^n) \\times \\mathbf{r} + r^n (\\nabla \\times \\mathbf{r})\\]\nStep 2: \\(~\\) Compute Each Term\n\n\\(\\nabla \\times \\mathbf{r}\\)\n\n\\[\\nabla \\times \\mathbf{r} = \\nabla \\times (x\\mathbf{i} + y\\mathbf{j} + z\\mathbf{k}) = \\mathbf{0}\\]\n\n\\(\\nabla r^n\\)\n\nWe compute:\n\\[\\nabla r^n = n r^{n-1} \\nabla r\\]\nand since:\n\\[\\nabla r = \\left( \\frac{x}{r}, \\frac{y}{r}, \\frac{z}{r} \\right) = \\frac{\\mathbf{r}}{r}\n\\Rightarrow \\nabla r^n = n r^{n-1} \\cdot \\frac{\\mathbf{r}}{r} = n r^{n-2} \\mathbf{r}\\]\n\nCompute the cross product\n\n\\[(\\nabla r^n) \\times \\mathbf{r} = (n r^{n-2} \\mathbf{r}) \\times \\mathbf{r} = n r^{n-2} (\\mathbf{r} \\times \\mathbf{r}) = n r^{n-2} \\cdot \\mathbf{0} = \\mathbf{0}\\]\n\\(~\\)\n11. \\(~\\) Let \\(\\mathbf{v} = u(x,y) \\mathbf{i} - v(x,y) \\mathbf{j}\\) be an incompressible, irrotational velocity field\n\\((a)\\) Show that the functions \\(u\\) and \\(v\\) (which determine the component functions of \\(\\mathbf{v}\\)) satisfy the Cauchy-Riemann equations\n\\[\\frac{\\partial u}{\\partial x} = \\frac{\\partial v}{\\partial y}\\, \\;\\text{and}\\; \\,\n  \\frac{\\partial u}{\\partial y} = -\\frac{\\partial v}{\\partial x}\\]\n\\((b)\\) Show that \\(u\\) and \\(v\\) are harmonic, that is, that\n$$\\frac{\\partial^2 u}{\\partial x^2} + \\frac{\\partial^2 u}{\\partial y^2}=0\\, \\;\\text{and}\\; \\,\n\\frac{\\partial^2 v}{\\partial x^2} + \\frac{\\partial^2 v}{\\partial y^2} = 0$$\nSolution\nWe are given a 2D velocity field and told that this flow is:\n\nIncompressible (i.e., \\(\\nabla \\cdot \\mathbf{v} = 0\\))\nIrrotational (i.e., \\(\\nabla \\times \\mathbf{v} = 0\\))\n\n\\((a)\\)\nStep 1: \\(~\\) Incompressibility condition\nThe divergence of a 2D vector field \\(\\mathbf{v} = (u, -v)\\) is:\n\\[\\nabla \\cdot \\mathbf{v} = \\frac{\\partial u}{\\partial x} + \\frac{\\partial (-v)}{\\partial y}\n= \\frac{\\partial u}{\\partial x} - \\frac{\\partial v}{\\partial y}\\]\nIncompressibility implies:\n\\[\\frac{\\partial u}{\\partial x} - \\frac{\\partial v}{\\partial y} = 0\n\\quad \\Rightarrow \\quad\n\\boxed{\\frac{\\partial u}{\\partial x} = \\frac{\\partial v}{\\partial y}}\\]\nStep 2: \\(~\\) Irrotationality condition\nIn 2D, the scalar curl of a vector field \\(\\mathbf{v} = (u, -v)\\) is:\n\\[\\nabla \\times \\mathbf{v} = \\frac{\\partial (-v)}{\\partial x} - \\frac{\\partial u}{\\partial y}\n= -\\frac{\\partial v}{\\partial x} - \\frac{\\partial u}{\\partial y}\\]\nIrrotationality implies:\n\\[-\\frac{\\partial v}{\\partial x} - \\frac{\\partial u}{\\partial y} = 0\n\\quad \\Rightarrow \\quad\n\\boxed{\\frac{\\partial u}{\\partial y} = -\\frac{\\partial v}{\\partial x}} \\]\n\\((b)\\)\nStep 1: \\(~\\) Differentiate the Cauchy-Riemann equations\nStart with the first:\n\\[\\frac{\\partial u}{\\partial x} = \\frac{\\partial v}{\\partial y}\\]\nDifferentiate both sides with respect to \\(x\\):\n\\[\\frac{\\partial^2 u}{\\partial x^2} = \\frac{\\partial^2 v}{\\partial x \\partial y}\n\\quad\\text{(1)}\\]\nNow differentiate the second Cauchy-Riemann equation:\n\\[\\frac{\\partial u}{\\partial y} = -\\frac{\\partial v}{\\partial x}\\]\nDifferentiate both sides with respect to \\(y\\):\n\\[\\frac{\\partial^2 u}{\\partial y^2} = -\\frac{\\partial^2 v}{\\partial y \\partial x}\n\\quad\\text{(2)}\\]\nStep 2: \\(~\\) Add the second derivatives of \\(u\\)\nFrom equations (1) and (2):\n\\[\\frac{\\partial^2 u}{\\partial x^2} + \\frac{\\partial^2 u}{\\partial y^2}\n= \\frac{\\partial^2 v}{\\partial x \\partial y} - \\frac{\\partial^2 v}{\\partial y \\partial x} = 0\\]\n(because mixed partial derivatives are equal under continuity)\nSo,\n\\[\\boxed{\\frac{\\partial^2 u}{\\partial x^2} + \\frac{\\partial^2 u}{\\partial y^2} = 0}\\]\nStep 3: \\(~\\) Do the same for \\(v\\)\nStart again from:\n\\[\\frac{\\partial v}{\\partial y} = \\frac{\\partial u}{\\partial x}\n\\Rightarrow \\frac{\\partial^2 v}{\\partial y^2} = \\frac{\\partial^2 u}{\\partial x \\partial y}\n\\quad\\text{(3)}\\]\n\\[\\frac{\\partial v}{\\partial x} = -\\frac{\\partial u}{\\partial y}\n\\Rightarrow \\frac{\\partial^2 v}{\\partial x^2} = -\\frac{\\partial^2 u}{\\partial x \\partial y}\n\\quad\\text{(4)}\\]\nNow add:\n\\[\\frac{\\partial^2 v}{\\partial x^2} + \\frac{\\partial^2 v}{\\partial y^2}\n= -\\frac{\\partial^2 u}{\\partial x \\partial y} + \\frac{\\partial^2 u}{\\partial x \\partial y} = 0\\]\nThus:\n\\[\\boxed{\\frac{\\partial^2 v}{\\partial x^2} + \\frac{\\partial^2 v}{\\partial y^2} = 0}\\]\n\\(~\\)\n12. \\(~\\) Let a particle of mass \\(m\\) travel along a differentiable path \\(\\mathbf{x}\\) in a Newtonian vector field \\(\\mathbf{F}\\) (i.e., one that satisfies Newton’s second law \\(\\mathbf{F} = m\\mathbf{a}\\), where \\(\\mathbf{a}\\) is the acceleration of \\(\\mathbf{x}\\)). We define the angular momentum \\(\\mathbf{l}(t)\\) of the particle to be the cross product of the position vector and the linear momentum \\(m\\mathbf{v}\\), i.e.,\n\\[\\mathbf{l}(t) = \\mathbf{x}(t) \\times m \\mathbf{v}(t)\\]\n(Here \\(\\mathbf{v}\\) denotes the velocity of \\(\\mathbf{x}\\).) The torque about the origin of the coordinate system due to the force \\(\\mathbf{F}\\) is the cross product of position and force:\n\\[\\mathbf{M}(t) = \\mathbf{x}(t) \\times \\mathbf{F}(t)\\]\n\\((a)\\) Show that\n\\[\\frac{d\\mathbf{l}}{dt} = \\mathbf{M}\\]\nThus, we see that the rate of change of angular momentum is equal to the torque imparted to the particle by the vector field \\(\\mathbf{F}\\)\n\\((b)\\) Suppose that \\(\\mathbf{F}\\) is a central force (i.e., a force that always points directly towards or away from the origin). Show that in this case the angular momentum is conserved, that is, that it must remain constant\nSolution\n\\((a)\\)\nStart from the definition of angular momentum:\n\\[\\mathbf{l}(t) = \\mathbf{x}(t) \\times m\\mathbf{v}(t)\\]\nDifferentiate using the product rule for cross products:\n\\[\\frac{d\\mathbf{l}}{dt} = \\frac{d}{dt} \\left[ \\mathbf{x}(t) \\times m\\mathbf{v}(t) \\right]\n= \\dot{\\mathbf{x}}(t) \\times m\\mathbf{v}(t) + \\mathbf{x}(t) \\times m\\dot{\\mathbf{v}}(t)\\]\nSince \\(\\dot{\\mathbf{x}}(t) = \\mathbf{v}(t)\\), and \\(\\dot{\\mathbf{v}}(t) = \\mathbf{a}(t),\\) this becomes:\n\\[\\frac{d\\mathbf{l}}{dt} = \\mathbf{v}(t) \\times m\\mathbf{v}(t) + \\mathbf{x}(t) \\times m\\mathbf{a}(t)\\]\nNow note:\n\n\\(\\mathbf{v}(t) \\times \\mathbf{v}(t) = \\mathbf{0}\\) (any vector crossed with itself is zero)\nSo the first term vanishes\n\nThus:\n\\[\\frac{d\\mathbf{l}}{dt} = \\mathbf{x}(t) \\times m\\mathbf{a}(t)\n= \\mathbf{x}(t) \\times \\mathbf{F}(t) = \\mathbf{M}(t)\\]\n\\((b)\\)\nWhat is a central force?\nA central force is a vector field where the force always points directly toward or away from the origin. That is:\n\\[\\mathbf{F}(t) = f(r)\\, \\hat{\\mathbf{r}} = f(r) \\frac{\\mathbf{x}(t)}{r}\n\\quad \\text{where} \\quad r = \\|\\mathbf{x}(t)\\|\\]\nIn other words, \\(\\mathbf{F}(t)\\) is parallel to \\(\\mathbf{x}(t)\\), so:\n\\[\\mathbf{x}(t) \\times \\mathbf{F}(t) = \\mathbf{0}\\]\nRecall from part (a):\nWe showed that:\n\\[\\frac{d\\mathbf{l}}{dt} = \\mathbf{x}(t) \\times \\mathbf{F}(t) = \\mathbf{M}(t)\\]\nNow if \\(\\mathbf{F}\\) is a central force, then \\(\\mathbf{x}(t) \\times \\mathbf{F}(t) = \\mathbf{0},\\) so:\n\\[\\frac{d\\mathbf{l}}{dt} = \\mathbf{0}\\]\nThis implies that:\n\\[\\boxed{\\mathbf{l}(t) = \\text{constant vector}}\\]\n\\(~\\)",
    "crumbs": [
      "**Second Semester**",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Vector Calculus</span>"
    ]
  },
  {
    "objectID": "ch_10_Systems_of_Linear_Differential_Equations.html",
    "href": "ch_10_Systems_of_Linear_Differential_Equations.html",
    "title": "7  Systems of Linear Differential Equations",
    "section": "",
    "text": "7.1 Theory of Linear Systems",
    "crumbs": [
      "**First Semester**",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Systems of Linear Differential Equations</span>"
    ]
  },
  {
    "objectID": "ch_10_Systems_of_Linear_Differential_Equations.html#sec-10-1",
    "href": "ch_10_Systems_of_Linear_Differential_Equations.html#sec-10-1",
    "title": "7  Systems of Linear Differential Equations",
    "section": "",
    "text": "Here we confine our study to systems of first-order DEs that are special cases of systems that have the normal form\n\\[\n\\begin{aligned}\n  \\frac{d x_1}{dt} &= g_1(t, x_1, x_2, \\cdots, x_n) \\\\\n  \\frac{d x_2}{dt} &= g_2(t, x_1, x_2, \\cdots, x_n) \\\\\n   &\\;\\vdots \\\\\n  \\frac{d x_n}{dt} &= g_n(t, x_1, x_2, \\cdots, x_n)  \n\\end{aligned}\\]\nThis system is called a first-order system\nWhen each of the functions \\(\\,g_i\\), \\(i=1,\\cdots,n\\), is linear in the dependent variables \\(x_i\\), \\(i=1,\\cdots,n\\), \\(\\,\\) we get the normal form of a first-order system of linear equations\n\\[\n\\begin{aligned}\n  \\frac{d x_1}{dt}\n     &= a_{11}(t) x_1 +a_{12}(t) x_2 +\\cdots +a_{1n}(t) x_n +f_1(t) \\\\\n  \\frac{d x_2}{dt}\n     &= a_{21}(t) x_1 +a_{22}(t) x_2 +\\cdots +a_{2n}(t) x_n +f_2(t) \\\\\n     &\\;\\vdots \\\\\n   \\frac{d x_n}{dt} &= a_{n1}(t) x_1 +a_{n2}(t) x_2 +\\cdots +a_{nn}(t) x_n +f_n(t) \\\\\n\\end{aligned} \\tag{LS}\\label{eq:LS}\\]\nWe refer to \\(\\eqref{eq:LS}\\) simply as a linear system. We assume that the coefficient \\(a_{ij}(t)\\) as well as the functions \\(f_i(t)\\) are continuous on a common interval \\(I\\). When \\(f_i(t)=0\\), \\(i=1,\\cdots,n\\), \\(\\,\\) the linear system is said to be homogeneous; otherwise it is nonhomogeneous\nIf \\(\\,\\mathbf{x}\\), \\(\\mathbf{A}\\), and \\(\\mathbf{f}\\) denote the respective matrices\n\\[\n\\mathbf{x}=\n  \\begin{pmatrix} x_1 \\\\ x_2 \\\\ \\vdots \\\\ x_n \\end{pmatrix}, \\;\n\\mathbf{A}=\n\\begin{pmatrix}\n  a_{11} & a_{12} & \\cdots & a_{1n}\\\\\n  a_{21} & a_{22} & \\cdots & a_{2n}\\\\\n  \\vdots &        &        & \\vdots \\\\\n  a_{n1} & a_{n2} & \\cdots & a_{nn}\n\\end{pmatrix}, \\;\n\\mathbf{f} =\n\\begin{pmatrix} f_1 \\\\ f_2 \\\\ \\vdots \\\\ f_n \\end{pmatrix}\\]\nthen the system of linear first-order DEs can be written as\n\\[\\frac{d\\mathbf{x}}{dt}=\\mathbf{A}\\mathbf{x} +\\mathbf{f} \\tag{LSM}\\label{eq:LSM}\\]\nA solution vector of \\(\\eqref{eq:LSM}\\) can be interpreted geometrically as a set of parametric equations of a space curve. In the case \\(n=2\\) and \\(n=3\\), the equations \\(x_1=\\phi_1(t)\\), \\(x_2=\\phi_2(t)\\), and \\(x_1=\\phi_1(t)\\), \\(x_2=\\phi_2(t)\\), \\(x_3=\\phi_3(t)\\) represent curves in 2-space and 3-space, respectively. It is common practice to call such a solution curve a trajectory. The plane is also called the phase plane\nInitial-Value Problem\nLet \\(\\,t_0\\) denote a point on an interval \\(I\\) and\n\\[\\mathbf{x}_0=\\begin{pmatrix} \\gamma_1 \\\\ \\gamma_2 \\\\ \\vdots \\\\ \\gamma_n \\end{pmatrix}\\]\nwhere \\(\\gamma_i\\), \\(i=1,\\cdots,n\\) are given constants. Then the problem\n\\[\\frac{d\\mathbf{x}}{dt}=\\mathbf{A}(t)\\mathbf{x} +\\mathbf{f}(t), \\;\\;\n  \\mathbf{x}(t_0)=\\mathbf{x}_0\\]\nis an initial-value problem on the interval. Here there exists a unique solution of the initial-value problem if the entries of \\(\\mathbf{A}\\) and \\(\\mathbf{f}\\) are the continuous functions on a common interval \\(\\,I\\)\nSuperposition Principle\nLet \\(\\,\\mathbf{x}_1\\), \\(\\mathbf{x}_2\\), \\(\\cdots\\), \\(\\mathbf{x}_k\\,\\) be a set of solution vectors of the homogeneous system\n\\[\\frac{d\\mathbf{x}}{dt}=\\mathbf{A}(t)\\mathbf{x}\\]\non an interval \\(\\,I\\). Then the linear combination\n\\[\n\\mathbf{x}=c_1\\mathbf{x}_1 +c_2\\mathbf{x}_2 +\\cdots +c_k\\mathbf{x}_k\n\\]\nwhere \\(c_i\\), \\(i=1,2,\\cdots,k,\\) are arbitrary constants, is also a solution on the interval\nWe can introduce the concept of the Wronskian determinant as a test for linear independence. Let \\(\\,\\mathbf{x}_1\\), \\(\\mathbf{x}_2\\), \\(\\cdots\\), \\(\\mathbf{x}_n\\) be \\(n\\) solution vectors of the homogeneous system on \\(I\\). Then the set of solution vectors is linearly independent on \\(I\\,\\) if and only if the Wronskian\n\\[\\scriptsize\n  W(\\mathbf{x}_1, \\mathbf{x}_2, \\cdots, \\mathbf{x}_n)=\n  \\begin{vmatrix}\n    x_{11} &  x_{12} & \\cdots & x_{1n} \\\\\n    x_{21} &  x_{22} & \\cdots & x_{2n} \\\\\n    \\vdots &         &        & \\vdots \\\\\n    x_{n1} &  x_{n2} & \\cdots & x_{nn}\n  \\end{vmatrix} \\neq 0  \n\\]\nfor every \\(t\\) in the interval.\nFor every \\(t\\) in \\(I\\), either \\(W(\\mathbf{x}_1\\), \\(\\mathbf{x}_2\\), \\(\\cdots\\), \\(\\mathbf{x}_n)\\neq 0\\) or \\(W(\\mathbf{x}_1\\), \\(\\mathbf{x}_2\\), \\(\\cdots\\), \\(\\mathbf{x}_n)= 0\\). Thus if we can show that \\(W\\neq 0\\) for some \\(t_0\\) in \\(I\\), \\(\\,\\) then \\(W\\neq 0\\) for every \\(t\\), \\(\\,\\)hence the set of solutions is linearly independent on the interval\nAny set of \\(n\\) linearly independent solution vectors of the homogeneous system on an interval \\(I\\) is said to be a fundamental set of solutions. Then the general solution of the homogeneous system is\n\\[\\mathbf{x}=c_1 \\mathbf{x}_1 +c_2 \\mathbf{x}_2 +\\cdots +c_n \\mathbf{x}_n\\]\nwhere \\(c_i\\), \\(i=1,2,\\cdots,n\\) are arbitrary constants\nGeneral Solution of the Nonhomogeneous System\nFor nonhomogeneous systems, a particular solution \\(\\mathbf{x}_p\\) on an interval \\(I\\) is any vector, free of arbitrary parameters, whose entries are functions that satisfy system \\(\\text{(LSM)}\\). Let\n\\[\\mathbf{x}_h=c_1 \\mathbf{x}_1 +c_2 \\mathbf{x}_2 +\\cdots +c_n \\mathbf{x}_n\\]\ndenote the general solution of the associated homogeneous system. Then the general solution of the nonhomogeneous system is\n\\[\\mathbf{x}=\\mathbf{x}_h +\\mathbf{x}_p\\]",
    "crumbs": [
      "**First Semester**",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Systems of Linear Differential Equations</span>"
    ]
  },
  {
    "objectID": "ch_10_Systems_of_Linear_Differential_Equations.html#sec-10-2",
    "href": "ch_10_Systems_of_Linear_Differential_Equations.html#sec-10-2",
    "title": "7  Systems of Linear Differential Equations",
    "section": "7.2 Homogeneous Linear Systems",
    "text": "7.2 Homogeneous Linear Systems\n\\(~\\)\n\nFor the general homogeneous linear first-order system\n\\[\\frac{d\\mathbf{x}}{dt}=\\mathbf{A}\\mathbf{x} \\tag{HE}\\label{eq:HE}\\]\nwe are prompted to ask whether we can always find a solution of the form\n\\[\\mathbf{x}=\\begin{pmatrix} k_1 \\\\ k_2 \\\\ \\vdots \\\\ k_n \\end{pmatrix} e^{\\lambda t}\n  =\\mathbf{k} e^{\\lambda t} \\tag{SL}\\label{eq:SL}\\]\nIf \\(\\eqref{eq:SL}\\) is to be a solution vector of the system, then\n\\[\n\\begin{aligned}\n   \\displaystyle \\frac{d \\mathbf{x}}{dt}&= \\mathbf{A}\\mathbf{x} \\\\\n        &\\Downarrow \\;\\;\\mathbf{x}=\\mathbf{k}e^{\\lambda t} \\\\\n   \\lambda \\mathbf{k} e^{\\lambda t}&= \\mathbf{A} \\mathbf{k} e^{\\lambda t}\\\\\n        &\\Downarrow \\;\\; \\div \\,e^{\\lambda t} \\\\\n        (\\mathbf{A} -\\lambda \\mathbf{I}) \\mathbf{k} &= \\mathbf{0}\n\\end{aligned} \\tag{CE}\\label{eq:CE}\\]\nIn order for \\(\\eqref{eq:CE}\\) to have solutions other than the obvious trivial solution \\(\\mathbf{k}=\\mathbf{0}\\), \\(\\,\\)we must have\n\\[\\mathrm{det}(\\mathbf{A} -\\lambda\\mathbf{I})=0\\]\nThis polynomial equation of \\(\\lambda\\) is called the characteristic equation of \\(\\mathbf{A}\\); its solutions are the eigenvalues of \\(\\mathbf{A}\\). A solution \\(\\mathbf{k}\\neq \\mathbf{0}\\) of \\(\\eqref{eq:CE}\\) corresponding to an eigenvalue \\(\\lambda\\) is called an eigenvector of \\(\\mathbf{A}\\)\nDistinct Real Eigenvalues\nWhen \\(\\mathbf{A}\\) possesses \\(n\\) distinct real eigenvalues \\(\\lambda_i\\), \\(i=1,2,\\cdots,n\\), \\(\\,\\)then a set of \\(n\\) linearly independent eigenvectors \\(\\mathbf{k}_1\\), \\(\\mathbf{k}_2\\), \\(\\cdots\\), \\(\\mathbf{k}_n\\) can always be found and\n\\[\\mathbf{x}_1=\\mathbf{k}_1 e^{\\lambda_1 t},\n  \\mathbf{x}_2=\\mathbf{k}_2 e^{\\lambda_2 t}, \\cdots,\n  \\mathbf{x}_n=\\mathbf{k}_n e^{\\lambda_n t}\\]\nis a fundamental set of solutions of the homogeneous system \\(\\eqref{eq:HE}\\) on \\((-\\infty,\\infty)\\)\n\n\\(~\\)\nExample \\(\\,\\) Solve\n\\[\\left.\\begin{array}{cl}\n     &\\displaystyle\\frac{dx}{dt} = 2x +3y\\\\\n     &\\displaystyle\\frac{dy}{dt} = 2x +y\n  \\end{array}\\right. \\;\\; \\Rightarrow\n     \\;\\;\\displaystyle \\frac{d\\mathbf{x}}{dt}=\\mathbf{A}\\mathbf{x},\n     \\;\\mathbf{A}=\n     \\begin{pmatrix}\n       2 & 3\\\\\n       2 & 1\n     \\end{pmatrix}, \\;\\mathbf{x}=\\begin{pmatrix} x \\\\ y \\end{pmatrix}\\]\n\nFrom the characteristic equation\n\\[\\mathrm{det}\\,(\\mathbf{A} -\\lambda\\mathbf{I})=\n  \\begin{vmatrix}\n     2 -\\lambda & 3\\\\\n     2 & 1 -\\lambda\n  \\end{vmatrix}\n  =(\\lambda +1)(\\lambda -4)=0\n\\]\nNow for \\(\\lambda_1=-1\\),\n\\[\n  \\begin{pmatrix}\n     3 & 3\\\\\n     2 & 2\n  \\end{pmatrix}\n  \\begin{pmatrix} k_1 \\\\ k_2\n  \\end{pmatrix}\n  =\\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix}\n\\]\nThus \\(\\,k_1=-k_2\\). \\(\\,\\)When \\(k_2=-1\\), \\(\\,\\)the related eigenvector is\n\\[\\mathbf{k}_1=\\begin{pmatrix} \\;\\;1 \\\\ -1 \\end{pmatrix}\\]\nFor \\(\\lambda_2=4\\),\n\\[\n  \\begin{pmatrix}\n     -2 & \\;\\;3\\\\\n     \\;\\;2 & -3\n  \\end{pmatrix} \\begin{pmatrix} k_1 \\\\ k_2 \\end{pmatrix}\n  =\\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix}\n\\]\nso that \\(k_1=\\frac{3}{2}k_2\\), \\(\\,\\)and therefore with \\(k_2=2\\), \\(\\,\\)the corresponding eigenvector is\n\\[\\mathbf{k}_2=\\begin{pmatrix} 3 \\\\ 2 \\end{pmatrix}\\]\nSince we have found two linearly independent solutions\n\\[\\mathbf{x}_1=\\begin{pmatrix} \\;\\;1 \\\\ -1 \\end{pmatrix} e^{-t}\n  \\;\\text{ and }\\;\n  \\mathbf{x}_2=\\begin{pmatrix} 3 \\\\ 2 \\end{pmatrix}e^{4t}\\]\nthe general solution of the system is\n\\[\\mathbf{x}=c_1 \\mathbf{x}_1 +c_2\\mathbf{x}_2\\]\n\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nw = 6\nxl = np.arange(-w, w, 0.1)\nyl = np.arange(-w, w, 0.1)\nx, y = np.meshgrid(xl, yl)\n\nxdot = 2*x +3*y\nydot = 2*x +y\n\ny_1 = -xl\ny_2 = 2/3 *xl\n\n\nfig, ax = plt.subplots(figsize=(5, 5))\n\nax.streamplot(x, y, xdot, ydot, color='blue')\nax.plot(xl, y_1, 'r:', xl, y_2, 'r:')\nax.set_title('Phase Portrait')\nax.axis((-w, w, -w, w))\nax.set_xlabel('x')\nax.set_ylabel('y')\nax.grid()\nax.tick_params(axis='both', direction='in')\n\n\n\n\n\n\n\nFigure 7.1: Phase Portrait of Saddle-Point System\n\n\n\n\n\n\nimport pprint\n\nA = np.array([[2, 3], [2, 1]])\nld, k = np.linalg.eig(A)\n\nprint('Eigenvalues =', end = ' '); pprint.pprint(ld)\nprint('Eigenvectors ='); pprint.pprint(k)\n\nEigenvalues = array([ 4., -1.])\nEigenvectors =\narray([[ 0.83205029, -0.70710678],\n       [ 0.5547002 ,  0.70710678]])\n\n\n\nimport sympy\nsympy.init_printing()\n\nA_ = sympy.Matrix(A)\nA_.eigenvects()\n\n\\(\\displaystyle \\left[ \\left( -1, \\  1, \\  \\left[ \\left[\\begin{matrix}-1\\\\1\\end{matrix}\\right]\\right]\\right), \\  \\left( 4, \\  1, \\  \\left[ \\left[\\begin{matrix}\\frac{3}{2}\\\\1\\end{matrix}\\right]\\right]\\right)\\right]\\)\n\n\n\\(~\\)\nExample \\(\\,\\)Use python to obtain the phase portraits of the systems\n\\(~\\)\n\n\n\\(~\\)\n\n\\[\\text{Attractor}\\]\n\\[\n\\begin{aligned}\n  \\frac{dx}{dt} &=-\\frac{5}{2}x +2y\\\\\n  \\frac{dy}{dt} &= \\frac{3}{4}x -2y\n\\end{aligned}\\]\n\n\\[\\text{Repeller}\\]\n\\[\n\\begin{aligned}\n  \\frac{dx}{dt} &= 2x +2y\\\\\n  \\frac{dy}{dt} &= x +3y\n\\end{aligned}\\]\n\n\\(~\\)\n\n\n\\(~\\)\n\nRepeated Eigenvalues\nFor an \\(n \\times n\\) matrix \\(\\mathbf{A}\\), if \\(m\\) is a positive integer and \\((\\lambda -\\lambda_1)^m\\) is a factor of the characteristic equation while \\((\\lambda -\\lambda_1)^{m +1}\\) is not a factor, then \\(\\lambda_1\\) is said to be an eigenvalue of multiplicity \\(m\\)\n\nIt may be possible to find \\(m\\) linearly independent eigenvectors \\(\\,\\) \\(\\mathbf{k}_1\\), \\(\\mathbf{k}_2\\), \\(\\cdots\\), \\(\\mathbf{k}_m\\) corresponding to an eigenvalue \\(\\lambda_1\\) of multiplicity \\(m \\leq n\\). In this case, the general solution contains the linear combination\n\\[c_1 \\mathbf{k}_1 e^{\\lambda_1 t} +c_2 \\mathbf{k}_2 e^{\\lambda_1 t} +\\cdots\n     +c_m \\mathbf{k}_m e^{\\lambda_1 t}\\]\n\nIf there is only one eigenvector corresponding to the eigenvalue \\(\\lambda_1\\) of multiplicity \\(m\\), then \\(m\\) linearly independent solutions of the form\n\\[\n\\begin{aligned}\n  \\mathbf{x}_1 &= \\mathbf{k}_{11} e^{\\lambda_1 t}\\\\\n  \\mathbf{x}_2 &= \\mathbf{k}_{11} t e^{\\lambda_1 t} +\\mathbf{k}_{12} e^{\\lambda_1 t}\\\\\n  &\\vdots\\\\\n  \\mathbf{x}_m &= \\mathbf{k}_{11} \\frac{t^{m-1}}{(m-1)!} e^{\\lambda_1 t}\n   +\\mathbf{k}_{12} \\frac{t^{m-2}}{(m-2)!} e^{\\lambda_1 t} +\\cdots +\\mathbf{k}_{1m} e^{\\lambda_1 t}\n\\end{aligned}\\]\ncan always be found\n\n\\(~\\)\nExample \\(\\,\\) Solve\n\\[\n\\mathbf{x}' =\n  \\left(\\begin{array}{rrr}\n      1 & -2 & 2 \\\\\n     -2 &  1 &-2 \\\\\n      2 & -2 & 1\n  \\end{array}\\right)\\mathbf{x}\\]\n\nThe characteristic equation\n\\[\\mathrm{det}(\\mathbf{A}-\\lambda \\mathbf{I}) =\n  \\left|\\begin{array}{ccc}\n    1-\\lambda & -2 & \\;\\;2 \\\\\n   -2 &  1-\\lambda &-2 \\\\\n    \\;\\;2 & -2 & 1-\\lambda\n  \\end{array}\\right|=0\n\\]\nyields \\(-(\\lambda+1)^2(\\lambda-5)=0\\). \\(\\,\\) Then \\(\\lambda_1=\\lambda_2=-1\\) and \\(\\lambda_3=5\\)\nFor \\(\\lambda_1=\\lambda_2=-1\\), \\(\\,\\) we have\n\\[\n(\\mathbf{A} +\\mathbf{I}|\\mathbf{0}) =\n\\left(\\begin{array}{rrr|r}\n  2 & -2 &  2 & 0\\\\\n-2 &  2 & -2 & 0\\\\\n  2 & -2 &  2 & 0\n\\end{array}\\right)\n\\overset{\\text{row operations}}{\\Longrightarrow}\n\\left(\\begin{array}{rrr|r}\n  1 & -1 & 1 & 0\\\\ \\hline\n  0 &  0 & 0 & 0\\\\\n  0 &  0 & 0 & 0\n\\end{array}\\right)  \n\\]\nThe first row of the last matrix means \\(k_1 -k_2 +k_3=0\\). \\(\\,\\)By choosing \\(k_2=1\\), \\(k_3=0\\) and \\(k_2=1\\), \\(k_3=1\\), two linearly independent eigenvectors corresponding to \\(\\lambda_1\\)\n\\[\\mathbf{k}_1=\n\\left(\\begin{array}{r}\n  1\\\\\n  1\\\\\n  0\n\\end{array}\\right) \\; \\text{ and } \\;\n\\mathbf{k}_2=\n\\left(\\begin{array}{r}\n  0\\\\\n  1\\\\\n  1\n\\end{array}\\right)\n\\]\nare obtained\nLast, for \\(\\lambda_3=5\\), \\(\\,\\)the Gauss elimination\n\\[\n(\\mathbf{A} -5\\mathbf{I}|\\mathbf{0}) =\n\\left(\\begin{array}{rrr|r}\n-4 & -2 &  2 & 0\\\\\n-2 & -4 & -2 & 0\\\\\n  2 & -2 & -4 & 0\n\\end{array}\\right)\n\\overset{\\text{row operations}}{\\Longrightarrow}\n\\left(\\begin{array}{rrr|r}\n  1 & 0 &-1 & 0\\\\\n  0 & 1 & 1 & 0\\\\ \\hline\n  0 & 0 & 0 & 0\n\\end{array}\\right)  \n\\]\nimplies \\(k_1=k_3\\) and \\(k_2=-k_3\\). Picking \\(k_3=1\\) gives \\(k_1=1\\), \\(k_2=-1\\), and thus a third eigenvector is\n\\[\\mathbf{k}_3=\n\\left(\\begin{array}{r}\n  1\\\\\n-1\\\\\n  1\n\\end{array}\\right)\n\\]\nThus we conclude that the general solution is\n\\[\\mathbf{x}=\nc_1\\left(\\begin{array}{r}\n      1\\\\\n      1\\\\\n      0\n    \\end{array}\\right)e^{-t} +\nc_2\\left(\\begin{array}{r}\n      0\\\\\n      1\\\\\n      1\n    \\end{array}\\right)e^{-t} +\nc_3\\left(\\begin{array}{r}\n      1\\\\\n     -1\\\\\n      1\n    \\end{array}\\right)e^{5t}      \n\\]\n\\(~\\)\nExample \\(\\,\\) Solve\n\\[\n\\mathbf{x}' =\n  \\left(\\begin{array}{rrr}\n    2 & 1 & 6 \\\\\n    0 & 2 & 5 \\\\\n    0 & 0 & 2\n  \\end{array}\\right)\\mathbf{x}\n\\]\nThe characteristic equation \\(-(\\lambda -2)^3=0\\,\\) shows that \\(\\lambda_1=2\\,\\) is an eigenvalue of multiplicity three. \\(\\,\\)We have\n\\[(\\mathbf{A} -2\\mathbf{I}|\\mathbf{0}) =\n\\left(\\begin{array}{rrr|r}\n  0 & 1 & 6 & 0\\\\\n  0 & 0 & 5 & 0\\\\\n  0 & 0 & 0 & 0\n\\end{array}\\right)\n\\]\nTherefore \\(\\,0 \\cdot k_1 +k_2 +6k_3 =0\\) and \\(k_3=0\\;\\rightarrow\\;k_2=k_3=0\\). \\(\\,\\) The choice \\(k_1=1\\) gives the single eigenvector\n\\[\\mathbf{k}_{11}=\n\\left(\\begin{array}{r}\n  1\\\\\n  0\\\\\n  0\n\\end{array}\\right)\n\\]\nA second solution can be found of the form\n\\[\\mathbf{x}_2 =\\mathbf{k}_{11} t e^{2t} +\\mathbf{k}_{12} e^{2t} \\tag{S1}\\label{eq:S1}\\]\nTo see this, \\(\\,\\)we substitute \\(\\eqref{eq:S1}\\) into the system \\(\\mathbf{x}'=\\mathbf{A}\\mathbf{x}\\)\n\\[\\underbrace{(\\mathbf{A} -2\\mathbf{I})\\mathbf{k_{11}}}_{\\mathbf{0}}te^{2t}\n   +(\\mathbf{A}\\mathbf{k}_{12} -2\\mathbf{k}_{12} -\\mathbf{k}_{11})e^{2t}=0\\]\nSince this last equation is to hold for all values of \\(t\\), \\(\\,\\)we must have\n\\[(\\mathbf{A} -2\\mathbf{I})\\mathbf{k}_{12}=\\mathbf{k}_{11}\\]\nTo find \\(\\mathbf{x}_2\\), \\(\\,\\)we need only solve the additional system for the generalized eigenvector \\(\\mathbf{k}_{12}\\)\n\\[\n(\\mathbf{A} -2\\mathbf{I}|\\mathbf{k}_{11}) =\n\\left(\\begin{array}{rrr|r}\n  0 & 1 & 6 & 1\\\\\n  0 & 0 & 5 & 0\\\\\n  0 & 0 & 0 & 0\n\\end{array}\\right)\n\\]\nThen \\(\\,0\\cdot k_1 +k_2 +6k_3 =1\\) and \\(k_3=0\\;\\rightarrow\\;k_2=1, k_3=0\\). \\(\\,\\)The choice \\(k_1=0\\) gives\n\\[\\mathbf{k}_{12}=\n\\left(\\begin{array}{r}\n  0\\\\\n  1\\\\\n  0\n\\end{array}\\right)\n\\]\nA third solution can be found of the form\n\\[\\mathbf{x}_3 =\\mathbf{k}_{11} \\frac{t^2}{2} e^{2t}\n  +\\mathbf{k}_{12}te^{2t} +\\mathbf{k}_{13}e^{2t} \\tag{S2}\\label{eq:S2}\\]\nTo see this, we substitute \\(\\eqref{eq:S2}\\) into the system \\(\\mathbf{x}'=\\mathbf{A}\\mathbf{x}\\)\n\\[\n\\begin{aligned}\n\\underbrace{(\\mathbf{A} -2\\mathbf{I})\\mathbf{k_{11}}}_{\\mathbf{0}}\n   &\\frac{t^2}{2}e^{2t}\n     +\\underbrace{(\\mathbf{A}\\mathbf{k}_{12} -2\\mathbf{k}_{12} -\\mathbf{k}_{11})}_{\\mathbf{0}}\\,t e^{2t} \\\\\n   &+(\\mathbf{A}\\mathbf{k}_{13} -2\\mathbf{k}_{13} -\\mathbf{k}_{12}) e^{2t}=0\n\\end{aligned}\\]\nThen, \\(\\,\\)to find \\(\\mathbf{k}_{13}\\)\n\\[\n(\\mathbf{A} -2\\mathbf{I}|\\mathbf{k}_{12}) =\n\\left(\\begin{array}{rrr|r}\n  0 & 1 & 6 & 0\\\\\n  0 & 0 & 5 & 1\\\\\n  0 & 0 & 0 & 0\n\\end{array}\\right)\n\\]\nThen \\(\\,0\\cdot k_1 +k_2 +6k_3 =0\\,\\) and \\(\\,5k_3=1\\;\\rightarrow\\;k_2=-\\frac{6}{5}, \\, k_3=\\frac{1}{5}\\). \\(\\,\\)The choice \\(k_1=0\\) gives\n\\[\\mathbf{k}_{13}=\n\\left(\\begin{array}{r}\n  0\\\\\n-\\frac{6}{5}\\\\\n  \\frac{1}{5}\n\\end{array}\\right)\n\\]\nThus we see that the general solution is\n\\[\n\\begin{aligned}\n\\mathbf{x}=  \n&\\; c_1\\left(\\begin{array}{r}\n           1\\\\\n           0\\\\\n           0\n         \\end{array}\\right)e^{2t}+\nc_2\\left[\n         \\left(\\begin{array}{r}\n           1\\\\\n           0\\\\\n           0\n         \\end{array}\\right)te^{2t}+\n         \\left(\\begin{array}{r}\n           0\\\\\n           1\\\\\n           0\n         \\end{array}\\right)e^{2t}           \n    \\right]\n\\\\&+\nc_3\\left[\n         \\left(\\begin{array}{r}\n           1\\\\\n           0\\\\\n           0\n         \\end{array}\\right)\\frac{t^2}{2}e^{2t}+\n         \\left(\\begin{array}{r}\n           0\\\\\n           1\\\\\n           0\n         \\end{array}\\right)te^{2t}+\n         \\left(\\begin{array}{r}\n           0\\\\\n          -\\frac{6}{5}\\\\\n           \\frac{1}{5}\n         \\end{array}\\right)e^{2t}            \n    \\right]\n\\end{aligned}\n\\]\n\n\\(~\\)\nExample \\(\\,\\) Use python to obtain the phase portrait of the system\n\\[\n\\begin{aligned}\n  \\frac{dx}{dt} &= 3x -18y\\\\\n  \\frac{dy}{dt} &= 2x -9y\n\\end{aligned}\\]\n\nA = np.array([[3, -18], [2, -9]])\nld, k = np.linalg.eig(A)\n\nprint('Eigenvalues =', end = ' '); pprint.pprint(ld)\nprint('Eigenvectors ='); pprint.pprint(np.around(k, decimals=4))\n\nEigenvalues = array([-3., -3.])\nEigenvectors =\narray([[0.9487, 0.9487],\n       [0.3162, 0.3162]])\n\n\n\nA_ = sympy.Matrix(A)\nA_.eigenvects()\n\n\\(\\displaystyle \\left[ \\left( -3, \\  2, \\  \\left[ \\left[\\begin{matrix}3\\\\1\\end{matrix}\\right]\\right]\\right)\\right]\\)\n\n\n\nw = 6\nxl = np.arange(-w, w, 0.1)\nyl = np.arange(-w, w, 0.1)\nx, y = np.meshgrid(xl, yl)\n\nxdot = 3*x -18*y\nydot = 2*x -9*y\n\ny_1 = 1/3 *xl\n\nfig, ax = plt.subplots(figsize=(5, 5))\n\nax.streamplot(x, y, xdot, ydot, color='blue')\nax.plot(xl, y_1, 'r:')\nax.set_title('Phase Portrait')\nax.axis((-w, w, -w, w))\nax.set_xlabel('x')\nax.set_ylabel('y')\nax.grid()\nax.tick_params(axis='both', direction='in')\n\n\n\n\n\n\n\nFigure 7.2: Phase Portrait of Defective System\n\n\n\n\n\n\\(~\\)\nExample \\(\\,\\) The \\(5 \\times 5\\) matrix\n\\[\n\\mathbf{A}=\n\\begin{pmatrix}\n2 & 1 & 0 & 0 & 0\\\\\n0 & 2 & 0 & 0 & 0\\\\\n0 & 0 & 2 & 0 & 0\\\\\n0 & 0 & 0 & 2 & 1\\\\\n0 & 0 & 0 & 0 & 2\n\\end{pmatrix}\n\\]\nhas an eigenvalue \\(\\lambda_1=2\\) of multiplicity 5. Show that three linearly independent eigenvectors corresponding to \\(\\lambda_1\\) can be found. \\(\\,\\)And construct the generalized eigenvectors\n\nThe system\n\\[\\scriptsize(\\mathbf{A} -2\\mathbf{I}|\\mathbf{0}) =\n\\left(\\begin{array}{rrrrr|r}\n   0 & 1 & 0 & 0 & 0 & 0\\\\\n   0 & 0 & 0 & 0 & 0 & 0\\\\\n   0 & 0 & 0 & 0 & 0 & 0\\\\\n   0 & 0 & 0 & 0 & 1 & 0\\\\\n   0 & 0 & 0 & 0 & 0 & 0\n\\end{array}\\right)\n\\]\nimplies \\(k_2=0\\) and \\(k_5=0\\). Then we can pick \\((k_1 =1, k_3=k_4=0)\\), \\((k_3=1, k_1=k_4=0)\\) and \\((k_4=1, k_1=k_3=0)\\) in turn to build three linearly eigenvectors\n\\[\\scriptsize\\mathbf{k}_{11}=\n   \\left(\\begin{array}{r}\n     1\\\\ 0\\\\ 0\\\\ 0\\\\ 0\n   \\end{array}\\right),\\;\\;\n   \\mathbf{k}_{21}=\n   \\left(\\begin{array}{r}\n     0\\\\ 0\\\\ 1\\\\ 0\\\\ 0\n   \\end{array}\\right),\\;\\;\n   \\mathbf{k}_{31}=\n   \\left(\\begin{array}{r}\n     0\\\\ 0\\\\ 0\\\\ 1\\\\ 0\n   \\end{array}\\right)     \n\\]\nThe following systems\n\\[\\scriptsize(\\mathbf{A} -2\\mathbf{I}|\\mathbf{k}_{11}) =\n\\left(\\begin{array}{rrrrr|r}\n   0 & 1 & 0 & 0 & 0 & 1\\\\\n   0 & 0 & 0 & 0 & 0 & 0\\\\\n   0 & 0 & 0 & 0 & 0 & 0\\\\\n   0 & 0 & 0 & 0 & 1 & 0\\\\\n   0 & 0 & 0 & 0 & 0 & 0\n\\end{array}\\right),\\;\\;\n(\\mathbf{A} -2\\mathbf{I}|\\mathbf{k}_{31}) =\n\\left(\\begin{array}{rrrrr|r}\n   0 & 1 & 0 & 0 & 0 & 0\\\\\n   0 & 0 & 0 & 0 & 0 & 0\\\\\n   0 & 0 & 0 & 0 & 0 & 0\\\\\n   0 & 0 & 0 & 0 & 1 & 1\\\\\n   0 & 0 & 0 & 0 & 0 & 0\n\\end{array}\\right)\n\\]\nare solved to construct two generalized eigenvectors: The first system gives \\(k_2=1\\) and \\(k_5=0\\), and then, for \\(\\mathbf{k}_{12}\\), we can pick \\(k_1=k_3=k_4=0\\). \\(\\,\\)The second system yields \\(k_2=0\\) and \\(k_5=1\\), \\(\\,\\)we can also pick \\(k_1=k_3=k_4=0\\) for \\(\\mathbf{k}_{32}\\)\n\\[\\scriptsize \\mathbf{k}_{12}=\n   \\left(\\begin{array}{r}\n     0\\\\ 1\\\\ 0\\\\ 0\\\\ 0\n   \\end{array}\\right),\\;\\;\n   \\mathbf{k}_{32}=\n   \\left(\\begin{array}{r}\n     0\\\\ 0\\\\ 0\\\\ 0\\\\ 1\n   \\end{array}\\right)\\]\n\n\\(~\\)\n\nComplex Eigenvalues\nLet \\(\\mathbf{A}\\) be the coefficient matrix having real entries of the homogeneous system, and let \\(\\mathbf{k}_1\\) be an eigenvector corresponding to the complex eigenvalue \\(\\lambda_1=\\alpha +i\\beta\\), \\(~\\alpha\\) and \\(\\beta\\) real. \\(\\,\\)Then\n\\[\\mathbf{k}_1 e^{\\lambda_1 t}\\;\\text{ and }\\;\\bar{\\mathbf{k}}_1 e^{\\bar{\\lambda}_1 t}\\]\nare linearly independent solutions of the homogeneous system. \\(\\,\\)By defining\n\\[\\mathbf{b}_1=\\mathrm{Re}(\\mathbf{k}_1)\\;\\text{ and\n}\\; \\mathbf{b}_2=\\mathrm{Im}(\\mathbf{k}_1)\\]\nthe following vectors are also linearly independent solutions\n\\[\n\\begin{aligned}\n  \\mathbf{x}_1 &=\\left[\\mathbf{b}_1 \\cos\\beta t -\n  \\mathbf{b}_2 \\sin\\beta t \\right]e^{\\alpha t} \\\\\n  \\mathbf{x}_2 &=\\left[\\mathbf{b}_2 \\cos\\beta t\n   +\\mathbf{b}_1 \\sin\\beta t \\right]e^{\\alpha t}\n\\end{aligned}\\]\n\n\\(~\\)\nExample \\(\\,\\) Solve the initial value problem\n\\[\n\\left.\\begin{array}{rl}\n    &\\displaystyle \\frac{dx}{dt} = 2x +8y\\\\\n    &\\displaystyle \\frac{dy}{dt} = x -2y\n  \\end{array}\\right.,\\quad\n     x(0)=2, \\;\\; y(0)=-1\\]\n\nFirst we obtain the eigenvalues from\n\\[\\mathrm{det}(\\mathbf{A} -\\lambda\\mathbf{I})=\n      \\left|\n      \\begin{array}{cc}\n         2-\\lambda & 8 \\\\\n        -1 & -2 -\\lambda\n      \\end{array}\n      \\right|\n     =\\lambda^2 +4=0\\]\nThe eigenvalues are \\(\\lambda_1=2i\\) and \\(\\lambda_2=\\bar{\\lambda}_1\\)\n\n\\(~\\)\n\nA = np.array([[2, 8], [-1, -2]])\n\nA_ = sympy.Matrix(A)\nA_.eigenvects()\n\n\\(\\displaystyle \\left[ \\left( - 2 i, \\  1, \\  \\left[ \\left[\\begin{matrix}-2 + 2 i\\\\1\\end{matrix}\\right]\\right]\\right), \\  \\left( 2 i, \\  1, \\  \\left[ \\left[\\begin{matrix}-2 - 2 i\\\\1\\end{matrix}\\right]\\right]\\right)\\right]\\)\n\n\n\\(~\\)\n\nFor \\(\\lambda_1\\), \\(\\,\\)the system\n\\[\n  (\\mathbf{A} -2i\\mathbf{I}|\\mathbf{0}) =\n    \\left(\\begin{array}{cc|r}\n         2 -2i & 8 & 0\\\\\n         -1 & -2 -2i & 0\n    \\end{array}\\right)\n    \\overset{\\text{row operations}}{\\Longrightarrow}\n    \\left(\\begin{array}{cc|r}\n          1 & 2 +2i & 0\\\\ \\hline\n          0 & 0 & 0\n    \\end{array}\\right)  \n  \\]\ngives \\(k_1=-(2 +2i)k_2\\). \\(\\,\\)By choosing \\(k_2=-1\\), \\(\\,\\)we get\n\\[\\mathbf{k}_1=\\begin{pmatrix} 2 +2i\\\\ -1 \\end{pmatrix}\n    =\\underbrace{\\begin{pmatrix} \\;\\;\\,2\\\\ -1 \\end{pmatrix}}_{\\mathbf{b}_1}\n   +i\\underbrace{\\begin{pmatrix} 2\\\\ 0 \\end{pmatrix}}_{\\mathbf{b}_2}\\]\nNow we form the general solution\n\\[\\mathbf{x}=\nc_1\\left[\\begin{pmatrix} \\;\\;\\,2\\\\ -1 \\end{pmatrix}\\cos 2t\n        -\\begin{pmatrix} 2\\\\ 0 \\end{pmatrix}\\sin 2t \\right]\n+c_2\\left[\\begin{pmatrix} 2\\\\ 0 \\end{pmatrix}\\cos 2t\n        +\\begin{pmatrix} \\;\\;\\,2\\\\ -1 \\end{pmatrix}\\sin 2t \\right]\\]\nThe initial condition yields \\(2c_1 +2c_2=2\\), \\(-c_1=-1\\) \\(\\,\\)whose solution is \\(c_1=1\\), \\(c_2=0\\). \\(\\,\\)Thus solution is\n\\[\\mathbf{x}=\n    \\begin{pmatrix} \\;\\;\\,2\\\\ -1 \\end{pmatrix}\\cos 2t\n         -\\begin{pmatrix} 2\\\\ 0 \\end{pmatrix}\\sin 2t \\]\n\n\\(~\\)\n\nfrom scipy.integrate import solve_ivp\n\ndef myODE(t, y):\n    return [2*y[0] +8*y[1], -y[0] -2*y[1]]\n\ntf = 20.0\n\nsol = solve_ivp(myODE, [0, tf], [2, -1], \n                t_eval = np.linspace(0, tf, 200))\n\nfig, ax = plt.subplots(figsize=(6, 4))\n\nax.plot(sol.t, sol.y[0], 'r-', label = 'x')\nax.plot(sol.t, sol.y[1], 'b-', label = 'y')\nax.axis((0, tf, -4, 4))\nax.tick_params(axis='both', direction='in', pad=8)\nax.legend()\nax.set_xlabel('t')\nax.set_ylabel('x, y');\n\n\n\n\n\n\n\nFigure 7.3: Time Series of System with Complex Eigenvalues\n\n\n\n\n\n\nw = 6\n\nxl = np.arange(-w, w, 0.1)\nyl = np.arange(-w, w, 0.1)\nx, y = np.meshgrid(xl, yl)\n\nxdot = 2*x +8*y\nydot = -x -2*y\n\nfig, ax = plt.subplots(figsize=(5, 5))\n\nax.streamplot(x, y, xdot, ydot, color='blue')\nax.plot(sol.y[0], sol.y[1], 'r:')\n\nax.set_title('Phase Portrait')\nax.axis((-w, w, -w, w))\nax.set_xlabel('x')\nax.set_ylabel('y')\nax.grid()\nax.tick_params(axis='both', direction='in')\n\n\n\n\n\n\n\nFigure 7.4: Phase Portrait of System with Complex Eigenvalues",
    "crumbs": [
      "**First Semester**",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Systems of Linear Differential Equations</span>"
    ]
  },
  {
    "objectID": "ch_10_Systems_of_Linear_Differential_Equations.html#sec-10-3",
    "href": "ch_10_Systems_of_Linear_Differential_Equations.html#sec-10-3",
    "title": "7  Systems of Linear Differential Equations",
    "section": "7.3 Solution by Diagonalization",
    "text": "7.3 Solution by Diagonalization\n\nA homogeneous system\n\\[\n\\begin{pmatrix} \\dot{x}_1 \\\\ \\dot{x}_2 \\\\ \\vdots \\\\ \\dot{x}_n\n\\end{pmatrix} =\n  \\begin{pmatrix}\n   a_{11} & a_{12} & \\cdots & a_{1n}\\\\\n   a_{21} & a_{22} & \\cdots & a_{2n}\\\\\n   \\vdots &        &        & \\vdots\\\\\n   a_{n1} & a_{n2} & \\cdots & a_{nn}\n  \\end{pmatrix}\n  \\begin{pmatrix} x_1 \\\\ x_2 \\\\ \\vdots \\\\ x_n\n\\end{pmatrix}\\]\nis said to be coupled if each \\(\\dot{x}_i\\) is expressed as a linear combination of \\(x_1, x_2, \\cdots, x_n\\). \\(\\,\\)If \\(\\mathbf{A}\\) is diagonalizable, \\(\\,\\)the system can be decoupled so each \\(\\dot{x}_i\\) is expressed solely in terms of \\(x_i\\)\nIf \\(\\mathbf{A}\\) has \\(n\\) linearly independent eigenvectors, \\(\\,\\)we can find a matrix \\(\\mathbf{P}\\) such that \\(\\mathbf{P}^{-1}\\mathbf{A}\\mathbf{P}=\\mathbf{D}\\text{ }\\) where \\(\\text{ }\\mathbf{D}\\) is a diagonal matrix\nMaking the substitution \\(\\mathbf{x}=\\mathbf{P}\\mathbf{y}\\) in \\(\\mathbf{\\dot{x}}=\\mathbf{A}\\mathbf{x}\\),\n\\[\n\\begin{aligned}\n   \\mathbf{\\dot{x}} &= \\mathbf{A}\\mathbf{x}\\\\\n       &\\Downarrow \\;\n    \\mathbf{x}=\\mathbf{P}\\mathbf{y}\\\\\n    \\mathbf{P}\\mathbf{\\dot{y}} &=\n    \\mathbf{A}\\mathbf{P}\\mathbf{y}\\\\\n    \\mathbf{\\dot{y}} &=\n    \\mathbf{P}^{-1}\\mathbf{A}\\mathbf{P}\\mathbf{y}=\n    \\mathbf{D}\\mathbf{y}\n\\end{aligned}\\]\nThe last equation is the same as\n\\[\n  \\begin{pmatrix} \\dot{y}_1 \\\\ \\dot{y}_2 \\\\ \\vdots \\\\ \\dot{y}_n \\end{pmatrix}=\n  \\begin{pmatrix}\n   \\lambda_1 & 0 & \\cdots & 0\\\\\n   0 & \\lambda_2 & \\cdots & 0\\\\\n   \\vdots & & \\ddots & \\vdots\\\\\n   0 & 0 & \\cdots & \\lambda_n\n  \\end{pmatrix}\n  \\begin{pmatrix} y_1 \\\\ y_2 \\\\ \\vdots \\\\ y_n \\end{pmatrix}\n\\]\nThe general solution of \\(\\mathbf{\\dot{y}}=\\mathbf{D}\\mathbf{y}\\) can be written as\n\\[\\mathbf{y}=\\begin{pmatrix} c_1 e^{\\lambda_1 t} \\\\ c_2 e^{\\lambda_2 t} \\\\\n   \\vdots \\\\ c_n e^{\\lambda_n t} \\end{pmatrix}\\]\nThen the general solution of \\(\\mathbf{\\dot{x}} =\\mathbf{A}\\mathbf{x}\\) is obtained from \\(\\mathbf{x}=\\mathbf{P}\\mathbf{y}\\)\n\n\\(~\\)\nExample \\(\\,\\) Solve\n\\[\n\\mathbf{\\dot{x}} =\n  \\left(\\begin{array}{rrr}\n   -2 & -1 & 8 \\\\\n    0 & -3 & 8 \\\\\n    0 & -4 & 9\n  \\end{array}\\right)\\mathbf{x}\\]\n\nWe begin by finding the eigenvalues and corresponding eigenvectors of the coefficient matrix. \\(\\,\\)From \\(\\mathrm{det}(\\mathbf{A} -\\lambda\\mathbf{I})=0\\), \\(\\,\\)we get \\(\\lambda_1=-2\\), \\(\\lambda_2=1\\), and \\(\\lambda_3=5\\). \\(\\,\\)Since the eigenvalues are distinct, the eigenvectors are linearly independent. Solving \\((\\mathbf{A} -\\lambda_i\\mathbf{I})=\\mathbf{0}\\) for \\(i=1,2,\\) and \\(3\\) gives, respectively,\n\\[\n  \\mathbf{k}_1=\n  \\left(\\begin{array}{r}\n    1\\\\\n    0\\\\\n    0\n  \\end{array}\\right),\\;\\;\n  \\mathbf{k}_2=\n  \\left(\\begin{array}{r}\n    2\\\\\n    2\\\\\n    1\n  \\end{array}\\right),\\;\\;\n  \\mathbf{k}_3=\n  \\left(\\begin{array}{r}\n    1\\\\\n    1\\\\\n    1\n  \\end{array}\\right)    \n\\]\nThus a matrix that diagonalize \\(\\mathbf{A}\\) is\n\\[\n  \\mathbf{P}=\n  \\left(\\begin{array}{rrr}\n    1 & 2 & 1 \\\\\n    0 & 2 & 1 \\\\\n    0 & 1 & 1\n  \\end{array}\\right)\\]\nThe substitution \\(\\mathbf{x}=\\mathbf{P}\\mathbf{y}\\) in \\(\\mathbf{\\dot{x}}=\\mathbf{A}\\mathbf{x}\\) gives the decoupled system \\(\\mathbf{\\dot{y}}=\\mathbf{D}\\mathbf{y}\\). The entries on the main diagonal of \\(\\mathbf{D}\\) are the eigenvalues of \\(\\mathbf{A}\\) corresponding to the order in which the eigenvectors appear in \\(\\mathbf{P}\\). The general solution of the decoupled system is\n\\[\n  \\mathbf{y}=\n  \\left(\\begin{array}{l}\n    c_1 e^{-2t}\\\\\n    c_2 e^{t}\\\\\n    c_3 e^{5t}\n  \\end{array}\\right)\\]\nHence the solution of the given system is\n\\[\n\\begin{aligned}\n  \\mathbf{x}=\\mathbf{P}\\mathbf{y}&=\n  \\left(\\begin{array}{rrr}\n    1 & 2 & 1 \\\\\n    0 & 2 & 1 \\\\\n    0 & 1 & 1\n  \\end{array}\\right)\n  \\left(\\begin{array}{l}\n    c_1 e^{-2t}\\\\\n    c_2 e^{t}\\\\\n    c_3 e^{5t}\n  \\end{array}\\right) \\\\&=\n  c_1 \\left(\\begin{array}{l} 1\\\\ 0\\\\ 0 \\end{array}\\right) e^{-2t}+\n  c_2 \\left(\\begin{array}{l} 2\\\\ 2\\\\ 1 \\end{array}\\right) e^{t}+\n  c_3 \\left(\\begin{array}{l} 1\\\\ 1\\\\ 1 \\end{array}\\right) e^{5t}\n\\end{aligned}\\]\n\n\\(~\\)",
    "crumbs": [
      "**First Semester**",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Systems of Linear Differential Equations</span>"
    ]
  },
  {
    "objectID": "ch_10_Systems_of_Linear_Differential_Equations.html#sec-10-4",
    "href": "ch_10_Systems_of_Linear_Differential_Equations.html#sec-10-4",
    "title": "7  Systems of Linear Differential Equations",
    "section": "7.4 Nonhomogeneous Linear Systems",
    "text": "7.4 Nonhomogeneous Linear Systems\n\nThe general solution of a nonhomogeneous linear system \\(\\mathbf{\\dot{x}}=\\mathbf{A}\\mathbf{x} +\\mathbf{f}(t)\\) is \\(\\mathbf{x}=\\mathbf{x}_h +\\mathbf{x}_p\\) where \\(\\mathbf{x}_h\\) is the complementary function of the associated homogeneous linear system and \\(\\mathbf{x}_p\\) is any particular solution of the nonhomogeneous system\n\n\n7.4.1 Method of undetermined coefficients\n\nThe method of undetermined coefficients consists of making an educational guess about the form of a particular solution vector \\(\\mathbf{x}_p\\). The matrix version of undetermined coefficients is only applicable when the entries of \\(\\mathbf{A}\\) are constants and the entires of \\(\\mathbf{f}(t)\\) are constants, polynomials, exponential functions, sines and cosines, or finite sums and products of these functions\n\n\\(~\\)\nExample \\(\\,\\) Solve\n\\[\n   \\mathbf{\\dot{x}} =\n   \\left(\\begin{array}{rr}\n     -1 & 2 \\\\\n     -1 & 1\n   \\end{array}\\right)\\mathbf{x}+\n   \\left(\\begin{array}{r}\n     -8 \\\\ 3\n   \\end{array}\\right)\n\\]\n\nWe first solve the associated homogeneous system\n\\[\n\\mathbf{\\dot{x}} =\n\\left(\\begin{array}{rr}\n   -1 & 2 \\\\\n   -1 & 1\n\\end{array}\\right)\\mathbf{x}\n\\]\nThe characteristic equation of \\(\\mathbf{A}\\)\n\\[\n\\mathrm{det}(\\mathbf{A} -\\lambda\\mathbf{I})=\n\\left|\\begin{array}{cc}\n   -1 -\\lambda & 2 \\\\\n   -1 & 1 -\\lambda\n\\end{array}\\right|=\\lambda^2+1=0   \n\\]\nyields the complex eigenvalues \\(\\lambda_1=i\\) and \\(\\lambda_2=\\bar{\\lambda}_1=-i\\)\nFor \\(\\lambda_1=i\\),\n\\[\n(\\mathbf{A} -i\\mathbf{I}|\\mathbf{0})=\n    \\left(\\begin{array}{cc|r}\n         -1 -i & 2 & 0\\\\\n         -1 & 1 -i & 0\n    \\end{array}\\right)\n    \\overset{\\text{row operations}}{\\Longrightarrow}\n    \\left(\\begin{array}{cc|r}\n          1 & -1 +i & 0\\\\ \\hline\n          0 & 0 & 0\n    \\end{array}\\right)     \n\\]\ngives an eigenvector\n\\[\n\\mathbf{k}_1 =\n\\left(\\begin{array}{c}\n   1 -i \\\\ 1\n\\end{array}\\right)\n\\]\nNow we build the general solution of the associated homogeneous system\n\\[\\mathbf{x}_h=\n  c_1 \\left[ \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix} \\cos t\n            -\\begin{pmatrix}-1 \\\\\\;\\; 0 \\end{pmatrix} \\sin t \\right]\n+c_2 \\left[ \\begin{pmatrix}-1 \\\\\\;\\; 0 \\end{pmatrix} \\cos t\n            +\\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix} \\sin t \\right]\n\\]\nSince \\(\\mathbf{f}\\) is a constant vector, \\(\\,\\)we assume a constant particular solution vector\n\\[\\mathbf{x}_p=\\begin{pmatrix} a_1 \\\\ b_1 \\end{pmatrix}\\]\n\\[\n\\mathbf{0} =\n\\left(\\begin{array}{rr}\n   -1 & 2 \\\\\n   -1 & 1\n\\end{array}\\right)\\begin{pmatrix} a_1 \\\\ b_1 \\end{pmatrix}+\n\\left(\\begin{array}{r}\n   -8 \\\\ 3\n\\end{array}\\right)\n\\]\nSolving this algebraic system gives\n\\[\\mathbf{x}_p=\\begin{pmatrix} 14 \\\\ 11 \\end{pmatrix}\\]\n\n\\(~\\)\nExample \\(\\,\\) Solve\n\\[\n  \\mathbf{\\dot{x}} =\n  \\left(\\begin{array}{rr}\n    6 & 1 \\\\\n    4 & 3\n  \\end{array}\\right)\\mathbf{x}+\n  \\left(\\begin{array}{c}\n    6t \\\\ -10t +4\n  \\end{array}\\right)\n\\]\n\nThe eigenvalues and corresponding eigenvectors of the associated homogeneous system are found to be \\(\\lambda_1=2\\), \\(\\lambda_2=7\\),\n\\[\\mathbf{k}_1=\\begin{pmatrix} \\phantom{-}1 \\\\ -4 \\end{pmatrix} \\;\\text{ and } \\;\\mathbf{k}_2=\\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix}\\]\nHence the complementary function is\n\\[\\mathbf{x}_h=\n  c_1 \\begin{pmatrix} \\phantom{-}1 \\\\ -4 \\end{pmatrix} e^{2t}\n+c_2 \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix} e^{7t}\\]\nNow we shall try to find the particular solution of the form\n\\[\\mathbf{x}_p=\n   \\begin{pmatrix} a_2 \\\\ b_2 \\end{pmatrix} t\n  +\\begin{pmatrix} a_1 \\\\ b_1 \\end{pmatrix}\n\\]\nSubstituting this assumption into the given system and solving the resultant algebraic equations yields\n\\[\\mathbf{x}_p=\n   \\begin{pmatrix} -2 \\\\ \\phantom{-}6 \\end{pmatrix} t\n  +\\frac{1}{7}\\begin{pmatrix} -4 \\\\ \\;10 \\end{pmatrix}\n\\]\n\n\\(~\\)\nExample \\(\\,\\) Determine the form of a particular solution vector \\(\\mathbf{x}_p\\), \\(\\,\\)for the system\n\\[\n   \\mathbf{\\dot{x}} =\n   \\left(\\begin{array}{rr}\n     5 & 3 \\\\\n    -1 & 1\n   \\end{array}\\right)\\mathbf{x}+\n   \\left(\\begin{array}{c}\n     -2e^{-t} +1 \\\\ e^{-t} -5t +7\n   \\end{array}\\right)\\]\n\nBecause \\(\\mathbf{f}\\) can be written in matrix terms as\n\\[\\mathbf{f}=\n    \\begin{pmatrix} -2 \\\\ \\phantom{-}1 \\end{pmatrix} e^{-t}\n    +\\begin{pmatrix} \\phantom{-}0 \\\\ -5 \\end{pmatrix} t\n     +\\begin{pmatrix} 1 \\\\ 7 \\end{pmatrix}\\]\na natural assumption for a particular solution would be\n\\[\\mathbf{x}_p=\n    \\begin{pmatrix} a_3 \\\\ b_3 \\end{pmatrix} e^{-t}\n    +\\begin{pmatrix} a_2 \\\\ b_2 \\end{pmatrix} t\n     +\\begin{pmatrix} a_1 \\\\ b_1 \\end{pmatrix}\\]\n\n\\(~\\)\n\nThe method of undetermined coefficients for linear systems is not as straightforward as the last three examples would seem to indicate. But there are further difficulties\nFor example, in \\(2 \\times 2\\) system, if \\(\\mathbf{f}(t)\\) is a constant vector and \\(\\lambda=0\\) \\(\\,\\)is an eigenvalue of multiplicity one, \\(\\,\\)then \\(\\mathbf{x}_p\\) contains a constant vector. \\(\\,\\)We would ordinarily try a particular solution of the form\n\\[\\mathbf{x}_p=\n    \\begin{pmatrix} a_1 \\\\ b_1 \\end{pmatrix} t\\]\nThis is not the proper assumption for linear systems; it should be\n\\[\\mathbf{x}_p=\n  \\begin{pmatrix} a_2 \\\\ b_2 \\end{pmatrix} t\n   +\\begin{pmatrix} a_1 \\\\ b_1 \\end{pmatrix}\\]\nSimilarly, in the last Example, if we replace \\(e^{-t}\\) in \\(\\mathbf{f}(t)\\) by \\(e^{2t}\\) (\\(\\lambda=2\\) is an eigenvalue), \\(\\,\\)then the correct form of the particular solution vector is\n\\[\\mathbf{x}_p=\n   \\begin{pmatrix} a_4 \\\\ b_4 \\end{pmatrix} te^{2t}\n    +\\begin{pmatrix} a_3 \\\\ b_3 \\end{pmatrix} e^{2t}\n    +\\begin{pmatrix} a_2 \\\\ b_2 \\end{pmatrix} t\n     +\\begin{pmatrix} a_1 \\\\ b_1 \\end{pmatrix}\\]\n\n\\(~\\)\n\n\n7.4.2 Method of Variation of Parameters\n\nA Fundamental Matrix\nIf \\(\\mathbf{x}_1\\), \\(\\mathbf{x}_2\\), \\(\\cdots\\), \\(\\mathbf{x}_n\\) is a fundamental set of solutions of the homogeneous system \\(\\mathbf{\\dot{x}}=\\mathbf{A}\\mathbf{x}\\), then its general solution is the linear combination \\(\\mathbf{x}=c_1\\mathbf{x}_1 +c_2\\mathbf{x}_2 +\\cdots +c_n\\mathbf{x}_n\\) or\n\\[\\scriptsize\n\\mathbf{x}=\n  c_1\\begin{pmatrix} x_{11} \\\\ x_{21} \\\\ \\vdots \\\\ x_{n1} \\end{pmatrix}\n+c_2\\begin{pmatrix} x_{12} \\\\ x_{22} \\\\ \\vdots \\\\ x_{n2} \\end{pmatrix}\n+\\cdots\n+c_n\\begin{pmatrix} x_{1n} \\\\ x_{2n} \\\\ \\vdots \\\\ x_{nn} \\end{pmatrix}\n    =\\begin{pmatrix}\n       x_{11} & x_{12} & \\cdots & x_{1n}\\\\\n       x_{21} & x_{22} & \\cdots & x_{2n}\\\\\n       \\vdots &        & \\ddots & \\vdots\\\\\n       x_{n1} & x_{n2} & \\cdots & x_{nn}\\\\\n     \\end{pmatrix}\n     \\begin{pmatrix} c_1 \\\\ c_2 \\\\ \\vdots \\\\ c_n \\end{pmatrix}\\]\nIn other words, the general solution can be written as the product\n\\[\\mathbf{x}=\\boldsymbol{\\Phi}(t)\\mathbf{c}\\]\nwhere \\(\\mathbf{c}\\) is the column vector of constants and \\(\\boldsymbol{\\Phi}\\) is the fundamental matrix\nWe need to know two properties of a fundamental matrix:\n\nA fundamental matrix \\(\\boldsymbol{\\Phi}(t)\\) is nonsingular\nIf \\(\\boldsymbol{\\Phi}(t)\\) is a fundamental matrix of \\(\\mathbf{\\dot{x}}=\\mathbf{A}\\mathbf{x}\\), then\n\\[\\boldsymbol{\\dot{\\Phi}}(t)=\\mathbf{A}\\boldsymbol{\\Phi}(t)\\]\n\nVariation of Parameters\nWe ask whether it is possible to replace \\(\\mathbf{c}\\) by\n\\[\\mathbf{u}(t)= \\begin{pmatrix} u_1(t) \\\\ u_2(t) \\\\ \\vdots \\\\ u_n(t) \\end{pmatrix}\\;\\text{ so that }\\;\\mathbf{x}_p=\\boldsymbol{\\Phi}(t)\\mathbf{u}(t)\\]\nis a particular solution of \\(\\,\\mathbf{\\dot{x}}=\\mathbf{A}\\mathbf{x} +\\mathbf{f}(t)\\)\n\\[\n\\begin{aligned}\n  \\mathbf{\\dot{x}}&=\\mathbf{A}\\mathbf{x} +\\mathbf{f}(t)\\\\\n     &\\;\\Big\\Downarrow \\;\\;\\mathbf{x}_p=\\boldsymbol{\\Phi}(t)\\mathbf{u}(t)\\\\\n  \\boldsymbol{\\dot{\\Phi}}(t)\\mathbf{u}(t)\n     +\\boldsymbol{\\Phi}(t)\\mathbf{\\dot{u}}(t)&=\\mathbf{A}\\boldsymbol{\\Phi}(t)\\mathbf{u}(t) +\\mathbf{f}(t)\\\\\n     &\\;\\Big\\Downarrow \\;\\;\\boldsymbol{\\dot{\\Phi}}(t)=\\mathbf{A}\\boldsymbol{\\Phi}(t)\\\\\n  \\boldsymbol{\\Phi}(t)\\mathbf{\\dot{u}}(t)&=\\mathbf{f}(t)\\\\\n     &\\Downarrow \\\\\n  \\mathbf{u}(t)&=\\int\\boldsymbol{\\Phi}^{-1}(t)\\mathbf{f}(t)\\;dt +\\mathbf{c}_0\\\\\n     &\\Downarrow \\\\\n  \\mathbf{x}_p&=\\boldsymbol{\\Phi}(t)\\int\\boldsymbol{\\Phi}^{-1}(t)\\mathbf{f}(t)\\;dt\n    +\\boldsymbol{\\Phi}(t) \\mathbf{c}_0\n\\end{aligned}\\]\nThus the general solution is\n\\[\n\\mathbf{x}=\\boldsymbol{\\Phi}(t)\\bar{\\mathbf{c}}+\\boldsymbol{\\Phi}(t)\n   \\int\\boldsymbol{\\Phi}^{-1}(t)\\mathbf{f}(t)\\;dt\n\\]\nwhere \\(\\;\\bar{\\mathbf{c}}=\\mathbf{c} +\\mathbf{c}_0\\)\nIf we set \\(\\mathbf{x}_0=\\boldsymbol{\\Phi}(t_0)\\bar{\\mathbf{c}}\\,\\) with an initial condition \\(\\mathbf{x}(t_0)=\\mathbf{x}_0\\), \\(\\,\\)the solution of the initial value problem is\n\\[\n\\mathbf{x}=\\boldsymbol{\\Phi}(t)\\boldsymbol{\\Phi}^{-1}(t_0)\\mathbf{x}_0\n   +\\boldsymbol{\\Phi}(t)\\int_{t_0}^t\\boldsymbol{\\Phi}^{-1}(s)\\mathbf{f}(s)\\;ds\n\\]\n\n\\(~\\)\nExample \\(\\,\\) Find the general solution of the nonhomogeneous system\n\\[\n   \\mathbf{\\dot{x}} =\n   \\left(\\begin{array}{rr}\n    -3 & 1 \\\\\n     2 &-4\n   \\end{array}\\right)\\mathbf{x}+\n   \\left(\\begin{array}{c}\n     3t \\\\ e^{-t}\n   \\end{array}\\right)\\]\n\nThe solution vectors of the homogeneous system are then\n\\[\n\\mathbf{x}_1 =\n\\left(\\begin{array}{c} e^{-2t} \\\\ e^{-2t} \\end{array}\\right)\\;\\text{and}\\;\n\\mathbf{x}_2 =\n\\left(\\begin{array}{c} \\;\\;\\;\\;e^{-5t} \\\\ -2e^{-5t} \\end{array}\\right)\\]\nHence\n\\[\n\\boldsymbol{\\Phi}(t) =\n\\left(\\begin{array}{rr}\n   e^{-2t} & \\;\\;\\;\\;e^{-5t} \\\\\n   e^{-2t} & -2e^{-5t}\n\\end{array}\\right)\\;\\text{and}\\;\n\\boldsymbol{\\Phi}^{-1}(t) =\\frac{1}{3}\n\\left(\\begin{array}{rr}\n   2e^{2t} & \\;\\;\\;\\;e^{2t} \\\\\n   e^{5t} & -e^{5t}\n\\end{array}\\right)\\]\nThen we obtain\n\\[\n\\begin{aligned}\n     \\mathbf{x}_p &=\\boldsymbol{\\Phi}(t)\\int\\boldsymbol{\\Phi}^{-1}(t)\\mathbf{f}(t)\\;dt\\\\\n     &= \\left(\\begin{array}{rr}\n          e^{-2t} & \\;\\;\\;\\;e^{-5t}\\\\\n          e^{-2t} & -2e^{-5t}\n       \\end{array}\\right)\n     {\\Large\\int} \\frac{1}{3}\n       \\left(\\begin{array}{rr}\n          2e^{2t} & \\;\\;\\;\\;e^{2t} \\\\\n          e^{5t} & -e^{5t}\n       \\end{array}\\right)\n       \\left(\\begin{array}{c}\n          3t \\\\ e^{-t}\n       \\end{array}\\right)\\,dt\\\\\n     &=\\left(\\begin{array}{c}\n          \\frac{6}{5}t -\\frac{27}{50} +\\frac{1}{4}e^{-t}\\\\\n          \\frac{3}{5}t -\\frac{21}{50} +\\frac{1}{2}e^{-t}\n       \\end{array}\\right)\n\\end{aligned}\\]\n\n\n\n7.4.3 Diagonalization\n\nSuppose \\(\\mathbf{P}\\) is the matrix such that \\(\\mathbf{P}^{-1}\\mathbf{A}\\mathbf{P}=\\mathbf{D}\\), where \\(\\mathbf{D}\\) is a diagonal matrix. Substituting \\(\\mathbf{x}=\\mathbf{P}\\mathbf{y}\\) into the nonhomogeneous system gives\n\\[\\begin{aligned}\n  \\mathbf{\\dot{x}} &=\\mathbf{A}\\mathbf{x} +\\mathbf{f}\\\\\n    &\\;\\big\\Downarrow \\;\\mathbf{x}=\\mathbf{P}\\mathbf{y} \\\\\n  \\mathbf{P}\\mathbf{\\dot{y}}&=\\mathbf{A}\\mathbf{P}\\mathbf{y} +\\mathbf{f}\\\\\n  \\mathbf{\\dot{y}} &= \\mathbf{P}^{-1}\\mathbf{A}\\mathbf{P}\\mathbf{y} +\\mathbf{P}^{-1}\\mathbf{f}\\\\\n  \\mathbf{\\dot{y}} &= \\mathbf{D}\\mathbf{y} +\\mathbf{P}^{-1}\\mathbf{f}\\\\\n    &\\;\\big\\Downarrow \\;\\mathbf{g}=\\mathbf{P}^{-1}\\mathbf{f}\\\\\n  \\mathbf{\\dot{y}} &= \\mathbf{D}\\mathbf{y} +\\mathbf{g}\n\\end{aligned}\\]\nIn the last equation, \\(\\mathbf{g}\\) ia a column vector. So each differential equation has the form of \\(\\dot{y}_i=\\lambda_i y_i +g_i(t)\\), \\(i=1,\\cdots,n\\). \\(\\,\\)Unlike the procedure for solving a homogeneous system, \\(\\,\\)we now are required to compute the inverse of \\(\\mathbf{P}\\)",
    "crumbs": [
      "**First Semester**",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Systems of Linear Differential Equations</span>"
    ]
  },
  {
    "objectID": "ch_10_Systems_of_Linear_Differential_Equations.html#sec-10-5",
    "href": "ch_10_Systems_of_Linear_Differential_Equations.html#sec-10-5",
    "title": "7  Systems of Linear Differential Equations",
    "section": "7.5 Matrix Exponential",
    "text": "7.5 Matrix Exponential\n\nWe can define a matrix exponential \\(e^{\\mathbf{A}t}\\) so that \\(\\mathbf{\\dot{x}} =\\mathbf{A}\\mathbf{x}\\) has a solution \\(\\mathbf{x}=e^{\\mathbf{A}t}\\mathbf{c}\\)\n\\[e^{\\mathbf{A}t}=\\mathbf{I} +\\mathbf{A}t +\\mathbf{A}^2\\frac{t^2}{2!}\n  +\\cdots +\\mathbf{A}^k\\frac{t^k}{k!} +\\cdots =\\sum_{k=0}^\\infty \\mathbf{A}^k \\frac{t^k}{k!}\\]\nThe derivative of \\(e^{\\mathbf{A}t}\\) is obtained by differentiating term by term\n\\[\n\\begin{aligned}\n\\frac{d}{dt}e^{\\mathbf{A}t}\n&=\\frac{d}{dt}\\left[\\mathbf{I} +\\mathbf{A}t +\\mathbf{A}^2\\frac{t^2}{2!}\n  +\\cdots +\\mathbf{A}^k\\frac{t^k}{k!} +\\cdots \\right]\\\\\n     &=\\mathbf{A} +\\mathbf{A}^2t +\\mathbf{A}^3\\frac{t^2}{2!}\n     +\\cdots +\\mathbf{A}^{k +1}\\frac{t^k}{k!} +\\cdots\\\\\n     &=\\mathbf{A}\\left[\\mathbf{I} +\\mathbf{A}t +\\mathbf{A}^2\\frac{t^2}{2!}\n     +\\cdots +\\mathbf{A}^k\\frac{t^k}{k!} +\\cdots\\right]=\\mathbf{A}e^{\\mathbf{A}t}\n\\end{aligned}\\]\nNow we can prove that \\(\\mathbf{x}=e^{\\mathbf{A}t}\\mathbf{c}\\) is the solution of \\(\\mathbf{\\dot{x}} =\\mathbf{A}\\mathbf{x}\\) for every constant vector \\(\\mathbf{c}\\)\n\\[\\mathbf{\\dot{x}} =\\frac{d}{dt}e^{\\mathbf{A}t}\\mathbf{c}=\\mathbf{A}e^{\\mathbf{A}t}\\mathbf{c}\n  =\\mathbf{A}\\left(e^{\\mathbf{A}t}\\mathbf{c}\\right)=\\mathbf{A}\\mathbf{x}\\]\nIf we denote \\(e^{\\mathbf{A}t}\\) by the symbol \\(\\boldsymbol{\\Psi}(t)\\), then \\(\\frac{d}{dt}e^{\\mathbf{A}t}=\\mathbf{A}e^{\\mathbf{A}t}\\) is equivalent to \\(\\boldsymbol{\\dot{\\Psi}}(t)=\\mathbf{A}\\boldsymbol{\\Psi}(t)\\). In addition, \\(\\boldsymbol{\\Psi}(0)=e^{\\mathbf{A}0}=\\mathbf{I}\\) and so \\(\\mathrm{det}\\,\\boldsymbol{\\Psi}(0)\\neq 0\\). These two properties are sufficient to conclude that \\(\\boldsymbol{\\Psi}(t)\\) is a fundamental matrix of \\(\\mathbf{\\dot{x}} =\\mathbf{A}\\mathbf{x}\\). Then it is always nonsingular and \\(\\left(e^{\\mathbf{A}t}\\right)^{-1}=e^{-\\mathbf{A}t}\\)\nFor a nonhomogeneous system of linear first-order DEs, the general solution is\n\\[\n\\mathbf{x}=\\mathbf{x}_h +\\mathbf{x}_p =e^{\\mathbf{A}t}\\mathbf{c}\n   +e^{\\mathbf{A}t}\\int_{t_0}^te^{-\\mathbf{A}s}\\mathbf{f}(s)\\;ds\n\\]\nComputation of \\(e^{\\mathbf{A}t}\\): \\(\\,\\) Using the Laplace Transform\n\\(\\mathbf{x}=e^{\\mathbf{A}t}\\) is a solution of the initial value problem\n\\[\\mathbf{\\dot{x}}=\\mathbf{A}\\mathbf{x}, \\;\\;\\mathbf{x}(0)=\\mathbf{I}\\]\nIf \\(\\mathbf{x}(s)=\\mathcal{L}\\{e^{\\mathbf{A}t}\\}\\), \\(\\,\\)then the Laplace transform of the above initial value problem is\n\\[\n\\begin{aligned}\n   s\\mathbf{x}(s) -\\mathbf{I}&=\\mathbf{A}\\mathbf{x}(s)\\\\\n   (s\\mathbf{I} -\\mathbf{A})\\mathbf{x}(s)&=\\mathbf{I}\\\\\n   \\mathbf{x}(s)&=(s\\mathbf{I} -\\mathbf{A})^{-1}\n\\end{aligned}\\]\nIn other words,\n\\[e^{\\mathbf{A}t}=\\mathcal{L}^{-1}\\{(s\\mathbf{I} -\\mathbf{A})^{-1}\\}\\]\nComputation of \\(e^{\\mathbf{A}t}\\):\\(\\text{ }\\) \\(\\,\\) Using Powers \\(\\mathbf{A}^m\\)\nFor computing an arbitrary power \\(\\mathbf{A}^k\\), \\(k\\) a nonnegative integer, \\(\\,\\)we can write\n\\[\n  \\mathbf{A}^k=\\sum_{j=0}^{n -1}c_j(k)\\mathbf{A}^j\\;\\text{and}\\;\n     \\lambda^k=\\sum_{j=0}^{n -1}c_j(k)\\lambda^j\\]\nwhere the last expression is valid for the eigenvalues \\(\\lambda_1\\), \\(\\lambda_2\\), \\(\\cdots\\), \\(\\lambda_n\\). \\(\\,\\)If the eigenvalues are distinct, by setting \\(\\lambda=\\lambda_1,\\lambda_2,\\cdots,\\lambda_n\\) in the second expression, \\(\\,\\)we were able to find \\(c_j(k)\\) in the first expression by solving \\(n\\) equations in \\(n\\) unknowns. From the definition of \\(e^{\\mathbf{A}t}\\), \\(\\,\\)we have\n\\[\n  e^{\\mathbf{A}t}=\\sum_{k=0}^\\infty \\mathbf{A}^k \\frac{t^k}{k!}\\;\\text{and}\\;\n  e^{\\lambda t}=\\sum_{k=0}^\\infty \\lambda^k \\frac{t^k}{k!}\n\\]\nWe next use to replace \\(\\mathbf{A}^k\\) and \\(\\lambda^k\\) as finite sums followed by an interchange of the order of summations\n\\[\n\\begin{aligned}\n   e^{\\mathbf{A}t}&=\\sum_{k=0}^\\infty \\frac{t^k}{k!}\\left(\\sum_{j=0}^{n -1}c_j(k)\\mathbf{A}^j\\right)\n     =\\sum_{j=0}^{n -1}\\mathbf{A}^j\\left(\\sum_{k=0}^\\infty \\frac{t^k}{k!}c_j(k)\\right)\n     =\\sum_{j=0}^{n -1}\\mathbf{A}^j b_j\\\\\n   e^{\\lambda t}&=\\sum_{k=0}^\\infty\\frac{t^k}{k!}\\left(\\sum_{j=0}^{n -1}c_j(k)\\lambda^j\\right)\n     =\\sum_{j=0}^{n -1}\\lambda^j\\left(\\sum_{k=0}^\\infty\\frac{t^k}{k!}c_j(k)\\right)\n     =\\sum_{j=0}^{n -1}\\lambda^j b_j\n\\end{aligned}\\]\nAnalogous to how we used the eigenvalues of \\(\\mathbf{A}\\) to determine the \\(c_j\\), \\(\\,\\)we again use the eigenvalues to solve a system of equations to determine the \\(b_j\\)\n\n\\(~\\)\nExample \\(\\,\\) Compute \\(e^{\\mathbf{A}t}\\) for\n\\[\\mathbf{A} =\n   \\left(\\begin{array}{rr}\n     1 & -1\\\\\n     2 & -2\n   \\end{array}\\right)\\]\n\\(~\\)\n\nFirst we compute the matrix \\(s\\mathbf{I} -\\mathbf{A}\\) and then find its inverse\n\\[s\\mathbf{I} -\\mathbf{A}=\n\\left(\\begin{array}{cc}\n   s -1 & 1\\\\\n   -2 & s +2\n\\end{array}\\right)\\]\n\\[(s\\mathbf{I} -\\mathbf{A})^{-1}=\n\\left(\\begin{array}{cc}\n   \\frac{s +2}{s(s +1)} & \\frac{-1}{s(s +1)}\\\\\n   \\frac{2}{s(s +1)} & \\frac{s -1}{s(s +1)}\n\\end{array}\\right)\\]\nThen we decompose the entries of the last matrix into partial fractions\n\\[(s\\mathbf{I} -\\mathbf{A})^{-1}=\n\\left(\\begin{array}{cc}\n   \\frac{2}{s} -\\frac{1}{s +1} & -\\frac{1}{s} +\\frac{1}{s +1}\\\\\n   \\frac{2}{s} -\\frac{2}{s +1} & -\\frac{1}{s} +\\frac{2}{s +1}\n\\end{array}\\right)\\]\nTaking the inverse Laplace transform gives the desired result\n\\[e^{\\mathbf{A}t} =\n\\left(\\begin{array}{cc}\n   2 -e^{-t} & -1 +e^{-t} \\\\\n   2 -2e^{-t} & -1 +2e^{-t}\n\\end{array}\\right)\\]\n\n\\(~\\)\n\nA = sympy.Matrix([[1, -1], [2, -2]])\ns = sympy.symbols('s')\nt = sympy.symbols('t', real=True, positive=True)\n\nlp_sol = (s*sympy.eye(2) -A).inv()\nsympy.inverse_laplace_transform(lp_sol, s, t)\n\n\\(\\displaystyle \\left[\\begin{matrix}2 - e^{- t} & -1 + e^{- t}\\\\2 - 2 e^{- t} & -1 + 2 e^{- t}\\end{matrix}\\right]\\)\n\n\n\\(~\\)\nExample \\(\\,\\) Compute \\(e^{\\mathbf{A}t}\\) for\n\\[\\mathbf{A} =\n   \\left(\\begin{array}{rr}\n     -2 & 4\\\\\n     -1 & 3\n   \\end{array}\\right)\\] \\(~\\)\n\nThe eigenvalues of \\(\\mathbf{A}\\) are \\(\\lambda_1=-1\\) and \\(\\lambda_2=2\\). Now since \\(\\mathbf{A}\\) is a \\(2 \\times 2\\) matrix, \\(\\,\\)we have\n\\[e^{\\mathbf{A}t}=b_0\\mathbf{I} +b_1\\mathbf{A}\\;\\text{ and }\\;e^{\\lambda t}=b_0 +b_1\\lambda\\]\nSetting \\(\\lambda=-1\\) and \\(\\lambda=2\\) in the second equation gives two equations in the two unknowns \\(b_0\\) and \\(b_1\\). \\(\\,\\)Solving the system yields\n\\[\\scriptsize b_0=\\frac{1}{3}\\left[e^{2t} +2e^{-t}\\right], \\; b_1=\\frac{1}{3}\\left[e^{2t} -e^{-t}\\right]\\]\nSubstituting these values in the first equation and simplifying the entries yields\n\\[e^{\\mathbf{A}t} =\n\\left(\\begin{array}{cc}\n   -\\frac{1}{3}e^{2t} +\\frac{4}{3}e^{-t} & \\frac{4}{3}e^{2t} -\\frac{4}{3}e^{-t} \\\\\n   -\\frac{1}{3}e^{2t} +\\frac{1}{3}e^{-t} & \\frac{4}{3}e^{2t} -\\frac{1}{3}e^{-t}\n\\end{array}\\right)\\]\n\n\\(~\\)\n\nA = sympy.Matrix([[-2, 4], [-1, 3]])\nt = sympy.symbols('t', real = True, positive = True)\nsympy.exp(A*t)\n\n\\(\\displaystyle \\left[\\begin{matrix}- \\frac{e^{2 t}}{3} + \\frac{4 e^{- t}}{3} & \\frac{4 e^{2 t}}{3} - \\frac{4 e^{- t}}{3}\\\\- \\frac{e^{2 t}}{3} + \\frac{e^{- t}}{3} & \\frac{4 e^{2 t}}{3} - \\frac{e^{- t}}{3}\\end{matrix}\\right]\\)\n\n\n\\(~\\)",
    "crumbs": [
      "**First Semester**",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Systems of Linear Differential Equations</span>"
    ]
  },
  {
    "objectID": "ch_10_Systems_of_Linear_Differential_Equations.html#worked-exercises",
    "href": "ch_10_Systems_of_Linear_Differential_Equations.html#worked-exercises",
    "title": "7  Systems of Linear Differential Equations",
    "section": "Worked Exercises",
    "text": "Worked Exercises\n1. \\(\\phantom{1}\\) Compute \\(e^{\\mathbf{A}t}\\) for the matrix\n\\[\\begin{pmatrix}\n-1 & 1 & 1 \\\\\n-1 & 0 &  1\\\\\n-1 & 1 &  1\\\\\n\\end{pmatrix}\\]\nand then use the matrix exponential to solve the system \\(\\mathbf{\\dot{x}}=\\mathbf{Ax}\\)\n\\(~\\)\nSolution A\nStep 1: Characteristic Polynomial\n\\[\\color{red}{\\det(\\mathbf{A} - \\lambda \\mathbf{I}) = -\\lambda^3=0}\\]\nSo the eigenvalue is \\(\\lambda=0\\) with algebraic multiplicity 3. This means \\(\\mathbf{A}\\) is nilpotent (some power of \\(\\mathbf{A}\\) equals zero), and not diagonalizable\nStep 2: Matrix Exponential\nSince \\(\\mathbf{A}\\) is nilpotent of index 3 (i.e., \\(\\mathbf{A}^3 = \\mathbf{0}\\)), the matrix exponential can be computed using the truncated power series:\n\\[\\color{red}{e^{\\mathbf{A}t} = \\mathbf{I} + \\mathbf{A}t + \\frac{1}{2!} \\mathbf{A}^2 t^2}\\]\nWe compute:\n\\[\\mathbf{A}^2 = \\begin{pmatrix}\n-1 & 0 & 1 \\\\\n\\phantom{-}0 & 0 & 0 \\\\\n-1 & 0 & 1\n\\end{pmatrix}\\]\nThen:\n\\[\\color{red}{e^{\\mathbf{A}t} =\n\\begin{pmatrix}\n1 - t - \\frac{t^2}{2} & t & t + \\frac{t^2}{2} \\\\\n-t & 1 & t \\\\\n-t - \\frac{t^2}{2} & t & 1 + t +\\frac{t^2}{2}\n\\end{pmatrix}}\\]\nStep 3: Final Solution\n\\[\\mathbf{x}(t) = e^{\\mathbf{A}t} \\mathbf{x}_0 =\n\\begin{pmatrix}\n1 - t - \\frac{t^2}{2} & t & t + \\frac{t^2}{2} \\\\\n-t & 1 & t \\\\\n-t - \\frac{t^2}{2} & t & 1 + t +\\frac{t^2}{2}\n\\end{pmatrix}\n\\mathbf{x}_0\\]\nSolution B\nStep 1: Compute \\(s\\mathbf{I} - \\mathbf{A}\\)\n\\[\ns \\mathbf{I} - \\mathbf{A} =\n\\begin{pmatrix}\ns + 1 & -1 & -1 \\\\\n1 & s & -1 \\\\\n1 & -1 & s -1\n\\end{pmatrix}\\]\nStep 2: Inverse of \\(s\\mathbf{I} - \\mathbf{A}\\)\nAfter computing the inverse, we obtain:\n\\[\\displaystyle (s \\mathbf{I} - \\mathbf{A})^{-1} = \\dfrac{1}{s^3}\n\\begin{pmatrix}\ns^2 - s - 1 & s & s+1 \\\\\n-s & s^2 & s \\\\\n-s-1 & s & s^2 +s+1\n\\end{pmatrix}\n\\]\nStep 3: Inverse Laplace Transform\nSo the time-domain solution becomes:\n\\[e^{\\mathbf{A}t} =\n\\begin{pmatrix}\n1 - t - \\frac{t^2}{2} & t & t + \\frac{t^2}{2} \\\\\n-t & 1 & t \\\\\n-t - \\frac{t^2}{2} & t & 1 + t +\\frac{t^2}{2}\n\\end{pmatrix}\\]\nStep 3: Final Solution\n\\[\n\\mathbf{x}(t) = e^{\\mathbf{A}t} \\mathbf{x}_0 =\n\\begin{pmatrix}\n1 - t - \\frac{t^2}{2} & t & t + \\frac{t^2}{2} \\\\\n-t & 1 & t \\\\\nt - \\frac{t^2}{2} & t & 1 + t +\\frac{t^2}{2}\n\\end{pmatrix}\n\\mathbf{x}_0\\]\n2. \\(\\phantom{1}\\) Solve the given system\n\\[\\mathbf{\\dot{x}} = \\begin{pmatrix}\n1 & 1 & 0 \\\\\n1 & 1 & 0 \\\\\n0 & 0 & 3 \\\\\n\\end{pmatrix} \\mathbf{x} +\\begin{pmatrix}\ne^t \\\\\ne^{2t}\\\\\nte^{3t}\n\\end{pmatrix}\\]\n\\(~\\)\nSolution\nWe are given the nonhomogeneous system of differential equations:\n\\[\\dot{\\mathbf{x}} = \\mathbf{A} \\mathbf{x} + \\mathbf{f}(t),\n\\quad \\text{where} \\quad\n\\mathbf{A} = \\begin{pmatrix}\n1 & 1 & 0 \\\\\n1 & 1 & 0 \\\\\n0 & 0 & 3\n\\end{pmatrix}, \\quad\n\\mathbf{f}(t) = \\begin{pmatrix}\ne^t \\\\\ne^{2t} \\\\\nt e^{3t}\n\\end{pmatrix}\\]\nStep 1: Solve the Homogeneous System\nWe first solve:\n\\[\\dot{\\mathbf{x}}_h = \\mathbf{A} \\mathbf{x}_h\\]\nLet’s find the general solution to the homogeneous system using eigenvalues and eigenvectors of \\(\\mathbf{A}\\)\n\nCharacteristic Polynomial\nWe compute the determinant of this matrix:\n\\[\\det(\\mathbf{A} - \\lambda \\mathbf{I}) =\n  (3 - \\lambda) \\cdot \\begin{vmatrix}\n  1 - \\lambda & 1 \\\\\n  1 & 1 - \\lambda\n  \\end{vmatrix}=\\lambda(\\lambda-2)(\\lambda-3)=0\\]\nThus, the eigenvalues are:\n\\[\\color{blue}{\\lambda_1 = 0, \\,\\lambda_2=2, \\; \\lambda_3 = 3}\\]\nEigenvectors\nFor \\(\\lambda_1 = 0\\): Solve \\((\\mathbf{A} -0\\mathbf{I}) \\mathbf{v}_1 = \\mathbf{0}\\):\n\\[\\displaystyle\n  \\begin{pmatrix}\n                  1 & 1 & 0 \\\\\n                  1 & 1 & 0 \\\\\n                  0 & 0 & 3\n               \\end{pmatrix} \\mathbf{v}_1\n  = \\mathbf{0} \\Rightarrow\n  \\color{blue}{\\mathbf{v}_{1}\n  = \\begin{pmatrix}\n     -1 \\\\ \\phantom{-}1 \\\\ \\phantom{-}0\n  \\end{pmatrix}}\\]\nFor \\(\\lambda_2 = 2\\): Solve \\((\\mathbf{A} -2\\mathbf{I}) \\mathbf{v}_2 = \\mathbf{0}\\):\n\\[\\displaystyle\n  \\begin{pmatrix}\n                  -1 & \\phantom{-}1 & 0 \\\\\n                  \\phantom{-}1 & -1 & 0 \\\\\n                  \\phantom{-}0 & \\phantom{-}0 & 2\n               \\end{pmatrix} \\mathbf{v}_2\n  = \\mathbf{0} \\Rightarrow\n  \\color{blue}{\\mathbf{v}_{2}\n  = \\begin{pmatrix}\n     1 \\\\ 1 \\\\ 0\n  \\end{pmatrix}}\\]\nFor \\(\\lambda = 3\\):\nSolve \\((\\mathbf{A} - 3\\mathbf{I})\\mathbf{v}_3 = \\mathbf{0}\\):\n\\[\\displaystyle\\begin{pmatrix}\n  -2 & \\phantom{-}1 & \\phantom{-}0 \\\\\n  \\phantom{-}1 & -2 & \\phantom{-}0 \\\\\n  \\phantom{-}0 & \\phantom{-}0 & \\phantom{-}0\n  \\end{pmatrix} \\mathbf{v}_3 = \\mathbf{0}\n  \\Rightarrow \\color{blue}{\\mathbf{v}_3 = \\begin{pmatrix} 0 \\\\ 0 \\\\ 1 \\end{pmatrix}}\\]\n\nSo, the homogeneous solution is:\n\\[\\color{blue}{\n\\mathbf{x}_h(t) = c_1 \\begin{pmatrix}\n-1 \\\\ \\phantom{-}1 \\\\ \\phantom{-}0\n\\end{pmatrix} + c_2  \\begin{pmatrix}\n1 \\\\ 1 \\\\ 0\n\\end{pmatrix}e^{2t} + c_3  \\begin{pmatrix}\n0 \\\\ 0 \\\\ 1\n\\end{pmatrix}e^{3t}}\n\\]\nStep 2: Find a Particular Solution\nSo the fundamental matrix is:\n\\[\\color{blue}{\\Phi(t) =\n\\begin{pmatrix}\n-1 & e^{2t} & 0 \\\\\n\\phantom{-}1 & e^{2t} & 0 \\\\\n\\phantom{-}0 & 0 & e^{3t}\n\\end{pmatrix}}\\]\nThen\n\\[\\Phi(t)^{-1} =\n\\begin{pmatrix}\n-\\frac{1}{2} & \\frac{1}{2} & 0 \\\\\n\\frac{1}{2} e^{-2t} & \\frac{1}{2}e^{-2t} & 0 \\\\\n0 & 0 & e^{-3t}\n\\end{pmatrix}\\]\nWe compute:\n\\[\\displaystyle \\mathbf{x}_p(t) = \\Phi(t) \\int \\Phi(t)^{-1} \\mathbf{f}(t) \\, dt\\]\nCompute \\(\\Phi(t)^{-1} \\mathbf{f}(t)\\):\n\\[\\Phi(t)^{-1} \\mathbf{f}(t) =\n\\begin{pmatrix}\n-\\frac{1}{2} e^t +\\frac{1}{2}e^{2t}\\\\\n\\frac{1}{2}e^{-t} +\\frac{1}{2} \\\\\nt\n\\end{pmatrix}\\]\nNow integrate:\n\\[\\displaystyle \\color{blue}{\\int \\Phi(t)^{-1} \\mathbf{f}(t) \\, dt = \\begin{pmatrix}\n-\\frac{1}{2} e^t +\\frac{1}{4}e^{2t}\\\\\n\\frac{1}{2}t -\\frac{1}{2}e^{-t} \\\\\n\\frac{1}{2}t^2\n\\end{pmatrix}}\n\\]\nLet’s simplify and write the particular solution in general form:\n\\[ \\displaystyle\n\\color{blue}{\\mathbf{x}_p(t) = \\Phi(t) \\int \\Phi(t)^{-1} \\mathbf{f}(t) \\, dt = \\begin{pmatrix}\n-\\frac{1}{4} e^{2t} +\\frac{1}{2}te^{2t}\\\\\n-e^t +\\frac{1}{4}e^{2t} +\\frac{1}{2} te^{2t} \\\\\n\\frac{1}{2}t^2 e^{3t}\n\\end{pmatrix}}\\]\nStep 3: Final General Solution\n\\[\\displaystyle\n\\mathbf{x}(t) =\\mathbf{x}_h(t) + \\mathbf{x}_p(t)\\]\n\\(~\\)\n3. \\(~\\) Use diagonalization to solve the given system\n\\[\\mathbf{x}' = \\begin{pmatrix}\n1 & \\frac{1}{4}\\\\\n1 & 1\n\\end{pmatrix} \\mathbf{x}\\]\nSolution\nStep 1: \\(~\\) Find eigenvalues\nSolve \\(~\\det(\\mathbf{A} - \\lambda \\mathbf{I}) = 0\\):\n\\[\\det\\begin{pmatrix} 1 - \\lambda & \\frac{1}{4} \\\\ 1 & 1 - \\lambda \\end{pmatrix}\n= (1 - \\lambda)^2 - \\frac{1}{4} = 0\\]\n\\[(1 - \\lambda)^2 = \\frac{1}{4}\n\\Rightarrow 1 - \\lambda = \\pm \\frac{1}{2}\n\\Rightarrow \\lambda_1 = \\frac{1}{2}, \\quad \\lambda_2 = \\frac{3}{2}\\]\nStep 2: \\(~\\) Find eigenvectors\nFor \\(~\\lambda_1 = \\frac{1}{2}\\):\nSolve \\(~(A - \\frac{1}{2}I)\\mathbf{v} = 0\\) and so one eigenvector is:\n\\[\\mathbf{v}_1 = \\begin{pmatrix} -\\frac{1}{2} \\\\ \\phantom{-}1 \\end{pmatrix}\n\\quad \\text{(or any scalar multiple)}\\]\nFor \\(~\\lambda_2 = \\frac{3}{2}\\):\nSo one eigenvector is:\n\\[\\mathbf{v}_2 = \\begin{pmatrix} \\frac{1}{2} \\\\ 1 \\end{pmatrix}\\]\nStep 3: \\(~\\) Form matrix \\(\\mathbf{P}\\) and diagonal \\(\\mathbf{D}\\)\nLet:\n\\[\n\\mathbf{P} = \\begin{pmatrix}\n-\\frac{1}{2} & \\frac{1}{2} \\\\\n\\phantom{-}1 & 1\n\\end{pmatrix}, \\quad\n\\mathbf{D} = \\begin{pmatrix}\n\\frac{1}{2} & 0 \\\\\n0 & \\frac{3}{2}\n\\end{pmatrix}\\]\nThen:\n\\[\\mathbf{A} = \\mathbf{P D P}^{-1}\\]\nStep 4: \\(~\\) Solve system using diagonalization\nLet \\(\\mathbf{x} = \\mathbf{P} \\mathbf{y}\\), then:\n\\[\\mathbf{x}’ = \\mathbf{A} \\mathbf{x}\n\\Rightarrow \\mathbf{P} \\mathbf{y}’ = \\mathbf{A P} \\mathbf{y}\n\\Rightarrow \\mathbf{y}’ = \\mathbf{D} \\mathbf{y}\\]\nSo the system becomes:\n\\[\n\\mathbf{y}' = \\mathbf{D} \\mathbf{y} \\Rightarrow\n\\begin{cases}\ny_1' = \\frac{1}{2} y_1 \\\\\ny_2' = \\frac{3}{2} y_2\n\\end{cases}\n\\;\\Rightarrow\\;\n\\begin{cases}\ny_1(t) = c_1 e^{\\frac{1}{2}t} \\\\\ny_2(t) = c_2 e^{\\frac{3}{2}t}\n\\end{cases}\\]\nThen:\n\\[\\mathbf{x}(t) = \\mathbf{P} \\mathbf{y}(t)\n= c_1 e^{\\frac{1}{2}t} \\begin{pmatrix} -\\frac{1}{2} \\\\ \\phantom{-}1 \\end{pmatrix}\n    +   c_2 e^{\\frac{3}{2}t} \\begin{pmatrix} \\frac{1}{2} \\\\ 1 \\end{pmatrix}\\]\n\\(~\\)\n4. \\(~\\) Use diagonalization to solve the given system\n\\[\\mathbf{x}' = \\begin{pmatrix}\n0 & 2 & 0\\\\\n2 & 0 & 2\\\\\n0 & 2 & 0\n\\end{pmatrix} \\mathbf{x}\\]\nSolution\nStep 1: \\(~\\) Find eigenvalues of \\(\\mathbf{A}\\)\nWe solve the characteristic polynomial:\n\\[\\det(\\mathbf{A} - \\lambda \\mathbf{I}) = 0\\]\nThe determinant is:\n\\[\\begin{vmatrix}\n-\\lambda & \\phantom{-}2 & \\phantom{-}0\\\\\n\\phantom{-}2 & -\\lambda & \\phantom{-}2\\\\\n\\phantom{-}0 & \\phantom{-}2 & -\\lambda\n\\end{vmatrix}\n= -\\lambda \\cdot \\begin{vmatrix}\n-\\lambda & \\phantom{-}2\\\\\n\\phantom{-}2 & -\\lambda\n\\end{vmatrix}\n    -   2 \\cdot \\begin{vmatrix}\n2 & \\phantom{-}2\\\\\n0 & -\\lambda\n\\end{vmatrix}\n= -\\lambda(\\lambda^2 - 4) + 4\\lambda = -\\lambda^3 + 8\\lambda\\]\nSet equal to \\(0\\):\n\\[-\\lambda^3 + 8\\lambda = 0 \\Rightarrow \\lambda(-\\lambda^2 + 8) = 0\n\\Rightarrow \\lambda = 0,\\ \\pm 2\\sqrt{2}\\]\nStep 2: \\(~\\) Find eigenvectors\n\nFor \\(\\lambda = 0\\):\nFrom the system:\n\n\\(2y = 0 \\Rightarrow y = 0\\)\nThen \\(~2x + 2z = 0 \\Rightarrow x + z = 0 \\Rightarrow z = -x\\)\n\nSo eigenvector:\n\\[\\mathbf{v}_0 = \\begin{pmatrix} \\phantom{-}1 \\\\ \\phantom{-}0 \\\\ -1 \\end{pmatrix}\\]\nFor \\(\\lambda = 2\\sqrt{2}\\):\nSolve \\((\\mathbf{A} - 2\\sqrt{2} \\mathbf{I}) \\mathbf{v} = \\mathbf{0}\\)\nSo:\n\\[\\mathbf{v}_+ = \\begin{pmatrix} 1 \\\\ \\sqrt{2} \\\\ 1 \\end{pmatrix}\\]\nFor \\(\\lambda = -2\\sqrt{2}\\):\n\\[\\mathbf{v}_- = \\begin{pmatrix} \\phantom{-}1 \\\\ -\\sqrt{2} \\\\ \\phantom{-}1 \\end{pmatrix}\\]\n\nStep 3: \\(~\\) Construct the matrices \\(\\mathbf{P}\\) and \\(\\mathbf{D}\\)\nWe construct matrix P using the eigenvectors as columns:\n\\[\\mathbf{P} = \\begin{pmatrix}\n1 & \\phantom{-}1 & \\phantom{-}1 \\\\\n\\sqrt{2} & -\\sqrt{2} & 0 \\\\\n1 & \\phantom{-}1 & -1\n\\end{pmatrix}\\]\nThen the diagonal matrix is:\n\\[\\mathbf{D} = \\begin{pmatrix}\n2\\sqrt{2} & \\phantom{-}0 & 0 \\\\\n0 & -2\\sqrt{2} & 0 \\\\\n0 & \\phantom{-}0 & 0\n\\end{pmatrix}\\]\nSo, we have:\n\\[\\mathbf{A} = \\mathbf{P D P}^{-1}\\]\nStep 4: \\(~\\) Solve system using diagonalization\nLet \\(\\mathbf{x} = \\mathbf{P} \\mathbf{y}\\), then:\n\\[\\mathbf{x}’ = \\mathbf{A} \\mathbf{x}\n\\Rightarrow \\mathbf{P} \\mathbf{y}’ = \\mathbf{A P} \\mathbf{y}\n\\Rightarrow \\mathbf{y}’ = \\mathbf{D} \\mathbf{y}\\]\nSo the system becomes:\n\\[\\begin{cases}\ny_1’(t) = 2\\sqrt{2} y_1(t) \\\\\ny_2’(t) = -2\\sqrt{2} y_2(t) \\\\\ny_3’(t) = 0\n\\end{cases}\n\\quad \\Rightarrow \\quad\n\\begin{cases}\ny_1(t) = c_1 e^{2\\sqrt{2} t} \\\\\ny_2(t) = c_2 e^{-2\\sqrt{2} t} \\\\\ny_3(t) = c_3\n\\end{cases}\\]\nStep 5: \\(~\\) Final general solution\nNow substitute back:\n\\[\\mathbf{x}(t) = P \\mathbf{y}(t)\n= c_1 \\begin{pmatrix} \\phantom{-}1 \\\\ \\phantom{-}0 \\\\ -1 \\end{pmatrix}\n  + c_2 e^{2\\sqrt{2}t} \\begin{pmatrix} 1 \\\\ \\sqrt{2} \\\\ 1 \\end{pmatrix}\n    +   c_3 e^{-2\\sqrt{2}t} \\begin{pmatrix} \\phantom{-}1 \\\\ -\\sqrt{2} \\\\ \\phantom{-}1 \\end{pmatrix}\n\\]\n5. \\(~\\) Compute \\(e^{\\mathbf{A}t}\\) for the coefficient matrix and find the general solution of the given system\n\\[\\mathbf{x}' = \\begin{pmatrix}\n2 & 8\\\\\n0 & 4\n\\end{pmatrix} \\mathbf{x} + \\begin{pmatrix}\n2 \\\\ 16t\n\\end{pmatrix}\\]\nSolution\nStep 1: \\(~\\) Solve the homogeneous system\nWe begin by solving the homogeneous system:\n\\[\\mathbf{x}' = \\mathbf{A}\\mathbf{x}\\]\nWe need to compute \\(e^{\\mathbf{A}t}\\)\nStep 1.1: \\(~\\) Diagonalize \\(\\mathbf{A}\\)\n\\(A = \\begin{pmatrix} 2 & 8 \\\\ 0 & 4 \\end{pmatrix}\\) is upper triangular, so eigenvalues are on the diagonal:\n\\[\\lambda_1 = 2,\\; \\lambda_2 = 4\\]\nEigenvector for \\(\\lambda = 2\\):\n\\[(\\mathbf{A} - 2\\mathbf{I}) = \\begin{pmatrix} 0 & 8 \\\\ 0 & 2 \\end{pmatrix} \\Rightarrow v^{(1)} = \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix}\\]\nEigenvector for \\(\\lambda = 4\\):\n\\[(\\mathbf{A} - 4\\mathbf{I}) = \\begin{pmatrix} -2 & 8 \\\\ \\phantom{-}0 & 0 \\end{pmatrix}\n\\Rightarrow -2x + 8y = 0 \\Rightarrow v^{(2)} = \\begin{pmatrix} 4 \\\\ 1 \\end{pmatrix}\\]\nConstruct \\(\\mathbf{P}\\) and \\(\\mathbf{D}\\):\n\\[\\mathbf{P} = \\begin{pmatrix} 1 & 4 \\\\ 0 & 1 \\end{pmatrix}, \\quad\n\\mathbf{D} = \\begin{pmatrix} 2 & 0 \\\\ 0 & 4 \\end{pmatrix}, \\quad\n\\mathbf{P}^{-1} = \\begin{pmatrix} 1 & -4 \\\\ 0 & \\phantom{-}1 \\end{pmatrix}\\]\nStep 1.2: \\(~\\) Compute the matrix exponential\n\\[e^{\\mathbf{A}t} = \\mathbf{P} e^{\\mathbf{D}t} \\mathbf{P}^{-1} =\n\\begin{pmatrix} 1 & 4 \\\\ 0 & 1 \\end{pmatrix}\n\\begin{pmatrix} e^{2t} & 0 \\\\ 0 & e^{4t} \\end{pmatrix}\n\\begin{pmatrix} 1 & -4 \\\\ 0 & \\phantom{-}1 \\end{pmatrix}\\]\nMultiply:\n\\[e^{\\mathbf{A}t} =\n\\begin{pmatrix}\ne^{2t} & 4e^{2t}(e^{2t} - 1) \\\\\n0 & e^{4t}\n\\end{pmatrix}\\]\nStep 2: \\(~\\) Particular solution by variation of parameters\nThe general solution is:\n\\[\\mathbf{x}(t) = e^{At} \\mathbf{c} + \\mathbf{x}_p(t)\\]\nwhere:\n\\[\\mathbf{x}_p(t) = e^{At} \\int_0^t e^{-As} \\mathbf{f}(s)\\, ds\\]\nStep 2.1: \\(~\\) Compute \\(e^{-\\mathbf{A}s} \\mathbf{f}(s)\\)\n\\[e^{-\\mathbf{A}s} \\mathbf{f}(s) =\n\\begin{pmatrix}\n2e^{-2s} - 64s(e^{2s} - 1)e^{-4s} \\\\\n16s e^{-4s}\n\\end{pmatrix}\\]\nNow integrate each component:\nStep 2.2: \\(~\\) Compute the integrals\nFrom direct integration:\n\\[\\int_0^t \\left( 2e^{-2s} - 64s(e^{2s} - 1)e^{-4s} \\right) ds = (32t + 15)e^{-2t} - (16t + 4)e^{-4t} - 11\\]\n\\[\\int_0^t 16s e^{-4s} ds = (-4t - 1)e^{-4t} + 1\\]\nStep 2.3: \\(~\\) Multiply by \\(e^{\\mathbf{A}t}\\)\nNow compute:\n\\[\\mathbf{x}_p(t) = e^{\\mathbf{A}t}\n\\begin{pmatrix}\n(32t + 15)e^{-2t} - (16t + 4)e^{-4t} - 11 \\\\\n(-4t - 1)e^{-4t} + 1\n\\end{pmatrix}\\]\nAfter simplification, the final particular solution is:\n\\[\n\\mathbf{x}_p(t) =\n\\begin{pmatrix}\n16t + 11 - 15e^{2t} + 4e^{4t} \\\\\ne^{4t} - 4t - 1\n\\end{pmatrix}\\]\nFinal General Solution\n\\[\n\\mathbf{x}(t) = e^{\\mathbf{A}t} \\mathbf{c} +\n\\begin{pmatrix}\n16t + 11 - 15e^{2t} + 4e^{4t} \\\\\ne^{4t} - 4t - 1\n\\end{pmatrix}\n\\]\nwhere:\n\\[\ne^{\\mathbf{A}t} \\mathbf{c} = \\begin{pmatrix}\ne^{2t} & 4e^{2t}(e^{2t} - 1) \\\\\n0 & e^{4t}\n\\end{pmatrix}\n\\begin{pmatrix} c_1 \\\\ c_2 \\end{pmatrix}\\]\n\\(~\\)\n6. \\(~\\) Compute \\(e^{\\mathbf{A}t}\\) for the coefficient matrix and find the general solution of the given system\n\\[\\mathbf{x}' = \\begin{pmatrix}\n\\phantom{-}3 & 1\\\\\n-1 & 1\n\\end{pmatrix} \\mathbf{x} + \\begin{pmatrix}\n-2 \\\\ \\phantom{-}1\n\\end{pmatrix} e^{2t}\\]\nSolution\nStep 1: \\(~\\) Solve the homogeneous system\nStep 1.1: \\(~\\) Find eigenvalues of \\(\\mathbf{A}\\)\nCompute the characteristic polynomial:\n\\[\\det(\\mathbf{A} - \\lambda \\mathbf{I}) =\n\\begin{vmatrix} 3 - \\lambda & 1 \\\\ -1 & 1 - \\lambda \\end{vmatrix}\n= (3 - \\lambda)(1 - \\lambda) + 1\n= (\\lambda - 2)^2\\]\nSo, \\(\\lambda = 2\\) is a repeated eigenvalue\nStep 1.2: \\(~\\) Find generalized eigenvectors\nEigenvector for \\(\\lambda = 2\\):\n\\[(\\mathbf{A} - 2\\mathbf{I}) = \\begin{pmatrix} \\phantom{-}1 & \\phantom{-}1 \\\\ -1 & -1 \\end{pmatrix}\n\\Rightarrow  \\mathbf{v}_1 = \\begin{pmatrix} \\phantom{-}1 \\\\ -1 \\end{pmatrix}\\]\nGeneralized eigenvector:\nWe solve \\((\\mathbf{A} - 2\\mathbf{I})\\mathbf{v}_2 = \\mathbf{v}_1\\)\n\\[\\begin{pmatrix} \\phantom{-}1 & \\phantom{-}1 \\\\ -1 & -1 \\end{pmatrix} \\mathbf{v}_2 = \\begin{pmatrix} \\phantom{-}1 \\\\ -1 \\end{pmatrix}\n\\Rightarrow \\mathbf{v}_2 = \\begin{pmatrix} 0 \\\\ 1 \\end{pmatrix}\\]\nStep 1.3: \\(~\\) Matrix exponential for defective matrix\nIf \\(\\mathbf{A} = \\mathbf{PJP}^{-1}\\) with Jordan form \\(\\mathbf{J}\\), then:\n\\[e^{\\mathbf{A}t} = \\mathbf{P} e^{\\mathbf{J}t} \\mathbf{P}^{-1}\\]\nWe use:\n\\[\\mathbf{J} = \\begin{pmatrix} 2 & 1 \\\\ 0 & 2 \\end{pmatrix}, \\quad\ne^{\\mathbf{J}t} = e^{2t} \\begin{pmatrix} 1 & t \\\\ 0 & 1 \\end{pmatrix}\\]\nWe form:\n\\[\\mathbf{P} = \\begin{pmatrix} \\phantom{-}1 & 0 \\\\ -1 & 1 \\end{pmatrix}, \\quad\n\\mathbf{P}^{-1} = \\begin{pmatrix} 1 & 0 \\\\ 1 & 1 \\end{pmatrix}\\]\nNow compute:\n\\[e^{\\mathbf{A}t} = \\mathbf{P} e^{\\mathbf{J}t} \\mathbf{P}^{-1}\n= \\begin{pmatrix} \\phantom{-}1 & 0 \\\\ -1 & 1 \\end{pmatrix}\ne^{2t} \\begin{pmatrix} 1 & t \\\\ 0 & 1 \\end{pmatrix}\n\\begin{pmatrix} 1 & 0 \\\\ 1 & 1 \\end{pmatrix}\n=e^{2t}\n\\begin{pmatrix}\n1 + t & t \\\\\n-t & -t + 1\n\\end{pmatrix}\\]\nThe inverse of the matrix \\(e^{\\mathbf{A}t}\\) is:\n\\[e^{-2t}\\begin{pmatrix}\n1 - t & -t  \\\\\nt  & t + 1\n\\end{pmatrix}\n\\]\nStep 2: \\(~\\) Variation of parameters\nThe general solution is:\n\\[\\mathbf{x}(t) = e^{\\mathbf{A}t} \\mathbf{c} + \\mathbf{x}_p(t),\n\\quad\n\\mathbf{x}_p(t) = e^{\\mathbf{A}t} \\int_0^t e^{-\\mathbf{A}s} \\mathbf{f}(s)\\, ds\\]\nThe particular solution is:\n\\[\\mathbf{x}_p(t) =\ne^{2t}\\begin{pmatrix}\n-\\dfrac{t(t + 4)}{2}  \\\\\n\\;\\;\\;\\;\\dfrac{t(t + 2)}{2}\n\\end{pmatrix}\\]",
    "crumbs": [
      "**First Semester**",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Systems of Linear Differential Equations</span>"
    ]
  },
  {
    "objectID": "ch_11_Systems_of_Nonlinear_Differential_Equations.html",
    "href": "ch_11_Systems_of_Nonlinear_Differential_Equations.html",
    "title": "8  Systems of Nonlinear Differential Equations",
    "section": "",
    "text": "8.1 Autonomous Systems\nA system of first-order differential equations is called autonomous when the system can be written in the form\n\\[\n    \\begin{aligned}\n    \\frac{dx_1}{dt} &=g_1(x_1,x_2,\\cdots,x_n) \\\\\n    \\frac{dx_2}{dt} &=g_2(x_1,x_2,\\cdots,x_n) \\\\\n        & \\;\\vdots \\\\\n    \\frac{dx_n}{dt} &=g_n(x_1,x_2,\\cdots,x_n)\n    \\end{aligned}\n\\]\nNotice that the independent variable \\(t\\) does not appear explicitly on the right-hand side of each differential equation\nExample \\(~\\) The displacement angle \\(\\theta\\) for a pendulum satisfies the nonlinear second-order differential equation\n\\[\\frac{d^2 \\theta}{dt^2} +\\frac{g}{l}\\sin\\theta=0\\]\nIf we let \\(x=\\theta\\) and \\(y=\\dot{\\theta}\\), \\(~\\) this second-order differential equation can be written as the autonomous system\n\\[\\dot{x} = y,\\;\\; \\dot{y} = -\\frac{g}{l}\\sin x\\]\nExample \\(~\\)Find all critical points of the following plane autonomous system\n\\[\n\\begin{aligned}\nx'&= x^2 +y^2 -6\\\\\ny'&= x^2 -y\n\\end{aligned}\\]",
    "crumbs": [
      "**First Semester**",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Systems of Nonlinear Differential Equations</span>"
    ]
  },
  {
    "objectID": "ch_11_Systems_of_Nonlinear_Differential_Equations.html#sec-11-1",
    "href": "ch_11_Systems_of_Nonlinear_Differential_Equations.html#sec-11-1",
    "title": "8  Systems of Nonlinear Differential Equations",
    "section": "",
    "text": "Second-Order DE as a System\nAny second-order differential equation \\(x''=g(x,x')\\) can be written as an autonomous system. If we let \\(y=x'\\), the second-order differential equation becomes the system of two first-order equations\n\\[\n  \\begin{aligned}\n      x' &= y\\\\\n      y' &= g(x,y)\n  \\end{aligned}\n  \\]\n\n\n\n\n\n\nPlane Autonomous System\nWhen \\(n=2\\), the system is called a plane autonomous system, and we write the system as\n\\[\n\\begin{aligned}\n   \\frac{dx}{dt} &= P(x,y)\\\\\n   \\frac{dy}{dt} &= Q(x,y)\n\\end{aligned}\n\\]\nIf \\(P\\), \\(Q\\), and the first-order partial derivatives \\(\\partial P/\\partial x\\), \\(\\partial P/\\partial y\\), \\(\\partial Q/\\partial x\\), and \\(\\partial Q/\\partial y\\) are continuous in a region \\(R\\) of the plane, then a solution to the plane autonomous system that satisfies \\(\\mathbf{x}(0)=\\mathbf{x}_0\\) is unique and one of three basic types:\n\nA constant solution, \\(\\mathbf{x}(t)=\\mathbf{x}_0\\) for all \\(t\\). A constant solution is called a critical or stationary point\n\\[\\begin{aligned}\n   P(x,y) &= 0 \\\\\n   Q(x,y) &= 0\n\\end{aligned}\\]\nNote that since \\(\\mathbf{x}'=\\mathbf{0}\\), a critical point is a solution of the system of algebraic equations\nAn arc, \\(\\mathbf{x}(t)\\) - a plane curve that does not cross itself\n\n\n\n\n\nA periodic solution or cycle, \\(\\mathbf{x}(t +p)=\\mathbf{x}(t)\\)",
    "crumbs": [
      "**First Semester**",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Systems of Nonlinear Differential Equations</span>"
    ]
  },
  {
    "objectID": "ch_11_Systems_of_Nonlinear_Differential_Equations.html#sec-11-2",
    "href": "ch_11_Systems_of_Nonlinear_Differential_Equations.html#sec-11-2",
    "title": "8  Systems of Nonlinear Differential Equations",
    "section": "8.2 Stability of Linear Systems",
    "text": "8.2 Stability of Linear Systems\nIf \\(\\mathbf{x}_1\\) is a critical point of a plane autonomous system and \\(\\mathbf{x}=\\mathbf{x}(t)\\) is a solution satisfying \\(\\mathbf{x}(0)=\\mathbf{x}_0\\), when \\(\\mathbf{x}_0\\) is placed near \\(\\mathbf{x}_1\\)\n\n\n\n\n\n\nIt may return to the critical point\nIt may remain close to the critical point without returning\nIt may move away from the critical point\n\n\nStability Analysis\n\nA careful geometric analysis of the solutions to the linear plane autonomous system\n\\[\n\\begin{aligned}\n   x'&= ax +by\\\\\n   y'&= cx +dy\n\\end{aligned}\n\\]\nin terms of the eigenvalues and eigenvectors of the coefficient matrix\n\\[\\mathbf{A}=\n\\begin{pmatrix}\na & b\\\\\nc & d\n\\end{pmatrix}\n\\]\ndrives the stabilty analysis\nTo ensure that \\(\\mathbf{x}_0=(0,\\,0)\\) is the only critical point, we will assume that the determinant \\(\\Delta = ad -bc \\neq 0\\). If \\(\\tau = a + d\\) is the trace of matrix \\(\\mathbf{A}\\), then the characteristic equation, \\(\\mathrm{det}(\\mathbf{A} -\\lambda\\mathbf{I})=0,\\) can be rewritten as\n\\[\\lambda^2 -\\tau\\lambda +\\Delta =0\\]\nTherefore the eigenvalues of \\(\\mathbf{A}\\) are\n\\[\\lambda =\\frac{\\tau \\pm \\sqrt{\\tau^2 -4\\Delta}}{2}\\]\nand the usual three cases for these roots occur according to whether \\(\\tau^2 -4\\Delta\\) is positive, negative, or zero\n\n\nExample \\(~\\) Find the eigenvalues of the linear system\n\\[\n    \\begin{aligned}\n       x'&= -x +y\\\\\n       y'&= cx -y\n    \\end{aligned}\n\\]\nin terms of \\(c\\), and use a numerical solver to discover the shapes of solutions corresponding to the cases \\(c=\\frac{1}{4}\\), \\(4\\), \\(0\\), and \\(-9\\). \\(\\mathbf{A}\\) has trace \\(\\tau=-2\\) and determinant \\(\\Delta=1 -c\\), and so the eigenvalues are\n\\[ \\lambda =-1 \\pm \\sqrt{c} \\]\nThe nature of the eigenvalues is therefore determined by the value of \\(c\\)\n\nimport numpy as np\nimport sympy as sp\nsp.init_printing(use_unicode=True)\n\nc = sp.Symbol('c')\nA = sp.Matrix([[-1, 1], [c, -1]])\nA.eigenvects()\n\n\\(\\displaystyle \\left[ \\left( - \\sqrt{c} - 1, \\  1, \\  \\left[ \\left[\\begin{matrix}\\frac{- \\sqrt{c} - 1}{c} + \\frac{1}{c}\\\\1\\end{matrix}\\right]\\right]\\right), \\  \\left( \\sqrt{c} - 1, \\  1, \\  \\left[ \\left[\\begin{matrix}\\frac{\\sqrt{c} - 1}{c} + \\frac{1}{c}\\\\1\\end{matrix}\\right]\\right]\\right)\\right]\\)\n\n\n\nimport matplotlib.pyplot as plt\n\nw = 1\nxp = np.linspace(-w, w, 50)\nyp = np.linspace(-w, w, 50)\nx, y = np.meshgrid(xp, yp)\n\nc_ = np.array([1/4, 4, 0, -9])\nc_title = [r'$c=\\frac{1}{4}$', r'$c=4$', r'$c=0$', r'$c=-9$']\n\nfig = plt.figure(figsize=(6, 6))\n\nfor i in range(4):\n    \n    ax = fig.add_subplot(2, 2, i +1)\n\n    xdot = -x +y\n    ydot = c_[i]*x -y\n    \n    if c_[i] &gt;= 0.0:\n        y_1 = -np.sqrt(c_[i]) *xp\n        y_2 = np.sqrt(c_[i]) *xp\n        ax.plot(xp, y_1, 'r:', xp, y_2, 'g:')    \n    ax.streamplot(x, y, xdot, ydot, color='blue')\n\n    ax.set_title(f'{c_title[i]}')\n\n    ax.axis((-w, w, -w, w))\n    ax.set_aspect(aspect='equal')\n    ax.grid()\n    \n    ax.tick_params(axis='both', direction='in', \n        labelsize=8)     \n    ax.xaxis.set_ticks(np.linspace(-w, w, 5))\n    ax.yaxis.set_ticks(np.linspace(-w, w, 5))\n    \n    if i &gt;= 2: \n        ax.set_xlabel(r'$x$')\n    if i == 0 or i == 2: \n        ax.set_ylabel(r'$y$')\n\n\n\n\n\n\n\n\n\nTrajectory behaviors in phase portraits can be explained with eigenvalue-eigenvector of \\(\\mathbf{A}\\)\nReal Distinct Eigenvalues, \\(\\tau^2 -4\\Delta &gt; 0\\)\n\\[\n  \\begin{aligned}\n      \\mathbf{x}(t) &= c_1\\mathbf{k}_1 e^{\\lambda_1 t} +c_2\\mathbf{k}_2 e^{\\lambda_2 t}\\\\\n      &\\;\\big\\Downarrow \\;{\\scriptstyle \\lambda_1 &gt;\\lambda_2}\\\\\n      &= e^{\\lambda_1 t} \\left[c_1\\mathbf{k}_1 +c_2\\mathbf{k}_2 e^{(\\lambda_2 -\\lambda_1)t} \\right ] \\\\\n      &\\;\\big\\Downarrow \\;\\,{\\scriptstyle t \\to \\infty}\\\\\n      &\\simeq c_1\\mathbf{k}_1 e^{\\lambda_1 t}\n  \\end{aligned}\n\\]\n\nBoth eigenvalues negative, \\(\\tau^2 -4\\Delta &gt; 0\\), \\(\\tau&lt;0\\), \\(\\Delta&gt;0\\)\nStable Node: Since both eigenvalues are negative, it follows that \\(\\lim_{t \\to \\infty} \\mathbf{x}(t)=\\mathbf{0}\\) in the direction of \\(\\mathbf{k}_1\\) when \\(c_1 \\neq 0\\) or in the direction of \\(\\mathbf{k}_2\\) when \\(c_1=0\\)\nBoth eigenvalues positive, \\(\\tau^2 -4\\Delta &gt; 0\\), \\(\\tau&gt;0\\), \\(\\Delta&gt;0\\)\nUnstable Node: \\(\\mathbf{x}(t)\\) becomes unbounded in the direction of \\(\\mathbf{k}_1\\) when \\(c_1 \\neq 0\\) or in the direction of \\(\\mathbf{k}_2\\) when \\(c_1=0\\)\nEigenvalues have opposite signs, \\(\\tau^2 -4\\Delta &gt; 0\\), \\(\\Delta&lt;0\\)\nSaddle Point: When \\(c_1=0\\), \\(\\mathbf{x}(t)\\) will approach \\(\\mathbf{0}\\) along the line determined by \\(\\mathbf{k}_2\\). If \\(\\mathbf{x}(0)\\) does not lie on the line determined by \\(\\mathbf{k}_2\\), the direction determined by \\(\\mathbf{k}_1\\) serves as an asymtote for \\(\\mathbf{x}(t)\\)\n\n\nExample \\(~\\) Classify the critical point \\((0,0)\\) of each of the following linear system \\(\\mathbf{x}'=\\mathbf{A}\\mathbf{x}\\) as either a stable node, an unstable node, or a saddle point\n\n\\((\\text{a})\\) \\(\\begin{pmatrix} -2 & -2\\\\ -2 & -5 \\end{pmatrix}\\), \\(\\;\\) \\((\\text{b})\\) \\(\\begin{pmatrix} -1 & -2\\\\ \\;\\;3 & \\;\\;4 \\end{pmatrix}\\), \\(\\;\\) \\((\\text{c})\\) \\(\\begin{pmatrix} 2 & -1\\\\ 3 & -2 \\end{pmatrix}\\)\n\n\nimport numpy as np\n\nw = 1\nxp = np.linspace(-w, w, 6)\nyp = np.linspace(-w, w, 6)\nx, y = np.meshgrid(xp, yp)\n\nA = np.array([[[-2, -2], [-2, -5]], \n              [[-1, -2], [3, 4]], \n              [[2, -1], [3, -2]]])\nA_title = ['Stable Node', 'Unstable Node', 'Saddle Point']\n\nfig = plt.figure(figsize=(4, 12))\n\nfor i in range(3):\n    xdot = A[i,0,0]*x +A[i,0,1]*y\n    ydot = A[i,1,0]*x +A[i,1,1]*y\n\n    lamda, v = np.linalg.eig(A[i])\n\n    if lamda[0] &gt;= lamda[1]:\n        y_1 = v[1,0]/v[0,0]*xp\n        y_2 = v[1,1]/v[0,1]*xp\n    else:\n        y_1 = v[1,1]/v[0,1]*xp\n        y_2 = v[1,0]/v[0,0]*xp       \n\n    ax = fig.add_subplot(3, 1, i +1)\n    \n    ax.plot(xp, y_1, 'r:', xp, y_2, 'g:')    \n    ax.streamplot(x, y, xdot, ydot, color='blue')\n\n    ax.set_title(A_title[i])\n\n    ax.axis((-w, w, -w, w))\n    ax.set_aspect(aspect='equal')\n    ax.grid()\n    \n    ax.tick_params(axis='both', direction='in', pad=5)     \n    ax.xaxis.set_ticks(np.linspace(-w, w, 5))\n    ax.set_ylabel(r'$y$')\n    \n    ax.yaxis.set_ticks(np.linspace(-w, w, 5))\n    if i == 2:\n        ax.set_xlabel(r'$x$')\n\n\n\n\n\n\n\n\n\nA Repeated Real Eigenvalue, \\(\\tau^2 -4\\Delta = 0\\)\nThe general solution takes on one of two different forms depending on whether one or two linearly independent eigenvectors can be found for the repeated eigenvalues \\(\\lambda_1\\)\n\nTwo linearly independent eigenvectors\nIf \\(\\mathbf{k}_1\\) and \\(\\mathbf{k}_2\\) are two linearly independent eigenvectors corresponding to \\(\\lambda_1\\), then the general solution is given by\n\\[\\mathbf{x}(t)=c_1\\mathbf{k}_1 e^{\\lambda_1 t} +c_2\\mathbf{k}_2 e^{\\lambda_1 t}=\n\\left(c_1\\mathbf{k}_1 + c_2\\mathbf{k}_2 \\right) e^{\\lambda_1 t}\\]\nIf \\(\\lambda_1&lt;0\\), the \\(\\mathbf{x}(t)\\) approaches \\(\\mathbf{0}\\) along the line determined by the vector \\(c_1\\mathbf{k}_1 + c_2\\mathbf{k}_2\\) and the critical point is a degenerate stable node. The arrows are reversed when \\(\\lambda_1&gt;0\\), and the critical point is a degenerate unstable node\nA single linearly independent eigenvectors\nWhen only a single linearly independent eigenvector \\(\\mathbf{k}_{11}\\) exists, the general solution is given by\n\\[\\begin{aligned}\n\\mathbf{x}(t)&=c_1\\mathbf{k}_{11} e^{\\lambda_1 t} +c_2\\left(\\mathbf{k}_{11} te^{\\lambda_1 t}\n        +\\mathbf{k}_{12} e^{\\lambda_1 t}\\right)\\\\\n     &=te^{\\lambda_1 t}\\left[c_2 \\mathbf{k}_{11} +\\frac{1}{t} \\left(c_1\\mathbf{k}_{11}\n        +c_2\\mathbf{k}_{12}\\right) \\right]\n\\end{aligned}\\]\nwhere \\((\\mathbf{A} -\\lambda_1\\mathbf{I})\\mathbf{k}_{12}=\\mathbf{k}_{11}\\). If \\(\\lambda_1&lt;0\\), then \\(\\lim_{t \\to \\infty} te^{\\lambda_1 t}=0\\) and it follows that \\(\\mathbf{x}(t)\\) approaches \\(\\mathbf{0}\\) in the line determined by \\(\\mathbf{k}_{11}\\). The critical point is again a degenerate stable node. When \\(\\lambda_1&gt;0\\), \\(\\mathbf{x}(t)\\) becomes unbounded as \\(t\\) increases, and the critical point is a degenerate unstable node\n\n\nExample \\(~\\) Classify the critical point \\((0,0)\\) of each of the following linear system \\(\\mathbf{x}'=\\mathbf{A}\\mathbf{x}\\)\n\n\\((\\text{a})\\) \\(\\begin{pmatrix} 1 & 0\\\\ 0 & 1 \\end{pmatrix}\\), \\(\\;\\) \\((\\text{b})\\) \\(\\begin{pmatrix} 3 & -18\\\\ 2 &\\; -9 \\end{pmatrix}\\), \\(\\;\\) \\((\\text{c})\\) \\(\\begin{pmatrix} \\;\\;2 & 4\\\\ -1 & 6 \\end{pmatrix}\\)\n\n\nw = 1\nxp = np.linspace(-w, w, 6)\nyp = np.linspace(-w, w, 6)\nx, y = np.meshgrid(xp, yp)\n\nA = np.array([[[1, 0], [0, 1]], \n              [[3, -18], [2, -9]], \n              [[2, 4], [-1, 6]]])\nA_title = ['Degenerate Unstable', \n           'Degenerate Stable', \n           'Degenerate Unstable']\n\nfig = plt.figure(figsize=(4, 12))\n\nfor i in range(3):\n   \n    xdot = A[i,0,0]*x +A[i,0,1]*y\n    ydot = A[i,1,0]*x +A[i,1,1]*y\n\n    lamda, v = np.linalg.eig(A[i])\n\n    ax = fig.add_subplot(3, 1, i +1)\n\n    if i != 0:\n        y_1 = v[1,0] /v[0,0] *xp\n        ax.plot(xp, y_1, 'r:')\n            \n    ax.streamplot(x, y, xdot, ydot, color='blue')\n\n    ax.set_title(A_title[i])\n\n    ax.axis((-w, w, -w, w))\n    ax.set_aspect(aspect='equal')\n    ax.grid()\n    \n    ax.tick_params(axis='both', direction='in', pad=5)     \n    ax.xaxis.set_ticks(np.linspace(-w, w, 5))\n    ax.set_ylabel(r'$y$')\n    \n    ax.yaxis.set_ticks(np.linspace(-w, w, 5))\n    if i == 2: ax.set_xlabel(r'$x$')           \n\n\n\n\n\n\n\n\n\nComplex Eigenvalues, \\(\\tau^2 -4\\Delta &lt; 0\\)\nIf \\(\\lambda_1=\\alpha +i\\beta\\) and \\(\\bar{\\lambda}_1\\) are the complex eigenvalues and \\(\\mathbf{k}_1=\\mathbf{b}_1 +i\\mathbf{b}_2\\) is a complex eigenvector corresponding to \\(\\lambda_1\\), the general solution can be written as \\(\\mathbf{x}=c_1\\mathbf{x}_1 +c_2\\mathbf{x}_2\\)\n\\[\n\\begin{aligned}\n  \\mathbf{x}_1(t) &=e^{\\alpha t}\\left(\\mathbf{b}_1\\cos\\beta t -\\mathbf{b}_2\\sin\\beta t\\right) \\\\\n\\mathbf{x}_2(t) &=e^{\\alpha t}\\left(\\mathbf{b}_2\\cos\\beta t +\\mathbf{b}_1\\sin\\beta t\\right)\n\\end{aligned}\\]\n\nPure imaginary roots, \\(\\tau^2 -4\\Delta &lt; 0\\), \\(~\\tau=0\\)\nCenter: When \\(\\alpha=0\\), all solutions are ellipses with center at the origin and are periodic with period \\(p=2\\pi/\\beta\\). The critical point is called a center\nNonezero real part, \\(\\tau^2 -4\\Delta &lt; 0\\), \\(~\\tau\\neq 0\\)\nSpiral Point: When \\(\\alpha&lt;0\\), \\(e^{\\alpha t}\\to 0\\), and the elliptical-like solution spirals closer and closer to the origin. The critical point is called a stable spiral point. When \\(\\alpha&gt;0\\), the effect is the opposite. An elliptical-like solution is driven farther and farther from the origin, and the critical point is now called an unstable spiral point\n\n\nExample \\(~\\) Classify the critical point \\((0,0)\\) of each of the following linear system \\(\\mathbf{x}'=\\mathbf{A}\\mathbf{x}\\)\n\n\\((\\text{a})\\) \\(\\begin{pmatrix} -1 & 2\\\\ -1 & 1 \\end{pmatrix}\\),\\(\\;\\) \\((\\text{b})\\) \\(\\begin{pmatrix} -1 & -4\\\\ \\;\\;1 & -1 \\end{pmatrix}\\)\n\n\nw = 1\nxp = np.linspace(-w, w, 6)\nyp = np.linspace(-w, w, 6)\nx, y = np.meshgrid(xp, yp)\n\nA = np.array([[[-1, 2], [-1, 1]],\n              [[-1, -4], [1, -1]]])\nA_title = ['Center', 'Stable Spiral']\n\nfig = plt.figure(figsize=(4, 8))\n\nfor i in range(2):\n\n    ax = fig.add_subplot(2, 1, i +1)\n    \n    xdot = A[i,0,0]*x +A[i,0,1]*y\n    ydot = A[i,1,0]*x +A[i,1,1]*y\n\n    lamda, v = np.linalg.eig(A[i])\n            \n    ax.streamplot(x, y, xdot, ydot, color='blue')\n\n    ax.set_title(A_title[i])\n\n    ax.axis((-w, w, -w, w))\n    ax.set_aspect(aspect='equal')\n    ax.grid()\n    \n    ax.tick_params(axis='both', direction='in', pad=5)     \n    ax.xaxis.set_ticks(np.linspace(-w, w, 5))\n    ax.set_ylabel(r'$y$')\n    \n    ax.yaxis.set_ticks(np.linspace(-w, w, 5))\n    if i == 1: ax.set_xlabel(r'$x$')\n\n\n\n\n\n\n\n\n\n\n\n\n\nFor a linear plane autonomous system \\(\\mathbf{x}'=\\mathbf{A}\\mathbf{x}\\) with \\(\\mathrm{det}\\,\\mathbf{A}\\neq 0\\), let \\(\\mathbf{x}\\) denote the solution that satisfies the initial condition \\(\\mathbf{x}(0)=\\mathbf{x}_0\\), where \\(\\mathbf{x}_0\\neq\\mathbf{0}\\)\n\n\\(\\lim_{t \\to \\infty}\\mathbf{x}(t)=\\mathbf{0}\\) if and only if the eigenvalues of \\(\\mathbf{A}\\) have negative real parts. This will occur when \\(\\Delta&gt;0\\) and \\(\\tau&lt;0\\)\n\\(\\mathbf{x}(t)\\) is periodic if and only if the eigenvalues of \\(\\mathbf{A}\\) are pure imaginary. This will occur when \\(\\Delta&gt;0\\) and \\(\\tau=0\\)\nIn all other cases, given any neighborhood of the origin, there is at least one \\(\\mathbf{x}_0\\) in the neighborhood for which \\(\\mathbf{x}(t)\\) becomes unbounded as \\(t\\) increases",
    "crumbs": [
      "**First Semester**",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Systems of Nonlinear Differential Equations</span>"
    ]
  },
  {
    "objectID": "ch_11_Systems_of_Nonlinear_Differential_Equations.html#sec-11-3",
    "href": "ch_11_Systems_of_Nonlinear_Differential_Equations.html#sec-11-3",
    "title": "8  Systems of Nonlinear Differential Equations",
    "section": "8.3 Linearization and Local Stability",
    "text": "8.3 Linearization and Local Stability\nHere we will use linearization as a means of analyzing nonlinear DEs and nonlinear systems; the idea is to replace them by linear DEs and linear systems. Let \\(\\mathbf{x}_1\\) be a critical point of an autonomous system, and let \\(\\mathbf{x}=\\mathbf{x}(t)\\) denote the solution that satisfies the initial condition \\(\\mathbf{x}(0)=\\mathbf{x}_0\\), where \\(\\mathbf{x}\\neq\\mathbf{x}_1\\).\n\n\\(\\mathbf{x}_1\\) is a stable critical point\nwhen, given any \\(\\rho &gt; 0\\), there is a \\(r&gt;0\\) such that if \\(\\mathbf{x}_0\\) satisfies \\(|\\mathbf{x}_0 -\\mathbf{x}_1|&lt;r\\), then \\(\\mathbf{x}(t)\\) satisfies \\(|\\mathbf{x}(t) -\\mathbf{x}_1|&lt;\\rho\\) \\(\\,\\) for all \\(t&gt;0\\). If, in addition, \\(\\lim_{t \\to \\infty} \\mathbf{x}(t)=\\mathbf{x}_1\\) whenever \\(|\\mathbf{x}_0 -\\mathbf{x}_1|&lt;r\\), we call \\(\\mathbf{x}_1\\) an asymptotically stable critical point\n\\(\\mathbf{x}_1\\) is an unstable critical point\nif there is \\(\\rho&gt;0\\) with the property that, for any \\(r&gt;0\\), there is at least one \\(\\mathbf{x}_0\\) that satisfies \\(|\\mathbf{x}_0 -\\mathbf{x}_1|&lt;r\\), yet the corresponding solution \\(\\mathbf{x}(t)\\) satisfies \\(|\\mathbf{x}(t) -\\mathbf{x}_1|\\geq\\rho\\) \\(\\,\\) for at least one \\(t&gt;0\\)\n\nExample \\(~\\) Show that \\((0,0)\\) is a stable critical point of the nonlinear plane autonomous system\n\\[\nx' = -y -x\\sqrt{x^2 +y^2}, \\;\\; y' = x -y\\sqrt{x^2 +y^2}\\]\nFrom the formulas \\(r^2=x^2 +y^2\\) and \\(\\theta=\\tan^{-1}(y/x)\\), we obtain\n\\[\n  \\frac{dr}{dt}=\\frac{1}{r}\\left(x\\frac{dx}{dt}+y\\frac{dy}{dt} \\right),\\;\n  \\frac{d\\theta}{dt}=\\frac{1}{r^2}\\left( -y\\frac{dx}{dt} +x\\frac{dy}{dt}\\right)\\]\nSubstituting for \\(dx/dt\\) and \\(dy/dt\\) in the expressions for \\(dr/dt\\) and \\(d\\theta/dt\\), we obtain\n\\[\\frac{dr}{dt} = -r^2, \\; \\frac{d\\theta}{dt} = 1\\]\nUsing separation of variables, we see that the solution of the system is\n\\[ r=\\frac{1}{t +c_1},\\;\\;\\theta=t +c_2 \\]\nfor \\(r\\neq 0\\). If \\(\\mathbf{x}(0)=(r_0,\\theta_0)\\) is the initial condition in polar coordinates, then\n\\[ r=\\frac{r_0}{r_0 t +1},\\;\\;\\theta=t +\\theta_0 \\]\nNote that \\(r\\leq r_0\\) \\(\\,\\) for \\(t\\geq 0\\), and \\(r\\) approaches \\(0\\) as \\(t\\) increases. Therefore, given \\(\\rho&gt;0\\), a solution that starts less than \\(\\rho\\) from the origin remains within \\(\\rho\\) of the origin for all \\(t\\geq 0\\). Hence the critical point \\((0,0)\\) is asymptotically stable\nExample \\(~\\) When expressed in polar coordinates, a plane autonomous system takes the form\n\\[\n\\begin{aligned}\n  \\frac{dr}{dt} &= 0.05\\,r(3 -r)\\\\\n  \\frac{d\\theta}{dt} &= -1\n\\end{aligned}\n\\]\nShow that \\((x,y)=(0,0)\\) is unstable critical point\nWe see that \\(dr/dt=0\\) when \\(r=0\\) and can conclude that \\((x,y)=(0,0)\\) is a critical point. \\(~\\) The differential equation can be solved using separation of variables. If \\(r(0)=r_0\\) and \\(r_0\\neq 0,\\) then\n\\[ r=\\frac{3}{1 +c_0 e^{-0.15t}} \\]\nwhere \\(c_0=(3 -r_0)/r_0\\). Since\n\\[\\lim_{t \\to \\infty}\\frac{3}{1 +c_0 e^{-0.15t}}=3\\]\nit follows that no matter how close to \\((0,0)\\) a solution starts, the solution will leave a disk of radius \\(\\epsilon\\) about the origin. Therefore \\((0,0)\\) is an unstable critical point\n\nLinearization\n\nWe replace the term \\(\\mathbf{g}(\\mathbf{x})\\) in the original autonomous system \\(\\mathbf{x}'=\\mathbf{g}(\\mathbf{x})\\) by a linear term \\(\\mathbf{A}(\\mathbf{x} -\\mathbf{x}_1)\\) that most closely approximates \\(\\mathbf{g}(\\mathbf{x})\\) in a neighborhood of \\(\\mathbf{x}_1\\). This replacement process is called linearization\nWhen \\(\\mathbf{x}_1\\) is a critical point of a plane autonomous system\n\\[\n\\begin{aligned}\nx' &= P(x,y)\\\\\ny' &= Q(x,y)\n\\end{aligned}\n\\]\n\\(P(x_1,y_1)=Q(x_1,y_1)=0\\) and we have\n\\[\n\\begin{aligned}\nx' &= P(x,y)\\simeq \\left.\\frac{\\partial P}{\\partial x}\\right|_{(x_1,y_1)}(x -x_1)\n         +\\left.\\frac{\\partial P}{\\partial y}\\right|_{(x_1,y_1)}(y -y_1)\\\\\ny' &= Q(x,y)\\simeq \\left.\\frac{\\partial Q}{\\partial x}\\right|_{(x_1,y_1)}(x -x_1)\n         +\\left.\\frac{\\partial Q}{\\partial y}\\right|_{(x_1,y_1)}(y -y_1)\n\\end{aligned}\n\\]\nThe original system \\(\\mathbf{x}'=\\mathbf{g}(\\mathbf{x})\\) may be approximated in a neighborhood of \\(\\mathbf{x}_1\\) by the linear system \\(\\mathbf{x}'=\\mathbf{A}(\\mathbf{x} -\\mathbf{x}_1)\\), where\n\\[\\mathbf{A}=\n\\begin{pmatrix}\n\\left.\\frac{\\partial P}{\\partial x}\\right|_{(x_1,y_1)} & \\left.\\frac{\\partial P}{\\partial y}\\right|_{(x_1,y_1)}\\\\\n\\left.\\frac{\\partial Q}{\\partial x}\\right|_{(x_1,y_1)} & \\left.\\frac{\\partial Q}{\\partial y}\\right|_{(x_1,y_1)}\n\\end{pmatrix} = \\mathbf{g}'(\\mathbf{x}_1)\n\\]\n\nStability Criteria for Plane Autonomous System\n\n\nIf the eigenvalues of \\(\\mathbf{A}=\\mathbf{g}'(\\mathbf{x}_1)\\) have negative real part, then \\(\\mathbf{x}_1\\) is an asymptotically stable critical point\nIf the eigenvalues of \\(\\mathbf{A}=\\mathbf{g}'(\\mathbf{x}_1)\\) have positive real part, then \\(\\mathbf{x}_1\\) is an unstable critical point\n\nExample \\(~\\) Classify the criticl points of the following plane autonomous system\n\\[\n\\begin{aligned}\nx'&= x^2 +y^2 -6\\\\\ny'&= x^2 -y\n\\end{aligned}\n\\]\nThe critical points are \\((\\sqrt{2},2)\\) and \\((-\\sqrt{2},2)\\) and the Jacobian matrix is\n\\[\n\\mathbf{g}'(\\mathbf{x})=\n\\begin{pmatrix}\n2x & \\;2y\\\\\n2x &-1\n\\end{pmatrix}\n\\]\nand so\n\\[\n\\begin{aligned}\n    \\mathbf{A}_1 &=\\mathbf{g}'({\\scriptsize(\\sqrt{2},2)})=\n    {\\scriptsize\\begin{pmatrix}\n    2\\sqrt{2} & \\;\\;4\\\\\n    2\\sqrt{2} &-1\n    \\end{pmatrix}} \\\\\n    \\mathbf{A}_2 &=\\mathbf{g}'({\\scriptsize(-\\sqrt{2},2)})={\\scriptsize\\begin{pmatrix}\n    -2\\sqrt{2} & \\phantom{-}4\\\\\n    -2\\sqrt{2} &-1\n    \\end{pmatrix}}\n\\end{aligned}\n\\]\nSince the determinant of \\(\\mathbf{A}_1\\) is negative, \\(\\mathbf{A}_1\\) has a positive real eigenvalue. Therefore \\((\\sqrt{2},2)\\) is an unstable critical point. \\(\\mathbf{A}_2\\) has a positive determinant and a negative trace, and so both eigenvalues have negative real parts. It follows that \\((-\\sqrt{2},2)\\) is a stable critical point\n\nClassifying Critical Points\n\n\n\n\n\n\nExample \\(~\\) The second-order differential equation \\(mx'' +kx +k_1x^3 =0\\), for \\(k&gt;0\\), represents a general model for the free, undamped oscillations of a mass \\(m\\) attached to a nonlinear spring. If \\(k=1\\) and \\(k_1=-1\\), the spring is called soft and the plane autonomous system corresponding to the nonlinear second-order equation \\(x'' +x -x^3=0\\) is\n\\[\n\\begin{aligned}\nx'&= y\\\\\ny'&= x^3 -x\n\\end{aligned}\n\\]\nFind and classify the critical points\nSince \\(x^3 -x =x(x^2 -1)\\), the critical points are \\((0,0)\\), \\((1,0)\\), and \\((-1,0)\\). The corresponding Jacobian matrices are\n\\[\n\\mathbf{A}_1=\\mathbf{g}'((0,0))=\n\\begin{pmatrix}\n\\;\\;0 & 1\\\\\n-1 & 0  \n\\end{pmatrix}\\;\n\\]\nand\n\\[\n\\mathbf{A}_2=\\mathbf{g}'((1,0))=\\mathbf{g}'((-1,0))=\n\\begin{pmatrix}\n  0 & 1\\\\\n  2 & 0\n\\end{pmatrix}\n\\]\nSince \\(\\mathrm{det}\\,\\mathbf{A}_2&lt;0\\), critical points \\((1,0)\\) and \\((-1,0)\\) are both saddle points. The eigenvalues of \\(\\mathbf{A}_1\\) are \\(\\pm i\\), and the status of the critical points at \\((0,0)\\) remains in doubt. It may be either a stable spiral, an unstable spiral, or a center\n\nPhase-Plane Method\n\nThe linearization method can provide useful information on the local behavior of solutions near critical points. It is of little help if we are interested in solutions whose initial condition \\(\\mathbf{x}(0)=\\mathbf{x}_0\\) is not close to a critical point or if we wish to obtain a global view of the family of solution curves. The phase-plane method is based on the fact that\n\\[\\frac{dy}{dx}=\\frac{dy/dt}{dx/dt}=\\frac{Q(x,y)}{P(x,y)}\\]\nand it attempts to find \\(y\\) as a function of \\(x\\) using one of the methods available for solving first-order DEs\nExample \\(~\\) Use the phase-plane method to determine the nature of the solutions to \\(x'' +x -x^3 =0\\) in a neighborhood of \\((0,0)\\)\nIf we let \\(dx/dt=y\\), then \\(dy/dt=x^3 -x\\). From this we obtain the first-order DE\n\\[\\frac{dy}{dx}=\\frac{x^3 -x}{y} \\]\nwhich can be solved by separation of variables. Integrating gives\n\\[ y^2=\\frac{x^4}{2}-x^2+c \\]\nIf \\(\\mathbf{x}(0)=(x_0,0)\\), where \\(0&lt;x_0&lt;1\\), then \\(c=-\\frac{x_0^4}{2} +x_0^2\\), and so\n\\[ y^2=\\frac{(2 -x^2 -x_0^2)(x_0^2 -x^2)}{2} \\]\nNote that \\(y=0\\) when \\(x=-x_0\\). In addition, the right-hand side is positive when \\(-x_0&lt;x&lt;x_0\\), and so each \\(x\\) has two corresponding values of \\(y\\). The solution \\(\\mathbf{x}(t)\\) that satisfies \\(\\mathbf{x}(0)=(x_0,0)\\) is therefore periodic, and so \\((0,0)\\) is a center\n\nfrom scipy.integrate import solve_ivp\n\ndef myODE(t, y):\n    return [y[1], y[0]**3 -y[0]]\n\ndef plotPhasePortrait(ax, g, td, x0, y0, color):\n\n    for i in range(x0.shape[0]):\n        sol = solve_ivp(g, [0, td[-1]], \n               [x0[i], y0], t_eval = td)    \n        ax.plot(x0[i], y0, color +'x')\n        ax.plot(sol.y[0], sol.y[1], color +'-', lw=0.2)    \n\ntd = np.linspace(0, 50, 2000)    \n        \nx0_0, y0_0 = np.linspace( 0.1, 1.0, 10),  0.0\nx0_1, y0_1 = np.linspace( 1.6, 2.6, 30), -3.0  \nx0_2, y0_2 = np.linspace(-2.6,-1.6, 30),  3.0\n\nfig, ax = plt.subplots(figsize=(6, 6))\n\nplotPhasePortrait(ax, myODE, td, x0_0,  y0_0, 'k')\nplotPhasePortrait(ax, myODE, td, x0_1,  y0_1, 'b')\nplotPhasePortrait(ax, myODE, td, x0_2,  y0_2, 'r')\n    \nax.set_title(r'Phase Portrait: $\\times$ - initial conditions')\n\nw = 3\nax.axis((-w, w, -w, w))\nax.set_aspect(aspect='equal')\nax.grid()\n   \nax.tick_params(axis='both', direction='in', pad=5)     \nax.xaxis.set_ticks(np.linspace(-w, w, 5))\nax.yaxis.set_ticks(np.linspace(-w, w, 5))\nax.set_xlabel(r'$x$')\nax.set_ylabel(r'$y$')\n\nplt.show()",
    "crumbs": [
      "**First Semester**",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Systems of Nonlinear Differential Equations</span>"
    ]
  },
  {
    "objectID": "ch_11_Systems_of_Nonlinear_Differential_Equations.html#sec-11-4",
    "href": "ch_11_Systems_of_Nonlinear_Differential_Equations.html#sec-11-4",
    "title": "8  Systems of Nonlinear Differential Equations",
    "section": "8.4 Autonomous Systems as Mathematical Models",
    "text": "8.4 Autonomous Systems as Mathematical Models\nMany applications from physics and biology give rise to nonlinear autonomous second-order DEs \\(x''=g(x,x')\\)\n\nNonlinear Pendulum\n\\[ \\frac{d^2\\theta}{dt^2} +\\frac{g}{l}\\sin\\theta=0 \\]\nWhen we let \\(x=\\theta\\) and \\(y=\\theta'\\), this second-order differential equation may be rewritten as the dynamical system\n\\[\n  \\begin{aligned}\n  x' &= y\\\\\n  y' &= -\\frac{g}{l}\\sin x\n  \\end{aligned}\n\\]\nThe critical points are \\((\\pm k\\pi,0)\\), and the Jacobian matrix is easily shown to be\n\\[\\mathbf{g}'((\\pm k \\pi,0))=\n  \\begin{pmatrix}\n  0 & 1\\\\\n  (-1)^{k+1}\\frac{g}{l} & 0\n  \\end{pmatrix}\n\\]\nIf \\(k=2n+1\\), \\(\\Delta&lt;0\\), and so all critical points \\((\\pm(2n+1)\\pi,0)\\) are saddle points. When \\(k=2n\\), the eigenvalues are pure imaginary, and so the nature of these critical points remains in doubt. From\n\\[\\frac{dy}{dx}=\\frac{dy/dt}{dx/dt}=-\\frac{g}{l}\\frac{\\sin x}{y}\\]\nit follows that\n\\[\\displaystyle y^2=\\frac{2g}{l}\\cos x +c \\]\nIf \\(\\mathbf{x}(0)=(x_0,0)\\), then\n\\[ y^2=\\frac{2g}{l}(\\cos x -\\cos x_0)\\]\nNote that \\(y=0\\) when \\(x=-x_0\\), and that \\((2g/l)(\\cos x -\\cos x_0)&gt;0\\) for \\(|x|&lt;|x_0|&lt;\\pi\\). Thus each such \\(x\\) has two corresponding values of \\(y\\), and so the solution \\(\\mathbf{x}(t)\\) that satisfies \\(\\mathbf{x}(0)=(x_0,0)\\) is periodic. We may conclude that \\((0,0)\\) is a center. In the case of large initial velocities, the pendulum spins in complete circles about the pivot\n\n\nw1 = 2*np.pi\nw2 = np.pi\nxp = np.linspace(-w1, w1, 100)\nyp = np.linspace(-w2, w2, 100)\nx, y = np.meshgrid(xp, yp)\n\ng = 9.8\nl = 8\nxdot = y\nydot = -g/l*np.sin(x)\n\nfig, ax = plt.subplots(figsize=(6, 3))\n      \nax.streamplot(x, y, xdot, ydot, \n             linewidth=0.5, density=2, color='blue')\n\nax.set_title('Nonlinear Pendulum')\n \nax.axis((-w1, w1, -w2, w2))\nax.set_aspect(aspect='equal')\nax.grid()\n    \nax.tick_params(axis='both', direction='in', pad=5)     \nax.xaxis.set_ticks(np.linspace(-w1, w1, 5))\nax.xaxis.set_ticklabels([r'$-4\\pi$', r'$-2\\pi$', r'$0$',\n                         r'$2\\pi$', r'$4\\pi$'])\nax.set_xlabel(r'$x$')\nax.yaxis.set_ticks(np.linspace(-w2, w2, 3))\nax.yaxis.set_ticklabels([r'$-\\pi$', r'$0$', r'$\\pi$'])\nax.set_ylabel(r'$y$')\n\nplt.show()\n\n\n\n\n\n\n\n\n\nPeriodic Solutions of the Pendulum DE\nSuppose a bead with mass \\(m\\) slides along a thin wire whose shape is described by the function \\(z=f(x)\\). A wide variety of nonlinear oscillations can be obtained by changing the shape of the wire and by making different assumptions about the forces acting on the bead\n\n\n\n\n\nThe tangential force \\(\\mathbf{F}\\) due to the weight \\(W=mg\\) \\(\\,\\) has a magnitude \\(mg\\sin\\theta\\), and therefore the \\(x\\)-component of \\(\\mathbf{F}\\) is \\(F_x=-mg\\sin\\theta\\cos\\theta\\). Since \\(f'(x)=\\tan\\theta\\), \\(\\,\\) we can use the identity \\(1+\\tan^2\\theta=\\sec^2\\theta\\) to conclude that\n\\[\n\\begin{aligned}\nF_x &= -mg\\sin\\theta\\cos\\theta\n=-mg\\tan\\theta\\cos^2\\theta \\\\\n&=-mg\\frac{f'(x)}{1 +\\left[f'(x)\\right]^2}\n\\end{aligned}\n\\]\nA damping force \\(\\mathbf{D}\\), in the direction opposite to the motion, is a constant multiple of the velocitcy of the bead. The \\(x\\)-component of \\(\\mathbf{D}\\) is therefore\n\\[ D_x=-\\beta\\frac{dx}{dt} \\]\nIt follows from Newton’s second law that\n\\[mx''=-mg\\frac{f'(x)}{1 +\\left[f'(x)\\right]^2}-\\beta x' \\]\nand the corresponding plane autonomous system is\n\\[\n  \\begin{aligned}\n  x' &= y\\\\\n  y' &= -g\\frac{f'(x)}{1 +\\left[f'(x)\\right]^2}-\\frac{\\beta}{m} y\n  \\end{aligned}\n\\]\nIf \\(\\mathbf{x}_1=(x_1,y_1)\\) is a critical point of the system, \\(y_1=0\\) and therefore \\(f'(x_1)=0\\). The bead must therefore be at rest at a point on the wire where the tangent line is horizontal. When \\(f\\) is twice differentiable, the Jacobian matrix at \\(\\mathbf{x}_1\\) is\n\\[\n  \\mathbf{g}'(\\mathbf{x}_1)=\n  \\begin{pmatrix}\n  0 & \\phantom{-}1\\\\\n  -g f''(x_1) & -\\frac{\\beta}{m}\n  \\end{pmatrix}\n\\]\nand so\n\\[\n  \\begin{aligned}\n  \\tau &=\n  -\\beta/m \\\\\n  \\Delta &= gf''(x_1) \\\\\n  \\tau^2 -4\\Delta &= \\beta^2/m^2 -4gf''(x_1)\n  \\end{aligned}\n\\]\nWe can make the following conclusions:\n\n\\(f''(x_1)&lt;0\\) : \\(~\\) A relative maximum therefore occurs at \\(x=x_1\\), and since \\(\\Delta&lt;0\\), \\(\\,\\) an unstable saddle point occurs at \\(\\mathbf{x}_1=(x_1,0)\\)\n\\(f''(x_1)&gt;0\\) and \\(\\beta&gt;0\\): \\(~\\) A relative maximum therefore occurs at \\(x=x_1\\), and since \\(\\tau&lt;0\\) and \\(\\Delta&gt;0\\), \\(\\,\\mathbf{x}_1=(x_1,0)\\) is a stable critical point.\nIf \\(\\beta^2 &gt; 4gm^2f''(x_1)\\), the system is overdamped and the critical point is a stable node. If \\(\\beta^2 &lt; 4gm^2f''(x_1)\\), the system is underdamped and the critical point is a stable spiral point. The exact nature of the stable critical point is still in doubt if \\(\\beta^2 = 4gm^2f''(x_1)\\)\n\\(f''(x_1)&gt;0\\) and the system is undamped (\\(\\beta=0\\)):\nIn this case the eigenvalues are pure imaginary, but the phase-plane method can be used to show that the critical point is a center. Therefore solutions with \\(\\mathbf{x}(0)=(x(0),x'(0))\\) near \\(\\mathbf{x}_1=(x_1,0)\\) are periodic\n\n\nExample \\(~\\) A bead is released from the position \\(x(0)=x_0\\) on the curve \\(z=\\cosh(x)\\) with initial velocity \\(x'(0)=\\nu_0\\). Use the phase-plane method to show that the resulting solution is periodic when the system is undamped\n\nw = 10\nxp = np.linspace(-w, w, 200)\nyp = np.linspace(-w, w, 200)\nx, y = np.meshgrid(xp, yp)\n\ng = 9.8\nm = 1\nbeta = [0, 0.25, 0.5]\n\nsb_title = [r'$\\beta=0$', r'$\\beta=0.25$', r'$\\beta=0.5$']\n\ndef df(x):\n    return np.sinh(x)\n\nfig = plt.figure(figsize=(5, 15))\n\nfor i in range(3):\n    \n    xdot = y\n    ydot = -g*df(x) /(1 +df(x)**2) -beta[i] /m *y\n\n    ax = fig.add_subplot(3, 1, i +1)\n    ax.streamplot(x, y, xdot, ydot, density=1.5, \n                  linewidth=0.5, color='blue')\n\n    ax.set_title(sb_title[i])\n\n    ax.axis((-w, w, -w, w))\n    ax.set_aspect(aspect='equal')\n    ax.grid()\n    \n    ax.tick_params(axis='both', direction='in', pad=5)     \n    ax.xaxis.set_ticks(np.linspace(-w, w, 5))\n    ax.set_ylabel(r'$y$')\n    ax.yaxis.set_ticks(np.linspace(-w, w, 5))\n    if i == 2:\n        ax.set_xlabel(r'$x$')\n    \nplt.show()    \n\n\n\n\n\n\n\n\n\nLotka-Volterra Predator-Prey Model\nThere are many predator-prey models that lead to plane autonomous systems with at least one periodic solution. The first such model was constructed independently by pioneer biomathematicians A. Lotka (1925) and V. Volterra (1926). If \\(x\\) denotes the number of predators and \\(y\\) denotes the number of prey, then the Lotka-Volterra model takes the form\n\\[\n  \\begin{aligned}\n  x' &= -ax +bxy = x(-a +by)\\\\\n  y' &= -cxy +dy = y(-cx +d)\n  \\end{aligned}\n\\]\nwhere \\(a\\), \\(b\\), \\(c\\), and \\(d\\) are positive constants. The critical points of this plane autonomous system are \\((0,0)\\) and \\((d/c,a/b)\\), and the corresponding Jocobian matrices are\n\\[\n\\begin{aligned}\n  \\mathbf{A}_1 &= \\mathbf{g}'\\left((0,0)\\right)=\n    \\begin{pmatrix} -a & 0 \\\\ \\phantom{-}0 & d \\end{pmatrix} \\\\\n  \\mathbf{A}_2 &= \\mathbf{g}'\\left((d/c,a/b)\\right)=\n   \\begin{pmatrix} 0 & bd/c \\\\ -ac/b & 0 \\end{pmatrix}\n\\end{aligned}\n\\]\nThe critical point \\((0,0)\\) is a saddle point. Since \\(\\mathbf{A}_2\\) has pure imaginary eigenvalues \\(\\lambda=\\pm i\\sqrt{ad}\\), the critical point \\((d/c,a/b)\\) may be a center. This possibility can be investigated using the phase-plane method. Since\n\\[ \\frac{dy}{dx}=\\frac{y(-cx +d)}{x(-a +by)} \\]\nwe may separate variables and obtain\n\\[ \\int\\frac{-a +by}{y}\\,dy=\\int\\frac{-cx +d}{x}\\,dx \\]\nso that\n\\[ \\left(x^d e^{-cx}\\right)\\left(y^a e^{-by}\\right)=F(x)G(y)=c_0 \\]\nTypical graphs of the nonnegative functions \\(F(x)=x^d e^{-cx}\\) and \\(G(y)=y^a e^{-by}\\) are shown in\n\n\n\n\n\nIt is not hard to show that \\(F(x)\\) has an absolute maximum at \\(x=d/c\\), whereas \\(G(y)\\) has an absolute maximum at \\(y=a/b\\). These graphs can be used to establish the following properties of a solution curve that orginates at a noncritical point \\((x_0,y_0)\\) in the first quadrant\n\nIf \\(y=a/b\\), \\(F(x)G(a/b)=c_0\\) has two solutions \\(x_m\\) and \\(x_M\\) that satisfy \\(x_m&lt;d/c&lt;x_M\\)\nIf \\(x_m&lt;x_1&lt;x_M\\) and \\(x=x_1\\), \\(\\,\\) then \\(F(x_1)G(y)=c_0\\) has exactly two solutions \\(y_1\\) and \\(y_2\\) that satisfy \\(y_1&lt;a/b&lt;y_2\\)\nIf \\(x\\) is outside the interval \\([x_m,x_M]\\), \\(\\,\\) then \\(F(x)G(y)=c_0\\) has no solutions\n\n\nExample \\(~\\) If we let \\(a=0.1\\), \\(b=0.002\\), \\(c=0.0025\\), and \\(d=0.2\\) in the Lotka-Volterra predator-prey model, the critical point in the first quadrant is \\((d/c,a/b)=(80,50)\\), and we know that this critical point is a center. The eigenvalues of \\(\\mathbf{g}'((80,50))\\) are \\(\\lambda=\\pm(\\sqrt{2}/10)i\\), and so the solutions near the critical point have period \\(p\\simeq 10\\sqrt{2}\\pi\\)\n\na = 0.1\nb = 0.002\nc = 0.0025\nd = 0.2\n\ndef myODE(t, y):\n    return [-a*y[0] +b*y[0]*y[1], -c*y[0]*y[1] +d*y[1]]\n\nfig, ax = plt.subplots(figsize=(6, 6))\n\ntd = 80\nt_eval = np.linspace(0, td, 300)\nx0 = 0\ny0 = 50\nfor i in range(15):\n    x0 = x0 +5\n    sol = solve_ivp(myODE, [0, td], [x0, y0], \n         t_eval=t_eval)    \n    ax.plot(x0, y0, 'ro')\n    ax.plot(sol.y[0], sol.y[1], 'b-', linewidth=1)\n    \nax.set_title('Lotka-Volterra model')\n\nw = 400\nax.axis((0, w, 0, w))\nax.set_aspect(aspect='equal')\nax.grid()\n   \nax.tick_params(axis='both', direction='in', pad=5)     \nax.xaxis.set_ticks(np.linspace(0, w, 5))\nax.set_xlabel(r'$x$, predator')\nax.yaxis.set_ticks(np.linspace(0, w, 5))\nax.set_ylabel(r'$y$, prey')\nplt.show()\n\n\n\n\n\n\n\n\n\nLotka-Volterra Competition Model\nA competitive interaction occurs when two or more species compete for the food, water, light, and species resources of an ecosystem. A number of mathematical models have been constructed that offer insights into conditions that permit coexistence. If \\(x\\) denotes the number in species I and \\(y\\) denotes the number in species II, then the Lotka-Volterra model takes the form\n\\[\n  \\begin{aligned}\n  x' &= \\frac{r_1}{K_1} x(K_1 -x -\\alpha_{12}y)\\\\\n  y' &= \\frac{r_2}{K_2} y(K_2 -y -\\alpha_{21}x)\n  \\end{aligned}\n\\]\nNote that in the absence of species II (\\(y=0\\)), \\(x'=(r_1/K_1)x(K_1 -x)\\), and so the first population grows logistically and approaches the steady-state population \\(K_1\\). A similar statement holds for species II growing in the absence of speices I. The term \\(-\\alpha_{21}xy\\) in the second equation stems from the competitive effect of species I on species II. The model therefore assumes that this rate of inhibition is directly proportional to the number of possible competitive pairs \\(xy\\) at a particular \\(t\\)\nThis plane autonomous system has critical points at \\((0,0)\\), \\((K_1,0)\\), and \\((0,K_2)\\). When \\(\\alpha_{12}\\alpha_{21}\\neq 0\\), the lines \\(K_1 -x -\\alpha_{12}y=0\\) and \\(K_2 -y -\\alpha_{21}x=0\\) intersect to produce a fourth critical point \\(\\hat{\\mathbf{x}}=(\\hat{x},\\hat{y})\\)\n\\[ \\hat{x}=\\frac{K_1 -\\alpha_{12}K_2}{1 -\\alpha_{12}\\alpha_{21}},\\;\n\\hat{y}=\\frac{K_2 -\\alpha_{21}K_1}{1 -\\alpha_{12}\\alpha_{21}}\\]\n\nExample \\(~\\) A competitive interaction is described by the Lotka-Volterra competition model\n\\[\n\\begin{aligned}\nx' &= 0.004 x\\,(50 -x -0.75y)\\\\\ny' &= 0.001 y\\,(100 -y -3.0x)\n\\end{aligned}\n\\]\nFind and classify all critical points of the system\n\nw = 120\nxp = np.linspace(0, w, 200)\nyp = np.linspace(0, w, 200)\nx, y = np.meshgrid(xp, yp)\n\nr1_K1 = 0.004\nr2_K2 = 0.001\nK1 = 50\nK2 = 100\na_12 = np.array([0.75, 0.4])\na_21 = np.array([3.00, 1.5])\n\nfig = plt.figure(figsize=(6, 12))\n\nfor i in range(2):\n    \n    ax = fig.add_subplot(2, 1, i +1)\n    \n    xdot = r1_K1*x*(K1 -x -a_12[i]*y)\n    ydot = r2_K2*y*(K2 -y -a_21[i]*x)\n\n    cpt_x = np.array([0, K1, 0, (K1 -a_12[i]*K2)\n                      /(1 -a_12[i]*a_21[i])])\n    cpt_y = np.array([0, 0, K2, (K2 -a_21[i]*K1)\n                      /(1 -a_12[i]*a_21[i])])\n\n    ax.streamplot(x, y, xdot, ydot, density=1.5, \n                  linewidth=0.5, color='blue')\n    ax.plot(cpt_x, cpt_y, 'ro')\n    ax.set_title(r'$\\alpha_{12}=$' +str(a_12[i]) \n                 +r'$, \\alpha_{21}=$' +str(a_21[i]))\n    ax.axis((0, w, 0, w))\n    ax.set_aspect(aspect='equal')\n    ax.grid()\n    \n    ax.tick_params(axis='both', direction='in', pad=5)     \n    ax.xaxis.set_ticks(np.linspace(0, w, 4))\n    ax.set_xlabel(r'$x$')\n    ax.yaxis.set_ticks(np.linspace(0, w, 4))\n    ax.set_ylabel(r'$y$')\n    \nplt.show()",
    "crumbs": [
      "**First Semester**",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Systems of Nonlinear Differential Equations</span>"
    ]
  },
  {
    "objectID": "ch_11_Systems_of_Nonlinear_Differential_Equations.html#sec-11-5",
    "href": "ch_11_Systems_of_Nonlinear_Differential_Equations.html#sec-11-5",
    "title": "8  Systems of Nonlinear Differential Equations",
    "section": "8.5 Periodic Solutions, Limit Cycles, and Global Stability",
    "text": "8.5 Periodic Solutions, Limit Cycles, and Global Stability\nIn this section, we will investigate the existence of periodic solutions of nonlinear plane autonomous systems and introduce special periodic solutions called limit cycles\nAn analysis of critical points using linearization can provide valuable information on the behavior of solutions near critical points and insight into a variety of biological and physical phenomena. However there are some inherent limitations to this approach. When the eigenvalues of the Jacobian matrix are pure imaginary, we cannot conclude that there are periodic solutions near the critical point\nThe first goal of this section is to determine conditions under which we can either exclude the possibility of periodic solutions or assert their existence\nA second goal is to determine conditions under which an asymptotically stable critical point is globally stable: \\(\\lim_{t \\to \\infty}\\mathbf{x}(t)=\\mathbf{x}_1\\) for all initial conditions in a simply connected region \\(\\mathbb{R}\\)\n\nNegative Criteria\n\nTheorem If a plane autonomous system has a periodic solution \\(\\mathbf{x}=\\mathbf{x}(t)\\) in a simply connected region \\(\\mathbb{R}\\), then the system has at least one critical point inside the corresponding simple closed curve \\(C\\). If there is a single critical point inside \\(C\\), then that critical point cannot be a saddle point\nCorollary If a simply connected region \\(\\mathbb{R}\\) either contains no critical points of a plane autonomous system or contains a single saddle point, then there are no periodic solutions in \\(\\mathbb{R}\\)\nExample \\(~\\) Show that the Lotka-Volterra competition model\n\\[\n\\begin{aligned}\nx' &= 0.004 x\\,(50 -x -0.75y)\\\\\ny' &= 0.001 y\\,(100 -y -3.0x)\n\\end{aligned}\n\\]\nhas no periodic solutions in the first quadrant\nAnother sometimes useful result can be formulated in terms of the divergence of the vector field\n\\[ \\mathbf{v}(x,y)=\\left(P(x,y), Q(x,y)\\right) \\]\n\nBendixson Negative Criterion\n\nIf \\(\\displaystyle\\mathrm{div}\\,\\mathbf{v}=\\frac{\\partial P}{\\partial x} +\\frac{\\partial Q}{\\partial y}\\) does not change sign in a simply connected region \\(\\mathbb{R}\\), then the plane autonomous system has no periodic solutions in \\(\\mathbb{R}\\)\nProof \\(~\\) Suppose, to the contrary, that there is a periodic solution \\(\\mathbf{x}=\\mathbf{x}(t)\\) in \\(\\mathbb{R}\\), and let \\(C\\) be the resulting simple closed curve and \\(R_1\\) the region bounded by \\(C\\). By using Green’s theorem, we obtain\n\\[ \\int_C -Q(x,y)\\,dx +P(x,y)\\,dy=\\iint_{R_1}\n  \\left(\\frac{\\partial P}{\\partial x} +\\frac{\\partial Q}{\\partial y}\\right)\\, dxdy \\]\nSince \\(\\mathbf{x}=\\mathbf{x}(t)\\) is a solution with period \\(p\\), \\(\\,\\) we have \\(x'(t)=P(x(t),y(t))\\) and \\(y'(t)=Q(x(t),y(t))\\), and so\n\\[\n\\begin{aligned}\n\\int_C -&Q(x,y)\\,dx +P(x,y)\\,dy \\\\\n   &= \\int_0^p \\left[-Q(x(t),y(t))\\,x'(t) +P(x(t),y(t))\\,y'(t) \\right]\\,dt\\\\\n   &= \\int_0^p -QP +PQ \\,dt = 0\n\\end{aligned}\n\\]\nSince \\(\\displaystyle\\mathrm{div}\\,\\mathbf{v}=\\frac{\\partial P}{\\partial x} +\\frac{\\partial Q}{\\partial y}\\) is continuous and does not change sign in \\(\\mathbb{R}\\), it follows that either \\(\\mathrm{det}\\,\\mathbf{v}\\geq 0\\) or \\(\\mathrm{det}\\,\\mathbf{v}\\leq 0\\) in \\(\\mathbb{R}\\), and so\n\\[\n  \\iint_{R_1} \\left(\\frac{\\partial P}{\\partial x} +\\frac{\\partial Q}{\\partial y}\\right)\\, dxdy \\neq 0\n\\]\nThis contradiction establishes that there are no periodic solutions in \\(\\mathbb{R}\\)\nExample \\(~\\) The sliding bead satisfies the differential equation\n\\[ mx''=-mg\\frac{f'(x)}{1 +\\left[f'(x)\\right]^2}-\\beta x' \\]\nShow that there are no periodic solutions\nSolution \\(~\\) The corresponding plane autonomous system is\n\\[\n\\begin{aligned}\n   x' &= y\\\\\n   y' &= -g\\frac{f'(x)}{1 +\\left[f'(x)\\right]^2}-\\frac{\\beta}{m} y\n\\end{aligned}\n\\]\nand so \\(\\displaystyle\n  \\mathrm{div}\\,\\mathbf{v}=\\frac{\\partial P}{\\partial x} +\\frac{\\partial Q}{\\partial y}=-\\frac{\\beta}{m}&lt;0\\)\n\nDulac Negative Criterion\n\nIf \\(\\delta(x,y)\\) has continuous first partial derivatives in a simply connected region \\(\\mathbb{R}\\) and \\(\\displaystyle \\frac{\\partial (\\delta P)}{\\partial x} +\\frac{\\partial (\\delta Q)}{\\partial y}\\) does not change sign in \\(\\mathbb{R}\\), \\(\\,\\) then the plane autonomous system has no periodic solutions in \\(\\mathbb{R}\\)\nExample \\(~\\) Use \\(\\delta(x,y)=1/(xy)\\) to show that the Lotka-Volterra competition equations\n\\[\n\\begin{aligned}\nx' &= \\frac{r_1}{K_1} x(K_1 -x -\\alpha_{12}y)\\\\\ny' &= \\frac{r_2}{K_2} y(K_2 -y -\\alpha_{21}x)\n\\end{aligned}\n\\]\nhave no periodic solutions in the first quadrant\nSolution \\(~\\) If \\(\\delta(x,y)=1/(xy)\\), then\n\\[ \\frac{\\partial (\\delta P)}{\\partial x} +\\frac{\\partial (\\delta Q)}{\\partial y}=\n  \\frac{r_1}{K_1}\\left(-\\frac{1}{y}\\right) +\\frac{r_2}{K_2}\\left(-\\frac{1}{x}\\right) &lt;0 \\]\nin the first quadrant\n\nPositive Criteria\nA region \\(\\mathbb{R}\\) is called an invariant region for a plane autonomous system if, whenever \\(\\mathbf{x}_0\\) is in \\(\\mathbb{R}\\), the solution \\(\\mathbf{x}=\\mathbf{x}(t)\\) remains in \\(\\mathbb{R}\\). If \\(\\mathbf{n}(x,y)\\) denotes a normal vector on the boundary that points inside the region, then \\(\\mathbb{R}\\) will be an invariant region for the plane autonomous system provided \\(\\mathbf{v}(x,y)\\cdot\\mathbf{n}(x,y)\\geq 0\\) for all points \\((x,y)\\) in the boundary\n\n\n\n\n\n\n\nPoincaré-Benedixon I\nLet \\(\\mathbb{R}\\) be an invariant region for a plane autonomous system and suppose that \\(\\mathbb{R}\\) has no critical points on its boundary\n\\((\\text{a})\\) \\(~\\) If \\(\\mathbb{R}\\) is a type I region that has a single unstable node or an unstable spiral point in its interior, then there is at least one periodic solution in \\(\\mathbb{R}\\)\n\\((\\text{b})\\) \\(~\\) If \\(\\mathbb{R}\\) is a type II region that contains no critical points of the system, then there is at least one periodic solution in \\(\\mathbb{R}\\)\nIn either of the two cases, if \\(\\mathbf{x}=\\mathbf{x}(0)\\) is a nonperiodic solution in \\(\\mathbb{R}\\), then \\(\\mathbf{x}(t)\\) spirals toward a cycle that is a solution to the system. This periodic solution is called a limit cycle\n\nExample \\(~\\) The Van der Pol equation is a nonlinear second-order differential equation that arises in electronics, and as a plane autonomous system it takes the form\n\\[\\begin{aligned}\nx' &= y\\\\\ny' &= -\\mu (x^2 -1)y -x\n\\end{aligned}\\]\nShow that the Van der Pol DE has a periodic solution when \\(\\mu&gt;0\\)\n\nw = 3\nxp = np.linspace(-w, w, 100)\nyp = np.linspace(-w, w, 100)\nx, y = np.meshgrid(xp, yp)\n\nfig = plt.figure(figsize=(5, 10))\n\nmu = np.array([1, 3])\nvdp_title = [r'$\\mu=1$', r'$\\mu=3$']\n\nfor i in range(2):\n    \n    ax = fig.add_subplot(2, 1, i +1)\n    \n    xdot = y\n    ydot = -mu[i]*(x*x -1)*y -x\n\n    ax.streamplot(x, y, xdot, ydot, density=1.5,  \n       linewidth=0.5, color='blue')\n    ax.plot(0, 0, 'ro')\n    ax.set_title(vdp_title[i])\n\n    ax.axis((-w, w, -w, w))\n    ax.set_aspect(aspect='equal')\n    ax.grid()\n    \n    ax.tick_params(axis='both', direction='in', pad=5)     \n    ax.xaxis.set_ticks(np.linspace(-w, w, 4))\n    ax.set_xlabel(r'$x$')\n    ax.yaxis.set_ticks(np.linspace(-w, w, 4))\n    ax.set_ylabel(r'$y$')\n    \nplt.show()\n\n\n\n\n\n\n\n\nWe will assumme that there is a Type I invariant region \\(\\mathbb{R}\\) for the corresponding plane autonomous system and that this region contains \\((0,0)\\) in its interior. The only critical point is \\((0,0)\\), and the Jacobian matrix is given by\n\\[\n\\mathbf{g}'((0,0))=\n\\begin{pmatrix}\n\\;\\;0 & 1\\\\\n-1 & \\mu\n\\end{pmatrix}\n\\]\nTherefore, \\(\\tau=\\mu\\), \\(\\Delta=1\\), and \\(\\tau^2 -4\\Delta=\\mu^2-4\\). Since \\(\\mu &gt;0\\), the critical point is either an unstable spiral point or an unstable node. By part (a) of Poincaré-Benedixon I, the system has at least one periodic solution in \\(\\mathbb{R}\\)\n\ntd = 15\nw = 5\nmu_ = np.array([1, 2.5])\nx0 = np.array([[0.5, 0.5], [3, 3], [-3,-3], [0, 0.1]])\n\ndef myODE(t, y):\n    return [y[1], -mu*(y[0]*y[0] -1)*y[1] -y[0]]\n\nvdp_title = [r'$\\mu=1$', r'$\\mu=2.5$']\nvdp_line = ['b:', 'r:', 'k:', 'g:']\n\nfig = plt.figure(figsize=(5, 10))\n\nfor i in range(2):\n\n    ax = fig.add_subplot(2, 1, i +1)\n    \n    mu = mu_[i]\n    for j in range(4):\n        y0 = x0[j]\n        t_eval = np.linspace(0, td, 500)\n        sol = solve_ivp(myODE, [0, td], y0, t_eval=t_eval)    \n        ax.plot(sol.y[0], sol.y[1], vdp_line[j], \n            linewidth=1)\n        ax.plot(y0[0], y0[1], 'ro')\n    \n    ax.set_title(vdp_title[i])\n\n    ax.axis((-w, w, -w, w))\n    ax.set_aspect(aspect='equal')\n    ax.grid()\n    \n    ax.tick_params(axis='both', direction='in', pad=5)     \n    ax.xaxis.set_ticks(np.linspace(-w, w, 4))\n    ax.set_xlabel(r'$x$')\n    ax.yaxis.set_ticks(np.linspace(-w, w, 4))\n    ax.set_ylabel(r'$y$')\n    \nplt.show()\n\n\n\n\n\n\n\n\n\nPoincaré-Benedixon II\nLet \\(\\mathbb{R}\\) be a type I invariant region for a plane autonomous system that has no periodic solutions in \\(\\mathbb{R}\\)\n\\((\\text{a})\\) \\(~\\) If \\(\\mathbb{R}\\) has a finite number of nodes or spiral points, then given any initial position \\(\\mathbf{x}_0\\) in \\(\\mathbb{R}\\), \\(\\lim_{t \\to \\infty} \\mathbf{x}(t)=\\mathbf{x}_1\\) for some critical point \\(\\mathbf{x}_1\\)\n\\((\\text{b})\\) \\(~\\) If \\(\\mathbb{R}\\) has a single stable node or stable spiral point \\(\\mathbf{x}_1\\) in its interior and no critical points on its boundary, then \\(\\lim_{t \\to \\infty} \\mathbf{x}(t)=\\mathbf{x}_1\\) for all initial positions \\(\\mathbf{x}_0\\) in \\(\\mathbb{R}\\): globally stable\n\nExample \\(~\\) Empirical evidence suggests that the plane autonomous system\n\\[\n\\begin{aligned}\nx' &= x^2y -x +1\\\\\ny' &= -x^2y +\\frac{1}{2}\n\\end{aligned}\n\\]\nhas a type I invariant region \\(\\mathbb{R}\\) that lies inside the reactangle defined by \\(0\\leq x \\leq 2\\), \\(0 \\leq y \\leq 1\\)\n\\((\\text{a})\\) \\(~\\) Use the Benedixson negative criterion to show that there are no periodic solutions in R\n\\((\\text{b})\\) \\(~\\) If \\(\\mathbf{x}_0\\) is in \\(\\mathbb{R}\\) and \\(\\mathbf{x}=\\mathbf{x}(t)\\) is the solution satisfying \\(\\mathbf{x}(t)=\\mathbf{x}_0\\), use Poincaré-Benedixon II to find \\(\\lim_{t \\to \\infty}\\mathbf{x}(t)\\)",
    "crumbs": [
      "**First Semester**",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Systems of Nonlinear Differential Equations</span>"
    ]
  },
  {
    "objectID": "ch_11_Systems_of_Nonlinear_Differential_Equations.html#worked-exercises",
    "href": "ch_11_Systems_of_Nonlinear_Differential_Equations.html#worked-exercises",
    "title": "8  Systems of Nonlinear Differential Equations",
    "section": "Worked Exercises",
    "text": "Worked Exercises",
    "crumbs": [
      "**First Semester**",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Systems of Nonlinear Differential Equations</span>"
    ]
  },
  {
    "objectID": "ch_12_Orthogonal_Functions_and_Fourier_Series.html",
    "href": "ch_12_Orthogonal_Functions_and_Fourier_Series.html",
    "title": "11  Orthogonal Functions and Fourier Series",
    "section": "",
    "text": "11.1 Orthogonal Functions\n\\(~\\)\nExample \\(\\displaystyle \\left\\{ \\frac{1}{\\sqrt{2\\pi}}, \\frac{\\cos x}{\\sqrt{\\pi}}, \\frac{\\cos 2x}{\\sqrt{\\pi}},\\cdots \\right\\}\\), \\(\\;\\; -\\pi \\leq x\\leq \\pi\\)\n\\(~\\)\n\\(~\\)",
    "crumbs": [
      "**Second Semester**",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Orthogonal Functions and Fourier Series</span>"
    ]
  },
  {
    "objectID": "ch_12_Orthogonal_Functions_and_Fourier_Series.html#sec-12-1",
    "href": "ch_12_Orthogonal_Functions_and_Fourier_Series.html#sec-12-1",
    "title": "11  Orthogonal Functions and Fourier Series",
    "section": "",
    "text": "Inner Product\nIf \\(\\,\\mathbf{u}=u_1\\mathbf{i} +u_2\\mathbf{j} +u_3\\mathbf{k}\\,\\) and \\(\\,\\mathbf{v}\n=v_1\\mathbf{i} +v_2\\mathbf{j} +v_3\\mathbf{k}\\,\\) are two vectors in \\(\\mathbb{R}^3\\), \\(\\,\\)then inner product is defined:\n\\[(\\mathbf{u},\\mathbf{v})=u_1v_1 +u_2v_2 +u_3v_3=u_iv_i\\]\nThe inner product \\((\\mathbf{u},\\mathbf{v})\\) possesses the following properties:\n\n\\((\\mathbf{u},\\mathbf{v})=(\\mathbf{v},\\mathbf{u})\\)\n\\((k\\mathbf{u},\\mathbf{v})\n=k(\\mathbf{u},\\mathbf{v})\\), \\(\\;k:\\;\\) a scalar\n\\((\\mathbf{u},\\mathbf{u})=0\\,\\) if \\(\\,\\mathbf{u}=\\mathbf{0}\\,\\) and \\(\\,(\\mathbf{u},\\mathbf{u})&gt;0\\,\\) if \\(\\,\\mathbf{u}\\neq\\mathbf{0}\\)\n\\((\\mathbf{u}+\\mathbf{v},\\mathbf{w})=(\\mathbf{u},\\mathbf{w}) +(\\mathbf{v},\\mathbf{w})\\)\n\nInner Product of Functions\nThe inner product of two functions \\(f_1\\) and \\(f_2\\) on an interval \\([a,b]\\) is the number\n\\[ (f_1,f_2)=\\int_a^b f_1(x)\\, f_2(x) \\,dx\\]\nOrthogonal Functions\nTwo functions \\(\\,f_1\\) and \\(\\,f_2\\) are said to be orthogonal on \\([a,b]\\,\\) if\n\\[(f_1,f_2)=\\int_a^b f_1(x)\\, f_2(x) \\,dx=0\\]\nOrthogonal Sets\nA set of real-valued functions \\(\\{\\phi_0(x),\\phi_1(x),\\phi_2(x),\\cdots\\}\\,\\) is said to be orthogonal on \\([a,b]\\,\\) if\n\\[(\\phi_m,\\phi_n)=\\int_a^b \\phi_m(x) \\, \\phi_n(x) \\,dx=0, \\;\\;m\\neq n\\]\nWith the property that \\(\\|\\phi_n(x)\\|=1\\), \\(\\,\\)then \\(\\{\\phi_n(x)\\}\\) is said to be an orthonormal set on the interval\n\n\n\n\n\nOrthogonal Series Expansion\nSuppose \\(\\left\\{ \\phi_n(x)\\right\\}\\) is an infinite orthogonal set of functions on \\([a,b]\\). \\(\\,\\)If \\(\\,y=f(x)\\,\\) is a function defined on \\([a,b]\\),\n\\[\n\\begin{aligned}\n   \\color{blue}{f(x)}\n     &\\; \\color{blue}{= c_0\\phi_0(x) +c_1\\phi_1(x) +\\cdots +c_n\\phi_n(x)+\\cdots}\\\\\n     &\\; {\\scriptsize\\Downarrow}\\\\\n    {\\scriptsize \\int_a^b f(x) \\phi_n(x)\\,dx}\n     &\\; {\\scriptsize =c_0\\int_a^b \\phi_0(x) \\phi_n(x)\\,dx\n      +c_1\\int_a^b \\phi_1(x)\\phi_n(x)\\,dx +\\cdots +c_n\\int_a^b \\phi_n^2(x)\\,dx+\\cdots}\\\\\n     &\\; {\\scriptsize = c_n\\int_a^b \\phi_n^2(x)\\,dx} \\\\\n     &\\; {\\scriptsize\\Downarrow} \\\\\n  {\\color{red}{c_n}} & {\\color{red}{= \\frac{\\displaystyle \\int_a^b f(x)\\,\\phi_n(x)\\,dx}{\\displaystyle \\int_a^b \\phi_n^2(x)\\,dx},\\;\\;n=0,1,2,\\cdots}}\n\\end{aligned}\\]\nOrthogonal Set/Weight Function\nA set of real-valued functions \\(\\{\\,\\phi_0(x),\\phi_1(x),\\phi_2(x),\\cdots\\}\\,\\) is said to be orthogonal with respect to a weight function \\(w(x)\\) on \\([a,b]\\,\\) if\n\\[(\\phi_m,\\phi_n)=\\int_a^b w(x)\\phi_m(x) \\phi_n(x) \\,dx=0, \\;\\;m\\neq n\\]\nWith this inner product notation,\n\\[ f(x)=\\sum_{n=0}^\\infty c_n \\phi_n(x)\\]\nwhere \\(~{\\scriptsize c_n=\\frac{\\displaystyle\\int_a^b f(x)w(x)\\phi_n(x)\\,dx}{\\displaystyle\\int_a^b w(x) \\phi_n^2(x)\\,dx}}\\)",
    "crumbs": [
      "**Second Semester**",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Orthogonal Functions and Fourier Series</span>"
    ]
  },
  {
    "objectID": "ch_12_Orthogonal_Functions_and_Fourier_Series.html#sec-12-2",
    "href": "ch_12_Orthogonal_Functions_and_Fourier_Series.html#sec-12-2",
    "title": "11  Orthogonal Functions and Fourier Series",
    "section": "11.2 Fourier Series",
    "text": "11.2 Fourier Series\n\nTrigonometric Series\n\\[ \\left\\{ 1, \\,\\cos\\frac{\\pi}{p}x, \\,\\cos\\frac{2\\pi}{p}x, \\,\\cos\\frac{3\\pi}{p}x,\\cdots,\n\\,\\sin\\frac{\\pi}{p}x, \\,\\sin\\frac{2\\pi}{p}x, \\,\\sin\\frac{3\\pi}{p}x, \\,\\cdots \\right\\} \\]\nFourier Series\nThe Fourier series of a function \\(f\\) defined on the interval \\((-p,p)\\,\\) is given by\n \\[ f(x)=\\frac{a_0}{2} +\\sum_{n=1}^\\infty \\left( a_n\\cos\\frac{n\\pi}{p}x +b_n\\sin\\frac{n\\pi}{p}x \\right)\\] \nwhere\n\\[\n\\begin{aligned}\n   a_n &= \\frac{1}{p}\\int_{-p}^p f(x)\\cos\\frac{n\\pi}{p}x \\,dx, \\quad a_0 = \\frac{1}{p}\\int_{-p}^p f(x)\\,dx \\\\\n   b_n &= \\frac{1}{p}\\int_{-p}^p f(x)\\sin\\frac{n\\pi}{p}x \\,dx\n\\end{aligned}\\]\nAt a point of discontinuity, \\(\\,\\)the Fourier series converges to the average \\(\\displaystyle {\\scriptsize \\;\\frac{f(x^+)+f(x^-)}{2}}\\)\n\n\\(~\\)\nExample\n\\[\\begin{aligned}\nf(x) &=\n  \\begin{cases}\n    \\;0, & -\\pi \\leq x \\leq 0 \\\\\n    \\;1, & \\;\\;\\, 0 &lt; x \\leq \\pi\n  \\end{cases} \\\\\n  &\\Downarrow \\\\\nf(x)&= \\frac{1}{2}\n+\\sum_{m=1}^\\infty \\frac{2}{(2m -1)\\pi} \\sin{(2m -1)x}\n\\end{aligned}\\]\n\\(~\\)\n\nfrom sympy import fourier_series, Piecewise, pi, plot\nfrom sympy.abc import x\n\nf = Piecewise((0, x &lt; 0), (1, True))\ns = fourier_series(f, (x, -pi, pi))\ns.truncate(5)\n\n\\(\\displaystyle \\frac{2 \\sin{\\left(x \\right)}}{\\pi} + \\frac{2 \\sin{\\left(3 x \\right)}}{3 \\pi} + \\frac{2 \\sin{\\left(5 x \\right)}}{5 \\pi} + \\frac{2 \\sin{\\left(7 x \\right)}}{7 \\pi} + \\frac{1}{2}\\)\n\n\n\ns3 = s.truncate(n = 3)\ns6 = s.truncate(n = 6)\ns9 = s.truncate(n = 9)\n\np = plot(f, s3, s6, s9, (x, -1.5 *pi, 1.5 *pi), show=False, legend=True)\n\np[0].label = 'f'\np[1].label = 'n = 3'\np[2].label = 'n = 6'\np[3].label = 'n = 9'\n\np.show()\n\n\n\n\n\n\n\n\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nplt.rcParams['font.size'] = 10\n\nx_t1 = [-2*np.pi, -np.pi, -np.pi, 0, 0, np.pi, np.pi, 2*np.pi]\ny_t1 = [1, 1, 0, 0, 1, 1, 0, 0]\n\nx_t2 = [-np.pi, 0, 0, np.pi]\ny_t2 = [0, 0, 1, 1]\n\nx = np.linspace(-2 *np.pi, 2 *np.pi, 1000)\n\ndef example_plot(func, ns):\n    \n    fig, axes = plt.subplots(2, 2, figsize=(7.5, 7))\n    i = 0\n    for n in ns:\n        ax = axes[i//2, i%2]\n        ax.plot(x_t1, y_t1, 'b:')\n        ax.plot(x_t2, y_t2, 'g-', lw=3)\n        ax.plot(x, func(x, n), c='darkorange')\n        ax.set_xticks([-2.0 *np.pi, -np.pi, 0, np.pi, 2.0 *np.pi])\n        ax.set_xticklabels([r'$-2\\pi$', r'$-\\pi$', '0', \n          r'$\\pi$', r'$2\\pi$'])\n        ax.set_xlim(-2.0 *np.pi, 2.0 *np.pi)\n        ax.set_ylim(-0.2, 1.2)\n        ax.set_title(f\"$n=${n}\")\n    \n        i += 1\n\n\ndef rect_wave(x, n): \n    y = 0.5\n    for m in range(1, n +1):\n        y = y +2.0 /((2.0*m -1) *np.pi) *np.sin((2.0*m -1)*x)\n    return y \n    \nexample_plot(rect_wave, ns=[1, 3, 6, 9])\n\n\n\n\n\n\n\nFigure 11.1: Fourier Series\n\n\n\n\n\n\\(~\\)\n\nThe overshooting by the partial sums from the functional values near a point of discontinuity dose not smooth out but remains fairly constant. This behavior of a Fourier series near a point at which \\(f\\) is discontinuous is known as the Gibbs phenomenon\n\n\\(~\\)",
    "crumbs": [
      "**Second Semester**",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Orthogonal Functions and Fourier Series</span>"
    ]
  },
  {
    "objectID": "ch_12_Orthogonal_Functions_and_Fourier_Series.html#sec-12-3",
    "href": "ch_12_Orthogonal_Functions_and_Fourier_Series.html#sec-12-3",
    "title": "11  Orthogonal Functions and Fourier Series",
    "section": "11.3 Fourier Cosine and Sine Series",
    "text": "11.3 Fourier Cosine and Sine Series\n\n\n\n\n\n\nIf \\(\\,f\\,\\) is even, \\(~\\) then\n\\(\\displaystyle\\phantom{xx} \\int_{-a}^a f(x)\\,dx = 2\\int_0^a f(x)\\,dx\\)\nIf \\(\\,f\\,\\) is odd, \\(\\,\\)then\n\\(\\displaystyle\\phantom{xx} \\int_{-a}^a f(x)\\,dx = 0\\)\nThe Fourier series of an even function on the interval \\((-p,p)\\,\\) is the cosine series\n\\[ f(x)=\\frac{a_0}{2} +\\sum_{n=1}^\\infty a_n \\cos\\frac{n\\pi}{p}x \\]\n\\(\\text{where }\\,\\displaystyle a_0=\\frac{2}{p} \\int_0^p f(x)\\,dx,\\; a_n=\\frac{2}{p} \\int_0^p f(x) \\cos\\frac{n\\pi}{p}x \\,dx\\)\nThe Fourier series of an odd function on the interval \\((-p,p)\\,\\) is the sine series\n\\[ f(x)=\\sum_{n=1}^\\infty b_n \\sin\\frac{n\\pi}{p}x\\;\\;\\;\\text{where}\\;\\displaystyle b_n=\\frac{2}{p} \\int_0^p f(x) \\sin\\frac{n\\pi}{p}x \\,dx \\]\n\n\\(~\\)\nExample\n\\[\n  \\begin{aligned}\n    f(x) &= x, \\;\\;0 &lt; x &lt; 2 \\\\\n      &\\Downarrow \\\\\n    f(x) &=\n     \\frac{4}{\\pi}\\sum_{n=1}^\\infty\n       \\frac{(-1)^{n +1}}{n} \\sin{\\frac{n\\pi}{2}x} \\\\\n      &\\,\\text{or} \\\\\n    f(x) &= 1 -\\frac{8}{\\pi^2}\\sum_{m=1}^\\infty\n      \\frac{1}{(2m -1)^2} \\cos{\\frac{(2m -1)\\pi}{2}x}\n  \\end{aligned}\\]\n\\(~\\)\n\nx_t1 = [-4,-2,-2, 2, 2, 4]\ny_t1 = [ 0, 2,-2, 2,-2, 0]\n\nx_t2 = [0, 2]\ny_t2 = [0, 2]\n    \nx = np.linspace(-4, 4, 1000)\n\ndef example_plot2(func, ns):\n    fig, axes = plt.subplots(2, 2, figsize=(7.5, 7))\n    i = 0\n    for n in ns:\n        ax = axes[i//2, i%2]    \n        ax.plot(x_t1, y_t1, 'b:') \n        ax.plot(x_t2, y_t2, 'g-', lw=3)          \n        ax.plot(x, func(x, n), c='darkorange') \n        ax.set_xlim(-4, 4)\n        ax.set_title(f\"$n=${n}\")\n    \n        i += 1\n\n\ndef lamp_sin(x, n): \n    y = 0\n    for m in range(1, n +1): \n        y = y +4.0 /np.pi *(-1)**(m +1)/m *np.sin(m *np.pi /2 *x)\n    return y \n    \nexample_plot2(lamp_sin, ns=[1, 3, 6, 9])\n\n\n\n\n\n\n\nFigure 11.2: Sine Series\n\n\n\n\n\n\nx_t1 = [-4,-2, 0, 2, 4]\ny_t1 = [ 0, 2, 0, 2, 0]\n\ndef lamp_cos(x, m):  \n    y = 1\n    for m in range(1, m +1): \n        y = y -8.0 /(np.pi *(2*m -1))**2.0 *np.cos((2*m -1) *np.pi /2 *x)\n    return y \n    \nexample_plot2(lamp_cos, ns=[1, 3, 6, 9])\n\n\n\n\n\n\n\nFigure 11.3: Cosine Series\n\n\n\n\n\n\\(~\\)\n\nHalf-Range Expansion\n\nThroughout the preceding discussion, \\(\\,\\)it was understood that a function \\(\\,f\\,\\) was defined on an interval with the origin as midpoint\nHowever, in many instances, \\(\\,\\)we are interested in representing a function that is defined only for \\(\\,0 &lt; x &lt; L~\\) by a trigonometric series\n\nThis can be done in many different ways by supplying an arbitrary definition of the function on the interval \\(-L &lt; x &lt; 0\\). \\(\\,\\)For brevity we condider three most important cases:\n\nReflect the graph of the function about the \\(y\\)-axis onto \\(-L &lt; x &lt; 0\\)\nReflect the graph of the function through the origin onto \\(-L &lt; x &lt; 0\\)\nDefine \\(f\\) on \\(-L &lt; x &lt; 0\\;\\) by \\(\\,f(x)=f(x+L)\\)",
    "crumbs": [
      "**Second Semester**",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Orthogonal Functions and Fourier Series</span>"
    ]
  },
  {
    "objectID": "ch_12_Orthogonal_Functions_and_Fourier_Series.html#sec-12-4",
    "href": "ch_12_Orthogonal_Functions_and_Fourier_Series.html#sec-12-4",
    "title": "11  Orthogonal Functions and Fourier Series",
    "section": "11.4 Complex Fourier Series",
    "text": "11.4 Complex Fourier Series\n\nComplex Fourier Series\nThe complex Fourier series of functions \\(\\,f\\) defined on an interval \\((-p,p)\\,\\) is given by\n\\[ f(x)=\\sum_{n=-\\infty}^\\infty c_n e^{\\frac{in\\pi}{p}x} \\]\nwhere\n\\[ c_n=\\frac{1}{2p}\\int_{-p}^p f(x) e^{-\\frac{in\\pi}{p}x}\\,dx \\]\nFundamental Frequency\nThe Fourier series define a periodic function and the fundamental period of that function is \\(T=2p\\). \\(\\,\\)Since \\(p=T/2\\),\n\\[ \\frac{a_0}{2} +\\sum_{n=1}^\\infty (a_n \\cos n\\omega x +b_n \\sin n\\omega x)\\;\\;\\]\n\\[\\text{and}\\]\n\\[ \\sum_{n=-\\infty}^\\infty c_n e^{in\\omega x}\\]\nwhere \\(\\,\\omega=2\\pi/T\\,\\) is called the fundamental angular frequency\nFrequency Spectrum\nIf \\(\\,f\\) is periodic and has fundamental period \\(T\\), \\(\\,\\)the plot of the points \\((n\\omega, |c_n|)\\) \\(\\,\\)is called the frequency spectrum of \\(\\;f\\)\n\n\\(~\\)\nExample \\(\\,\\) Expand \\(\\;f(x)=e^{-x}\\), \\(\\;-\\pi&lt;x&lt;\\pi\\;\\) in a complex Fourier series and find the frequency spectrum\n\\(~\\)",
    "crumbs": [
      "**Second Semester**",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Orthogonal Functions and Fourier Series</span>"
    ]
  },
  {
    "objectID": "ch_12_Orthogonal_Functions_and_Fourier_Series.html#sec-12-5",
    "href": "ch_12_Orthogonal_Functions_and_Fourier_Series.html#sec-12-5",
    "title": "11  Orthogonal Functions and Fourier Series",
    "section": "11.5 Sturm-Liouville Problem",
    "text": "11.5 Sturm-Liouville Problem\n\nSelf-Adjoint Form\n\\[\\begin{aligned}\n  a(x) \\frac{d^2 y}{dx^2}\n   +b(x) \\frac{dy}{dx}\n  &+\\left[ \\lambda c(x) +d(x) \\right] y\n   = 0,\\;\\;a(x)\\neq 0 \\\\\n  &\\Downarrow \\\\\n  {\\scriptsize \\frac{d}{dx}\\left[ \\exp\\left(\\int\\frac{b(x)}{a(x)}dx\\right)\\frac{dy}{dx} \\right ]}\n  &{\\scriptsize +\\left\\{ \\lambda \\frac{c(x)}{a(x)}\\exp\\left(\\int\\frac{b(x)}{a(x)}dx\\right)\n   +\\frac{d(x)}{a(x)}\\exp\\left(\\int\\frac{b(x)}{a(x)}dx\\right)\\right \\} y = 0} \\\\\n  &\\Downarrow \\\\     \n   \\color{red}{\\frac{d}{dx}\\left[r(x)\\frac{dy}{dx}\\right]}&\\color{red}{+\\left[\\lambda p(x) +q(x)\\right]y=0}\n\\end{aligned}\\]\nEigenvalues and Eigenfunctions\nAn orthogonal set of functions can be generated by solving a two-point boundary-value problem involving a linear second-order differential equation containing a parameter \\(\\lambda\\)\nThe boundary-value problem\n\\[ y''+\\lambda y =0, \\;y(0)=0, \\;y(L)=0~\\]\npossessed nontrivial solutions only when the parameter \\(\\lambda\\) took on the values\n\\[ \\lambda_n= \\left(\\frac{n\\pi}{L}\\right)^2, \\;\\;n=1,2,3,\\cdots \\]\ncalled eigenvalues. The corresponding nontrivial solutions\n\\[ y_n=c_n\\sin \\frac{n\\pi x}{L} \\]\nare called the eigenfunctions\nRegular Sturm-Liouville Problem\nLet \\(p\\), \\(q\\), \\(r\\), and \\(r'\\) be real-valued functions continuous on an interval \\([a,b]\\), \\(\\,\\)and let \\(r(x) &gt; 0~\\) and \\(\\,p(x)&gt;0~\\) for every \\(\\,x\\,\\) in the interval. \\(\\,\\)Then\n\\[ \\frac{d}{dx}\\left[r(x)\\frac{dy}{dx}\\right]+\\left[\\lambda p(x) +q(x)\\right]y=0 \\]\nsubject to\n\\[\\begin{aligned}\n  A_1 y(a) +B_1 y'(a)&= 0\\\\\n  A_2 y(b) +B_2 y'(b)&= 0\n\\end{aligned}\\]\nis said to be a regular Sturm-Liouville problem\nProperties of the Regular Sturm-Liouville problem\n\nThere are  an infinite number of real eigenvalues that can be arranged in increasing order \\(\\lambda_1&lt;\\lambda_2&lt;\\lambda_3&lt;\\cdots&lt;\\lambda_n&lt;\\cdots\\;\\;\\) such that \\(\\lambda_n \\to \\infty\\;\\) as \\(n\\to\\infty\\)\nFor each eigenvalue \\(\\lambda_i\\), \\(~\\)there is only one eigenfunction (except for nonzero constant multiples)\nEigenfunctions corresponding to different eigenvalues are linearly independent\nThe set of eigenfunctions corresponding to the set of eigenvalues is orthogonal with respect to the weight function \\(p(x)\\) on the interval \\([a,b]\\)\n\nProof of 4\n\\[\\scriptstyle   \n  \\begin{aligned}\n   \\frac{d}{dx}\\left[r(x)\\frac{dy_m}{dx}\\right]\n   &+\\left(\\lambda_m p(x) +q(x)\\right) y_m \\;=\\; 0 \\\\\n   \\frac{d}{dx}\\left[r(x)\\frac{dy_n}{dx}\\right]\n   &+\\left(\\lambda_n p(x) +q(x)\\right) y_n \\;=\\; 0 \\\\\n   &\\Downarrow \\\\\n   \\left( \\lambda_m -\\lambda_n \\right ) p(x) y_m y_n \\;\n   &=\\; y_m \\frac{d}{dx}\\left[r(x)\\frac{dy_n}{dx}\\right] -y_n \\frac{d}{dx}\\left[r(x)\\frac{dy_m}{dx}\\right] \\\\\n   &\\Downarrow \\\\\n   \\left( \\lambda_m -\\lambda_n \\right) \\int_a^b p(x) y_m y_n \\,dx \\;\n   & =\\; \\left.y_m r(x) \\frac{d y_n}{dx} \\right|_a^b\n    \\;-\\;\\int_a^b \\frac{d y_m}{dx} r(x) \\frac{d y_n}{dx} \\,dx \\;-\\;\\left.y_n r(x) \\frac{d y_m}{dx} \\right|_a^b\n    \\;+\\;\\int_a^b \\frac{d y_n}{dx} r(x) \\frac{d y_m}{dx} \\,dx \\\\\n   & =\\; r(b)\\left[ y_m(b) y_n'(b) - y_n(b) y_m'(b) \\right ] \\;-\\;r(a)\\left[ y_m(a) y_n'(a) - y_n(a) y_m'(a) \\right ] \\\\\n   & =\\; 0 \\\\\n   &\\Downarrow \\\\\n   \\int_a^b p(x) y_m y_n \\,dx \\;\n   & =\\; 0, \\;\\;\\lambda_m \\neq \\lambda_n\n\\end{aligned}\\]\n\n\\(~\\)\nExample \\(\\,\\) Solve the boundary-value problem\n\\[ y'' +\\lambda y=0, \\;\\;y(0)=0, \\;\\;y(1) +y'(1)=0 \\]\nSolution\n\\[\n\\begin{aligned}\n  y &= c_1\\cos\\sqrt{\\lambda}\\,x\n      +c_2\\sin\\sqrt{\\lambda}\\,x \\\\ \\\\\n    &\\;\\Downarrow \\;y(0)=0,\\;y(1)+y'(1)=0 \\\\ \\\\\n     c_1\\cos 0 &+c_2\\sin 0 = 0 \\;\\rightarrow \\;c_1=0 \\\\\n     c_2\\sin\\sqrt{\\lambda}\n    &+c_2\\sqrt{\\lambda}\\cos\\sqrt{\\lambda}=0\n     \\;\\rightarrow \\;c_2 \\neq 0, \\;\\tan\\sqrt{\\lambda}=-\\sqrt{\\lambda}\n\\end{aligned}\\]\n\\(~\\)\n\nx = np.linspace(0, 4*np.pi, 1000)  \ny = np.tan(x)\n\nthreshold = 100\ny[y &gt; threshold] = np.inf\ny[y &lt;-threshold] = np.inf\n\ndef examle_plot3(eigenvalues):\n    plt.figure(figsize=(6, 5))\n    plt.plot(x, y, linewidth=1.2, color=\"blue\")\n    plt.plot(x,-x, linewidth=1.2, color=\"green\")\n    plt.scatter(eigenvalues,-eigenvalues, color='red')\n    plt.xticks([0, np.pi, 2.0*np.pi, 3.0*np.pi, 4.0*np.pi], \n        ['0', r'$\\pi$', r'$2\\pi$', r'$3\\pi$', r'$4\\pi$'], \n        fontsize=16)\n    plt.yticks(fontsize=16)\n    plt.xlim(0, 4.0*np.pi)\n    plt.ylim(-15, 15)\n    plt.grid()\n    plt.show()\n\n\nfrom scipy import optimize\n\nsqrt_eigenvalues = np.zeros(4)\nfor i in range(4):\n    sqrt_eigenvalues[i] = np.round(optimize.brentq(\n      lambda x : np.tan(x) +x, \n      (i +0.5001) *np.pi, \n      (i +0.9999) *np.pi), 4)\n\nprint(f'{sqrt_eigenvalues =}')\n\nsqrt_eigenvalues =array([ 2.0288,  4.9132,  7.9787, 11.0855])\n\n\n\nexamle_plot3(sqrt_eigenvalues)\n\n\n\n\n\n\n\n\n\ndef example_plot3_(x, n_eig, sqrt_eig, eigfunc):\n\n    plt.figure(figsize=(6, 5))\n    for n in range(n_eig):\n        plt.plot(x, eigfunc(x, sqrt_eig[n]), label='$X_%d(x)$' % (n +1))\n     \n    plt.xlim(0, 1)\n    plt.ylim(-1.2, 1.2)\n    plt.grid(ls=':')    \n    plt.legend(loc='upper left', bbox_to_anchor=(1, 1.03))\n    plt.xlabel('$x$')\n    plt.ylabel('$X_n(x)$')\n    plt.title(r'$X_n(x)=\\sin\\sqrt{\\lambda_n} x$')\n    plt.show()\n\ndef eigfunc(x, sqrt_eig):  \n    return np.sin(sqrt_eig *x)\n\nx = np.linspace(0, 1, 150)\nexample_plot3_(x, len(sqrt_eigenvalues), sqrt_eigenvalues, eigfunc)\n\n\n\n\n\n\n\nFigure 11.4: Eigenfunctions\n\n\n\n\n\n\\(~\\)\n\nSingular Sturm-Liouville Problem\nThere are several important conditions under which we seek nontrivial solutions\n\n\\(r(a)=0\\;\\) and \\(\\;A_2y(b)+B_2y'(b)=0\\)\n\\(A_1y(a)+B_1y'(a)=0\\;\\) and \\(\\;r(b)=0\\)\n\\(r(a)=r(b)=0\\;\\) and no boundary condition is specified at either \\(x=a\\;\\) or \\(\\;x=b\\)\n\\(r(a)=r(b)\\;\\) and \\(\\,y(a)=y(b)\\), \\(\\;y'(a)=y'(b)\\)\n\n\\[\\scriptsize\n\\begin{aligned}\n   \\left( \\lambda_m -\\lambda_n \\right ) \\int_a^b p(x) y_m y_n \\,dx \\,\n   & \\,= r(b)\\left[ y_m(b) y_n'(b) - y_n(b) y_m'(b) \\right ] \\\\\n   & \\text{ }-r(a)\\left[ y_m(a) y_n'(a) - y_n(a) y_m'(a) \\right ] = 0 \\\\\n   & \\Downarrow \\\\\n    \\int_a^b p(x) y_m y_n \\,dx \\,\n   & \\,= 0, \\;\\;\\lambda_m \\neq \\lambda_n\n\\end{aligned}\\]\nParametric Bessel Equation:\n\\[ x^2y'' +xy' +(\\color{red}{\\alpha^2}x^2 -\\color{blue}{n^2})y=0,\\;\\; n=0,1,2,\\cdots\\]\n\\[ \\frac{d}{dx}\\left[xy'\\right] +\\left(\\alpha^2x -\\frac{n^2}{x}\\right)y=0 \\]\nLegendre Equation:\n\\[(1-x^2)y''-2xy'+\\color{red}{n(n+1)}y=0,\\;\\; n=0,1,2,\\cdots \\]\n\\[ \\frac{d}{dx}\\left[(1-x^2)y'\\right] +n(n +1)y = 0 \\]\nConsider \\(\\,y'' +\\lambda y =0~\\) subject to the periodic boundary condition \\(~y(-L)=y(L)\\), \\(\\,y'(-L)=y'(L)\\). \\(\\,\\)Show that the eigenfunctions are\n\\[ \\left\\{ 1,\\cos\\frac{\\pi}{L}x, \\cos\\frac{2\\pi}{L}x, \\cdots, \\sin\\frac{\\pi}{L}x, \\sin\\frac{2\\pi}{L}x, \\,\\cdots \\right\\}\\]\nThis set, \\(\\,\\)which is orthogonal on \\(\\,[-L,L]\\), \\(\\,\\)is the basis for the Fourier Series\n\n\\(~\\)",
    "crumbs": [
      "**Second Semester**",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Orthogonal Functions and Fourier Series</span>"
    ]
  },
  {
    "objectID": "ch_12_Orthogonal_Functions_and_Fourier_Series.html#sec-12-6",
    "href": "ch_12_Orthogonal_Functions_and_Fourier_Series.html#sec-12-6",
    "title": "11  Orthogonal Functions and Fourier Series",
    "section": "11.6 Bessel and Legendre Series",
    "text": "11.6 Bessel and Legendre Series\n\n11.6.1 Fourier-Bessel Series\n\nThe parametric Bessel differential equation is\n \\[ \\frac{d}{dx}\\left[xy'\\right] +\\left(\\alpha^2x -\\frac{n^2}{x}\\right)y=0 \\] \nin which \\(~r(x)=x\\), \\(~p(x)=x\\), \\(~q(x)=-n^2/x\\), \\(\\,\\)and \\(~\\lambda=\\alpha^2\\). \\(\\,\\)The general solution of this equation is\n\\[ \\color{blue}{y=c_1J_n(\\alpha x)} +c_2Y_n(\\alpha x) \\]\nNow \\(r(0)=0\\), \\(\\,\\)and of two solutions \\(J_n(\\alpha x)\\) and \\(Y_n(\\alpha x)\\), \\(\\,\\) only \\(J_n(\\alpha x)\\) is bounded at \\(x=0\\)\nThe eigenvalues \\(\\lambda_i=\\alpha_i^2, \\;i=1,2,3,\\cdots,\\) \\(\\,\\)are defined by means of a boundary condition at \\(x=b\\):\n \\[A_2J_n(\\alpha b) +B_2\\alpha J_n'(\\alpha b) = 0 \\] \nFor any choice of \\(A_2\\) and \\(B_2\\), \\(\\,\\)not both zero, \\(\\,\\)it is known that the above boundary condition gives an infinite number of roots \\(x_i=\\alpha_i b\\). The eigenvalues are then \\(\\lambda_i=\\alpha_i^2=(x_i/b)^2\\) \nDifferential Recurrence Relations\n\\[\n\\begin{aligned}\nxJ_n'(x)&=xJ_{n-1}(x) -nJ_n(x)\\\\\nxJ_n'(x)&=nJ_n(x)-xJ_{n+1}(x)\\\\\n&\\Downarrow \\\\\n\\frac{d}{dx}\\left[ x^n J_n(x) \\right ]&= x^n J_{n-1}(x)\\\\\n\\frac{d}{dx}\\left[ x^{-n} J_n(x) \\right ]&=-x^{-n} J_{n+1}(x)\n\\end{aligned}\\]\nSquare Norm\n\\[\n\\begin{aligned}\n  \\frac{d}{dx}\\left[ xy' \\right] &+\\left( \\alpha^2x -\\frac{n^2}{x} \\right)y = 0\\\\\n  &\\Downarrow \\;\\times \\;2xy' \\\\\n  \\frac{d}{dx}\\left[ xy' \\right]^2 &+\\left( \\alpha^2x^2 -n^2 \\right) \\frac{d}{dx}[y]^2 =0\\\\\n  &\\Downarrow {\\tiny \\text{integrating by parts on}\\; [0,b]}\\\\\n  \\left[ \\left[xy' \\right]^2 +(\\alpha^2x^2 -n^2)y^2 \\right]_0^b &=2\\alpha^2 \\int_0^b xy^2\\,dx \\\\\n  &\\Downarrow \\; {\\tiny y=J_n(\\alpha x),\\;y'=\\alpha J_n'(\\alpha x), \\;J_n(0)=0\\,\\;\\text{for}\\; n&gt;0} \\\\\n  \\color{blue}{2\\alpha^2 \\int_0^b xJ_n^2(\\alpha x)\\,dx}\\; &\\color{blue}{= \\alpha^2b^2 \\left[ J_n'(\\alpha b) \\right]^2\n  +\\left(\\alpha^2b^2 -n^2\\right)\\left[ J_n(\\alpha b) \\right]^2}\n\\end{aligned}\\]\nConsider three cases of the boundary condition\n\\[A_2J_n(\\alpha b) +B_2\\alpha J_n'(\\alpha b) = 0\\]\n\n\\(~\\)\n\nCase I :  \\(~~A_2=1, \\,B_2 = 0 \\Rightarrow J_n(\\alpha b)=0\\;\\) \\(\\Rightarrow\\) \\(\\;\\lambda_i=\\alpha_i^2 =(x_i/b)^2 &gt;0\\)\n\\[\n\\begin{aligned}\n    2\\alpha_i^2 \\int_0^b xJ_n^2(\\alpha_i x)\\,dx &= \\alpha_i^2b^2 \\left[ J_n'(\\alpha_i b) \\right]^2 +(\\alpha_i^2b^2 -n^2)\\left[ J_n(\\alpha_i b) \\right]^2 \\\\\n    &\\Downarrow \\;xJ_n'(x)=n\\underbrace{J_n(x)}_{=0} -xJ_{n+1}(x) \\\\\n    \\color{blue}{\\| J_n(\\alpha_i x) \\|^2 = \\int_0^b xJ_n^2(\\alpha_i x)}&\\color{blue}{\\,dx\\; =\\frac{b^2}{2} J_{n+1}^2(\\alpha_i b)}\n\\end{aligned}\\]\n\n\\(~\\)\n\nfrom scipy.special import jv, jn_zeros\nfrom mpl_toolkits.axisartist.axislines import SubplotZero\n\ndef make_plot_form():\n\n    fig = plt.figure(figsize=(7, 8))\n    ax1 = SubplotZero(fig, 211)\n    ax2 = SubplotZero(fig, 212)\n\n    fig.add_subplot(ax1)\n    ax1.axis[\"left\", \"right\", \"bottom\", \"top\"].set_visible(False)\n    ax1.axis[\"xzero\", \"yzero\"].set_visible(True)\n    ax1.axis[\"xzero\", \"yzero\"].set_axisline_style(\"-|&gt;\")\n\n    ax1.set_xticks([2, 4, 6, 8, 10])\n    ax1.text(12, -0.03, r'$\\alpha$', fontsize=14)\n\n    fig.add_subplot(ax2)\n    ax2.axis[\"left\", \"right\", \"bottom\", \"top\"].set_visible(False)\n    ax2.axis[\"xzero\", \"yzero\"].set_visible(True)\n    ax2.axis[\"xzero\", \"yzero\"].set_axisline_style(\"-|&gt;\")\n\n    ax2.set_xticks([0.2, 0.4, 0.6, 0.8, 1.0])\n    ax2.text(1.1, -0.03, r'$x$', fontsize=14)\n\n    r1 = np.linspace(0, 11, 100) \n    r2 = np.linspace(0, 1, 100)       \n\n    return ax1, ax2, r1, r2\n\ndef make_plot(case_no, ax1, ax2, r1, r2, mm, bc_txt, eigfn_txt):\n\n    alpha_0m = eval_alpha(mm)\n\n    y1 = eval_bc(r1)\n    ax1.plot(r1, y1, label=bc_txt)\n    ax1.plot(alpha_0m, np.zeros(mm), 'ro')\n\n    for m in range(mm):\n        ax1.text(alpha_0m[m] -0.2, 0.1, rf'$\\alpha_{m +1}$', \n            fontsize=12)            \n    ax1.legend(bbox_to_anchor=(1, 1), loc=\"upper left\")\n\n    x_zeros = eval_x_zeros(mm)\n    for m in range(mm):   \n        y2 = eval_eigfn(alpha_0m[m]*r2)\n        ax2.plot(r2, y2, label=eigfn_txt +rf'$(\\alpha_{m +1}x)$')\n        if case_no == 1:\n          ax2.plot(x_zeros[:m +1] /alpha_0m[m], np.zeros(m +1), 'x')\n        elif case_no == 2:\n          ax2.plot(x_zeros[:m] /alpha_0m[m], np.zeros(m), 'x')\n        elif case_no == 3:\n          if m != 0:\n            ax2.plot(x_zeros[:m] /alpha_0m[m], np.zeros(m), 'x')\n \n    ax2.legend(bbox_to_anchor=(1, 1), loc=\"upper left\")\n\n\ndef eval_eigfn(x):\n    return jv(0, x)\n\ndef eval_bc(alpha):\n    return eval_eigfn(alpha)\n\ndef eval_alpha(mm):\n    return jn_zeros(0, mm)\n\ndef eval_x_zeros(mm):\n    return eval_alpha(mm)\n\ndef example_plot4():\n\n    ax1, ax2, r1, r2 = make_plot_form()\n\n    ax1.set_ylim(-0.5, 1.1)\n    ax1.set_yticks([-0.5, 0.5, 1.0])\n\n    ax2.set_ylim(-0.5, 1.1)   \n    ax2.set_yticks([-0.5, 0.5, 1.0])\n\n    mm = 3\n\n    make_plot(1, ax1, ax2, r1, r2, mm, r'$J_0(\\alpha)$', '$J_0$')\n\n    plt.show()\n\nexample_plot4()\n\n\n\n\nCase I: \\(~n=0, A_2 = 1, B_2 = 0, b=1\\)\n\n\n\n\n\ndef eval_eigfn(x):\n    return jv(1, x)\n\ndef eval_bc(alpha):\n    return eval_eigfn(alpha)\n\ndef eval_alpha(mm):\n    return jn_zeros(1, mm)\n\ndef eval_x_zeros(mm):\n    return eval_alpha(mm)           \n\ndef example_plot4_():\n\n    ax1, ax2, r1, r2 = make_plot_form()  \n\n    ax1.set_ylim(-0.5, 1.1)\n    ax1.set_yticks([-0.5, 0.5, 1.0])\n\n    ax2.set_ylim(-0.5, 1.1)   \n    ax2.set_yticks([-0.5, 0.5, 1.0])\n\n    mm = 3\n\n    make_plot(1, ax1, ax2, r1, r2, mm, r'$J_1(\\alpha)$', '$J_1$')\n\n    plt.show()\n\nexample_plot4_()\n\n\n\n\nCase I: \\(~n=1, A_2 = 1, B_2 = 0, b=1\\)\n\n\n\n\n\\(~\\)\n\nCase II : \\(\\,\\)(\\(A_2=h, \\,B_2 = b \\Rightarrow h\\geq 0\\))\n \\[~ hJ_n(\\alpha b) +\\alpha bJ_n'(\\alpha b)=0 \\;\\Rightarrow \\;\\lambda_i=\\alpha_i^2 =\\left(\\frac{x_i}{b}\\right)^2 \\;\\text{ except for }\\;\nh=0\\; \\text{ and } \\; n=0 \\] \n\\[\n  \\begin{aligned}\n     2\\alpha_i^2 \\int_0^b xJ_n^2(\\alpha_i x)\\,dx\n      &= \\alpha_i^2b^2 \\left[ J_n'(\\alpha_i b) \\right]^2 +(\\alpha_i^2b^2 -n^2)\\left[ J_n(\\alpha_i b) \\right]^2 \\\\ \\\\\n     &\\Downarrow \\;\\alpha_ibJ_n'(\\alpha_i b)=-hJ_n(\\alpha_i b) \\\\ \\\\\n     \\color{blue}{\\| J_n(\\alpha_i x) \\|^2 = \\int_0^b xJ_n^2(\\alpha_i x)\\,dx} \\; & \\color{blue}{=\\frac{\\alpha_i^2b^2 -n^2 +h^2}{2\\alpha_i^2}\n     J_n^2(\\alpha_i b) }\n  \\end{aligned}\\]\n\n\\(~\\)\n\nfrom scipy.special import j0, j1\nfrom scipy.optimize import fsolve\n\ndef eval_eigfn(x):\n    return j0(x)\n\ndef eval_bc(alpha):\n    return j0(alpha) -alpha *j1(alpha)\n\ndef eval_alpha(mm):\n    alpha_0m = np.zeros(mm)\n    for i in range(mm):\n      alpha_0m[i] = fsolve(eval_bc, 1 +3*i)[0]\n    \n    return alpha_0m\n\ndef eval_x_zeros(mm):\n    return jn_zeros(0, mm -1)    \n\ndef example_plot4__():\n\n    ax1, ax2, r1, r2 = make_plot_form()  \n\n    ax1.set_ylim(-4, 4)\n    ax1.set_yticks([-3, -2, -1, 1, 2, 3])\n\n    ax2.set_ylim(-0.5, 1.2)   \n    ax2.set_yticks([-0.5, 0.5, 1.0])\n\n    mm = 4\n\n    make_plot(2, ax1, ax2, r1, r2, mm, \n        r\"$J_0(\\alpha) +\\alpha J_0'(\\alpha)$\", '$J_0$')    \n\n    plt.show()\n\nexample_plot4__()\n\n\n\n\nCase II: \\(~n=0, h = 1, b=1, \\;J_0(\\alpha) + \\alpha J_0' (\\alpha)=0\\)\n\n\n\n\n\\(~\\)\n\nCase III : \\(\\,\\)(\\(h=0\\) and \\(n=0\\)) :  \\(~~J_0'(\\alpha b)=0\\;\\) \nIt is the only situation for which \\(\\lambda=0\\) is an eigenvalue\n\\[\n\\begin{aligned}\n  \\color{blue}{\\alpha_1 = 0 \\rightarrow\n   \\| J_0(\\alpha_1 x) \\|^2}\\; &\\color{blue}{= \\int_0^b x\\,dx =\\frac{b^2}{2}} \\\\ \\\\  \n   2\\alpha_i^2 \\int_0^b xJ_0^2(\\alpha_i x)\\,dx &= \\alpha_i^2b^2 \\left[ J_0'(\\alpha_i b) \\right]^2 +(\\alpha_i^2b^2 -0^2)\\left[ J_0(\\alpha_i b) \\right]^2 \\\\\n   &\\Downarrow \\\\\n   \\color{blue}{\\alpha_i,\\; i=2,3,4,\\cdots \\rightarrow\n   \\| J_0(\\alpha_i x) \\|^2} \\; &\\color{blue}{ =\\int_0^b xJ_0^2(\\alpha_i x)\\,dx =\\frac{b^2}{2}J_0^2(\\alpha_i b)}\n\\end{aligned}\\]\n\n\\(~\\)\n\nfrom scipy.special import jnp_zeros\n\ndef eval_eigfn(x):\n    return j0(x)\n\ndef eval_bc(alpha):\n    return -j1(alpha)\n\ndef eval_alpha(mm):\n    return np.append([0], jnp_zeros(0, mm -1))\n\ndef eval_x_zeros(mm):\n    return jn_zeros(0, mm -1)      \n\ndef example_plot4___():\n\n    ax1, ax2, r1, r2 = make_plot_form()   \n  \n    ax1.set_ylim(-0.8, 0.5)   \n    ax1.set_yticks([-0.5, 0.5])\n  \n    ax2.set_ylim(-0.5, 1.2)   \n    ax2.set_yticks([-0.5, 0.5, 1.0])\n  \n    mm = 4\n\n    make_plot(3, ax1, ax2, r1, r2, mm, r\"$J_0'(\\alpha)$\", '$J_0$')    \n\n    plt.show()\n\nexample_plot4___()\n\n\n\n\nCase III: \\(~n=0, h = 0, b=1, \\;J_0'(\\alpha)=0\\)\n\n\n\n\n\\(~\\)\n\nThe set \\(\\{J_n(\\alpha_i x), \\;i=0,1,2,\\cdots\\}\\) is orthogonal with respect to the weight function \\(p(x)=x~\\) on \\(~[0, b]\\)\n \\[\\int_{0}^b x\\, J_n(\\alpha_i x) \\, J_n(\\alpha_j x)\\,dx=0,\\; i\\neq j\\] \nThe Fourier-Bessel series of a function \\(\\,f\\) defined on the interval \\((0, b)\\) is given by\n \\[ f(x)=\\sum_{i=1}^\\infty c_i J_n(\\alpha_i x)\\] \nwhere\n \\[ c_i=\\frac{\\displaystyle\\int_0^b x\\,f(x)\\,J_n(\\alpha_i x)\\,dx}{\\displaystyle\\int_0^b x \\, J_n^2(\\alpha_i x)\\,dx} =\\color{blue}{\\frac{1}{\\| J_n(\\alpha_i x)\\|^2}}\\int_0^b x\\,f(x) \\, J_n(\\alpha_i x)\\,dx \\]\n\n\\(~\\)\n\n\n11.6.2 Fourier-Legendre Series\n\nLegendre’s differential equation is\n\\[(1-x^2)y'' -2xy'+n(n+1)y=0\\;\\]\n\\[\\text{or}\\]\n \\[\\frac{d}{dx}\\left[ (1-x^2)y' \\right]+n(n+1)y=0\\] \nin which \\(~r(x)=1-x^2, \\,p(x)=1, \\,\\lambda=n(n+1),\\) and \\(~q(x)=0\\)\n The Legendre polymomials \\(P_n(x)\\) are the only solutions that are bounded on the interval \\([\\text{-}1, \\,1]\\) \n\n\\(~\\)\n\nfrom scipy.special import eval_legendre\n\nfig = plt.figure(figsize=(5, 5))\nax = SubplotZero(fig, 111)\n\nfig.add_subplot(ax)\nax.axis[\"left\", \"right\", \"bottom\", \"top\"].set_visible(False)\nax.axis[\"xzero\", \"yzero\"].set_visible(True)\nax.axis[\"xzero\", \"yzero\"].set_axisline_style(\"-|&gt;\")\n\nax.set_xticks([-1, -0.5, 0.5, 1], ['-1', '', '', '1'])\nax.set_yticks([-1, -0.5, 0.5, 1], ['-1', '', '', '1'])\nax.grid(ls=':')\nax.text(-0.1, 1.3, r'$P_n(x)$', fontsize=14)\nax.text(1.3, -0.05, r'$x$', fontsize=14)\nax.set_xlim(-1.2, 1.2)  \nax.set_ylim(-1.2, 1.2)  \n\nx = np.linspace(-1, 1, 100)\nfor n in range(5):\n    y = eval_legendre(n, x)\n    ax.plot(x, y, label=f'$P_{n}(x)$')\n\nax.legend(loc='lower left', bbox_to_anchor=(1, 0))\n\nplt.show()\n\n\n\n\n\n\n\nFigure 11.5: Legendre Polynomials\n\n\n\n\n\n\\(~\\)\n\nThe set \\(\\{P_n(x), \\;n=0,1,2,\\cdots\\}\\) is orthogonal with respect to the weight function \\(p(x)=1\\) on \\([\\text{-}1,\\,1]\\)\n \\[\\int_{-1}^1 P_m(x)P_n(x)\\,dx=0,\\; m\\neq n\\] \nThe Fourier-Legendre series of a function \\(\\,f\\) defined on the interval \\((\\text{-}1,1)\\) is given by\n \\[ f(x)=\\sum_{n=0}^\\infty c_n P_n(x)\\] \nwhere\n \\[ c_n=\\frac{\\displaystyle\\int_{-1}^1 f(x)P_n(x)\\,dx}{\\displaystyle\\int_{-1}^1 P_n^2(x)\\,dx} =\\color{blue}{\\frac{2n+1}{2}}\\int_{-1}^1 f(x)P_n(x)\\,dx \\] \n\n\n\n11.6.3 Other Important Equations\n\\(~\\)\n\nHermite\n\\[y'' -2x y' +2n y=0\\]\nLaguerre\n\\[xy'' +(1-x)y' +ny=0\\]\nChebyshev\n\\[(1-x^2) y'' -xy' +n^2y = 0\\]\nHypergeometric\n\\[x(1-x)y'' +[c - (a + b +1)x y'] -ab y = 0\\]\n\n\\(~\\)",
    "crumbs": [
      "**Second Semester**",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Orthogonal Functions and Fourier Series</span>"
    ]
  },
  {
    "objectID": "ch_12_Orthogonal_Functions_and_Fourier_Series.html#worked-exercises",
    "href": "ch_12_Orthogonal_Functions_and_Fourier_Series.html#worked-exercises",
    "title": "11  Orthogonal Functions and Fourier Series",
    "section": "Worked Exercises",
    "text": "Worked Exercises\n1. \\(~\\) Suppose the function \\(y=f(x)\\), \\(0 &lt; x &lt; L\\), given in figure is expanded in a cosine series, in a sine series, and in a Fourier series. Sketch the periodic extension to which each series converges\n\n\\[f(x) = \\begin{cases}\n0 & 0 \\displaystyle \\leq x &lt; \\frac{L}{2} \\\\\n\\displaystyle -\\sin \\frac{2\\pi x}{L} & \\displaystyle \\frac{L}{2} \\leq x \\leq L\n\\end{cases}\\]\nSolution\n1. Cosine Series (Even Extension)\n\nA cosine series is the Fourier cosine series, which assumes that the function is extended evenly about \\(x=0\\)\nThe extended function is even, meaning \\(f(-x) = f(x)\\)\nThe domain is extended to \\([-L, L]\\) and periodically repeated with period \\(2L\\)\n\nSo the full even extension on \\([-L, L]\\):\n\\[f_{\\text{even}}(x) = \\begin{cases}\n\\sin\\left(\\frac{2\\pi x}{L}\\right) & -L \\le x &lt; -\\frac{L}{2} \\\\\n0 & -\\frac{L}{2} \\le x &lt; \\frac{L}{2} \\\\\n-\\sin\\left(\\frac{2\\pi x}{L}\\right) & \\frac{L}{2} \\le x \\le L\n\\end{cases}\\]\nThen repeat with period \\(2L\\)\n2. Sine Series (Odd Extension)\n\nA sine series extends the function oddly about \\(x=0\\)\nThe extended function is odd, so \\(f(-x) = -f(x)\\)\nDefined on \\([-L, L]\\) and repeated every \\(2L\\)\n\nSo the odd extension is:\n\\[f_{\\text{odd}}(x) = \\begin{cases}\n\\sin\\left(\\frac{2\\pi x}{L}\\right) & -L \\le x &lt; -\\frac{L}{2} \\\\\n0 & -\\frac{L}{2} \\le x &lt; \\frac{L}{2} \\\\\n-\\sin\\left(\\frac{2\\pi x}{L}\\right) & \\frac{L}{2} \\le x \\le L\n\\end{cases}\\]\nThen repeated every \\(2L\\)\n3. Full Fourier Series (Periodic Extension)\n\nThe full Fourier series uses both sine and cosine terms\nThe function is extended periodically with period \\(L\\)\n\nThis shape repeats every \\(L\\) units\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Define function f(x) on [0, L]\nL = 2 * np.pi  # for easy visualization, let L = 2π\nx = np.linspace(-2*L, 2*L, 2000)\n\n# Define the base function f(x) on [0, L]\ndef f_base(x):\n    x_mod = x % L\n    return np.where(\n        x_mod &lt; L/2,\n        0,\n        -np.sin(2 * np.pi * x_mod / L)\n    )\n\n# Even extension (cosine series)\n\ndef f_even(x):\n    # Reflect across x = 0 and repeat with period 2L\n    x_mod = np.mod(x, 2*L)\n    reflected = np.where(x_mod &lt; L, x_mod, 2*L - x_mod)\n    return f_base(reflected)\n\n# Odd extension (sine series)    \n\ndef f_odd(x):\n    # Reflect and invert across x = 0, repeat with period 2L\n    x_mod = np.mod(x, 2*L)\n    reflected = np.where(x_mod &lt; L, x_mod, 2*L - x_mod)\n    sign = np.where(x_mod &lt; L, 1, -1)\n    return sign * f_base(reflected)\n\n# Full periodic extension (Fourier series)\ndef f_periodic(x):\n    return f_base(x)\n\n# Redefine x-axis from -L to 2L\nx_zoom = np.linspace(-L, 2*L, 1000)\n\nxticks = [-L, -L/2, 0, L/2, L, 3*L/2, 2*L]\nxtick_labels = [r\"$-L$\", r\"$-\\frac{L}{2}$\", \n  r\"$0$\", r\"$\\frac{L}{2}$\", r\"$L$\", r\"$\\frac{3L}{2}$\", r\"$2L$\"]\n\n# Plotting with labeled ticks\nfig, axs = plt.subplots(3, 1, figsize=(6, 8), sharex=True)\n\naxs[0].plot(x_zoom, f_even(x_zoom), color='blue')\naxs[0].set_title('Even Extension (Cosine Series)')\naxs[0].grid(True)\naxs[0].set_xlim([-L, 2*L])\naxs[0].set_ylim([-1.1, 1.1])\n\naxs[1].plot(x_zoom, f_odd(x_zoom), color='green')\naxs[1].set_title('Odd Extension (Sine Series)')\naxs[1].grid(True)\naxs[1].set_xlim([-L, 2*L])\naxs[1].set_ylim([-1.1, 1.1])\n\naxs[2].plot(x_zoom, f_periodic(x_zoom), color='red')\naxs[2].set_title('Periodic Extension (Full Fourier Series)')\naxs[2].grid(True)\naxs[2].set_xlim([-L, 2*L])\naxs[2].set_ylim([-1.1, 1.1])\n\n# Set common x-axis ticks and labels\naxs[2].set_xticks(xticks)\naxs[2].set_xticklabels(xtick_labels, fontsize=12)\nplt.xlabel('x')\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\\(~\\)\n2. \\(~\\) Find the half-range cosine and sine expansions of the given function:\n\\[ f(x)=x(2-x), \\;\\;0 &lt; x &lt; 2\\]\nSolution\n1. Half-Range Cosine Series (Even Extension)\nWe write \\(f(x)\\) on \\(0 &lt; x &lt; 2\\) as a cosine series:\n\\[f(x) \\sim \\frac{a_0}{2} + \\sum_{n=1}^{\\infty} a_n \\cos\\left(\\frac{n\\pi x}{L}\\right)\\] where \\(L = 2\\), and the coefficients are:\n\\[a_0 = \\frac{1}{L} \\int_0^L f(x) \\, dx = \\frac{1}{2} \\int_0^2 x(2 - x) \\, dx\\]\n\\[a_n = \\frac{1}{L} \\int_0^L f(x) \\cos\\left(\\frac{n\\pi x}{L}\\right) \\, dx = \\frac{1}{2} \\int_0^2 x(2 - x) \\cos\\left(\\frac{n\\pi x}{2}\\right) \\, dx\\]\n\nCompute \\(a_0\\):\n\n\\[a_0 = \\frac{1}{2} \\int_0^2 (2x - x^2) \\, dx = \\frac{1}{2} \\left[ x^2 - \\frac{x^3}{3} \\right]_0^2 = \\frac{1}{2} \\left(4 - \\frac{8}{3} \\right) = \\frac{2}{3}\\]\n\nCompute \\(a_n\\):\n\nWe split the integral:\n\\[\\begin{aligned}\na_n &= \\frac{1}{2} \\left[ 2 \\int_0^2 x \\cos\\left(\\frac{n\\pi x}{2}\\right) dx - \\int_0^2 x^2 \\cos\\left(\\frac{n\\pi x}{2}\\right) dx \\right] \\\\\n&= \\frac{-4 \\left[(-1)^n + 1\\right]}{\\pi^2 n^2}\n\\end{aligned}\\]\nTherefore, the final simplified form is:\n\\[a_n =\n\\begin{cases}\n\\displaystyle \\frac{-8}{\\pi^2 n^2}, & \\text{if } n \\text{ is even} \\\\\n0, & \\text{if } n \\text{ is odd}\n\\end{cases}\n\\quad \\text{for } n \\ge 1\\]\n\\(~\\)\n2. Half-Range Sine Series (Odd Extension)\nWe write \\(f(x)\\) on \\(0 &lt; x &lt; 2\\) as a sine series:\n\\[f(x) \\sim  \\sum_{n=1}^{\\infty} a_n \\sin\\left(\\frac{n\\pi x}{L}\\right)\\] where \\(L = 2\\), and the coefficients are:\n\\[b_n = \\frac{1}{L} \\int_0^L f(x) \\sin\\left(\\frac{n\\pi x}{L}\\right) \\, dx = \\frac{1}{2} \\int_0^2 x(2 - x) \\sin\\left(\\frac{n\\pi x}{2}\\right) \\, dx\\]\nThe exact expression for the sine series coefficients \\(b_n\\) is:\n\\[b_n =\n\\begin{cases}\n\\displaystyle \\frac{-4\\pi n \\sin(n\\pi) - 8\\cos(n\\pi) + 8}{\\pi^3 n^3}, & n \\ne 0 \\\\\n0, & n = 0\n\\end{cases}\\]\nSo the final simplified form is:\n\\[b_n =\n\\begin{cases}\n\\displaystyle \\frac{16}{\\pi^3 n^3}, & \\text{if } n \\text{ is odd} \\\\\n0, & \\text{if } n \\text{ is even}\n\\end{cases}\n\\quad \\text{for } n \\ge 1\\]\n\\(~\\)\n3. \\(~\\) (a) Find the eigenvalues and eigenfunctions of the boundary value problem\n\\[ y'' +2y' +(\\lambda +1) y =0, \\;\\;y(0)=0, \\;y(1)=0\\]\n\nPut the differential equation in self-adjoint form, (c) Give an orthogonality relation\n\nSolution (a)\nStep 1: \\(~\\) Solve the ODE\nGiven:\n\\[y''+ 2y' + (\\lambda + 1)y = 0\\]\nLet’s reduce it to a homogeneous constant-coefficient equation.\nThe characteristic equation is:\n\\[p^2 + 2p + (\\lambda + 1) = 0\n\\; \\Rightarrow \\; p = -1 \\pm \\sqrt{1 - \\lambda}\\]\nWe’ll split into cases based on the sign of \\(1 - \\lambda\\):\nStep 2.1: \\(~\\lambda &gt; 1\\)\nLet \\(\\lambda = 1 + \\mu^2\\) with \\(\\mu &gt; 0\\)\nThen:\n\\[p = -1 \\pm i\\mu\\]\nThe general solution is:\n\\[y(x) = e^{-x} \\left[ A \\cos(\\mu x) + B \\sin(\\mu x) \\right]\\]\nApply boundary conditions:\n\n\\(y(0) = e^{0}\\,A = A = 0 \\Rightarrow A = 0\\)\n\\(y(1) = e^{-1} B \\sin(\\mu) = 0\\)\n\nSince \\(e^{-1} \\ne 0\\) and \\(B \\ne 0\\) (nontrivial solution), we require:\n\\[\\sin(\\mu) = 0 \\; \\Rightarrow \\; \\mu = n\\pi, \\; n = 1, 2, 3, \\dots\\]\nSo:\n\\[\\lambda_n = 1 + \\mu^2 = 1 + n^2 \\pi^2\\]\nAnd the corresponding eigenfunctions:\n\\[y_n(x) = e^{-x} \\sin(n\\pi x)\\]\n\\(~\\)\nStep 2.2: \\(~\\lambda &lt; 1\\)\nThe characteristic equation is:\n\\[p^2 + 2p + (\\lambda + 1) = 0 \\Rightarrow p = -1 \\pm \\sqrt{1 - \\lambda}\\]\nSince \\(\\lambda &lt; 1\\), the discriminant \\(\\sqrt{1 - \\lambda}\\) is real and positive, so the roots are real and distinct. Let:\n\\[\\alpha = \\sqrt{1 - \\lambda} &gt; 0\\]\nThen the general solution is:\n\\[y(x) = A e^{(-1 + \\alpha)x} + B e^{(-1 - \\alpha)x}\\]\nApply boundary conditions:\n\n\\(y(0) = A + B = 0 \\Rightarrow B = -A\\)\n\nSo:\n\\[y(x) = A \\left(e^{(-1 + \\alpha)x} - e^{(-1 - \\alpha)x} \\right)\\]\n\nApply \\(y(1) = 0\\):\n\n\\[y(1) = A\\left(e^{-1 + \\alpha} - e^{-1 - \\alpha} \\right) = 0\n\\Rightarrow e^{-1 + \\alpha} = e^{-1 - \\alpha} \\Rightarrow \\alpha = 0\\]\nBut we assumed \\(\\alpha &gt; 0\\), so this is a contradiction. For \\(\\lambda &lt; 1\\), the only solution satisfying the boundary conditions is the trivial solution \\(y(x) = 0\\) . Therefore, no eigenvalue exists for \\(\\lambda &lt; 1\\)\n\\(~\\)\nStep 2.3: \\(~\\lambda = 1\\)\nIn this case:\n\\[p = -1 \\pm \\sqrt{1 - \\lambda} = -1 \\pm 0 = -1 \\quad \\text{(repeated root)}\\]\nThe general solution becomes:\n\\[y(x) = (A + Bx) e^{-x}\\]\nApply boundary conditions:\n\n\\(y(0) = A = 0 \\Rightarrow y(x) = Bx e^{-x}\\)\n\\(y(1) = Be^{-1} = 0 \\Rightarrow B = 0\\)\n\nSo again, the only solution is the trivial one \\(\\Rightarrow\\) no eigenvalue at \\(\\lambda = 1\\)\nConclusion\nEigenvalues only exist for \\(\\lambda &gt; 1\\), specifically \\(\\lambda_n = 1 + n^2 \\pi^2,\\; n = 1, 2, 3, \\dots\\). The corresponding eigenfunctions are \\(y_n(x) = e^{-x} \\sin(n\\pi x)\\)\n\\(~\\)\nSolution (b)\nWe are given:\n\\[y'' + 2y' + (\\lambda + 1)y = 0\\]\nWe want to rewrite this in self-adjoint form, that is, in the general form:\n\\[\\frac{d}{dx} \\left[ p(x) y' \\right] + [\\lambda w(x) +q(x)] y = 0\\]\nOr more specifically:\n\\[\\frac{d}{dx}\\left[ p(x) y’ \\right] + \\lambda w(x) y = 0\\]\nStep 1: \\(~\\) Multiply by an integrating factor\nWe remove the first derivative term by multiplying the equation by an integrating factor \\(\\mu(x)\\) such that:\n\\[\\mu(x)y'' + 2\\mu(x)y' + \\mu(x)(\\lambda + 1)y = 0\\]\nWe want:\n\\[\\mu(x) y'' + 2\\mu(x) y' = \\frac{d}{dx} \\left[ \\mu(x) y’ \\right]\\]\nSo the condition is:\n\\[\\frac{d}{dx} \\mu(x) = 2 \\mu(x) \\Rightarrow \\mu(x) = e^{2x}\\]\nStep 2: \\(~\\) Multiply the ODE by \\(\\mu(x) = e^{2x}\\):\n\\[e^{2x} y’’ + 2e^{2x} y’ + (\\lambda + 1)e^{2x} y = 0\n\\Rightarrow \\frac{d}{dx} \\left( e^{2x} y’ \\right) + (\\lambda + 1) e^{2x} y = 0\\]\nSelf-adjoint form:\n\\[\n\\frac{d}{dx} \\left( e^{2x} y' \\right) + (\\lambda + 1) e^{2x} y = 0\\]\n\\(~\\)\nSolution (c)\nFrom part \\((a)\\), the eigenfunctions were:\n\\[y_n(x) = e^{-x} \\sin(n\\pi x)\\]\nAnd from part \\((b)\\), the weight function (from the self-adjoint form) is:\n\\[w(x) = e^{2x}\\]\nThen, the eigenfunctions \\(y_n(x)\\) are orthogonal with respect to the weight function \\(w(x) = e^{2x}\\) on the interval \\([0,1]\\)\nOrthogonality relation:\n\\[\n\\int_0^1 e^{2x} y_m(x) y_n(x) \\, dx = 0 \\quad \\text{for } m \\ne n\\]\nor equivalently:\n\\[\n\\int_0^1 e^{2x} e^{-x} \\sin(m\\pi x) \\cdot e^{-x} \\sin(n\\pi x) \\, dx = 0, \\quad m \\ne n\n\\]\n\\(~\\)\n4. \\(~\\) \\((a)~\\) Find the Fourier series of \\(f\\) on the given interval\n\\[f(x) = |x|, \\;\\;-\\pi \\le x \\le \\pi\\]\n\\((b)~\\) Use the result of (a) and Parseval’s Theorem to evaluate the following series:\n\\[ 1 + \\frac{1}{3^4} + \\frac{1}{5^4} + \\frac{1}{7^4} + \\cdots\\]\nParseval’s Theorem \\(~\\) If \\(f\\) is piecewise continuous, of period \\(2\\pi\\), then\n\\[\\frac{1}{\\pi} \\int_0^{2\\pi} \\left[ f(x) \\right]^2 \\,dx = \\frac{1}{2}a_0^2 + \\sum_{n=1}^\\infty \\left( a_n^2 + b_n^2 \\right)\\]\nwhere \\(a_0\\), \\(a_n\\), and \\(b_n\\) are Fourier coefficients\nSolution\n\\((a)~\\)\nStep 1: \\(~\\) Determine symmetry\nSince\n\\[f(-x) = |-x| = |x| = f(x)\\]\nthe function is even\nSo the Fourier series of \\(f(x)\\) will have the form:\n\\[f(x) \\sim \\frac{a_0}{2} + \\sum_{n=1}^{\\infty} a_n \\cos(nx)\\]\n(No sine terms appear, because \\(\\sin(nx)\\) is odd)\nStep 2: \\(~\\) Compute the coefficients\nConstant term:\n\\[a_0 = \\frac{1}{\\pi} \\int_{-\\pi}^{\\pi} |x| \\, dx = \\frac{2}{\\pi} \\int_{0}^{\\pi} x \\, dx = \\frac{2}{\\pi} \\cdot \\left[ \\frac{x^2}{2} \\right]_0^\\pi = \\pi\\]\nCosine coefficients:\n\\[a_n = \\frac{1}{\\pi} \\int_{-\\pi}^{\\pi} |x| \\cos(nx) \\, dx\n= \\frac{2}{\\pi} \\int_{0}^{\\pi} x \\cos(nx) \\, dx\\]\nNow evaluate:\n\\[\\int_0^\\pi x \\cos(nx)\\, dx = \\left. \\frac{x \\sin(nx)}{n} \\right|_0^\\pi + \\left. \\frac{\\cos(nx)}{n^2} \\right|_0^\\pi\n= 0 + \\frac{(-1)^n - 1}{n^2}\\]\nThen:\n\\[a_n = \\frac{2}{\\pi} \\cdot \\frac{(-1)^n - 1}{n^2}\n= \\begin{cases}\n0 & \\text{if } n \\text{ even} \\\\\n-\\dfrac{4}{\\pi n^2} & \\text{if } n \\text{ odd}\n\\end{cases}\\]\nFinal Answer: \\(~\\) Fourier series of \\(f(x) = |x|\\)\n\\[\\begin{aligned}\nf(x) &= \\frac{\\pi}{2} - \\frac{4}{\\pi} \\sum_{\\substack{n=1\\\\n\\text{ odd}}}^\\infty \\frac{1}{n^2} \\cos(nx) \\\\\n&= \\frac{\\pi}{2} - \\frac{4}{\\pi} \\left( \\frac{\\cos x}{1^2} + \\frac{\\cos 3x}{3^2} + \\frac{\\cos 5x}{5^2} + \\cdots \\right)\n\\end{aligned}\\]\n\\((b)~\\)\nStep 1: \\(~\\) Recall the Fourier Series for \\(f(x) = |x|\\)\nFrom part \\((a)\\), the Fourier series is:\n\\[f(x) = |x| \\sim \\frac{\\pi}{2} - \\frac{4}{\\pi} \\sum_{n=1,3,5,\\dots}^\\infty \\frac{\\cos(nx)}{n^2}\\]\nSo the coefficients are:\n\n\\(a_0 = \\pi \\Rightarrow \\frac{a_0^2}{2} = \\frac{\\pi^2}{2}\\)\n\\(a_n = -\\frac{4}{\\pi n^2}~\\) for odd \\(n\\), and \\(a_n = 0\\) for even \\(n\\)\n\\(b_n = 0~\\) for all \\(n\\) (because \\(f(x)\\) is even)\n\nStep 2: \\(~\\) Apply Parseval’s Theorem\nRecall Parseval’s identity for \\(f\\) with period \\(2\\pi\\):\n\\[\\frac{1}{\\pi} \\int_0^{2\\pi} f(x)^2 dx = \\frac{1}{2} a_0^2 + \\sum_{n=1}^\\infty \\left( a_n^2 + b_n^2 \\right)\\]\n\nOver \\([0, \\pi]\\), \\(~f(x) = x\\)\nOver \\([\\pi, 2\\pi]\\), \\(~f(x) = 2\\pi - x\\)\n\nSo:\n\\[\\frac{1}{\\pi} \\int_0^{2\\pi} f(x)^2 dx = \\frac{1}{\\pi} \\left( \\int_0^\\pi x^2 dx + \\int_\\pi^{2\\pi} (2\\pi - x)^2 dx \\right)\\]\nChange variable \\(u = 2\\pi - x\\) in second integral, \\(du = -dx\\), it becomes:\n\\[\\int_\\pi^{2\\pi} (2\\pi - x)^2 dx = \\int_0^\\pi x^2 dx \\]\nSo:\n\\[\\frac{1}{\\pi} \\int_0^{2\\pi} f(x)^2 dx = \\frac{2}{\\pi} \\int_0^\\pi x^2 dx = \\frac{2\\pi^2}{3}\\]\nStep 3: \\(~\\) Compute RHS from Fourier coefficients\nRecall:\n\n\\(a_0 = \\pi \\Rightarrow \\frac{a_0^2}{2} = \\frac{\\pi^2}{2}\\)\nFor odd \\(n\\), \\(~a_n = -\\frac{4}{\\pi n^2}\\), so \\(~a_n^2 = \\frac{16}{\\pi^2 n^4}\\)\n\nSo:\n\\[\\sum_{n=1}^\\infty a_n^2 = \\frac{16}{\\pi^2} \\sum_{n=1,3,5,\\dots}^\\infty \\frac{1}{n^4}\\]\nPlug into Parseval’s identity:\n\\[\\frac{2\\pi^2}{3} = \\frac{\\pi^2}{2} + \\frac{16}{\\pi^2} \\sum_{n=1,3,5,\\dots}^\\infty \\frac{1}{n^4}\\]\nSubtract \\(\\frac{\\pi^2}{2}\\) from both sides:\n\\[\\frac{2\\pi^2}{3} - \\frac{\\pi^2}{2} = \\frac{\\pi^2}{6}\n= \\frac{16}{\\pi^2} \\sum_{n=1,3,5,\\dots}^\\infty \\frac{1}{n^4}\\]\nSolve for the sum:\n\\[\\sum_{n=1,3,5,\\dots}^\\infty \\frac{1}{n^4} = \\frac{\\pi^4}{96}\\]\n\\(~\\)\n5. \\(~\\) The root-mean-square value of a function \\(f(x)\\) defined over an interval \\((a, b)\\) is given by\n\\[\\text{RMS}(f) = \\sqrt{\\frac{\\displaystyle\\int_a^b \\,f^2(x)\\,dx}{b-a}}\\]\nShow that the RMS value of \\(f\\) over the interval \\((-p, p)\\) is given by\n\\[\\text{RMS}(f) = \\sqrt{\\frac{1}{4} a_0^2 +\\frac{1}{2} \\sum_{n=1}^\\infty \\left(a_n^2 + b_n^2 \\right)}\\]\nwhere \\(a_0\\), \\(a_n\\), and \\(b_n\\) are the Fourier coefficients, respectively\nSolution\nStep 1: \\(~\\) Fourier Series on \\((-p, p)\\)\nAssume \\(f(x)\\) has a Fourier series expansion on \\((-p, p)\\), with period \\(2p\\):\n\\[f(x) = \\frac{a_0}{2} + \\sum_{n=1}^\\infty \\left[ a_n \\cos\\left(\\frac{n\\pi x}{p}\\right) + b_n \\sin\\left(\\frac{n\\pi x}{p}\\right) \\right]\\]\nStep 2: \\(~\\) Parseval’s Theorem (general interval)\nParseval’s identity for a function with period \\(2p\\) is:\n\\[\\frac{1}{p} \\int_{-p}^{p} f^2(x)\\, dx = \\frac{1}{2} a_0^2 + \\sum_{n=1}^\\infty \\left( a_n^2 + b_n^2 \\right)\\]\nNow multiply both sides by \\(\\frac{1}{2}\\) to match the RMS definition, which has \\(\\frac{1}{2p}\\):\n\\[\\frac{1}{2p} \\int_{-p}^{p} f^2(x)\\, dx\n= \\frac{1}{4} a_0^2 + \\frac{1}{2} \\sum_{n=1}^\\infty \\left( a_n^2 + b_n^2 \\right)\\]\nTake square root of both sides:\n\\[\\text{RMS}(f) = \\sqrt{ \\frac{1}{2p} \\int_{-p}^p f^2(x)\\,dx }\n= \\sqrt{ \\frac{1}{4} a_0^2 + \\frac{1}{2} \\sum_{n=1}^\\infty \\left( a_n^2 + b_n^2 \\right) }\\]\n\\(~\\)\n6. \\(~\\) \\((a)~\\) Find the eigenvalues and eigenfunctions of the boundary-value problem\n\\[y''+y'+\\lambda y=0, \\;y(0)=0, \\; y(2)=0\\]\n\\((b)~\\) Put the differential equation in self-adjoint form\n\\((c)~\\) Give an orthogonality condition\nSolution\n\\((a)\\)\nStep 1: \\(~\\) Solve the ODE\nConsider the characteristic equation:\n\\[r^2 + r + \\lambda = 0\n\\Rightarrow r = \\frac{-1 \\pm \\sqrt{1 - 4\\lambda}}{2}\\]\nWe consider three cases depending on the discriminant \\(D = 1 - 4\\lambda\\):\nCase 1: \\(~\\lambda &lt; \\frac{1}{4} \\Rightarrow D &gt; 0\\)\nReal and distinct roots:\n\\[r_{1,2} = \\frac{-1 \\pm \\sqrt{1 - 4\\lambda}}{2}\\]\nGeneral solution:\n\\[y(x) = c_1 e^{r_1 x} + c_2 e^{r_2 x}\\]\nApply boundary conditions:\n\n\\(y(0) = c_1 + c_2 = 0 \\Rightarrow c_2 = -c_1\\)\n\\(y(2) = c_1 (e^{r_1 \\cdot 2} - e^{r_2 \\cdot 2}) = 0\\)\n\nThis implies \\(e^{2r_1} = e^{2r_2} \\Rightarrow r_1 = r_2\\), but they are distinct \\(\\Rightarrow\\) only trivial solution. So, no nontrivial solution for \\(\\lambda &lt; \\frac{1}{4}\\)\nCase 2: \\(~\\lambda = \\frac{1}{4} \\Rightarrow D = 0\\)\nDouble root:\n\\[r = -\\frac{1}{2}\n\\Rightarrow y(x) = (c_1 + c_2 x) e^{-x/2}\\]\nApply BCs:\n\n\\(y(0) = c_1 = 0\\)\n\\(y(2) = c_2 \\cdot 2 e^{-1} = 0 \\Rightarrow c_2 = 0\\)\n\nSo again, only trivial solution \\(\\Rightarrow\\) no eigenvalue at \\(\\lambda = \\frac{1}{4}\\)\nCase 3: \\(~\\lambda &gt; \\frac{1}{4} \\Rightarrow D &lt; 0\\)\nNow roots are complex:\n\\[r = -\\frac{1}{2} \\pm i \\mu, \\quad \\text{where } \\mu = \\frac{\\sqrt{4\\lambda - 1}}{2}\\]\nThen the general solution becomes:\n\\[y(x) = e^{-x/2} \\left( c_1 \\cos(\\mu x) + c_2 \\sin(\\mu x) \\right)\\]\nApply BCs:\n\n\\(y(0) = e^0 (c_1) = 0 \\Rightarrow c_1 = 0\\)\n\\(y(2) = e^{-1} \\cdot c_2 \\sin(2\\mu) = 0\\)\n\nSo \\(\\sin(2\\mu) = 0 \\Rightarrow 2\\mu = n\\pi \\Rightarrow \\mu = \\frac{n\\pi}{2}, \\; n = 1,2,3,\\dots\\)\nThen:\n\\[\\mu = \\frac{\\sqrt{4\\lambda - 1}}{2} = \\frac{n\\pi}{2}\n\\Rightarrow \\lambda_n = \\frac{1 + n^2 \\pi^2}{4}\\]\nEigenvalues and Eigenfunctions:\n\nEigenvalues: \\[\\boxed{\\lambda_n = \\frac{1 + n^2 \\pi^2}{4}, \\quad n = 1,2,3,\\dots}\\]\nEigenfunctions: \\[\\boxed{y_n(x) = e^{-x/2} \\sin\\left( \\frac{n\\pi x}{2} \\right)}\\]\n\n\\((b)~\\)\nWe want to rewrite the equation:\n\\[y'' + y' + \\lambda y = 0\\]\nin the form:\n\\[\\frac{d}{dx} \\left[ p(x) y’ \\right] + q(x) y + \\lambda w(x) y = 0\\]\nWe do this by finding an integrating factor \\(\\mu(x)\\) such that:\n\\[\\mu(x) y'' + \\mu(x) y' = \\frac{d}{dx} \\left[ \\mu(x) y’ \\right]\\]\nWe want:\n\\[\n\\begin{aligned}\n\\frac{d}{dx} [\\mu(x) y'] &= \\mu(x) y'' + \\mu'(x) y' = \\mu(x) y'' + \\mu(x) y'\\\\ &\\Rightarrow \\mu'(x) = \\mu(x) \\Rightarrow \\mu(x) = e^x\n\\end{aligned}\\]\nMultiply the original equation by \\(e^x\\):\n\\[e^x y'' + e^x y' + \\lambda e^x y = 0\n\\Rightarrow \\frac{d}{dx} \\left( e^x y’ \\right) + \\lambda e^x y = 0\\]\nSo the self-adjoint form is:\n\\[\\boxed{\n\\frac{d}{dx} \\left( e^x y’ \\right) + \\lambda e^x y = 0\n}\\]\n\\((c)~\\) Orthogonality Condition\nIn a Sturm-Liouville problem, eigenfunctions corresponding to distinct eigenvalues are orthogonal with respect to the weight function \\(w(x)\\)\nFrom \\((b)\\), we see the weight function is:\n\\[w(x) = e^x\\]\nSo, the orthogonality condition is:\n\\[\\boxed{\n\\int_0^2 e^x y_m(x) y_n(x)\\, dx = 0 \\quad \\text{if } m \\ne n\n}\\]\n\\(~\\)\n7. \\(~\\) Find the eigenfuctions and the equation that defines the eigenvalues for the given boundary-value problem:\n\\[ y''+\\lambda y = 0, \\;y(0) +y'(0) = 0, \\; y(1)=0\\]\nSolution\nStep 1: \\(~\\) General Solution of the ODE\nWe consider \\(\\lambda &gt; 0\\), since we typically expect nontrivial eigenfunctions. Let \\(\\lambda = \\mu^2\\), \\(~\\) with \\(\\mu &gt; 0\\). Then the general solution to the differential equation\n\\[y'' + \\mu^2 y = 0\\]\nis:\n\\[y(x) = A \\cos(\\mu x) + B \\sin(\\mu x)\\]\nand\n\\[y'(x) = -A \\mu \\sin(\\mu x) + B \\mu \\cos(\\mu x)\\]\nStep 2: \\(~\\) Apply the Boundary Conditions\n\nCondition 1: \\(~y(0) + y'(0) = 0\\)\n\n\\[A + B \\mu = 0 \\quad \\Rightarrow \\quad A = -B \\mu\\]\n\nCondition 2: \\(~y(1) = 0\\)\n\n\\[y(1) = A \\cos(\\mu) + B \\sin(\\mu)\n= B\\left( \\sin(\\mu) - \\mu \\cos(\\mu) \\right)\\]\nWe seek nontrivial solutions \\(B \\ne 0\\), so we must have:\n\\[\\boxed{\n\\sin(\\mu) = \\mu \\cos(\\mu)\n}\\]\nThis is the equation that defines the eigenvalues \\(\\lambda = \\mu^2\\)\nStep 3: \\(~\\) Eigenfunctions\nSo the eigenfunctions are (up to constant multiples):\n\\[\\boxed{\ny_\\mu(x) = \\sin(\\mu x) - \\mu \\cos(\\mu x)\n}\\]\nwhere \\(\\mu\\) satisfies \\(\\sin(\\mu) = \\mu \\cos(\\mu)\\), and the eigenvalue is \\(\\lambda = \\mu^2\\)\nFinal Answer\n\nEigenvalue equation:\n\n\\[\\boxed{ \\sin(\\mu) = \\mu \\cos(\\mu) \\quad \\text{with } \\lambda = \\mu^2 }\\]\n\nEigenfunctions:\n\n\\[\\boxed{ y(x) = \\sin(\\mu x) - \\mu \\cos(\\mu x) }\\]\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy.optimize import fsolve\n\n# Define the function whose roots we want to find: \n# f(mu) = sin(mu) - mu*cos(mu)\ndef f(mu):\n  return np.sin(mu) - mu *np.cos(mu)\n\n# Find the first few roots numerically using initial guesses\ninitial_guesses = [0.5, 4.5, 7.5, 11]\nmu_roots = [fsolve(f, guess)[0] for guess in initial_guesses]\nlambda_roots = [mu**2 for mu in mu_roots]\n\n# Define eigenfunction for given mu\ndef eigenfunction(mu, x):\n  return np.sin(mu *x) - mu *np.cos(mu *x)\n\n# Plot the first few eigenfunctions on [0, 1]\nx_vals = np.linspace(0, 1, 500)\nplt.figure(figsize=(6, 4))\n\nfor i, mu in enumerate(mu_roots):\n    y_vals = eigenfunction(mu, x_vals)\n    plt.plot(x_vals, y_vals, label=fr\"$\\mu_{i +1} \\approx {mu:6.3f}$\")\n\nplt.title(\"First Few Eigenfunctions\")\nplt.xlabel(\"x\")\nplt.ylabel(\"y(x)\")\nplt.legend()\nplt.grid(True)\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\\(~\\)\n8. \\(~\\) Expand \\(f(x)=x^4\\), \\(-1&lt;x&lt;1\\) in a Fourier-Legendre series\nSolution\nStep 1: \\(~\\) Legendre Polynomials\nThe first few Legendre polynomials are:\n\\[\\begin{aligned}\nP_0(x) &= 1 \\\\\nP_1(x) &= x \\\\\nP_2(x) &= \\frac{1}{2}(3x^2 - 1) \\\\\nP_3(x) &= \\frac{1}{2}(5x^3 - 3x) \\\\\nP_4(x) &= \\frac{1}{8}(35x^4 - 30x^2 + 3) \\\\\n\\end{aligned}\\]\nSince \\(f(x) = x^4\\) is a polynomial of degree 4, the Legendre expansion will have at most 5 nonzero terms:\n\\[f(x) = a_0 P_0(x) + a_1 P_1(x) + a_2 P_2(x) + a_3 P_3(x) + a_4 P_4(x)\\]\nBut since \\(f(x) = x^4\\) is even, and odd Legendre polynomials are odd functions (so their integrals with even functions vanish), we expect:\n\\[a_1 = a_3 = 0\\]\nStep 2: \\(~\\) Compute coefficients\nWe compute:\n\\[a_n = \\frac{2n+1}{2} \\int_{-1}^1 x^4 P_n(x)\\, dx\\]\n\n\\(a_0\\)\n\n\\[a_0 = \\frac{1}{2} \\int_{-1}^{1} x^4 \\cdot 1 \\, dx = \\frac{1}{2} \\cdot \\int_{-1}^1 x^4 dx = \\frac{1}{2} \\cdot \\frac{2}{5} = \\frac{1}{5}\\]\n\n\\(a_2\\)\n\n\\[a_2 = \\frac{5}{4} \\cdot \\int_{-1}^{1} (3x^6 - x^4) dx\n= \\frac{4}{7} \\]\n\n\\(a_4\\)\n\n\\[a_4 = \\frac{9}{16} \\cdot \\int_{-1}^{1} (35x^8 - 30x^6 + 3x^4) dx\n= \\frac{8}{35} \\]\nFinal Answer: \\(~\\) Fourier–Legendre Expansion of \\(f(x) = x^4\\)\n\\[\\boxed{\nx^4 = \\frac{1}{5} P_0(x) + \\frac{4}{7} P_2(x) + \\frac{8}{35} P_4(x)\n}\\]\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nx = np.linspace(-1, 1, 400)\nP0 = np.ones_like(x)\nP2 = 0.5 *(3*x**2 -1)\nP4 = (1/8) *(35*x**4 -30*x**2 +3)\n\nf = (1/5)*P0 +(4/7)*P2 +(8/35)*P4\n\nplt.figure(figsize=(6, 4))\nplt.plot(x, x**4)\nplt.plot(x, f, \n  label=r'$ \\frac{1}{5} P_0(x) +\\frac{4}{7} P_2(x) +\\frac{8}{35} P_4(x)$')\nplt.axhline(0, color='gray', lw=0.7)\nplt.axvline(0, color='gray', lw=0.7)\nplt.title(\"Legendre Polynomial Combination\")\nplt.xlabel(\"x\")\nplt.ylabel(\"f(x)\")\nplt.legend()\nplt.grid(True)\nplt.show()\n\n\n\n\n\n\n\n\n\\(~\\)\n9. \\(~\\) Expand the given function in a Fourier-Bessel series using Bessel functions of the same order as in the indicated boundary condition:\n\\[ f(x)=x^2, \\;\\;0&lt;x&lt;3, \\;\\;J_0'(3\\alpha)=0\\]\nSolution\nStep 1: \\(~\\) Understand the eigenfunctions and the orthogonality\nThis is a Fourier-Bessel series of order \\(0\\) with a Neumann boundary condition \\(J_0’(3\\alpha_n) = 0\\), so the eigenfunctions are:\n\\[\\phi_n(x) = J_0(\\alpha_n x)\\]\nwhere \\(\\alpha_n\\) are such that \\(J_0’(3\\alpha_n) = 0\\), or equivalently \\(\\alpha_n = \\frac{\\beta_n}{3}\\), with \\(\\beta_n\\) being the \\(n\\)-th positive zero of \\(J_0’(z)\\)\nThese \\(\\phi_n(x)\\) are orthogonal with respect to the weight \\(x\\) on \\((0, 3)\\):\n\\[\\int_0^3 x J_0(\\alpha_m x) J_0(\\alpha_n x) dx = 0 \\quad \\text{for } m \\ne n\\]\nStep 2: \\(~\\) General Fourier-Bessel expansion formula\nThe coefficients \\(a_n\\) are given by:\n\\[a_n = \\frac{\\displaystyle\\int_0^3 x f(x) J_0(\\alpha_n x) dx}{\\displaystyle\\int_0^3 x J_0^2(\\alpha_n x) dx}\\]\nHere, \\(f(x) = x^2\\), so the numerator becomes:\n\\[\\int_0^3 x \\cdot x^2 \\cdot J_0(\\alpha_n x) dx = \\int_0^3 x^3 J_0(\\alpha_n x) dx\\]\nStep 3: \\(~\\) Define the coefficients\nLet’s write:\n\n\\(\\alpha_n = \\frac{\\beta_n}{3}\\), where \\(\\beta_n\\) is the \\(n\\)-th zero of \\(J_0’\\)\nThen the expansion is:\n\n\\[f(x) = x^2 = \\sum_{n=1}^{\\infty} a_n J_0\\left( \\frac{\\beta_n}{3} x \\right)\\]\nwith\n\\[a_n = \\frac{\\displaystyle\\int_0^3 x^3 J_0\\left( \\frac{\\beta_n}{3} x \\right) dx}{\\displaystyle\\int_0^3 x J_0^2\\left( \\frac{\\beta_n}{3} x \\right) dx}\\]\nNow, do a substitution to simplify:\nLet \\(u = \\frac{\\beta_n}{3} x \\Rightarrow x = \\frac{3}{\\beta_n} u\\), and when \\(x = 0\\), \\(u = 0\\); when \\(x = 3\\), \\(u = \\beta_n\\). Then:\n\n\\(dx = \\frac{3}{\\beta_n} du\\)\n\\(x^3 = \\left(\\frac{3}{\\beta_n}\\right)^3 u^3\\)\n\nSo numerator becomes:\n\\[\\int_0^3 x^3 J_0\\left( \\frac{\\beta_n}{3} x \\right) dx = \\left( \\frac{3}{\\beta_n} \\right)^4 \\int_0^{\\beta_n} u^3 J_0(u) du\\]\nSimilarly, denominator becomes:\n\\[\\int_0^3 x J_0^2\\left( \\frac{\\beta_n}{3} x \\right) dx = \\left( \\frac{3}{\\beta_n} \\right)^2 \\int_0^{\\beta_n} u J_0^2(u) du\\]\nSo the coefficient is:\n\\[a_n = \\frac{\\displaystyle\\left( \\frac{3}{\\beta_n} \\right)^4 \\int_0^{\\beta_n} u^3 J_0(u) du}{\\displaystyle\\left( \\frac{3}{\\beta_n} \\right)^2 \\int_0^{\\beta_n} u J_0^2(u) du} = \\left( \\frac{3}{\\beta_n} \\right)^2 \\cdot \\frac{\\displaystyle \\int_0^{\\beta_n} u^3 J_0(u) du }{\\displaystyle \\int_0^{\\beta_n} u J_0^2(u) du }\\]",
    "crumbs": [
      "**Second Semester**",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Orthogonal Functions and Fourier Series</span>"
    ]
  },
  {
    "objectID": "ch_x1_Parabolic_PDEs.html",
    "href": "ch_x1_Parabolic_PDEs.html",
    "title": "12  Parabolic Partial Differential Equations",
    "section": "",
    "text": "12.1 Introduction to Partial Differential Equations",
    "crumbs": [
      "**Second Semester**",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Parabolic Partial Differential Equations</span>"
    ]
  },
  {
    "objectID": "ch_x1_Parabolic_PDEs.html#sec-x1-1",
    "href": "ch_x1_Parabolic_PDEs.html#sec-x1-1",
    "title": "12  Parabolic Partial Differential Equations",
    "section": "",
    "text": "Most physical phenomena, \\(\\,\\)whether in the domain of fluid dynamics, electricity, magnetism, mechanics, optics, or heat flow, \\(\\,\\)can be described in general by partial differential equations(PDEs): \\(\\,\\)in fact, \\(\\,\\)most of mathematical physics are PDEs\nWhat Are PDEs?\nA partial differential equation is an equation that contains partial derivatives. In contrast to ordinary differential equations (ODEs), where the unknown function depends on only one variable, in PDEs, the unknown function depends on several variables: \\(\\,u(x,t)\\) depends both on location \\(x\\) and time \\(t\\)\nA few well-known PDEs\n\\[\n\\begin{aligned}\n   u_t &= u_{xx}, &&\\scriptsize\\text{heat equation in one dimension} \\\\[5pt]\n   u_t &= u_{xx}+u_{yy}, &&\\scriptsize\\text{heat equation in two dimension} \\\\[5pt]\n   u_{tt} &= u_{xx}+u_{yy}+u_{zz}, &&\\scriptsize\\text{wave equation in three dimension} \\\\\n   u_{rr} &+\\frac{1}{r}u_r+\\frac{1}{r^2}u_{\\theta\\theta}=0, &&\\scriptsize\\text{Laplace's equation in polar coordinates}\n\\end{aligned}\\]\nThe unknown function \\(u\\) always depends on more than one variable. \\(\\,\\)The variable \\(u\\) (which we differentiate) is called the dependent variable, \\(\\,\\)whereas the ones differentiate with respect to are called the independent variables\nClassification is an important concept because the general theory and methods of solution apply only to a given class of equations. \\(\\,\\)Six basic classifications are:\n\nOrder of the PDE\n\\[\n\\begin{aligned}\n  u_t &= u_{\\color{red}{xx}}, && \\scriptsize \\text{second order} \\\\\n  u_{\\color{red}{t}} &= u_{\\color{red}{x}}, && \\scriptsize \\text{first order} \\\\\n  u_t &= uu_{\\color{red}{xxx}}+\\sin x, && \\scriptsize \\text{third order}\n  \\end{aligned}\\]\nNumber of Independent Variables\n\\[\n\\begin{aligned}\n  u_t &= u_{xx}, && \\scriptsize \\text{two variables: } \\,x \\,\\text{and } \\,t \\\\\n  u_t &= u_{rr}+\\frac{1}{r}u_{r}+\\frac{1}{r^2}u_{\\theta\\theta}, && \\scriptsize \\text{three variables: } \\,t, \\,r \\,\\text{and } \\,\\theta\n\\end{aligned}\\]\nLinearity\n\\[\n\\begin{aligned}\n  & u_{tt} = e^{-t}u_{xx}+\\sin t, && \\scriptsize \\text{linear} \\\\\n  & \\color{blue}{uu_{xx}}+u_t=0, && \\scriptsize \\text{nonlinear} \\\\\n  & u_{xx}+yu_{yy}=0, && \\scriptsize \\text{linear} \\\\\n  & xu_x+yu_y +\\color{blue}{u^2}=0, && \\scriptsize \\text{nonlinear}\n  \\end{aligned}\\]\n\nA second-order linear equation in two variables is an equation of the form:\n\\[Au_{xx} +Bu_{xy} +Cu_{yy} +Du_x +Eu_y +Fu = G \\tag{SL}\\label{eq:SL}\\]\nwhere \\(A\\), \\(B\\), \\(C\\), \\(D\\), \\(E\\), \\(F\\), and \\(G\\) can be constants or given functions of \\(x\\) and \\(y\\)\n\nHomogeneity\n\n\\(\\eqref{eq:SL}\\) is called homogeneous if the right hand side \\(G(x,y)\\) is identically zero for all \\(x\\) and \\(y\\).\nIf \\(G(x,y)\\) is not identically zero, then the equation is nonhomogeneous\n\nKinds of Coefficients\nThe coefficients in \\(\\eqref{eq:SL}\\) are constants, then \\(\\eqref{eq:SL}\\) is said to have constant coefficients (otherwise, \\(\\,\\)variable coefficients)\nThree Basic Types of Linear Equations\n\nParabolic\n\\(B^2-4AC=0,~\\) heat flow and diffusion processes\n\\(u_t=u_{xx}\\)\nHyperbolic\n\\(B^2-4AC&gt;0,~\\) vibrating systems and wave motion\n\\(u_{tt}=u_{xx}, \\;u_{\\xi\\eta}=0\\)\nElliptic\n\\(B^2-4AC &lt; 0,~\\) steady-state phenomena\n\\(u_{xx}+u_{yy}=0\\)\n\nIn the case of variable coefficients, the situation can change from point to point\n\\(yu_{xx}+u_{yy}=0\\)",
    "crumbs": [
      "**Second Semester**",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Parabolic Partial Differential Equations</span>"
    ]
  },
  {
    "objectID": "ch_x1_Parabolic_PDEs.html#sec-x1-2",
    "href": "ch_x1_Parabolic_PDEs.html#sec-x1-2",
    "title": "12  Parabolic Partial Differential Equations",
    "section": "12.2 Diffusion-Type Problems (Parabolic Equations)",
    "text": "12.2 Diffusion-Type Problems (Parabolic Equations)\n\n\n\n\n\n\nThe Mathematical Model of the Heat-Flow\nThe description of our physical problem requires three types of equations\n\nThe PDE describing the physical phenomenon of heat flow\nThe boundary conditions describing the physical nature of our problem on the boundaries\nThe initial condition describing the physical phenomenon at the start of the experiment\n\nThe Heat Equation\nThe basic equation of one-dimensional heat flow is\n\\[u_t=\\alpha u_{xx},\\;0&lt;x&lt;L, \\;0&lt;t&lt;\\infty \\tag{HE}\\label{eq:HE}\\]\nThis equation is derived from the basic conservation of energy\nBoundary Conditions\nAll physical problems have boundaries of some kind, \\(\\,\\)so we must describe mathematically what goes on there in order to adequately describe the problem. Since the temperature \\(\\,u\\,\\) was fixed for all time \\(t&gt;0~\\) at \\(~T_1\\) and \\(T_2~\\) at the two ends \\(\\,x=0~\\) and \\(~x=L\\)\n\\[\n\\begin{array}{r}\nu(0,t) = T_1\\\\\nu(L,t) = T_2\n\\end{array}, \\;\\; 0 &lt; t &lt; \\infty\n\\tag{BCs}\\label{eq:BCs}\\]\nInitial Condition\nAll physical problems must start from some value of time (generally called \\(t=0\\)), \\(\\,\\)so we must specify the physical apparatus at this time. Since we started monitoring the rod temperature from time the rod has achieved a constant temperature \\(T_0\\), \\(\\,\\)we have\n\\[ u(x,0) = T_0, \\;\\; 0 \\leq x \\leq L \\tag{IC}\\label{eq:IC}\\]\nBy writing \\(\\eqref{eq:HE}\\), \\(\\eqref{eq:BCs}\\), and \\(\\eqref{eq:IC}\\) together, \\(\\,\\)we have what is called an initial-boundary-value problem. The interesting here, which is not at all obvious, is that there is only one function \\(u(x,t)\\) that satisfies the problem\nMore Diffusion-Type Equations\n\nLateral Heat Loss Proportional to the Temperature Difference\nThe equation\n\\[u_t=\\alpha u_{xx} -\\beta (u -u_0),\\;\\beta&gt;0\\]\ndescribes heat flow in the rod with both diffusion \\(\\alpha u_{xx}\\) along the rod and heat loss (or gain) across the lateral sides of the rod. \\(\\,\\)Heat loss (\\(u&gt;u_0\\)) or gain (\\(u&lt;u_0\\)) is proportional to the difference between the temperature \\(u(x,t)\\) and the surrounding medium \\(u_0\\)\nInternal Heat Source\nThe nonhomogeneous equation\n\\[u_t=\\alpha u_{xx}+f(x,t)\\]\ncorresponds to the situation where the rod is being supplied with an internal heat source\nDiffusion-Convection Equation\nSuppose a pollutant is being carried along in a stream moving with velocity \\(v\\). \\(\\,\\)The rate of change \\(u_t\\) is measured by the diffusion-convection equation\n\\[u_t=\\alpha u_{xx}-vu_x\\]\nThe term \\(\\alpha u_{xx}\\) is the diffusion contribution and \\(-vu_x\\) is the convection component",
    "crumbs": [
      "**Second Semester**",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Parabolic Partial Differential Equations</span>"
    ]
  },
  {
    "objectID": "ch_x1_Parabolic_PDEs.html#sec-x1-3",
    "href": "ch_x1_Parabolic_PDEs.html#sec-x1-3",
    "title": "12  Parabolic Partial Differential Equations",
    "section": "12.3 Derivation of the Heat Equation",
    "text": "12.3 Derivation of the Heat Equation\n\\(~\\)\n\n\n\n\n\n\nSuppose we have a one-dimension rod of length \\(L\\) for which we make the following assumptions:\n\nThe rod is made of a single homogeneous conducting material\nThe rod is laterally insulated (heat flows only in the \\(x\\)-direction)\nThe rod is thin (the temperature at all points of a cross section is constant)\n\nIf we apply the principle of conservation of energy to the segment \\([x,x+\\Delta x]\\), \\(\\,\\)we can claim\n\\[\n\\begin{aligned}\n  \\text{Net Change} & \\text{ of Heat inside } [x,x+\\Delta x] \\;=\\; \\\\\n   &\\text{Net Flux of Heat across the Boundaries} \\;+ \\\\\n   &\\qquad\\qquad\\text{Total Heat Generated inside }[x,x+\\Delta x]\n  \\end{aligned}\\]\n\nThe total amount of heat inside \\([x,x+\\Delta x]\\) at any time \\(\\,t\\,\\) is measured by\n\\[ \\text{Total Heat inside } [x,x+\\Delta x] = \\int_x^{x+\\Delta x} \\rho c_p A u(s,t)\\,ds \\]\nWe can write the conservation of energy via calculus as\n\n\\[\\scriptsize\n\\begin{aligned}\n  \\frac{d}{dt}\\int_x^{x +\\Delta x}\n   &\\rho c_p A u(s,t)\\,ds = \\rho c_p A \\int_x^{x +\\Delta x} u_t(s,t)\\,ds \\\\\n   &=kA \\left[ u_x(x+\\Delta x,t) -u_x(x,t) \\right] +A\\int_x^{x +\\Delta x} F(s,t)\\,ds \\\\ \\\\\n  &\\big\\downarrow {\\; \\text{Mean Value Theorem}\\;\\int_a^b f(x)\\,dx=f(\\xi)(b-a),\\;a&lt;\\xi&lt;b } \\\\ \\\\\n  \\rho c_p A u_t(\\xi_1,t) \\Delta x\n   &= kA \\left[ u_x(x+\\Delta x,t) -u_x(x,t) \\right] +AF(\\xi_2,t)\\Delta x, \\;\\;x&lt;\\xi_1, \\xi_2&lt;x+\\Delta x\\\\ \\\\\n  &\\big\\downarrow \\\\ \\\\\n  u_t(\\xi_1,t)\n  &=\\frac{k}{\\rho c_p} \\left\\{ \\frac{u_x(x+\\Delta x,t) -u_x(x,t)}{\\Delta x} \\right\\} +\\frac{1}{\\rho c_p} F(\\xi_2,t) \\\\ \\\\\n  &\\big\\downarrow \\;\\Delta x \\to 0 \\\\ \\\\\n   {\\normalsize u_t(x,t)}\\; & {\\normalsize =\\alpha u_{xx}(x,t) +f(x,t)}\n\\end{aligned}\\]",
    "crumbs": [
      "**Second Semester**",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Parabolic Partial Differential Equations</span>"
    ]
  },
  {
    "objectID": "ch_x1_Parabolic_PDEs.html#sec-x1-4",
    "href": "ch_x1_Parabolic_PDEs.html#sec-x1-4",
    "title": "12  Parabolic Partial Differential Equations",
    "section": "12.4 Boundary Conditions for Diffusion-Type Problems",
    "text": "12.4 Boundary Conditions for Diffusion-Type Problems\n\nWhen describing the various types of boundary conditions that can occur for heat-flow problems, three basic types generally come to mind\nType 1 BC (Temperature specified on the boundary)\n\n\n\n\n\n\n\n\n\n\n\nOf course, we’d have an initial temperature to get this experiment started, but in this case, the effects of our IC would vanish after a short period of time, and the resulting temperature inside the circle would depend on the boundary temperature\n\nType 2 BC (Flux specified - including the special case of insulated boundaries)\n\nInsulated boundaries are those that do not allow any heat flow to pass, and, hence, the normal derivative (inward or outward) must be zero on the boundary (since the normal derivative is proportional to the flux)\nIn the case of the one-dimensional rod with insulated ends at \\(x=0\\) and \\(x=L\\), the BCs are\n\\[\\begin{array}{r}\n  u_x(0,t) = 0\\\\\n  u_x(L,t) = 0\n\\end{array},\\;\\; 0 &lt; t &lt;\\infty \\]\nIn two-dimensional domains, an insulated boundary would mean that the normal derivative of the temperature across the boundary is zero. For example, if the circular disc insulated on the boundary, then the BC would be\n\\[u_r(R,\\theta,t)=0\\; \\text{ for } \\;0\\leq \\theta &lt;2\\pi\\; \\text{ and }\\;0&lt;t&lt;\\infty\\]\nOn the other hand, if we specify the amount of heat entering across the boundary of our disc, the BC is\n\\[u_r(R,\\theta,t)=f(\\theta,t)\\]\n\nType 3 BC (Temperature of the surrounding medium specified)\n\n\n\n\n\n\nBy specifying these types of BCs, \\(\\,\\)we cannot say the boundary temperatures of the rod will be the same as the liquid temperature \\(g_1(t)\\) and \\(g_2(t)\\),\nbut we do know (Newton’s law of cooling) that whenever the rod temperature at one of the boundaries is less than the respective liquid temperatures, \\(\\,\\)then heat will flow into the rod at a rate proportional to this differance\nIn other words, for the one-dimensional rod with boundaries at \\(x=0\\) and \\(L\\), \\(\\,\\)Newton’s law of cooling states:\n\\[ \\text{Outward Flux of Heat (at } x=0\\text{) }=h[u(0,t) -g_1(t)] \\]\n\\[ \\text{Outward Flux of Heat (at } x=L\\text{) }=h[u(L,t) -g_2(t)] \\]\nwhere \\(\\,h\\) is a heat transfer coefficient\nFourier’s law gives us another representation for the outward flux of heat. In our one-dimensional problem, \\(\\,\\)Fourier’s law takes the form:\n\\[ \\text{Outward Flux of Heat (at }x=0\\text{) } \\displaystyle =k\\frac{\\partial u(0,t)}{\\partial x}\\]\n\\[ \\text{Outward Flux of Heat (at }x=L\\text{) }\\displaystyle =-k\\frac{\\partial u(L,t)}{\\partial x}\\]\nwhere \\(k\\) is the thermal conductivity of the material\nFinally if we use the two experssions for heat flux, \\(\\,\\)we have our desired BCs in purely mathematical terms;\n\\[\\begin{array}{l}\n\\displaystyle\\frac{\\partial u(0,t)}{\\partial x}\n  = \\phantom{-}\\frac{h}{k} \\left[ u(0,t) -g_1(t) \\right]\\\\[8pt]\n\\displaystyle\\frac{\\partial u(L,t)}{\\partial x}\n= -\\frac{h}{k} \\left[ u(L,t) -g_2(t) \\right]\n\\end{array},\\;\\; 0 &lt; t &lt; \\infty\\]\nIn higher dimensions, we have similar BCs; for example, if the boundary of a circular disc is interfaced with a moving liquid that has a temperature \\(g(\\theta,t)\\), \\(\\,\\)our BC would be\n\\[\\frac{\\partial u}{\\partial r}(R,\\theta, t)=-\\frac{h}{k} \\left[ u(R,\\theta,t) -g(\\theta,t) \\right]\\]",
    "crumbs": [
      "**Second Semester**",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Parabolic Partial Differential Equations</span>"
    ]
  },
  {
    "objectID": "ch_x1_Parabolic_PDEs.html#sec-x1-5",
    "href": "ch_x1_Parabolic_PDEs.html#sec-x1-5",
    "title": "12  Parabolic Partial Differential Equations",
    "section": "12.5 Separation of Variables",
    "text": "12.5 Separation of Variables\n\nSeparation of variables is one of the oldest techniques for solving initial-boundary-value problems and applies to problems where\n\nThe PDE is linear and homogeneous (not necessarily constant coefficients)\nThe boundary conditions are of the form\n\\[\\begin{aligned}\n  \\alpha u_x(0,t) +\\beta u(0,t)&= 0\\\\\n  \\gamma u_x(L,t) +\\delta u(L,t)&= 0\n\\end{aligned}\\]\nwhere \\(\\alpha\\), \\(\\beta\\), \\(\\gamma\\), and \\(\\delta\\) are constants (boundary conditions of this form are called linear homogeneous BCs)\n\nWe wish to find the function \\(u(x,t)\\) that satisfies the following four conditions:\n\\[\n\\begin{aligned}\n    u_t &= \\alpha u_{xx} && 0 &lt; x &lt; L,\\; 0 &lt; t &lt; \\infty \\\\\n    u(0, t) &= 0 && 0 &lt; t &lt; \\infty \\\\\n    u(L, t) &= 0 && \\\\\n    u(x, 0) &= \\phi(x) && 0 \\leq x \\leq L\n\\end{aligned}\\]\n\n\n\n\n\n\nSTEP 1 \\(\\,\\) To begin, \\(\\,\\)we look for solutions of the form \\(u(x,t)=X(x)T(t)\\) by substituting \\(X(x)T(t)\\) into the PDE\n\\[\n\\begin{aligned}\n  X(x)T'(t)&=\\alpha X''(x)T(t) \\\\\n  &\\Downarrow \\\\\n  \\frac{T'(t)}{\\alpha T(t)}&= \\frac{X''(x)}{X(x)}=-\\lambda &lt;0\\\\\n  &\\Downarrow {\\scriptsize \\text{We essentially change a second-order PDE to two ODEs}}\\\\\n  T' &+\\alpha\\lambda T= 0\\\\\n  X'' &+\\lambda X = 0 \\\\\n  &\\Downarrow \\\\\n  T(t)&=a_1 e^{-\\alpha\\lambda t}\\\\\n  X(x)&=a_2 \\sin \\sqrt{\\lambda}x +a_3 \\cos \\sqrt{\\lambda}x\\\\\n  &\\Downarrow \\\\\n  u(x,t)&=e^{-\\alpha\\lambda t} \\left[ A \\sin \\sqrt{\\lambda}x +B\\cos\\sqrt{\\lambda} x \\right]\n\\end{aligned}\\]\nAt this point, \\(\\,\\)we have an infinite number of functions that satisfy the PDE\nSTEP 2 \\(\\,\\)The next step is to choose a certain subset of our current crop of solutions\n\\[e^{-\\alpha\\lambda t} \\left[ A \\sin \\sqrt{\\lambda}x +B\\cos\\sqrt{\\lambda} x \\right]\\; \\tag{12.1}\\]\nthat satisfy the boundary conditions\n\\[\n\\begin{aligned}\nu(0,t) &= 0 \\\\\nu(L,t) &= 0\n\\end{aligned}\\]\nTo do this, \\(\\,\\)we substitute Equation 12.1 into these BCs, \\(\\,\\)getting\n\\[\n\\begin{aligned}\n  u(0,t) & =Be^{-\\alpha\\lambda t}=0\\;\\Rightarrow B=0\\\\\n  u(L,t) & =Ae^{-\\alpha\\lambda t}\\sin\\sqrt{\\lambda}L=0 \\;\\Rightarrow \\sin\\sqrt{\\lambda}L=0,\\; A\\neq 0 \\\\\n  &\\Downarrow \\\\\n\\lambda &= \\left(\\frac{\\pi}{L}\\right)^2, \\left(\\frac{2\\pi}{L}\\right)^2,\\left(\\frac{3\\pi}{L}\\right)^2,\\cdots \\\\\n&\\Downarrow \\\\\n\\lambda_n &=\\left(\\frac{n\\pi}{L}\\right)^2,\\;n=1,2,3,\\cdots \\\\ \\text{ }\n\\end{aligned}\\]\nWe have now finished the second step; we have an infinite number of functions\n\\[ u_n(x,t)=c_n e^{-\\alpha\\left(\\frac{n\\pi}{L}\\right)^2 t} \\sin\\left( \\frac{n\\pi}{L}x \\right), \\;n=1,2,3,\\cdots \\]\neach one satisfying the PDE and the BCs\nSTEP 3 \\(\\,\\)The last step is to add the fundamental solutions\n\\[ u(x,t)=\\sum_{n=1}^\\infty c_n e^{-\\alpha\\left(\\frac{n\\pi}{L}\\right)^2 t} \\sin\\left( \\frac{n\\pi}{L}x \\right) \\tag{12.2}\\]\nin such a way (pick the coefficients \\(c_n\\)) that the initial condition\n\\[u(x,0)=\\phi(x)\\]\nis satisfied. \\(\\,\\)Substituting the sum into the IC gives\n\\[ \\phi(x)=\\sum_{n=1}^\\infty c_n \\sin\\left( \\frac{n\\pi}{L} x \\right) \\]\nNow the problem becomes how to find the coefficients \\(c_n\\). \\(\\,\\)This is actually very easy\n\nOne uses properties of the functions known as orthogonality:\n\\[ \\int_0^L \\sin\\left(\\frac{m\\pi}{L} x \\right)\\sin\\left(\\frac{n\\pi}{L}x \\right)\\,dx =\n\\begin{cases} 0 &  m\\neq n \\\\ \\frac{L}{2} & m=n \\end{cases}\\]\nWe multiply each side of Equation 12.2 by \\(\\sin(m\\pi x/L)\\) and integrate from zero to \\(L\\); \\(\\,\\)we get\n\\[ \\int_0^L \\phi(x) \\sin \\left( \\frac{m\\pi}{L} x\\right)\\, dx=c_m\\int_0^L \\sin^2\\left( \\frac{m\\pi}{L} x\\right)\\,dx = \\frac{L}{2}c_m \\]\nWe’re done; \\(\\,\\)the solution is\n\\[ u(x,t)=\\sum_{n=1}^\\infty c_n e^{-\\alpha\\left(\\frac{n\\pi}{L}\\right)^2 t} \\sin\\left( \\frac{n\\pi}{L}x \\right)\\;\\;\\text{ where }\\; c_n =\\frac{2}{L} \\int_0^L \\phi(x) \\sin \\left( \\frac{n\\pi}{L} x\\right)\\, dx \\]\n\nNOTES\n\nObserve that the only difference between the Fourier sine expansion of \\(\\phi(x)\\) and the solution is the insertion of the time factor\n\\[ e^{-\\alpha\\left(\\frac{n\\pi}{L}\\right)^2 t} \\]\nin each term\nThe terms in the series get small very fast due to the factor \\(e^{-\\alpha\\left(\\frac{n\\pi}{L}\\right)^2 t}\\). \\(\\,\\)Hence, for long time periods, \\(\\,\\)the solution is approximately equal to the first term\n\\[ u(x,t)\\approx c_1 e^{-\\alpha\\left(\\frac{\\pi}{L}\\right)^2 t} \\sin\\left( \\frac{\\pi}{L}x \\right) \\]\n\n\\(~\\)\nExample \\(\\,\\)Solve the diffusion problem with insulated boundaries; \\(\\,\\) that is\n\\[\\begin{aligned}\n    u_t & = u_{xx} && 0 &lt; x &lt; 1,\\; 0 &lt; t &lt; \\infty\\\\\n    u_x(0,t) & = 0 && 0 &lt; t &lt;\\infty\\\\\n    u_x(1,t) & = 0 && \\\\\n    u(x,0) & = 1 + \\cos\\pi x +0.5\\cos 3\\pi x && 0 \\leq x \\leq 1\n  \\end{aligned}\\]\n\\(~\\)",
    "crumbs": [
      "**Second Semester**",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Parabolic Partial Differential Equations</span>"
    ]
  },
  {
    "objectID": "ch_x1_Parabolic_PDEs.html#sec-x1-6",
    "href": "ch_x1_Parabolic_PDEs.html#sec-x1-6",
    "title": "12  Parabolic Partial Differential Equations",
    "section": "12.6 Transforming Nonhomogeneous BCs into Homogeneous Ones",
    "text": "12.6 Transforming Nonhomogeneous BCs into Homogeneous Ones\n\nConsider heat flow in an insulated rod where the two ends are kept at constant temperatures \\(k_1\\) and \\(k_2\\):\n\\[\\begin{array} {r} u(0,t) = k_1\\\\ u(L,t) = k_2 \\end{array}, \\qquad\\quad 0&lt;t&lt;\\infty \\tag{12.3}\\]\nThe difficulty here is that since the BCs are not homogeneous, \\(\\,\\)we cannot solve this problem by separation of variables\nHowever, \\(\\,\\)it is obvious that the solution will have a steady-state solution (when \\(t=\\infty\\)) that varies linearly between the boundary temperatures \\(k_1\\) and \\(k_2\\)\n\n\n\n\n\nIn other words, \\(\\,\\)it seems reasonable to think of our temperature \\(u(x,t)\\) as the sum of two parts\n\\[\n\\begin{aligned}\n  u(x,t) &=\\underbrace{\\text{steady state}}_{\\text{Eventual Solution for Large Time}}\n      +\\underbrace{\\text{transient}}_{\\underset{\\text{(and will go to zero)}}{\\text{Part of the Solution that depends on the IC}}}\\\\\n      &\\Downarrow \\\\\n      &= \\left[ k_1 +\\frac{x}{L}(k_2 -k_1) \\right] +U(x,t)\n\\end{aligned}\\]\nBy substituting the above relation in the original problem Equation 12.3, \\(\\text{ }\\)we arrive at a new problem in \\(U(x,t)\\):\n\\[{U_t = \\alpha U_{xx},\\quad 0 &lt; x &lt; L,\\; 0 &lt; t &lt; \\infty} \\]\n\\[{\\scriptsize\\color{blue}{\\begin{array}{r}\n  U(0,t)=0 \\\\\n  U(L,t)=0\n\\end{array}, \\;\\; 0&lt;t&lt;\\infty}, \\;\\;\nU(x,0)=\\phi(x)\n-\\left[ k_1 +\\displaystyle\\frac{x}{L}(k_2 -k_1) \\right]\n=\\bar{\\phi}(x), \\;\\; 0\\leq x \\leq L}\\]\nTransforming Time Varying BCs to Zero BCs\n\nWhat about more realistic-type derivative BCs with time-varying right-hand sides? Consider the typical problem\n\\[\n\\begin{aligned}\n  u_t &= \\alpha u_{xx} && 0 &lt; x &lt; L, \\;0 &lt; t &lt;\\infty \\\\\n  u(0,t) &= g_1(t) && 0 &lt; t &lt; \\infty \\\\\n  u_x(L,t) +hu(L,t) &= g_2(t) && \\\\\n  u(x,0) &=\\phi(x) && 0 \\leq x \\leq L\n\\end{aligned} \\tag{12.4}\\]\nTo change these nonezero BCs to homogeneous ones, \\(\\,\\)we (after some trial and error) seek a solution of the form:\n\\[ u(x,t)=\\underbrace{A(t)\\left[1-\\frac{x}{L}\\right] +B(t)\\frac{x}{L}}_{S(x,t)}+U(x,t)\\]\nwhere \\(A(t)\\) and \\(B(t)\\) are chosen so that \\(S(x,t)\\) satisfies the BCs and thus\n\\[\\begin{array}{r} U(0,t) = 0\\\\ U_x(L,t)+hU(L,t) = 0 \\end{array}, \\quad 0&lt;t&lt;\\infty\\]\nSubstituting \\(S(x,t)\\) into the BCs gives\n\\[ \\begin{aligned}\n    S(0,t) &= g_1(t)\\\\\n    S_x(L,t)+hS(L,t) &= g_2(t)\n  \\end{aligned}, \\quad 0&lt;t&lt;\\infty \\]\nin which we get\n\\[\\begin{aligned}\n   A(t)&= g_1(t)\\\\\n   B(t)&= \\frac{g_1(t) +Lg_2(t)}{1+Lh}\n  \\end{aligned}, \\quad 0&lt;t&lt;\\infty\\]\nSo if we substitute this into the original problem Equation 12.4, \\(\\,\\)we get the transformed problem in \\(U(x,t)\\):\n\\[\n\\begin{aligned}\n  U_t &= \\alpha U_{xx} -\\color{red}{S_t} && 0 &lt; x &lt;L, \\;0 &lt; t &lt;\\infty \\\\\n  U(0,t) &= 0 && 0 &lt; t &lt; \\infty\\\\\n  U_x(L,t) +hU(L,t) &= 0 && \\\\\n  U(x,0) &= \\phi(x)-S(x,0) && 0 \\leq x \\leq L\n\\end{aligned}\\]\n\nWe now have our new problem with zero BCs but unfortunately the PDE is nonhomogeneous\nWe cannot solve this problem by separation of variables, \\(\\,\\)but we will solve it by integral transforms and eigenfuction expansions\n\n\n\n\\(~\\)\nExample \\(\\,\\)Solve the initial-boundary-value problem\n\\[\n  \\begin{aligned}\n    u_t &= \\alpha u_{xx} && 0 &lt; x &lt; 1, \\;0 &lt; t &lt;\\infty \\\\\n    u(0,t) &= 1 && 0 &lt; t &lt; \\infty\\\\\n    u_x(1,t) +hu(1,t) &= 1 && \\\\\n    u(x,0) &= \\sin\\pi x +x && 0 \\leq x \\leq 1\n  \\end{aligned}\\]\n\\(~\\)",
    "crumbs": [
      "**Second Semester**",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Parabolic Partial Differential Equations</span>"
    ]
  },
  {
    "objectID": "ch_x1_Parabolic_PDEs.html#sec-x1-7",
    "href": "ch_x1_Parabolic_PDEs.html#sec-x1-7",
    "title": "12  Parabolic Partial Differential Equations",
    "section": "12.7 Solving More Complicated Problems by Separation of Variables",
    "text": "12.7 Solving More Complicated Problems by Separation of Variables\n\nWe start with a one-dimensional heat-flow problem where one of the BCs contains derivatives\nHeat-Flow Problem with 3rd kind BC\n\n\n\n\n\nWe fix the temperature at the top of the rod at \\(u(0,t)=0\\) and immerse the bottom of the rod in a solution of water fixed at the same temperature of zero. The natural flow of heat says that the BC at \\(x=1\\) is\n\\[u_x(1,t)=-hu(1,t)\\]\nSuppose now the initial temperature of the rod is \\(u(x,0)=x,\\) \\(\\,\\)but instantaneously thereafter (\\(t&gt;0\\)), \\(\\,\\)we apply our BCs. To find the ensuing temperature, \\(\\,\\)we must solve the IBVP\n\\[\n\\begin{aligned}\n  u_t &= \\alpha u_{xx} && 0 &lt; x &lt; 1, \\;0 &lt; t &lt;\\infty \\\\\n  u(0,t) &= 0 && 0 &lt; t &lt; \\infty\\\\\n  u_x(1,t) +hu(1,t) &= 0 && \\\\\n  u(x,0) &= x && 0 \\leq x \\leq 1\n\\end{aligned}\\]\n\nSTEP 1\n\nSubstituting \\(u(x,t)=X(x)T(t)\\) into the PDE gives\n\\[u(x,t)=e^{-\\alpha\\lambda t} \\left[ A\\sin\\sqrt{\\lambda}x +B\\cos\\sqrt{\\lambda}x \\right]\\]\nfor any \\(\\lambda&gt;0\\) and any \\(A\\) and \\(B\\)\n\nSTEP 2\n\nSubstituting the solution into the BCs gives us conditions on \\(\\lambda\\), \\(A\\) and \\(B\\) \\(\\,\\)that must be satisfied;\n\\[\n\\begin{aligned}\nB e^{-\\alpha\\lambda t}&= 0\\; \\Rightarrow \\; B=0 \\\\\nA e^{-\\alpha\\lambda t}(\\sqrt{\\lambda} \\cos \\sqrt{\\lambda}\n  +h\\sin\\sqrt{\\lambda})&= 0 \\;\\Rightarrow \\; \\tan \\sqrt{\\lambda}\n  =-\\frac{\\sqrt{\\lambda}}{h}, \\; A \\neq 0\n\\end{aligned}\\]\nTo find \\(\\lambda\\), \\(\\,\\)we must find the intersections of the curves \\(\\tan\\sqrt{\\lambda}\\) and \\(-\\frac{\\sqrt{\\lambda}}{h}\\). \\(\\,\\)These values \\(\\lambda_1\\), \\(\\lambda_2\\), \\(\\cdots\\) \\(\\,\\)can be computed numerically for a given \\(h\\) and are called the eigenvalues of the boundary-value problem\n\\[\\begin{aligned}\nX''+\\lambda X &= 0 \\\\\nX(0)&=0 \\\\\nX'(1)+hX(1)&=0\n\\end{aligned} \\tag{12.5}\\]\nThe eigenvalue problem Equation 12.5 is a special case of the general Sturm-Liouville problem\n\n\\(~\\)\n\nimport numpy as np\nfrom scipy import optimize\nimport matplotlib.pyplot as plt\n\nplt.rcParams['font.size'] = 12\nplt.rcParams['xtick.direction'] = 'in'\nplt.rcParams['ytick.direction'] = 'in'\nplt.rcParams['text.usetex'] = True\nplt.rcParams['text.latex.preamble'] = r'\\usepackage{amsmath}'\n\ndef example_plot(h, n_eig, sqrt_eig):\n    \n    x = np.linspace(0, n_eig*np.pi, 1000)  \n    y = np.tan(x)\n\n    threshold = 100\n    y[y &gt; threshold] = np.inf\n    y[y &lt;-threshold] = np.inf\n\n    plt.figure(figsize=(7, 4))\n\n    plt.plot(x, y, linewidth=1.2, color=\"blue\")\n    plt.plot(x,-x, linewidth=1.2, color=\"green\")\n    plt.scatter(sqrt_eig,-sqrt_eig, color='red')\n    \n    plt.xticks([0, np.pi, 2*np.pi, 3*np.pi, 4*np.pi, 5*np.pi], \n            ['0',r'$\\pi$',r'$2\\pi$',r'$3\\pi$',r'$4\\pi$',r'$5\\pi$'])\n    plt.xlim(0, 5.0*np.pi)\n    plt.ylim(-20, 20)\n    \n    plt.xlabel(r'$\\sqrt{\\lambda}/h$')\n    plt.ylabel(r'$\\\\tan \\sqrt{\\lambda}$, $-\\sqrt{\\lambda}/h$')\n    plt.title(r'$h=%3.1f$' % h)\n    plt.grid()\n    \n    plt.show()\n\n\ndef calc_eig(h, n_eig):\n    return np.array([optimize.brentq(lambda x: np.tan(x) +x/h, \n      (i +0.5001)*np.pi, (i +1)*np.pi) for i in range(n_eig)])\n\nh = 1\nn_eig = 5\n\nsqrt_eig = calc_eig(h, n_eig)\nexample_plot(h, n_eig, sqrt_eig)\n\n\n\n\n\n\n\nFigure 12.1: Eigenvalues\n\n\n\n\n\nfor i in range(n_eig):\n  print(rf'$$ \\lambda_{i +1} = {np.round(sqrt_eig[i], 2)} $$')\n\\[ \\lambda_1 = 2.03 \\] \\[ \\lambda_2 = 4.91 \\] \\[ \\lambda_3 = 7.98 \\] \\[ \\lambda_4 = 11.09 \\] \\[ \\lambda_5 = 14.21 \\]\n\\(~\\)\n\nThe solutions of Equation 12.5 corresponding to the eigenvalues \\(\\lambda_n\\) are called eigenfunctions \\(X_n(x)\\)\n\\[ X_n(x)=\\sin\\sqrt{\\lambda_n} x \\]\n\n\\(~\\)\n\ndef example_plot2(x, n_eig, sqrt_eig, eigfunc):\n\n    plt.figure(figsize=(7, 4))\n    for n in range(n_eig):\n        plt.plot(x, eigfunc(x, sqrt_eig[n]), label=rf'$X_{n +1}(x)$')\n     \n    plt.xlim(0, 1)\n    plt.ylim(-1.2, 1.2)\n    plt.grid(ls=':')    \n    plt.legend(loc='upper left', bbox_to_anchor=(0, -0.1))\n    \n    plt.xlabel('$x$')\n    plt.ylabel('$X_n(x)$')\n    plt.title(r'$X_n(x)=\\sin\\sqrt{\\lambda_n} x$')\n    \n    plt.show()\n\n\ndef eigfunc(x, sqrt_eig):  \n    return np.sin(sqrt_eig *x)\n\nx = np.linspace(0, 1, 150)\nexample_plot2(x, n_eig, sqrt_eig, eigfunc)\n\n\n\n\n\n\n\nFigure 12.2: Eigenfunctions\n\n\n\n\n\n\\(~\\)\nSTEP 3\n\nWe now have an infinite number of solutions\n\\[u_n(x,t)=e^{-\\alpha\\lambda_n t} \\sin\\sqrt{\\lambda_n}x\\]\neach one satisfying the PDE and the BCs. \\(\\,\\)The final step is to add these functions together (the sum will still satisfy the PDE and BCs, since both the PDE and BCs are linear and homogeneous)\n\\[ u(x,t)=\\sum_{n=1}^\\infty c_n e^{-\\alpha\\lambda_n t} \\sin\\sqrt{\\lambda_n}x \\]\nin such a way that they agree with the IC\n\\[ u(x,0)=x=\\sum_{n=1}^\\infty c_n \\sin\\sqrt{\\lambda_n}x \\]\nTo find the coefficients \\(c_n\\), \\(\\,\\)we must multiply each side of the equation by \\(\\sin\\sqrt{\\lambda_m}x\\) and integrate \\(x\\) from \\(0\\) to \\(1\\)\n\\[{\\scriptsize\n\\begin{aligned}\n\\int_0^1 x\\sin\\sqrt{\\lambda_m}x\\,dx&= \\sum_{n=1}^\\infty c_n \\int_0^1 \\sin\\sqrt{\\lambda_n}x\\,\\sin\\sqrt{\\lambda_m}x \\,dx\\\\\n&\\Downarrow \\\\\n\\frac{\\sin\\sqrt{\\lambda_m}-\\sqrt{\\lambda_m}\\cos\\sqrt{\\lambda_m}}{\\lambda_m}&=c_m \\int_0^1 \\sin^2\\sqrt{\\lambda_m}x\\,dx =c_m\\left[ \\frac{\\sqrt{\\lambda_m}-\\sin\\sqrt{\\lambda_m}\\cos\\sqrt{\\lambda_m}}{2\\sqrt{\\lambda_m}} \\right ] \\\\\n&\\Downarrow\\\\\nc_n&=\\frac{2}{\\sqrt{\\lambda_n}} \\left[\\frac{\\sin\\sqrt{\\lambda_n} -\\sqrt{\\lambda_n}\\cos\\sqrt{\\lambda_n}}{\\sqrt{\\lambda_n}-\\sin\\sqrt{\\lambda_n}\\cos\\sqrt{\\lambda_n}} \\right]\n\\end{aligned}}\\]\nIn this problem, \\(\\,\\)the first five constants \\(c_n\\) have been computed:\n\ndef cal_c_n(sqrt_eig):  \n    \n    sin_sqrt_eig = np.sin(sqrt_eig)\n    cos_sqrt_eig = np.cos(sqrt_eig)\n    \n    return (2.0 /sqrt_eig \n           *(sin_sqrt_eig -sqrt_eig *cos_sqrt_eig)\n           /(sqrt_eig -sin_sqrt_eig *cos_sqrt_eig))\n\nc_n = cal_c_n(sqrt_eig)\nprint('$,\\;\\;$'.join([rf'$c_{i} = {c_i: 6.4f}$' \n              for i, c_i in enumerate(c_n, 1)]))\n\\(c_1 =  0.7292\\)\\(,\\;\\;\\)\\(c_2 = -0.1562\\)\\(,\\;\\;\\)\\(c_3 =  0.0614\\)\\(,\\;\\;\\)\\(c_4 = -0.0322\\)\\(,\\;\\;\\)\\(c_5 =  0.0197\\)\n&lt;&gt;:11: SyntaxWarning: invalid escape sequence '\\;'\n&lt;&gt;:11: SyntaxWarning: invalid escape sequence '\\;'\n/var/folders/4x/8kn2nym12cn7x7qmg_6s4b8h0000gn/T/ipykernel_75465/1942846608.py:11: SyntaxWarning: invalid escape sequence '\\;'\n  print('$,\\;\\;$'.join([rf'$c_{i} = {c_i: 6.4f}$'\n\ndef example_plot3(alpha, t, x, n_eig, sqrt_eig, eigfunc, u_solution):\n    plt.figure(figsize=(7, 4))\n    plt.plot(x, x, 'r:', label='$t=0.0$')\n    for tt in t:\n        plt.plot(x, u_solution(alpha, tt, x, n_eig, sqrt_eig, eigfunc),\n                 label='$t=%0.1f$' % tt)\n    \n    plt.xlim(0, 1)\n    plt.ylim(0, 1)\n    plt.legend(loc='upper left')\n    plt.xlabel('$x$')\n    plt.ylabel('$u(x,t)$')\n    \n    plt.show() \n\n\ndef u_solution(alpha, t, x, n_eig, sqrt_eig, eigfunc):\n    u = 0\n    for i in range(n_eig):\n        u += c_n[i] *np.exp(-alpha *sqrt_eig[i]**2 *t) \\\n                    *eigfunc(x, sqrt_eig[i])\n    return u    \n        \nalpha = 1\nt = np.linspace(0.1, 0.5, 4)\nexample_plot3(alpha, t, x, n_eig, sqrt_eig, eigfunc, u_solution)\n\n\n\n\n\n\n\nFigure 12.3: Solutions",
    "crumbs": [
      "**Second Semester**",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Parabolic Partial Differential Equations</span>"
    ]
  },
  {
    "objectID": "ch_x1_Parabolic_PDEs.html#sec-x1-8",
    "href": "ch_x1_Parabolic_PDEs.html#sec-x1-8",
    "title": "12  Parabolic Partial Differential Equations",
    "section": "12.8 Transforming Hard Equations into Easier Ones",
    "text": "12.8 Transforming Hard Equations into Easier Ones\n\n\n\n\n\n\nConsider the following problem:\n\\[\n\\begin{aligned}\n  u_t &= \\alpha u_{xx} \\color{blue}{-\\beta u} && 0 &lt; x &lt; 1, \\;0 &lt; t &lt;\\infty \\\\\n  u(0,t) &= 0 && 0 &lt; t &lt; \\infty\\\\\n  u(1,t) &= 0 && \\\\\n  u(x,0) &= \\phi(x) && 0 \\leq x \\leq 1\n\\end{aligned} \\tag{12.6}\\]\nwhere the term \\(-\\beta u\\) represents heat flow across the lateral boundary\nWe introduce a new temperature \\(w(x,t)\\) in place of \\(u(x,t)\\), \\(\\,\\)so that the PDE in \\(w\\) is simpler than the original one\nThe transformation is generally based on an intuitive feeling of how the solution of the original PDE behaves\nIn our problem Equation 12.6, \\(\\,\\)the temperature \\(u(x,t)\\) at any point \\(x_0\\) is changing as a result of two phenomena\n\ndiffusion of heat within the rod due to \\(\\alpha u_{xx}\\)\nheat flow across the lateral boundary due to \\(-\\beta u\\)\n\nThe important point is that if there were no diffusion within the rod (\\(\\alpha=0\\)) then the temperature at each point \\(x_0\\) would dump exponentially to zero according to\n\\[u(x_0,t)=u(x_0,0) e^{-\\beta t}\\]\nBy means of this observation, \\(\\,\\)we can essentially decompose the temperature \\(u(x,t)\\) into two factors\n\\[u(x,t)=e^{-\\beta t} w(x,t)\\]\nwhere \\(w(x,t)\\) would represent the temperature due to diffusion only\nWhen we substitute this expression into Equation 12.6, \\(\\,\\)we arrive at\n\\[\n\\begin{aligned}\n  w_t &= \\alpha w_{xx} && 0 &lt; x &lt; 1, \\;0 &lt; t &lt;\\infty \\\\\n  w(0,t) &= 0 && 0 &lt; t &lt; \\infty\\\\\n  w(1,t) &= 0 && \\\\\n  w(x,0) &= \\phi(x) && 0 \\leq x \\leq 1\n\\end{aligned}\\]\nThe diffusion-convection equation\n\\[u_t=\\alpha u_{xx}-vu_x\\]\n(\\(v\\) is a constant) can also be transformed to\n\\[w_t=\\alpha w_{xx}\\]\nIn this case, \\(\\,\\)the transformation is\n\\[ u(x,t)=\\exp\\left[ \\frac{v}{2\\alpha} \\left(x -\\frac{v}{2}t\\right) \\right] w(x,t)\\]\n\n\\(~\\)\nExample \\(\\,\\) Solve the diffusion-convection problem \\[\n  \\begin{aligned}\n    u_t &= u_{xx} -u_x && 0 &lt; x &lt; 1, \\;0 &lt; t &lt;\\infty \\\\\n    u(0,t) &= 0 && 0 &lt; t &lt; \\infty\\\\\n    u(1,t) &= 0 && \\\\\n    u(x,0) &= e^{x/2} && 0 \\leq x \\leq 1\n  \\end{aligned}\\]\nby transforming it into an easier one\n\\[\n  \\begin{aligned}\n    w_t &= w_{xx} && 0 &lt; x &lt; 1, \\;0 &lt; t &lt;\\infty \\\\\n    w(0,t) &= 0 && 0 &lt; t &lt; \\infty\\\\\n    w(1,t) &= 0 && \\\\\n    w(x,0) &= 1 && 0 \\leq x \\leq 1\n  \\end{aligned}\\]\nSolution\n\\[\\begin{aligned}\nu(x,t)&=\\exp\\left[ \\frac{1}{2}\\left( x -\\frac{1}{2}t \\right ) \\right] w(x,t) \\\\\n&\\Downarrow \\\\\nw_t=w_{xx},\\; w(0,t)&=0,\\;w(1,t)=0,\\;w(x,0)=1 \\\\\n&\\Downarrow \\\\\nw(x,t)&=2\\sum_{n=1}^\\infty \\frac{1-(-1)^n}{n\\pi} e^{-n^2\\pi^2 t}\\sin n\\pi x \\\\\n&\\Downarrow \\\\\nu(x,t)&= 2\\sum_{n=1}^\\infty \\frac{1-(-1)^n}{n\\pi} e^{-n^2\\pi^2 t +\\frac{1}{2}\\left( x-\\frac{1}{2}t \\right )}\\sin n\\pi x \\\\\n&=4\\sum_{m=1}^\\infty \\frac{1}{(2m-1)\\pi} e^{-(2m-1)^2\\pi^2 t +\\frac{1}{2}\\left( x-\\frac{1}{2}t \\right )}\\sin (2m-1)\\pi x    \n\\end{aligned}\\]\n\\(~\\)\n\ndef example_plot4(t, x, n_terms, u_solution):   \n    plt.figure(figsize=(6, 4))   \n    plt.plot(x, np.exp(x /2.0), 'r:', label='$t=0$')\n   \n    for tt in t:\n        plt.plot(x, u_solution(tt, x, n_terms), label='$t=%0.3f$' % tt)\n    \n    plt.xlim(0, 1)\n    plt.ylim(0, 2)\n    plt.legend(loc='upper left', bbox_to_anchor=(1.0, 1.0))\n    plt.xlabel('$x$')\n    plt.ylabel('$u(x,t)$')\n    plt.title(r'$\\alpha=1,\\;v=1$') \n\n    plt.show() \n\n\ndef u_solution(t, x, n_terms):  \n    u = 0\n    for m in range(1, n_terms +1):\n        a = (2.0 *m -1.0) *np.pi\n        u += 1 /a *np.exp(-a *a *t +(x -t /2.0) /2.0) *np.sin(a *x)\n    return 4.0*u\n\nx = np.linspace(0, 1, 150)\nt = np.array([0.001, 0.002, 0.004, 0.008, 0.05, 0.10, 0.15, 0.20])\n\nn_terms = 25\n\nexample_plot4(t, x, n_terms, u_solution)\n\n\n\n\n\n\n\nFigure 12.4: Solutions\n\n\n\n\n\n\\(~\\)",
    "crumbs": [
      "**Second Semester**",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Parabolic Partial Differential Equations</span>"
    ]
  },
  {
    "objectID": "ch_x1_Parabolic_PDEs.html#sec-x1-9",
    "href": "ch_x1_Parabolic_PDEs.html#sec-x1-9",
    "title": "12  Parabolic Partial Differential Equations",
    "section": "12.9 Solving Nonhomogeneous PDEs (Eigenfunction Expansion)",
    "text": "12.9 Solving Nonhomogeneous PDEs (Eigenfunction Expansion)\n\nConsider the nonhomogeneous problem\n\\[\n\\begin{aligned}\n  u_t &= \\alpha u_{xx} +f(x,t) && 0 &lt; x &lt; 1, \\;0 &lt; t &lt;\\infty \\\\\n  u(0,t) &= 0 && 0 &lt; t &lt; \\infty\\\\\n  u(1,t) &= 0 && \\\\\n  u(x,0) &= \\phi(x) && 0 \\leq x \\leq 1\n\\end{aligned} \\tag{12.7}\\]\nThe purpose of this section is to solve this problem by a method that is analogous to the method of variation of parameters in ODEs and is known as the eigenfunction expansion\n\nSTEP 1\n\nThe basic idea in this method is to decompose the heat source \\(f(x,t)\\) into simple components\n\\[ f(x,t)=\\sum_{n=1}^\\infty f_n(t) X_n(x)\\]\nIt turns out that the \\(X_n(x)\\) are the eigenfunctions of the Sturm-Liouville system we get when solving the associated homogeneous system by separation of variables\nIn this case, \\(\\,\\)the Sturm-Liouville problem we find when separating variables is\n\\[\\begin{aligned}\n&X'' +\\lambda X =0 \\\\\n&X(0)=0 \\\\\n&X(1)=0\n\\end{aligned}\\]\nand, \\(\\,\\)hence, \\(\\,\\)the \\(X_n(x)\\) are\n\\[X_n(x)=\\sin n\\pi x,\\;n=1,2,3,\\cdots\\]\nFinally, \\(\\,\\) to find the functions \\(\\,f_n(t)\\), \\(\\,\\)we merely multiply each side of this equation by \\(\\sin m\\pi x\\,\\) and integrate from zero to one:\n\\[\\begin{aligned}\n  \\int_0^1 f(x,t) \\sin m\\pi x \\,dx&= \\sum_{n=1}^\\infty f_n(t) \\int_0^1 \\sin m\\pi x\\, \\sin n\\pi x \\,dx=\\frac{1}{2}f_m(t)\\\\\n  &\\Downarrow\\;{\\scriptsize\\text{changing}\\;m \\;\\text{to}\\; n} \\\\\n  f_n(t)&=2\\int_0^1 f(x,t) \\sin n\\pi x\\, dx\n\\end{aligned}\\]\n\nSTEP 2\n\nThe responses \\(u_n(x,t)=T_n(t) \\sin n\\pi x\\,\\) to each of these individual components \\(\\,f_n(t)\\sin n\\pi x\\,\\) are added to construct the solution of our problem:\n\\[ u(x,t)=\\sum_{n=1}^\\infty u_n(x,t)=\\sum_{n=1}^\\infty T_n(t) \\sin n\\pi x \\]\nSubstituting \\(u(x,t)\\) and \\(f(x,t)\\) into Equation 12.7 gives us\n\\[{\\scriptsize\n\\begin{aligned}\n  \\sum_{n=1}^\\infty T_n'(t) \\sin n\\pi x&=-\\alpha \\sum_{n=1}^\\infty (n\\pi)^2\\, T_n(t) \\sin  n\\pi x +\\sum_{n=1}^\\infty f_n(t) \\sin n\\pi x \\\\\n  \\sum_{n=1}^\\infty T_n(0)\\sin n\\pi x&=\\phi(x) \\\\\n  &\\Downarrow \\\\\n   \\sum_{n=1}^\\infty\\underbrace{\\left[ T_n'+\\alpha(n\\pi)^2\\,T_n -f_n(t) \\right ]}_{=0}\\,&\\sin n\\pi x= 0 \\\\\n   T_n(0)=2\\int_0^1 \\phi(x)&\\sin n\\pi x\\,dx=a_n \\\\\n   &\\Downarrow \\\\\n   T_n(t)=a_n e^{-\\alpha(n\\pi)^2 t}&+\\int_0^t e^{-\\alpha(n\\pi)^2(t-\\tau)} f_n(\\tau)\\,d\\tau\n\\end{aligned}}\\]\nHence, the solution of our problem Equation 12.7 is\n\\[{\\scriptsize\n\\begin{aligned}\nu(x,t)&=\\sum_{n=1}^\\infty T_n(t) \\sin n\\pi x\n=\\underbrace{\\sum_{n=1}^\\infty a_n e^{-\\alpha(n\\pi)^2 t}\\sin n\\pi x }_{\\text{Transient Part due to the IC}}+\\underbrace{\\sum_{n=1}^\\infty \\int_0^t e^{-\\alpha(n\\pi)^2(t-\\tau)} f_n(\\tau)\\,d\\tau \\cdot \\sin n\\pi x}_{\\text{Forcing Part due to}\\, f(x,t)}\n\\end{aligned}}\\]\nThe eigenfunctions \\(X_n(x)\\) in the expansion change from problem to problem and depend on the PDE and BCs\n\n\\(~\\)\nExample \\(\\,\\)Find the solution\n\\[\n  \\begin{aligned}\n    u_t &= u_{xx} +\\sin\\sqrt{\\lambda_1} x && 0 &lt; x &lt; 1, \\;0 &lt; t &lt;\\infty \\\\\n    u(0,t) &= 0 && 0 &lt; t &lt; \\infty\\\\\n    u_x(1,t) +hu(1,t) &= 0 && \\\\\n    u(x,0) &= 0 && 0 \\leq x \\leq 1\n  \\end{aligned}\\]\nwhere \\(\\lambda_1\\) is the first root of the equation \\(\\,\\) \\(\\tan\\sqrt{\\lambda}=-\\sqrt{\\lambda}\\)\n\\(~\\)",
    "crumbs": [
      "**Second Semester**",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Parabolic Partial Differential Equations</span>"
    ]
  },
  {
    "objectID": "ch_x1_Parabolic_PDEs.html#sec-x1-10",
    "href": "ch_x1_Parabolic_PDEs.html#sec-x1-10",
    "title": "12  Parabolic Partial Differential Equations",
    "section": "12.10 \\(~\\)The Finite Sine and Cosine Transforms",
    "text": "12.10 \\(~\\)The Finite Sine and Cosine Transforms\n\nAn integral transformation is merely a transformation that assigns to one function \\(f(t)\\) a new function \\(F(s)\\) by means of a formula like\n\\[ F(s)=\\int_a^b K(s,t) \\,f(t)\\,dt \\]\nNote that we start with a function of \\(t\\) and end with a function of \\(s\\)\nThe function \\(K(s,t)\\) is called the kernel of the transformation and is the major ingredient that distinguishes one transform from another; \\(\\,\\) it is chosen so that the transform has certain desirable properties. The limits \\(a\\) and \\(b\\) also change from transformation to transformation\nWith every integral transform, there is an inverse transform that will reproduce that original function from its transform. \\(\\,\\)The transform and its inverse together form what is called a transform pair\nThe general philosophy behind integral transformation is that they eliminate partial derivatives with respect to one of the variables; \\(\\,\\)hence, \\(\\,\\)the new equation has one less variable\nIn other words, integral transformations change problems into easier ones. \\(\\,\\)The transformed problem is then solved, and its inverse is obtained to find the solution to the original problem\n\n\n\n\n\n\n\nWe first start with a function \\(f(x)\\) defined on an interval \\([0,L]\\). \\(\\,\\)The finite sine and cosine transforms of this function are defined by\n\\[\\begin{aligned}\n\\mathcal{F}_s[f]&=\\frac{2}{L} \\int_0^L f(x)\\,\\sin\\frac{n\\pi x}{L} \\,dx =b_n &&\\;{\\scriptsize\\text{Finite Sine Transform}} \\\\[8pt]\n\\mathcal{F}_c[f]&=\\frac{2}{L} \\int_0^L f(x)\\,\\cos\\frac{n\\pi x}{L} \\,dx =a_n &&\\;{\\scriptsize\\text{Finite Cosine Transform}}\n\\end{aligned}\\]\nThe student will note that these transforms do nothing more than transform a function into the Fourier sine and cosine coefficients. \\(\\,\\)The inverse transform of these transforms are the Fourier sine and cosine series\n\\[\\begin{aligned}\n  f(x) &=\\sum_{n=1}^{\\infty} b_n \\sin\\frac{n\\pi x}{L} &&\\;{\\scriptsize\\text{Inverse Sine Transform}} \\\\\n  f(x) &= \\frac{a_0}{2} +\\sum_{n=1}^\\infty a_n \\cos \\frac{n\\pi x}{L} &&\\;{\\scriptsize\\text{Inverse Cosine Transform}}\n  \\end{aligned}\\]\nNote that the summation in the inverse cosine starts at \\(n=0\\), \\(\\,\\)while the inverse sine starts at \\(n=1\\)\n\n\\(~\\)\nExamples of the Sine Transform\n\\[\\begin{aligned}\n   f(x)&=1, \\;\\;\\;\\; 0 \\leq x \\leq 1 \\\\\n   &\\Downarrow \\\\\n   \\scriptsize b_n=2 \\int_0^1 \\sin n\\pi x \\,dx &\\scriptsize=\n   \\begin{cases}\n     \\;\\;\\;\\;0& n\\; \\text{ even} \\\\\n     \\;^{\\displaystyle 4}/_{\\displaystyle n\\pi}& n\\; \\text{ odd }  \n   \\end{cases} \\\\\n   &\\Downarrow \\\\\n   f(x)&=\\frac{4}{\\pi} \\sum_{n=1}^\\infty \\left[ \\frac{1}{2n-1} \\right] \\sin (2n-1)\\pi x\n  \\end{aligned}\\]\n\n\n\n\n\n\\(~\\)\n\nProperties of the Finite Sine and Cosine Transforms\n\\[\\begin{aligned}\n  \\mathcal{F}_s [u_t]\n     &= \\frac{d \\mathcal{F}_s [u]}{dt} \\\\\n  \\mathcal{F}_s [u_{tt}]\n     &= \\frac{d^2 \\mathcal{F}_s [u]}{dt^2} \\\\ \\\\\n  \\mathcal{F}_s [u_x]\n     &= -\\frac{n\\pi}{L} \\mathcal{F}_c [u] \\\\\n  \\color{red}{\\mathcal{F}_s [u_{xx}]}\n     &\\color{red}{\\:= -\\left( \\frac{n\\pi}{L} \\right)^2 \\mathcal{F}_s [u] +\\frac{2n\\pi}{L^2}\n   \\left[ u(0,t) -(-1)^n u(L,t) \\right]}\\\\\n  \\mathcal{F}_c [u_x]\n     &= -\\frac{n\\pi}{L} \\mathcal{F}_s [u] +\\frac{2}{L} \\left[(-1)^n u(L,t) -u(0,t) \\right] \\\\\n  \\color{red}{\\mathcal{F}_c [u_{xx}]} & \\color{red}{\\:= -\\left(\\frac{n\\pi}{L}\\right)^2 \\mathcal{F}_c [u] +\\frac{2}{L} \\left[(-1)^n u_x(L,t) -u_x(0,t) \\right]} \\\\\n\\end{aligned}\\]\n\n\\(~\\)\nExample \\(\\,\\)Solve the general problem\n\\[\n  \\begin{aligned}\n    u_t &= \\alpha u_{xx} -\\beta u +f(x,t) && 0 &lt; x &lt; 1, \\;0 &lt; t &lt;\\infty \\\\\n    u(0,t) &= 0 && 0 &lt; t &lt; \\infty\\\\\n    u(1,t) &= 0 && \\\\\n    u(x,0) &= 0 && 0 \\leq x \\leq 1\n  \\end{aligned}\\]\n\\(~\\)",
    "crumbs": [
      "**Second Semester**",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Parabolic Partial Differential Equations</span>"
    ]
  },
  {
    "objectID": "ch_x1_Parabolic_PDEs.html#sec-x1-11",
    "href": "ch_x1_Parabolic_PDEs.html#sec-x1-11",
    "title": "12  Parabolic Partial Differential Equations",
    "section": "12.11 \\(~\\)Superposition (The Backbone of Linear Systems)",
    "text": "12.11 \\(~\\)Superposition (The Backbone of Linear Systems)\n\nFor an engineer who wishes to find the response \\(u\\) to a linear system from input \\(\\,f\\), \\(\\,\\)a common approach is\n\nBreak \\(\\,f\\) into elementary parts, \\(\\,f=\\sum f_k\\)\nFind the system response \\(u_k\\) to \\(f_k\\)\nAdd(superimpose) the simple responses \\(u_k\\) to get \\(\\,u=\\sum u_k\\)\n\nIt turns out if the system is linear, \\(\\,\\)then the sum \\(u\\) is the response we get if the function \\(\\,f\\) were imputted directly; \\(\\,\\)this is the principle of superposition\nSuperposition Used to Break an IBVP into Two Simpler Problems\nSuppose we have the linear problem\n\\[\n\\begin{aligned}\n  u_t &= u_{xx} +\\color{red}{\\sin \\pi x} && 0 &lt; x &lt; 1, \\;0 &lt; t &lt;\\infty \\\\\n  u(0,t) &= 0 && 0 &lt; t &lt; \\infty\\\\\n  u(1,t) &= 0 && \\\\\n  u(x,0) &= \\sin 2\\pi x && 0 \\leq x \\leq 1\n\\end{aligned}\\tag{P}\\label{eq:P}\\]\nHere, \\(\\,\\)we have an nonhomogeneous heat equation, \\(\\,\\)so separation of variables is not a viable method of attack\n\nWe could, \\(\\,\\)of course, \\(\\,\\)use the finite sine transform on the variable \\(x\\) or the Laplace transform on \\(t\\), \\(\\,\\)but still another idea would be to consider two subproblems\n\\[\n\\begin{aligned}\nu_t\n  &= u_{xx} +\\color{red}{\\sin \\pi x}\n  && 0 &lt; x &lt; 1, \\;0 &lt; t &lt;\\infty \\\\\nu(0,t) &= 0 && 0 &lt; t &lt; \\infty\\\\\nu(1,t) &= 0 && \\\\\nu(x,0) &= 0 && 0 \\leq x \\leq 1\n\\end{aligned}\\tag{A}\\label{eq:A}\\]\n\\[\\text{and} \\qquad\\qquad\\qquad\\]\n\\[\n\\begin{aligned}\n  u_t &= u_{xx} && 0 &lt; x &lt; 1, \\;0 &lt; t &lt;\\infty \\\\\n  u(0,t) &= 0 && 0 &lt; t &lt; \\infty\\\\\n  u(1,t) &= 0 && \\\\\n  u(x,0)\n  &= \\color{red}{\\sin 2\\pi x} && 0 \\leq x \\leq 1\n\\end{aligned}\\tag{B}\\label{eq:B}\\]\nThere two problems can be solved individually with a little effort, \\(\\,\\)and it should be clear here that the sum of the solutions to \\(\\eqref{eq:A}\\) and \\(\\eqref{eq:B}\\) is the solution to the original problem \\(\\eqref{eq:P}\\); \\(\\,\\)that is\n\\[u(x,t)=\\underbrace{\\frac{1}{\\pi^2} (1 -e^{-\\pi^2 t}) \\sin\\pi x}_{\\text{Solution to \\eqref{eq:A}}} +\\underbrace{e^{-(2\\pi)^2 t} \\sin 2\\pi x}_{\\text{Solution to \\eqref{eq:B}}}\\]\n\nSeparation of Variables and Integral Transforms as Superpositions\n\nIn separation of variables, \\(\\,\\)we generally break down the initial conditions into an infinite number of simple parts and find the response to each part. \\(\\,\\)We then sum these individual responses to find the solution to the problem\nOn the other hand, \\(\\,\\)integral transforms also use superposition, \\(\\,\\)for instance, \\(\\,\\)let’s show how the finite sine transform uses this principle. \\(\\,\\)Consider the nonhomogeneous heat equation\n\\[\n\\begin{aligned}\nu_t &= u_{xx} +f(x,t) && 0 &lt; x &lt; 1, \\;0 &lt; t &lt;\\infty \\\\\nu(0,t) &= 0 && 0 &lt; t &lt; \\infty\\\\\nu(1,t) &= 0 && \\\\\nu(x,0) &= 0 && 0 \\leq x \\leq 1\n\\end{aligned}\\]\nand its solution by use of the finite sine transform. \\(\\,\\)What we’re really doing is resolving the input \\(f(x,t)\\) into components, \\(\\,\\)finding the response \\(U_n\\) due to each component, \\(\\,\\)and adding these responses\n\n\\[{\n\\begin{aligned}\n  u_t &= u_{xx} +f(x,t)\\\\\n  &\\Downarrow\\,{\\scriptstyle \\text{finite sine transform,}} \\; {\\scriptstyle u(0,t) = u(1,t) =0} \\\\\n  \\sum_{n=1}^\\infty &\\left[ U_n' +(n\\pi)^2 U_n -F_n(t) \\right] \\sin n\\pi x =0 \\\\\n  &\\Downarrow \\, {\\scriptstyle u(x,0)=0}\\\\\n  U_n'(t) &+(n\\pi)^2 U_n(t) =F_n(t), \\;\\;\n     U_n(0) =0\\;\\;\\;\\;n=1,2,3,\\cdots \\\\\n  &\\Downarrow \\\\\n  U_n(t) &= e^{-(n\\pi)^2 t} \\int_0^t e^{(n\\pi)^2 \\tau} F_n(\\tau) \\,d\\tau\n    = \\int_0^t e^{-(n\\pi)^2 (t -\\tau)} F_n(\\tau) \\,d\\tau\\\\\n  &\\Downarrow \\\\\n  u(x,t) &=\\sum_{n=1}^\\infty U_n(t) \\sin n\\pi x\n\\end{aligned}}\\]\n\nNOTES\nIn the finite sine transform, \\(\\,\\)the resolutions were infinite series, \\(\\,\\)whereas in most other integral transforms, \\(\\,\\)the resolutions are integrals (continuous resolutions)\n\\(~\\)\nExample \\(\\,\\)Find four initial-boundary-value problems whose solutions sum to the solution of the following problem:\n\\[\n  \\begin{aligned}\n    u_t &= u_{xx} +f(x,t) && 0 &lt; x &lt; 1, \\;0 &lt; t &lt;\\infty \\\\\n    u(0,t) &= g_1(t) && 0 &lt; t &lt; \\infty\\\\\n    u(1,t) &= g_2(t) && \\\\\n    u(x,0) &= \\pi(x) && 0 \\leq x \\leq 1\n  \\end{aligned}\\]\n\\(~\\)\nExample \\(\\,\\)Solve the problem\n\\[\n  \\begin{aligned}\n    u_t &= u_{xx} +\\sin 3\\pi x && 0 &lt; x &lt; 1, \\;0 &lt; t &lt;\\infty \\\\\n    u(0,t) &= 0 && 0 &lt; t &lt; \\infty\\\\\n    u(1,t) &= 1 && \\\\\n    u(x,0) &= \\sin \\pi x && 0 \\leq x \\leq 1\n  \\end{aligned}\\]\n\\(~\\)",
    "crumbs": [
      "**Second Semester**",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Parabolic Partial Differential Equations</span>"
    ]
  },
  {
    "objectID": "ch_x1_Parabolic_PDEs.html#sec-x1-12",
    "href": "ch_x1_Parabolic_PDEs.html#sec-x1-12",
    "title": "12  Parabolic Partial Differential Equations",
    "section": "12.12 \\(~\\)The Fourier Integral and Transform",
    "text": "12.12 \\(~\\)The Fourier Integral and Transform\n\nAn integral transformation can be thought of as a resolution of a function into a certain spectrum of components:\nLet’s consider the resolution of a periodic function \\(f(x)\\) into Fourier series\n\\[ f(x)= \\frac{a_0}{2} +\\sum_{n=1}^\\infty \\left[ a_n\\cos \\frac{n\\pi x}{L} +b_n \\sin \\frac{n\\pi x}{L} \\right] \\]\nHere the coefficients \\(a_n\\) and \\(b_n\\) represent the amount of the function \\(f(x)\\) made up \\(\\displaystyle\\cos \\frac{n\\pi x}{L}\\) and \\(\\displaystyle\\sin \\frac{n\\pi x}{L}\\), \\(\\,\\)respectively\nWhile the square root\n\\[\\sqrt{a_n^2 +b_n^2}\\]\n(called the spectrum of the function) measures the amount of \\(\\,f(x)\\) with frequency \\(\\displaystyle\\frac{n\\pi}{L}\\)\nFunctions that are periodic can be resolved into infinite series (they have discrete spectrums)\nOf course, \\(\\,\\)if a function is defined only on a finite interval, \\(\\,\\)we could extend the function outside the interval in a periodic way, \\(\\,\\)so that a Fourier series representation could be obtained for the function inside the interval\nWhereas functions that are not periodic must be resolved into a continuous spectrum of values\nAlthough a nonperiodic function \\(f(x)\\) cannot be represented by an infinite series of sines and cosines, \\(\\,\\)we might write it as continuous analog of the Fourier series:\n\\[{\\scriptsize\n\\begin{aligned}\nf(x)&= \\frac{a_0}{2} +\\sum_{n=1}^\\infty \\left[ a_n\\cos \\frac{n\\pi x}{L} +b_n \\sin \\frac{n\\pi x}{L} \\right] \\\\\n&= {\\tiny \\frac{1}{2L} \\int_{-L}^L f(\\xi)\\,d\\xi +\\frac{1}{L}\n\\sum_{n=1}^\\infty \\left[ \\left( \\int_{-L}^L f(\\xi) \\cos\\frac{n\\pi\\xi}{L}\\,d\\xi \\right) \\cos \\frac{n\\pi x}{L}\n+\\left( \\int_{-L}^L f(\\xi) \\sin\\frac{n\\pi\\xi}{L}\\,d\\xi \\right) \\sin \\frac{n\\pi x}{L} \\right] }\\\\\n&\\Downarrow\\; {\\tiny \\omega_n =\\frac{n\\pi}{L}, \\;\\Delta\\omega=\\omega_{n+1} -\\omega_n=\\frac{\\pi}{L}}\\\\\n&= {\\tiny \\frac{1}{2\\pi} \\left( \\int_{-L}^L f(\\xi)\\,d\\xi\\right)\\Delta \\omega +\\frac{1}{\\pi}\n\\sum_{n=1}^\\infty \\left[ \\left( \\int_{-L}^L f(\\xi) \\cos\\omega_n\\xi\\,d\\xi \\right) \\cos \\omega_n x\n+\\left( \\int_{-L}^L f(\\xi) \\sin\\omega_n \\xi\\,d\\xi \\right) \\sin\\omega_n x \\right]\\Delta \\omega }\\\\  \n  &\\Downarrow\\;{\\tiny L \\to \\infty,\\;\\Delta \\omega \\to 0,\\; \\lim_{\\Delta\\omega \\to 0} \\sum_{n=1}^\\infty F(\\omega_n)\\,\\Delta\\omega=\\int_0^\\infty F(\\omega)\\,d\\omega }\\\\\n  \\color{red}{f(x)} \\,&\\color{red}{ =\\int_0^\\infty \\left[ a(\\omega) \\cos\\omega x +b(\\omega)\\sin\\omega x \\right]\\,d\\omega} \\\\\n  &\\color{blue}{a(\\omega)=\\frac{1}{\\pi} \\int_{-\\infty}^\\infty f(x)\\cos \\omega x\\, dx} \\\\\n  &\\color{blue}{b(\\omega)=\\frac{1}{\\pi} \\int_{-\\infty}^\\infty f(x)\\sin \\omega x\\, dx}\n\\end{aligned}}\\]\nwhere the functions \\(a(\\omega)\\) and \\(b(\\omega)\\) are known as the Fourier cosine and sine transforms and measure the cosine and sine component of \\(\\,f(x)\\) and\n\\[\\color{red}{\\sqrt{a^2(\\omega) +b^2(\\omega)}}\\]\nmeasures the \\(\\omega\\) frequency component of \\(f(x)\\) and is called the spectrum of \\(f(x)\\). \\(\\,\\)With this intutive explanation of the spectrum of a function, \\(\\,\\)we now get to the nuts and bolts of integral transforms\nThe Exponential Fourier Transform\nWe are now in a position to define what is generally known as the exponential Fourier transfrom:\n\\[{\\scriptsize\n\\begin{aligned}\n   f(x)\n   &=\\frac{1}{\\pi} \\int_0^\\infty \\int_{-\\infty}^\\infty f(\\xi)\\left[ \\cos\\omega\\xi \\cos\\omega x +\\sin\\omega \\xi \\sin\\omega x \\right]\\,d\\xi\\,d\\omega \\\\\n   &={\\scriptsize \\frac{1}{\\pi} \\int_0^\\infty \\int_{-\\infty}^\\infty f(\\xi) \\underbrace{\\color{blue}{\\cos\\omega(\\xi -x)}}_{\\cos\\omega(x-\\xi) \\; - \\;\\text{even function w.r.t. }\\omega } \\,d\\xi\\,d\\omega }\\\\\n   &{\\tiny = \\frac{1}{2\\pi} \\int_{-\\infty}^\\infty \\int_{-\\infty}^\\infty f(\\xi) \\cos\\omega(x -\\xi) \\,d\\xi\\,d\\omega }\n  {\\tiny = \\frac{1}{2\\pi} \\int_{-\\infty}^\\infty \\int_{-\\infty}^\\infty f(\\xi) \\left[ \\cos\\omega(x -\\xi) +\\color{blue}{i\\sin\\omega(x -\\xi)} \\right]\\,d\\xi\\,d\\omega }\\\\\n   &={\\scriptsize \\frac{1}{2\\pi} \\int_{-\\infty}^\\infty \\int_{-\\infty}^\\infty f(\\xi) e^{i\\omega (x -\\xi)} \\,d\\xi\\,d\\omega }\\\\\n   &=\\frac{1}{\\sqrt{2\\pi}} \\int_{-\\infty}^\\infty \\left[ \\color{red}{\\frac{1}{\\sqrt{2\\pi}}\\int_{-\\infty}^\\infty f(\\xi) e^{-i\\omega \\xi} \\,d\\xi} \\right] \\,e^{i\\omega x}\\,d\\omega  \\\\\n\\end{aligned}}\\]\n\n\\[\\begin{aligned}\n&\\Downarrow \\\\\n\\mathcal{F}[f(x)]&=\\frac{1}{\\sqrt{2\\pi}} \\int_{-\\infty}^\\infty f(x) e^{-i\\omega x}\\,dx = F(\\omega) \\\\\n\\mathcal{F}^{-1}[F(\\omega)]&=\\frac{1}{\\sqrt{2\\pi}} \\int_{-\\infty}^\\infty F(\\omega) e^{i\\omega x}\\,d\\omega = f(x)\n\\end{aligned}\\]\n\\(~\\)",
    "crumbs": [
      "**Second Semester**",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Parabolic Partial Differential Equations</span>"
    ]
  },
  {
    "objectID": "ch_x1_Parabolic_PDEs.html#sec-x1-13",
    "href": "ch_x1_Parabolic_PDEs.html#sec-x1-13",
    "title": "12  Parabolic Partial Differential Equations",
    "section": "12.13 \\(~\\)The Fourier Transform and its Application to PDEs",
    "text": "12.13 \\(~\\)The Fourier Transform and its Application to PDEs\n\nThe usefulness of the Fourier transform comes from the fact that it changes the operation of differentiation into multiplication; \\(\\,\\)that is, \\(\\,\\)partial differential equations are changed into ordinary differential equations\n\n\n\n\n\nUseful Properties of the Fourier Transform\nProperty 1 \\(\\,\\)Fourier Transform Pair\nThe Fourier transform of \\(\\;f(x), -\\infty &lt;x &lt;\\infty\\), \\(\\,\\)produces a new function \\(F(\\omega)\\):\n\\[ \\mathcal{F}[f(x)]=\\frac{1}{\\sqrt{2\\pi}} \\int_{-\\infty}^\\infty f(x) e^{-i\\omega x}\\,dx = F(\\omega)\\]\nand the inverse Fourier transform of \\(F(\\omega), -\\infty&lt;\\omega&lt;\\infty\\), \\(\\,\\)will produce the original function \\(f(x)\\):\n\\[\\displaystyle \\mathcal{F}^{-1}[F(\\omega)]=\\frac{1}{\\sqrt{2\\pi}} \\int_{-\\infty}^\\infty F(\\omega) e^{i\\omega x}\\,d\\omega = f(x)\\]\nFor example,\n\\[{\\scriptsize e^{-|x|} \\overset{\\mathcal{F}}{\\longrightarrow} \\sqrt{\\frac{2}{\\pi}} \\frac{1}{1+\\omega^2} \\overset{\\mathcal{F}^{-1}}{\\longrightarrow} e^{-|x|} }\\]\nProperty 2 \\(\\,\\)Linear Transformation\n\\[ \\mathcal{F} [af(x) +bg(x)] = a\\mathcal{F}[f(x)] +b\\mathcal{F}[g(x)] \\]\nProperty 3 \\(\\,\\)Transformation of Partial Derivatives\n\\[ \\mathcal{F}[f_x]=\\frac{1}{\\sqrt{2\\pi}} \\int_{-\\infty}^\\infty f_x e^{-i\\omega x}\\,dx = i\\omega \\mathcal{F}[f] \\]\n\\[ \\mathcal{F}[f_{xx}]=\\frac{1}{\\sqrt{2\\pi}} \\int_{-\\infty}^\\infty f_{xx} e^{-i\\omega x}\\,dx = -\\omega^2 \\mathcal{F}[f] \\]\nProperty 4 \\(\\,\\)Convolution Property\nEvery integral transform has what is called a convolution property. \\(\\,\\)The general idea is that the transform of a product of two functions \\(\\,f(x)g(x)\\,\\) is not the product of the individual transforms:\n\\[ \\mathcal{F}[f(x)g(x)]\\neq\\mathcal{F}[f(x)]\\mathcal{F}[g(x)] \\]\nHowever, \\(\\,\\)in transform theory, \\(\\,\\)there is something called the convolution \\(\\,f*g\\,\\) of two functions that more or less plays the role of the product:\n\\[ \\mathcal{F}[f*g]=\\mathcal{F}[f(x)]\\mathcal{F}[g(x)]\\]\nSo what is this mysterious convolution \\(\\,f*g\\)? \\(\\,\\)It’s given by formula\n\\[ (\\,f*g)(x)=\\frac{1}{\\sqrt{2\\pi}} \\int_{-\\infty}^\\infty f(x -\\xi) g(\\xi)\\,d\\xi \\]\nThe importance of the convolution in applications is due to the fact that quite often, \\(\\,\\)the final step in solving a PDE boils down to finding the inverse transform of some expression that we can interpret as the product of two transforms \\(\\mathcal{F}[f]\\mathcal{F}[g]\\)\nSolution of an Initial-Value Problem\nConsider the heat flow in an infinite rod where the initial temperature is \\(\\,u(x,0)=\\phi(x)\\)\n\\[\n\\begin{aligned}\n  u_t &=\\alpha u_{xx}\n    &&\\; \\color{red}{-\\infty &lt; x &lt; \\infty}, \\;0 &lt; t &lt; \\infty \\\\\n  u(x,0) &=\\phi(x) &&-\\infty &lt; x &lt; \\infty\n\\end{aligned}\\]\n\nSTEP 1 \\(\\,\\)Transforming the Problem\n\\[\\begin{aligned}\n\\mathcal{F}[u_t] &= \\alpha \\mathcal{F}[u_{xx}]\\\\\n\\mathcal{F}[u(x,0)] &= \\mathcal{F} [\\phi(x)]\\\\\n&\\Downarrow \\\\\n\\frac{dU(t)}{dt} &= -\\alpha\\omega^2 U(t)\\\\\nU(0)&=\\Phi(\\omega)\n\\end{aligned}\\]\nSTEP 2 \\(\\,\\)Solving the Transformed Problem\n\\[U(t)=\\Phi(\\omega) e^{-\\alpha \\omega^2 t}\\]\nSTEP 3 \\(\\,\\)Finding the Inverse Transform\n\\[\\begin{aligned}\nu(x,t)&= \\mathcal{F}^{-1}\\left[ \\Phi(\\omega) e^{-\\alpha\\omega^2 t} \\right]\\\\\n&= \\mathcal{F}^{-1}\\left[ \\Phi(\\omega) \\right] * \\mathcal{F}^{-1}\\left[ e^{-\\alpha\\omega^2 t} \\right]\\\\\n&= \\phi(x)*\\frac{1}{\\sqrt{2\\alpha t}} e^{-x^2/4\\alpha t}\\\\\n&= \\frac{1}{2\\sqrt{\\alpha\\pi t}} \\int_{-\\infty}^{\\infty} \\phi(\\xi) e^{-(x -\\xi)^2/4\\alpha t} \\,d\\xi\n\\end{aligned}\\]\nNote that the integrand is made up of two terms:\n\n\\(\\,\\)The initial temperature \\(\\phi(x)\\)\n\n\\(\\,\\)the Green’s function or impulse-response function\n\n\\[ G(x-\\xi,t)=\\frac{1}{2\\sqrt{\\alpha\\pi t}}e^{-(x -\\xi)^2/4\\alpha t}, \\;\\;\\lim_{t\\to 0}G(x-\\xi,t) = \\underbrace{\\delta(x-\\xi)}_{\\text{Dirac delta function}}\\]\nIt can be shown that this impulse-response function \\(G(x-\\xi,t)\\) \\(\\,\\)is the temperature response to an initial temperature impulse at \\(\\,x=\\xi\\)\n\n\n\n\n\nHence, \\(\\,\\)the interpretation of solution is that the initial temperature \\(\\,u(x,0)=\\phi(x)\\,\\) is decomposed into a continuum of impulses of \\(\\,\\phi(\\xi)\\,\\) at each point \\(x=\\xi\\) and the resulting temperature \\(\\,\\phi(\\xi)G(x-\\xi,t)\\,\\) is found. \\(\\,\\)These resulting temperatures are then added (integrated) to obtain the solution. This general idea is also superposition\n\\(~\\)\n\ndef example_plot5(alpha, xi, t, x):\n\n    plt.figure(figsize=(7, 5))\n\n    for tt in t:\n      plt.plot(x, 0.5 /np.sqrt(alpha *np.pi *tt) \n            *np.exp(-(x -xi)**2 /(4.0 *alpha *tt)), \n          label=f'$t={tt:0.2f}$')\n    plt.legend(loc='upper right')          \n\n    plt.text(x[0] +1, 1.5, \n      r'$$G(x-\\xi,t)=\\frac{1}{2\\sqrt{\\alpha\\pi t}}'\n      r'e^{-(x-\\xi)^2/4\\alpha t}$$')\n\n    plt.xlim(x[0], x[-1])\n    plt.ylim(0, 2)\n\n    plt.xlabel('$x$')\n    plt.ylabel(r'$G(x-\\xi,t)$')\n\n    plt.title(rf'$\\alpha={alpha:2.1f}, \\;\\xi={xi:2.1f}$')\n\n    plt.show()\n\nalpha = 1\nxi = 0\n\nt = [0.05, 0.1, 0.2, 0.4, 0.8, 1.6, 3.2]\nx = np.linspace(-6, 6, 1000)\n\nexample_plot5(alpha, xi, t, x) \n\n\n\n\n\n\n\nFigure 12.5: Impulse-response function\n\n\n\n\n\nNOTES\nThe major drawback of the Fourier transform is that all functions can not be transformed. \\(\\,\\)For example, \\(\\,\\)even simple functions like\n\\[\n\\begin{aligned}\nf(x) &= \\text{constant} \\\\\nf(x) &= e^x\\\\\nf(x) &= \\sin x\n\\end{aligned}\\]\ncannot be transformed, \\(\\,\\)since the integral\n\\[\\mathcal{F}[f]=\\frac{1}{\\sqrt{2\\pi}} \\int_{-\\infty}^\\infty f(x) e^{-i\\omega x}\\,dx\\]\ndoes not exist. \\(\\,\\)Only functions that damp to zero sufficiently fast as \\(|x| \\to\\infty\\) have transforms",
    "crumbs": [
      "**Second Semester**",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Parabolic Partial Differential Equations</span>"
    ]
  },
  {
    "objectID": "ch_x1_Parabolic_PDEs.html#sec-x1-14",
    "href": "ch_x1_Parabolic_PDEs.html#sec-x1-14",
    "title": "12  Parabolic Partial Differential Equations",
    "section": "12.14 \\(~\\)Sine and Cosine Transforms",
    "text": "12.14 \\(~\\)Sine and Cosine Transforms\n\nFourier Sine Transform Pair\n\\[\n\\begin{aligned}\n  \\mathcal{F}_s\\left[ f(x) \\right]\n    &= \\frac{2}{\\pi} \\int_0^\\infty f(x) \\sin\\omega x\\,dx =F(\\omega) \\\\\n  \\mathcal{F}_s^{-1} \\left[ F(\\omega) \\right ]\n    &= \\int_0^\\infty F(\\omega) \\sin\\omega x \\,d\\omega =f(x)\n\\end{aligned}\\]\nFourier Cosine Transform Pair\n\\[\n\\begin{aligned}\n  \\mathcal{F}_c \\left[ f(x) \\right] &= \\frac{2}{\\pi}\n    \\int_0^\\infty f(x) \\cos\\omega x\\,dx =F(\\omega) \\\\\n  \\mathcal{F}_c^{-1} \\left[ F(\\omega) \\right ]&=\n     \\int_0^\\infty F(\\omega) \\cos\\omega x \\,d\\omega =f(x)\n\\end{aligned}\\]\nThe next step is to list a few properties of these transforms that make them work\n\\[{\\scriptsize\n\\begin{aligned}\n  \\mathcal{F}_s\\left[ f'(x) \\right]  \n    &= \\frac{2}{\\pi} \\int_0^\\infty f'(x)\n      \\sin\\omega x\\,dx\\\\\n    &= \\left.\\frac{2}{\\pi} \\, f(x)\n      \\sin\\omega x \\,\\right|_0^\\infty\n     -\\omega\\frac{2}{\\pi}\\int_0^\\infty f(x)\n      \\cos\\omega x\\,dx\n     =-\\omega\\mathcal{F}_c\\left[ f(x) \\right]  \n    \\phantom{xxxxxxxxxx}\n\\end{aligned}}\\]\n\\[{\\scriptsize\n\\begin{aligned}\n  \\mathcal{F}_c\\left[ f'(x) \\right]\n    &= \\frac{2}{\\pi} \\int_0^\\infty f'(x)\n     \\cos\\omega x\\,dx \\\\\n    &= \\left.\\frac{2}{\\pi} \\, f(x)\n     \\cos\\omega x \\,\\right|_0^\\infty\n     +\\omega\\frac{2}{\\pi}\\int_0^\\infty f(x)\n      \\sin\\omega x\\,dx =-\\frac{2}{\\pi}\\,f(0)\n    +\\omega\\mathcal{F}_s\\left[ f(x) \\right]\n     \\phantom{xx}\n\\end{aligned}}\\]\n\\[{\\scriptsize \\begin{aligned}\n   \\color{red}{\\mathcal{F}_s\\left[ f''(x) \\right]}\n  &= \\frac{2}{\\pi} \\int_0^\\infty f''(x)\n   \\sin\\omega x\\,dx =\n  \\left.\\frac{2}{\\pi}f'(x) \\sin\\omega x\n   \\,\\right|_0^\\infty -\\omega\\frac{2}\n    {\\pi}\\int_0^\\infty f'(x) \\cos\\omega x\\,dx \\\\\n  &=\\color{red}{\\frac{2}{\\pi}\\omega \\,f(0)-\n   \\omega^2\\mathcal{F}_s\\left[ f(x) \\right]}\n\\end{aligned}}\\]\n\\[{\\scriptsize \\begin{aligned}\n  \\color{red}{\\mathcal{F}_c\\left[ f''(x) \\right]}\n  &= \\frac{2}{\\pi} \\int_0^\\infty f''(x)\n  \\cos\\omega x\\,dx =\\left.\\frac{2}{\\pi}f'(x)\n   \\cos\\omega x \\,\\right|_0^\\infty\n   +\\omega\\frac{2}{\\pi}\\int_0^\\infty f'(x)\n   \\sin\\omega x\\,dx \\\\\n  &=\\color{red}{-\\frac{2}{\\pi} \\,f'(0) -\n   \\omega^2\\mathcal{F}_c\\left[ f(x) \\right]}\n\\end{aligned}}\\]\nSolution of an Semi-infinite-Diffusion Problem via the Sine Transform\nThe problem we are interested in is the semi-infinite diffusion problem\n\\[\n\\begin{aligned}\n  u_t &= \\alpha u_{xx}\n   && \\color{red}{0&lt;x&lt;\\infty}, \\; 0 &lt; t &lt;\\infty \\\\\n  \\color{red}{u(0,t)} & \\color{red}{\\;= A}\n   && 0 &lt; t &lt; \\infty \\\\\n  u(x,0) &= 0 && 0 \\leq x &lt; \\infty\n\\end{aligned}\\]\n\n\n\n\n\nTo solve this, \\(\\,\\)we break into three simple steps. \\(\\,\\)First our strategy is to transform the \\(x\\)-variable via Fourier sine transform so that we get an ODE in time\n\n\\(~\\)\nSTEP 1\nWe start by transforming each side of the PDE;\n\\[\n\\begin{aligned}\n  \\mathcal{F}_s [u_t]\n    &=\\alpha \\mathcal{F}_s\\left[u_{xx} \\right] \\\\\n    &\\Downarrow\\;{\\scriptsize\n    \\mathcal{F}_s[u(x,t)]=U(t)} \\\\\n    \\frac{d}{dt}U(t)\n    &= \\alpha\\left [\\frac{2}{\\pi}\\omega u(0,t)\n     -\\omega^2\\mathcal{F}_s[u]  \\right ]=\\alpha\\left\n      [A\\frac{2}{\\pi}\\omega -\\omega^2 U(t)  \\right ]\\\\\n    &\\Downarrow \\;{\\scriptsize\n     \\mathcal{F}_s[u(x,0)]=U(0)}\\\\\n     \\frac{dU}{dt}&+\\alpha \\omega^2 U\n     =A\\frac{2}{\\pi}\\alpha\\omega, \\;U(0)=0\n\\end{aligned}\\]\nWe arrive at the ODE\nSTEP 2\nTo solve this IVP, \\(\\,\\)we could use a variety of elementary techniques from ordinary differential equations. \\(\\,\\)The solution is\n\\[ U(t)=A\\frac{2}{\\pi}\\left[\\frac{1 -e^{-\\alpha\\omega^2 t}}{\\omega} \\right]\\]\nWe have now found the sine transformation for the answer \\(\\,u(x,t)\\)\nSTEP 3\nTo find the solution, \\(\\,\\)we can either evaluate the inverse transform directly from the integral or else resort to the tables. \\(\\,\\)Using the tables, \\(\\,\\)we get\n\\[ u(x,t)=A\\, \\mathrm{erfc} \\left( \\frac{x}{2\\sqrt{\\alpha t}} \\right)\\]\nwhere \\(\\mathrm{erfc}(\\eta), \\;0 &lt; \\eta &lt; \\infty\\), \\(\\,\\)is called the complementary-error function and is given by\n\\[ \\mathrm{erfc}(\\eta)=\\frac{2}{\\sqrt{\\pi}} \\int_\\eta^\\infty e^{-\\beta^2}\\,d\\beta\\]\n\\(~\\)\n\nfrom scipy.special import erf, erfc\n\ndef example_plot6(A, alpha, t, x):\n    plt.figure(figsize=(7, 5))\n    for tt in t:\n        plt.plot(x, A *erfc(x /(2 *np.sqrt(alpha *tt))), \n          label=rf'$t={tt:0.1f}$')\n    plt.xlim(0, x[-1])\n    plt.ylim(0, 1.2)\n    plt.legend(loc='upper right')\n    plt.xlabel('$x$')\n    plt.ylabel('$u(x,t)$')\n    plt.text(2, 0.8, \n      r'$$u(x,t)=A\\, \\mathrm{erfc}' \n      r'\\left( \\dfrac{x}{2\\sqrt{\\alpha t}} \\right)$$')\n    plt.title(rf'$A={A:3.1f}, \\; \\alpha={alpha:3.1f}$')\n    plt.show()\n\nA = 1\nalpha = 1\n\nt = [0.1, 0.2, 0.4, 0.8, 1.6, 3.2]\nx = np.linspace(0, 8)\n\nexample_plot6(A, alpha, t, x)    \n\n\n\n\n\n\n\nFigure 12.6: Heat conduction in semi-infinite domain with fixed temperature at the boundary\n\n\n\n\n\n\\(~\\)\nExample \\(\\,\\)Solve by means of the cosine transform\n\\[\n\\begin{aligned}\nu_t &= \\alpha u_{xx} && \\color{red}{0 &lt; x &lt; \\infty}, \\;0 &lt; t &lt; \\infty\\\\\n\\color{red}{u_x(0,t)} &= 0 && 0 &lt; t &lt; \\infty\\\\\n\\color{red}{u(x,0)} &=\\color{red}{H(1-x)} && 0 \\leq x &lt; \\infty\n\\end{aligned}\\]\nwhere \\(H(x)\\) is the Heaviside function\n\\(~\\)\n\nfrom scipy import integrate\n\nx0 = [0, 1, 1, 8]\nu0 = [1, 1, 0, 0]\n\ndef example_plot6(alpha, t, x, u_solution):\n    plt.figure(figsize=(7, 5))\n    plt.plot(x0, u0, 'r:', label='$t=0.0$')\n    for tt in t:\n        u = u_solution(alpha, tt, x)\n        plt.plot(x, u, label='$t=%0.1f$' % tt)       \n    plt.xlim(0, x[-1])\n    plt.ylim(0, 1.1)\n    plt.legend(loc='upper right')\n    plt.xlabel('$x$')\n    plt.ylabel('$u(x,t)$')\n    plt.text(2, 0.35, \n        (r'$$u(x,t)=\\dfrac{2}{\\pi}\\,' \n         r'\\int_0^\\infty \\dfrac{\\sin\\omega}{\\omega}\\,' \n         r'\\exp(-\\alpha\\omega^2 t)\\, \\cos\\omega x \\,d\\omega$$'))\n    plt.title(r'$\\alpha=%3.1f$' % alpha)    \n    plt.show()\n\nalpha = 1\n\nt = [0.1, 0.2, 0.4, 0.8, 1.6, 3.2, 6.4]\nx = np.linspace(0, 8)\n\ndef u_solution(alpha, t, x):   \n    u = np.zeros_like(x)\n    for i, xx in enumerate(x):\n        f = lambda w : np.sin(w) /w *np.exp(-alpha*w*w*t) *np.cos(w*xx) \n        u[i], _ = integrate.quad(f, 0, np.inf) \n    return 2.0 /np.pi *u\n\nexample_plot6(alpha, t, x, u_solution)      \n\n\n\n\n\n\n\nFigure 12.7: Heat conduction in semi-infinite domain insulated at the boundary\n\n\n\n\n\n\\(~\\)",
    "crumbs": [
      "**Second Semester**",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Parabolic Partial Differential Equations</span>"
    ]
  },
  {
    "objectID": "ch_x1_Parabolic_PDEs.html#sec-x1-15",
    "href": "ch_x1_Parabolic_PDEs.html#sec-x1-15",
    "title": "12  Parabolic Partial Differential Equations",
    "section": "12.15 \\(~\\)The Laplace Transform",
    "text": "12.15 \\(~\\)The Laplace Transform\n\nThe Laplace transform\n\\[ \\mathcal{L}[f(t)]= \\int_0^\\infty f(t) e^{-st}\\,dt \\]\nis probably the only integral transform the student has seen before, \\(\\,\\)since it is a very powerful tool for transforming initial-value problems in ODE into algebraic equations\nNot only is the Laplace transform useful in transforming ODEs into algebraic equations, \\(\\,\\)but now we will use the Laplace transform to transform PDEs into ODEs\nThe major difference in applying the Laplace transform to PDEs in contrast to ODEs is that now when the original PDE is transformed, \\(\\,\\)the new resulting equation will be either a new PDE with one less independent variable or else an ODE in one variable\nWe must then decide how to solve this new problem (maybe by another transform, \\(\\,\\)by separation of variables, \\(\\,\\)and so on)\n\n\\(~\\)\nProperty 1 \\(\\,\\) Transform Pair\n\nThe Laplace transform and its inverse are given by\n\\[\\begin{aligned}\n\\mathcal{L}[f(t)] &= F(s) =\\int_0^\\infty f(t) e^{-st}\\,dt \\\\\n\\mathcal{L}^{-1}[F(s)] &= \\frac{1}{2\\pi i}\\int_{\\gamma-i\\infty}^{\\gamma+i\\infty} F(s)e^{st}\\,ds\n\\end{aligned}\\]\nThe Laplace transform has one major advantage over the Fourier transform in that the damping factor \\(e^{-st}\\) in the integrand allows us to transform a wider class of functions (the factor \\(e^{-i\\omega x}\\) in the Fourier transform doesn’t do any damping since its absolute value is one)\nIn fact, \\(\\,\\)the exact conditions that insure that a function \\(f(t)\\) has a Laplace transform are given by the following theorem:\n\\(\\,\\)\nIf we can find constants \\(\\,M\\) and \\(\\,a\\,\\) such that\n\\[ \\left|f(t)\\right| \\leq Me^{at} \\;\\text{ for } \\; t &gt; T, \\]\nthen \\(\\displaystyle \\int_0^\\infty f(t)\\,e^{-st}\\,dt~\\) exists for \\(s&gt;a\\)\nWe now list a few functions that have Laplace transforms\n\\[{\\scriptsize \\begin{aligned}\n  f(t)\n    &= 1\n    && (\\text{pick } M=1, a=0)\n      \\;\\;\\longrightarrow\\;\\; F(s)=\\frac{1}{s}\\\\\n  f(t)\n    &=e^{2t}\n    && (\\text{pick } M=1, a=2)\n      \\;\\;\\longrightarrow\\;\\; F(s)=\\frac{1}{s-2},\\;\\; s &gt; 2 \\\\\n  f(t)\n    &=\\sin \\omega t\n    && (\\text{pick } M=1, a=0)\n      \\;\\;\\longrightarrow\\;\\; F(s)=\\frac{\\omega}{s^2 +\\omega^2} \\\\\n  f(t)\n    &=e^{t^2}\n    && (\\text{doesn't have a Laplace transform})\n\\end{aligned}}\\]\nIn the definition of the Laplace transform, \\(\\,\\)the variable \\(s\\) is taken to be a real variable \\(0 &lt; s &lt; \\infty\\). \\(\\,\\)It is possible to extend this definition to complex values of \\(s\\) and, in fact, to evaluate the inverse Laplace transform\n\\[ \\mathcal{L}^{-1}[F(s)]=\\frac{1}{2\\pi i}\\int_{\\gamma-i\\infty}^{\\gamma+i\\infty} F(s)e^{st}\\,ds \\]\nWe must often resort to contour integration in the complex plane and the theory of residues. \\(\\,\\)We won’t bother ourselves with this topic here but will use the tables for finding inverse transforms\n\nProperty 2 \\(\\,\\) Transforms of Partial Derivatives\n\nSuppose we have a function \\(u(x,t)\\) of two variables. \\(\\,\\)Since the Laplace transform transforms the \\(t\\)-variable, \\(\\,\\)the rules of transformation for partial derivatives are\n\\(~\\)\n\\(\\displaystyle \\color{red}{\\mathcal{L}[u_t]}=\\int_0^\\infty u_t(x,t) e^{-st}\\,dt=\\color{red}{sU(x,s) -u(x,0)}\\)\n\\(\\displaystyle \\color{red}{\\mathcal{L}[u_{tt}]}=\\int_0^\\infty u_{tt}(x,t) e^{-st}\\,dt=\\color{red}{s^2U(x,s) -su(x,0) -u_t(x,0)}\\)\n\\(\\displaystyle \\mathcal{L}[u_x]=\\int_0^\\infty u_x(x,t) e^{-st}\\,dt=\\frac{\\partial U}{\\partial x}(x,s)\\)\n\\(\\displaystyle \\mathcal{L}[u_{xx}]=\\int_0^\\infty u_{xx}(x,t) e^{-st}\\,dt=\\frac{\\partial^2 U}{\\partial x^2}(x,s)\\)\n\nProperty 3 \\(\\,\\) Convolution Theory\n\nConvolution plays the same role here as it did in the Fourier transform, \\(\\,\\)but now the convolution is defined slightly differently. \\(\\,\\)The finite convolution of two functions \\(\\,f\\) and \\(g\\) is defined by\n\\[ \\color{red}{(f*g)(t)=\\int_0^t f(\\tau) g(t-\\tau)\\,d\\tau =\\int_0^t f(t- \\tau) g(\\tau)\\,d\\tau} \\]\nAs in the case of the infinite convolution, \\(\\,\\)the important property of this new convolution is that\n\\[ \\mathcal{L}[f*g]=\\mathcal{L}[f]\\,\\mathcal{L}[g] \\]\nor the equivalent formula\n\\[ \\mathcal{L}^{-1}\\left\\{\\mathcal{L}[f] \\,\\mathcal{L}[g] \\right\\}=f*g \\]\nFor example\n\\[ \\mathcal{L}^{-1}\\left[\\frac{1}{s} \\cdot \\frac{1}{s^2+1}\\right] = \\int_0^t \\sin \\tau \\,d\\tau=1-\\cos t \\]\n\nNOTES\nThe Laplace transform can also be applied to problems where the PDE is nonhomogeneous (in separation of variables, \\(\\,\\)the equation had to be homogeneous), \\(\\,\\)but the Laplace transform will generally work only if the equation has constant coefficients (in separation of variables, \\(\\,\\)we could have variable coefficients)\n\\(~\\)\n\nHeat Conduction in a Semi-infinite Medium\n\n\n\n\n\n\nConsider a large (deep) container of liquid that is insulated on the sides. Suppose the liquid has an initial temperature of \\(u_0\\) and that the temperature of the air above liquid is zero. Our goal is to find the temperature of the liquid at various depths of the container at different \\(t\\) values:\n\\[\\begin{aligned}\nu_t &= u_{xx} && 0 &lt; x &lt; \\infty,\\;0 &lt; t &lt; \\infty \\\\\nu_x(0,t) -u(0,t) &= 0 && 0 &lt; t &lt; \\infty \\\\\nu(x,0) &= u_0 && 0 &lt; x &lt;\\infty\n\\end{aligned}\\]\nTransforming our problem, \\(\\,\\)we arrive at an ODE in \\(x\\)\n\\[\n\\begin{aligned}\n  sU(x) -u_0 &= \\frac{d^2U}{dx^2} && 0 &lt; x &lt; \\infty \\\\[5pt]\n  \\frac{dU}{dx}(0) &= U(0) &&\n\\end{aligned}\\]\nWe first find the general solution of the ODE, \\(\\,\\)which is\n\\[ U(x)=c_1 e^{\\sqrt{s}x} +c_2 e^{-\\sqrt{s}x} +\\frac{u_0}{s} \\]\nSubstituting the BCs into this expression allows us to find the constants \\(c_1\\) and \\(c_2\\) (first note that \\(c_1=0\\,\\) or else the temperature will go to infinity as \\(x\\) gets large). \\(\\,\\)Finding \\(c_2\\) from the BC at \\(x=0\\,\\) gives us the answer for \\(U(x)\\)\n\\[ {\\scriptsize U(x)=\n  -u_0 \\left[ \\frac{e^{-\\sqrt{s}x}}{s(\\sqrt{s} +1)} \\right]\n   +\\frac{u_0}{s}\n  =\\frac{u_0}{s} -u_0\\left[ \\frac{e^{-\\sqrt{s}x}}{s}\n  -\\frac{e^{-\\sqrt{s}x}}{\\sqrt{s}(\\sqrt{s} +1)}\\right]} \\]\nNow for the last step, \\(\\,\\)to find \\(u(x,t)\\), \\(\\,\\)we compute\n\\[ {\\scriptsize u(x,t)\n=\\mathcal{L}^{-1}\\left[ U(x,s) \\right]\n=u_0 -u_0 \\left[ \\mathrm{erfc}\\left( \\frac{x}{2\\sqrt{t}} \\right)\n-\\mathrm{erfc}\\left( \\sqrt{t}+\\frac{x}{2\\sqrt{t}} \\right)\ne^{x+t}\\right]} \\]\nSee Inverse Laplace Transform Table\n\n\n\\(~\\)\n\ndef example_plot7(t, x, u0, u_solution):\n\n    plt.figure(figsize=(7, 5))\n    plt.plot(x, np.full_like(x, u0), label='$t=0$')\n    for tt in t:\n        plt.plot(x, u_solution(tt, x, u0), label='$t=%1.3f$' % tt)\n    plt.legend(loc='lower right')\n\n    plt.xlim(0, 5)\n    plt.ylim(0.4, 1.2)\n\n    plt.xlabel('$x$')\n    plt.ylabel('$u(x,t)$')\n    plt.title(r'$u_0=%3.1f$' % u0)\n\n    plt.show()\n\ndef u_solution(t, x, u0):\n    sqrt_t = np.sqrt(t)\n    x_2sqrt_t = x /(2 *sqrt_t)\n    return u0 -u0*(erfc(x_2sqrt_t) \n              -erfc(sqrt_t +x_2sqrt_t) *np.exp(x +t))\n\nu0 = 1\nt = np.logspace(-3, 0, 6)\nx = np.linspace(0, 5, 300)\n\nexample_plot7(t, x, u0, u_solution)    \n\n\n\n\n\n\n\nFigure 12.8: Heat conduction in semi-infinite domain with 3rd kind BC\n\n\n\n\n\n\\(~\\)",
    "crumbs": [
      "**Second Semester**",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Parabolic Partial Differential Equations</span>"
    ]
  },
  {
    "objectID": "ch_x1_Parabolic_PDEs.html#sec-x1-16",
    "href": "ch_x1_Parabolic_PDEs.html#sec-x1-16",
    "title": "12  Parabolic Partial Differential Equations",
    "section": "12.16 \\(~\\)The Convection Term \\(u_x\\) in the Diffusion Problems",
    "text": "12.16 \\(~\\)The Convection Term \\(u_x\\) in the Diffusion Problems\n\nSuppose now we consider the problem of finding the concentration of a substance upwards from the surface of the earth where the substance both diffuses through the air and is carried upwards (convected) by moving currents (moving with velocity \\(V\\))\nIn any case, \\(\\,\\)it is our purpose here to solve the diffusion-convection equation\n\\[ u_t = Du_{xx} -V u_x\\]\nTo get an idea of what solutions look like or how they behave with the convection term included, \\(\\,\\)let’s first work a problem that is pure convection (the diffusion term is zero)\nA typical problem would be dumping a substance into a clean air (moving with upward velocity \\(V\\)) and observing the concentration of the substance at the upperside. \\(\\,\\)Then the concentration of substance \\(u(x,t)\\) can be found by solving the following mathematical model:\n\\[\n\\begin{aligned}\n   u_t &= -Vu_x && \\phantom{xx}\\color{red}{0 &lt; x &lt; \\infty}, \\; 0 &lt; t &lt; \\infty \\\\ \\\\\n   u(0,t) &= P &&\\longleftarrow\\; \\text{Constant Input of the Substance} \\\\\n   u(x,0) &= 0 &&\\longleftarrow\\; \\text{Initially a Clean Air}\n\\end{aligned} \\tag{CV}\\label{eq:CV}\\]\nSince the \\(x\\)-variable is unbounded, \\(\\,\\)we use the Laplace transform on \\(t\\)\n\\[\\begin{aligned}\n  sU(x) &=-V\\frac{dU}{dx}, \\;\\;\\; 0 &lt; x &lt; \\infty \\\\\n  U(0) &= \\frac{P}{s}\\\\\n  &\\Downarrow \\\\\n  U(x) &=\\frac{P}{s} e^{-\\frac{x}{V}s}  \\\\\n  &\\Downarrow \\\\\n  u(x,t) &= P \\,H(t -x/V) \\\\\n  &\\Downarrow \\\\\n  u(x,t) &=\n  \\begin{cases}\n    P & x \\leq Vt \\\\\n    0 & x &gt; Vt  \n  \\end{cases}\n\\end{aligned}\\]\nTo see what happens when a moving wave diffuses, \\(\\,\\)we solve the following problem \\((P=1)\\)\n\\[\n\\begin{aligned}\n   u_t &= Du_{xx}-Vu_x && -\\infty &lt; x &lt; \\infty, \\;0 &lt; t &lt; \\infty \\\\\n   u(x,0) &= P\\left(1 -H(x)\\right) && -\\infty &lt; x &lt; \\infty\n\\end{aligned} \\tag{DC}\\label{eq:DC}\\]\nNote that in the new problem \\(\\eqref{eq:DC}\\), \\(\\,\\)we have moved the boundary to \\(-\\infty\\) (we now have an initial-value problem)\nIn this case, \\(\\,\\)we introduce a new coordinate \\(\\xi\\), \\(\\,\\)which moves along the \\(x\\)-axis with velocity \\(V\\). \\(\\,\\)In other words, \\(\\,\\)we now place our coordinate system so that it moves with the wave front (of course, \\(\\,\\)now when diffusion in addition to convection, \\(\\,\\)we won’t have a sharp wave front)\nMathematically this says that we change our space coordinate \\(\\,x\\,\\) to \\(\\,\\xi=x -Vt\\). \\(\\,\\)It’s now clear that when \\(\\xi=0,\\) \\(\\,\\)we are on the wave front\nTo begin, \\(\\,\\)we introduce new coordinates \\((\\xi, \\tau)\\) and use the chain rule to get the new IVP in terms of \\(\\xi\\) and \\(\\tau\\):\n\\[\\begin{aligned}\n  u_t&= D u_{xx} -Vu_x\\\\\n  &\\Downarrow\\; \\xi=x-Vt, \\;\\tau=t \\\\\n  u_t&= u_{\\xi}\\xi_t+u_\\tau\\tau_t=-Vu_\\xi+u_\\tau\\\\\n  u_x&= u_\\xi \\xi_x+u_\\tau\\tau_x =u_\\xi\\\\\n  u_{xx}&=(u_\\xi)_x=u_{\\xi\\xi}\\xi_x=u_{\\xi\\xi} \\\\\n  &\\Downarrow \\\\\n  -Vu_\\xi+u_\\tau &=Du_{\\xi\\xi} -Vu_\\xi\\\\\n  &\\big\\Downarrow \\;\\xi=x \\text{ at } t=0\\\\\n  u_\\tau &=D u_{\\xi\\xi},\\;\\;\\;-\\infty&lt;\\xi&lt;\\infty \\\\\n  u(\\xi,0)&=P(1-H(\\xi))=\\phi(\\xi)\n\\end{aligned}\\]\nThis problem has already been solved in Section 12.13 by the Fourier transform and has the solution\n\\[ {\\scriptsize u(\\xi,\\tau)=\\frac{1}{2\\sqrt{D\\pi\\tau}} \\int_{-\\infty}^{\\infty}\\phi(\\omega) e^{-(\\xi-\\omega)^2/4D\\tau}\\,d\\omega\n=\\frac{P}{2\\sqrt{D\\pi\\tau}} \\int_{-\\infty}^0 e^{-(\\xi-\\omega)^2/4D\\tau}\\,d\\omega } \\]\nwhere \\(\\,\\phi(\\omega)\\) is the initial condition\nBy letting\n\\[{\\beta=\\frac{\\xi -\\omega}{2\\sqrt{D\\tau}},\\;\\; d\\beta=-\\frac{1}{2\\sqrt{D\\tau}}d\\omega}\\]\nwe get the interesting result\n\\[\\begin{aligned}\n  \\displaystyle u(\\xi,\\tau)&=\\frac{P}{2} \\left[ \\frac{2}{\\sqrt{\\pi}} \\int_{\\frac{\\xi}{2\\sqrt{D\\tau}}}^\\infty e^{-\\beta^2}\\,d\\beta \\right]  \n   = {\\scriptsize \\begin{cases}\n  \\displaystyle\\;\\frac{P}{2} \\left[ 1+\\mathrm{erf}\\left( -\\frac{\\xi}{2\\sqrt{D\\tau}} \\right) \\right],& \\xi&lt;0 \\\\\n  \\displaystyle\\;\\frac{P}{2}\\, \\mathrm{erfc}\\left( \\frac{\\xi}{2\\sqrt{D\\tau}} \\right), & \\xi \\geq 0\n    \\end{cases}}\n  \\end{aligned}\\]\nFinally, \\(\\,\\)the solution of our problem in terms of the coordinates \\(x\\) and \\(t\\,\\) is\n\\[u(x,t)=\\begin{cases}\n  \\displaystyle\\;\\frac{P}{2} \\left[ 1+\\mathrm{erf}\\left( \\frac{Vt -x}{2\\sqrt{Dt}} \\right) \\right], & Vt&gt;x \\\\\n  \\displaystyle\\;\\frac{P}{2}\\, \\mathrm{erfc}\\left( \\frac{x-Vt}{2\\sqrt{Dt}} \\right), & Vt \\leq x\n\\end{cases}\\]\n\n\\(~\\)\n\ndef example_plot8(D, V, t, x, u_solution):\n    plt.figure(figsize=(8, 6))\n    plt.plot([-1, 0, 0, x[-1]], [1, 1, 0, 0], 'r:', label='$t=0.0$')\n    for tt in t:\n        x_tt = V*tt \n        plt.plot(x, u_solution(D, V, tt, x), label=rf'$t={tt:3.1f}$')\n        plt.plot([-1, x_tt, x_tt, x[-1]], [1, 1, 0, 0], 'k:') \n    plt.xlim(-1, x[-1])\n    plt.ylim(0, 1.2)\n    plt.legend(loc='upper left', bbox_to_anchor=(1.0, 1.0))\n    plt.xlabel('$x$')\n    plt.ylabel('$u(x,t)$')\n    plt.title(rf'$D={D:4.1f},\\;V={V:3.1f},\\;P=1.0$')    \n    plt.show()        \n\ndef u_solution(D, V, t, x):\n    x_t = (x -V*t) /(2.0 *np.sqrt(D*t))      \n    return [0.5*(1.0 +erf(-q)) if q &lt;= 0.0 else 0.5*erfc(q) for q in x_t] \n\nD = 0.1\nV = 1.0\n\nt = [2.0, 4.0, 6.0, 8.0]\nx = np.linspace(0, 16, 400)\n\nexample_plot8(D, V, t, x, u_solution)  \n\n\n\n\n\n\n\nFigure 12.9: Diffusion-convection Heat transfer in semi-infinite domain\n\n\n\n\n\n\\(~\\)",
    "crumbs": [
      "**Second Semester**",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Parabolic Partial Differential Equations</span>"
    ]
  },
  {
    "objectID": "ch_x1_Parabolic_PDEs.html#sec-x1-17",
    "href": "ch_x1_Parabolic_PDEs.html#sec-x1-17",
    "title": "12  Parabolic Partial Differential Equations",
    "section": "12.17 \\(~\\)Duhamel’s Principle",
    "text": "12.17 \\(~\\)Duhamel’s Principle\n\nQuite often, \\(\\,\\)it is important to find the temperature inside a medium due to time-varying boundary conditions. \\(\\,\\)For example, \\(\\,\\)consider an insulated rod with temperature specified as \\(\\,f(t)\\) on the right end\n\\[\n\\begin{aligned}\nu_t &= u_{xx} && 0 &lt; x &lt; 1, \\; 0 &lt; t &lt; \\infty \\\\\nu(0,t) &= 0 && 0 &lt; t &lt; \\infty \\\\\nu(1,t) &= \\color{red}{f(t)} && \\\\\nu(x,0) &= 0 && 0 \\leq x \\leq 1\n\\end{aligned} \\tag{D1}\\label{eq:D1}\\]\nWe may think that the solution to \\(\\eqref{eq:D1}\\) can be easily found once we know the solution to the simpler version (constant temperature on the boundaries)\n\\[\n\\begin{aligned}\nw_t &= w_{xx} && 0 &lt; x &lt; 1, \\; 0 &lt; t &lt; \\infty \\\\\nw(0,t) &= 0 && 0 &lt; t &lt; \\infty \\\\\nw(1,t) &= \\color{red}{1} && \\\\\nw(x,0) &= 0 && 0 \\leq x \\leq 1\n\\end{aligned} \\tag{D2}\\label{eq:D2}\\]\nIn fact, \\(\\,\\)if we solve \\(\\eqref{eq:D1}\\) and \\(\\eqref{eq:D2}\\) side by side by the Laplace transform, \\(\\,\\)we will see a striking result (Duhamel’s principle) that will give us the solution to \\(\\eqref{eq:D1}\\) in terms of the solution of \\(\\eqref{eq:D2}\\)\nSolving \\(\\eqref{eq:D2}\\) by Laplace transform, \\(\\,\\)we have\n\\[\\begin{aligned}\n\\frac{d^2W}{dx^2} &-sW = 0\\\\\n   W(0)&=0 \\\\\n   W(1)&=\\color{red}{1/s} \\\\\n       &\\Downarrow {\\scriptstyle \\text{Solve the ODE}}\\\\\n   W(x,s)&=\\color{red}{\\frac{1}{s}\n       \\left[ \\frac{\\sinh x\\sqrt{s}}{\\sinh \\sqrt{s}} \\right]} \\\\\n       &\\Downarrow {\\scriptstyle \\text{Find the inverse transform}}\\\\\n   w(x,t)=x +\\frac{2}{\\pi} & \\sum_{n=1}^\\infty\n   \\frac{(-1)^n}{n} e^{-(n\\pi)^2 t}\\sin n\\pi x\n\\end{aligned}\\]\nAnd we also obtain the following by transforming \\(\\eqref{eq:D1}\\):\n\\[\\begin{aligned}\n\\frac{d^2U}{dx^2} &-sU = 0\\\\\n  U(0)&=0 \\\\\n  U(1)&=\\color{red}{F(s)} \\\\\n   &\\Downarrow {\\scriptstyle \\text{Solve the ODE}}\\\\\n  U(x,s)&=F(s) \\left[ \\frac{\\sinh x\\sqrt{s}}{\\sinh \\sqrt{s}} \\right]\n         =F(s) s \\color{red}{\\frac{1}{s}\\left[ \\frac{\\sinh x\\sqrt{s}}{\\sinh \\sqrt{s}} \\right]}\n         =F(s) sW(x,s)\\\\\n   &\\big\\Downarrow \\;{\\scriptstyle \\mathcal{L}[w_t]=sW -w(x,0)=sW}\\\\\n  U(x,s)&=F(s) \\mathcal{L}[w_t] \\\\\n   &\\big\\Downarrow \\;{\\scriptstyle \\text{Find the inverse transform}}\\\\\n   \\color{red}{u(x,t)}\n     &= \\mathcal{L}^{-1} \\left[ F(s) \\mathcal{L}[w_t] \\right]\n     =f(t) * w_t(x,t) \\\\\n     &\\color{red}{\\;= \\int_0^t w_{t -\\tau}(x,t-\\tau) \\, f(\\tau) \\,d\\tau} \\\\\n     &= \\left. -w(x,t-\\tau)\\, f(\\tau) \\phantom{\\frac{}{}}\n      \\right |_{\\,0}^{\\,t} +\\int_0^t w(x,t-\\tau) f'(\\tau) \\,d\\tau\\\\\n     &= \\int_0^t w(x,t-\\tau) f'(\\tau)\\,d\\tau +w(x,t) \\, f(0)\n\\end{aligned}\\]\nWe have found the solution \\(u(x,t)\\) to the time-varying problem in terms of the solution to the ease problem (constant BCs). \\(\\,\\)The above result is known as Duhamel’s principle\n\n\\(~\\)\nNOTES\n\nThere are another interesting version of Duhamel’s principle that gives the answer to \\(\\eqref{eq:D1}\\) in terms of the solution \\(\\,v(x,t)\\) of the alternative simple\n\\[\n\\begin{aligned}\nv_t &= v_{xx} && 0 &lt; x &lt; 1, \\; 0 &lt; t &lt; \\infty \\\\\nv(0,t) &= 0 && 0 &lt; t &lt; \\infty \\\\\nv(1,t) &= \\color{red}{\\delta(t)} && \\\\\nv(x,0) &= 0 && 0 \\leq x \\leq 1\n\\end{aligned}\\]\nKnowing this formula\n\\[ \\color{red}{u(x,t)=\\int_0^t v(x,t-\\tau) \\,f(\\tau)\\,d\\tau} \\]\nallows us to find \\(u(x,t)\\) to an arbitrary boundary temperature \\(\\,f(t)\\) once we have carried out an experiment to determine the temperature response \\(\\,v(x,t)\\) from an impulse temperature",
    "crumbs": [
      "**Second Semester**",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Parabolic Partial Differential Equations</span>"
    ]
  },
  {
    "objectID": "ch_x1_Parabolic_PDEs.html#worked-exercises",
    "href": "ch_x1_Parabolic_PDEs.html#worked-exercises",
    "title": "12  Parabolic Partial Differential Equations",
    "section": "Worked Exercises",
    "text": "Worked Exercises\n1. \\(~\\) Solve the problem\n\\[\n\\begin{aligned}\nu_t &= \\alpha u_{xx}, \\;\\;a &lt; x &lt; b, \\;\\; t&gt;0\\\\\n&u(a,t)=0, \\;u(b,t)=0, \\;u(x,0)=(x-a)(b-x)\n\\end{aligned}\n\\]\nSolution\nStep 1: \\(~\\) Separation of Variables\nWe look for solutions of the form:\n\\[u(x,t) = X(x)T(t)\\]\nSubstitute into the PDE:\n\\[X(x) T’(t) = \\alpha X’’(x) T(t)\n\\; \\Rightarrow \\; \\frac{T’(t)}{\\alpha T(t)} = \\frac{X’’(x)}{X(x)} = -\\lambda\\]\nWe get two ODEs:\n\\[\\begin{aligned}\nT’(t) + \\alpha \\lambda T(t) &= 0 \\\\\nX’’(x) + \\lambda X(x) &= 0\n\\end{aligned}\\]\nwith boundary conditions:\n\\[X(a) = 0, \\quad X(b) = 0\\]\nStep 2: \\(~\\) Solve the Spatial Part\nWe solve:\n\\[X'' + \\lambda X = 0, \\quad X(a) = 0, \\quad X(b) = 0\\]\nThis is a standard Sturm–Liouville problem\nThe eigenfunctions are:\n\\[X_n(x) = \\sin\\left( \\frac{n\\pi (x - a)}{b - a} \\right), \\quad n = 1, 2, 3, \\dots\\]\nThe corresponding eigenvalues are:\n\\[\\lambda_n = \\left( \\frac{n\\pi}{b - a} \\right)^2\\]\nStep 3: \\(~\\) Solve the Temporal Part\n\\[T_n(t) = e^{-\\alpha \\lambda_n t} = \\exp\\left( -\\alpha \\left( \\frac{n\\pi}{b - a} \\right)^2 t \\right)\\]\nStep 4: \\(~\\) General Solution\n\\[u(x,t) = \\sum_{n=1}^\\infty B_n \\sin\\left( \\frac{n\\pi(x - a)}{b - a} \\right)\ne^{- \\alpha \\left( \\frac{n\\pi}{b - a} \\right)^2 t}\\]\nStep 5: \\(~\\) Apply Initial Condition\n\\[u(x,0) = (x - a)(b - x) = \\sum_{n=1}^\\infty B_n \\sin\\left( \\frac{n\\pi(x - a)}{b - a} \\right)\\]\nTo find coefficients \\(B_n\\), use Fourier sine series:\n\\[B_n = \\frac{2}{b - a} \\int_a^b (x - a)(b - x) \\sin\\left( \\frac{n\\pi(x - a)}{b - a} \\right) dx\\]\nLet’s use substitution: Let \\(\\xi = \\frac{x - a}{b - a}\\), so \\(x = (b-a)\\xi + a\\), \\(dx = (b-a) d\\xi\\), and when \\(x = a \\Rightarrow \\xi = 0, x = b \\Rightarrow \\xi = 1\\)\nThen:\n\\[(x - a)(b - x) = (b - a)^2 \\xi (1 - \\xi)\\]\nand:\n\\[B_n = 2 \\int_0^1 \\xi(1 - \\xi) \\sin(n\\pi \\xi) d\\xi =\n\\begin{cases}\n\\displaystyle \\frac{8}{n^3 \\pi^3}, & n \\text{ odd} \\\\\n0, & n \\text{ even}\n\\end{cases}\\]\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Parameters\na = 0\nb = 1\nalpha = 1\nt_values = [0, 0.05, 0.10, 0.15]  # Times to plot\nx_vals = np.linspace(a, b, 400)\nN_terms = 50  # Number of terms in Fourier series\n\n# Solution series function\ndef u_series(x, t, N=N_terms):\n    u = np.zeros_like(x)\n    for n in range(1, N + 1, 2):  # Only odd n\n        coeff = 8 /(n**3 * np.pi**3)\n        term = coeff *(np.sin(n *np.pi *(x - a) /(b - a)) \n          *np.exp(-alpha *(n *np.pi /(b - a))**2 *t))\n        u += term\n    return u\n\n# Plotting\nplt.figure(figsize=(7.5, 5))\nfor t in t_values:\n    u_vals = u_series(x_vals, t)\n    plt.plot(x_vals, u_vals, label=f\"t = {t}\")\n\nplt.title(\"Solution $u(x,t)$ of the Heat Equation\")\nplt.xlabel(\"x\")\nplt.ylabel(\"u(x,t)\")\nplt.legend()\nplt.grid(True)\nplt.show()\n\n\n\n\n\n\n\n\n\\(~\\)\n2. \\(~\\) Solve the problem\n\\[\n\\begin{aligned}\nu_t &= \\alpha u_{xx}, \\;\\; 0 &lt; x &lt; \\pi, \\;\\; t&gt;0\\\\\n&u_x(0,t)=0, \\;u_x(\\pi,t)=0, \\;u(x,0)=1 +\\frac{1}{2}\\cos 3x\n\\end{aligned}\n\\]\nSolution\nStep 1: \\(~\\) Separation of Variables\nLet’s look for a solution of the form:\n\\[u(x,t) = X(x)T(t)\\]\nPlug into the PDE:\n\\[X(x) T’(t) = \\alpha X’’(x) T(t)\n\\; \\Rightarrow \\; \\frac{T’(t)}{\\alpha T(t)} = \\frac{X’’(x)}{X(x)} = -\\lambda\\]\nWhich gives us two ODEs:\n\\[\n\\begin{aligned}\nT'(t) + \\alpha \\lambda T(t) &= 0 \\\\\nX''(x) + \\lambda X(x) &= 0\n\\end{aligned}\\]\nWith Neumann boundary conditions:\n\\[X'(0) = 0, \\quad X'(\\pi) = 0\\]\nStep 2: \\(~\\) Solve the Spatial ODE\n\\[X'' + \\lambda X = 0\\]\nSolutions depend on the value of \\(\\lambda\\):\n\nThe eigenfunctions satisfying Neumann BCs are:\n\n\\[X_n(x) =\n\\begin{cases}\n1, & n = 0 \\\\\n\\cos(nx), & n = 1, 2, 3, \\dots\n\\end{cases}\\]\n\nCorresponding eigenvalues:\n\n\\[\\lambda_n = n^2, \\quad n = 0, 1, 2, \\dots\\]\nStep 3: \\(~\\) Solve the Time Part\n\\[T_n(t) = e^{-\\alpha n^2 t}\\]\nStep 4: \\(~\\) General Solution\n\\[u(x,t) = \\frac{A_0}{2} +\\sum_{n=1}^\\infty A_n \\cos(nx) e^{-\\alpha n^2 t}\\]\nStep 5: \\(~\\) The initial condition is:\n\\[u(x,0) = 1 + \\cos 3x\\]\nThis already matches our eigenfunctions: * \\(1\\) corresponds to \\(\\frac{A_0}{2}\\) * \\(\\frac{1}{2}\\cos 3x\\) corresponds to \\(A_3\\cos(3x)\\)\nSo the expansion is:\n\\[u(x,t) = \\frac{A_0}{2} + A_3 \\cos(3x) e^{-9\\alpha t}\\]\nComparing with \\(u(x,0) = 1 + \\frac{1}{2}\\cos 3x\\), we get:\n\\[A_0 = 2, \\quad A_3 = \\frac{1}{2}\\]\nAll other \\(A_n = 0\\)\nFinal Answer:\n\\[u(x,t) = 1 + \\frac{1}{2}\\cos(3x) e^{-9\\alpha t}\\]\n\\(~\\)\n3. \\(~\\) Solve the problem \\[\n\\begin{aligned}\nu_{t} &= u_{xx} +F(x,t), \\;\\; -\\infty &lt; x &lt; \\infty, \\;\\; t&gt;0\\\\\n&u(x,0) = 0, \\;u(x, t) \\sim \\text{bounded}\\\\\n\\end{aligned}\n\\]\nSolution\nFor the heat equation on \\(\\mathbb{R}\\), the Green’s function (or heat kernel) is: \\[G(x,t) = \\frac{1}{\\sqrt{4\\pi t}} e^{-x^2/(4t)}\\]\nThe solution to the nonhomogeneous heat equation with zero initial condition is given by the Duhamel’s principle: \\[u(x,t) = \\int_0^t \\int_{-\\infty}^\\infty G(x - \\xi, t - \\tau) F(\\xi, \\tau) \\, d\\xi \\, d\\tau\\]\nSo,\n\\[\nu(x,t) = \\int_0^t \\int_{-\\infty}^\\infty \\frac{1}{\\sqrt{4\\pi(t - \\tau)}} \\exp\\left(-\\frac{(x - \\xi)^2}{4(t - \\tau)}\\right) F(\\xi, \\tau) \\, d\\xi \\, d\\tau\n\\]\n\\(~\\)\n4. \\(~\\) Solve the problem \\[\n\\begin{aligned}\nu_{t} &= u_{xx}, \\;\\; 0 &lt; x &lt; \\infty, \\;\\; t&gt;0\\\\\n&u(0, t) = g(t), \\;u(x, 0) = 0, \\;u(x, t) \\sim \\text{bounded}\\\\\n\\end{aligned}\n\\]\nSolution\nThis is a classical problem with the Dirichlet boundary condition on a semi-infinite domain.\nSince the initial condition is zero and the boundary condition is nonzero at \\(x = 0\\), we apply a Green’s function method for the semi-infinite domain\nThe appropriate solution is:\n\\[\nu(x,t) = \\int_0^t \\frac{x}{\\sqrt{4\\pi (t - \\tau)^3}} \\exp\\left(-\\frac{x^2}{4(t - \\tau)}\\right) g(\\tau) \\, d\\tau\n\\]\nThis is the Poisson kernel representation for the half-line heat problem with a source at the boundary\n\\(~\\)\n5. \\(~\\) Solve the problem\n\\[\\begin{aligned}\nu_t &= \\alpha u_{xx} -\\nu u_{x} -\\beta u, \\;\\;0 &lt; x &lt; 1, \\quad t&gt;0\\\\\n&u(0, t) = 0, \\;u(1, t) = 0, \\;u(x,0) = \\exp \\left(\\frac{\\nu}{2\\alpha}x\\right)\n\\end{aligned}\\]\nSolution\nStep 1: \\(~\\) Change of variables\nWe use the substitution:\n\\[u(x,t) = e^{r x - \\lambda t} w(x,t)\\]\nwith the goal of choosing \\(r\\) and so that the PDE for \\(w(x,t)\\) becomes the standard heat equation\nCompute derivatives:\n\\[\\begin{aligned}\nu_x &= r e^{r x - \\lambda t} w + e^{r x - \\lambda t} w_x = e^{r x - \\lambda t}(r w + w_x) \\\\\nu_{xx} &= e^{r x - \\lambda t} (r^2 w + 2r w_x + w_{xx})\\\\\nu_t &= e^{r x - \\lambda t} (-\\lambda w + w_t)\n\\end{aligned}\\]\nNow plug into the PDE:\n\\[u_t = \\alpha u_{xx} - \\nu u_x - \\beta u\\]\nSubstituting everything:\n\\[\\begin{aligned}\ne^{r x - \\lambda t} (-\\lambda w + w_t) &= \\alpha e^{r x - \\lambda t} (r^2 w + 2r w_x + w_{xx})\\\\ &- \\nu e^{r x - \\lambda t} (r w + w_x) - \\beta e^{r x - \\lambda t} w\n\\end{aligned}\\]\nCancel the common exponential \\(e^{r x - \\lambda t}\\):\n\\[-\\lambda w + w_t = \\alpha (r^2 w + 2r w_x + w_{xx}) - \\nu (r w + w_x) - \\beta w\\]\nGroup terms:\n\\[w_t = \\alpha w_{xx} + (2\\alpha r - \\nu) w_x + (\\alpha r^2 - \\nu r - \\beta + \\lambda) w\\]\nWe want to choose \\(r\\) and \\(\\lambda\\) so that:\n\nCoefficient of \\(w_x\\) is 0: \\(~2\\alpha r - \\nu = 0\\)\nCoefficient of \\(w\\) is 0: \\(~\\alpha r^2 - \\nu r - \\beta + \\lambda = 0\\)\n\nSolve for \\(r\\):\n\\[2\\alpha r = \\nu \\; \\Rightarrow \\; r = \\frac{\\nu}{2\\alpha}\\]\nPlug \\(r = \\frac{\\nu}{2\\alpha}\\) into the second equation to find \\(\\lambda\\):\n\\[\\lambda = \\alpha r^2 - \\nu r + \\beta = \\alpha \\left( \\frac{\\nu}{2\\alpha} \\right)^2 - \\nu \\cdot \\frac{\\nu}{2\\alpha} + \\beta\n= -\\frac{\\nu^2}{4\\alpha} + \\beta\\]\nStep 2: \\(~\\) Resulting equation for \\(w\\)\nWe now have:\n\\[u(x,t) = e^{\\frac{\\nu}{2\\alpha} x - \\lambda t} w(x,t),\n\\quad \\text{with } \\lambda = -\\frac{\\nu^2}{4\\alpha} + \\beta\\]\nThen \\(w(x,t)\\) satisfies the heat equation:\n\\[w_t = \\alpha w_{xx},\n\\quad 0 &lt; x &lt; 1,\\; t &gt; 0\\]\nwith boundary conditions:\n\\[w(0,t) = 0, \\quad w(1,t) = 0\\]\nand initial condition:\n\\[w(x,0) = u(x,0) \\cdot e^{ -\\frac{\\nu}{2\\alpha} x } = \\exp\\left( \\frac{\\nu}{2\\alpha} x \\right) \\cdot \\exp\\left( -\\frac{\\nu}{2\\alpha} x \\right) = 1\\]\nStep 3: \\(~\\) Solve the heat equation with constant initial condition\nWe solve:\n\\[\\begin{aligned}\nw_t &= \\alpha w_{xx}, \\quad 0 &lt; x &lt; 1, \\\\\nw(0,t) &= w(1,t) = 0, \\\\\nw(x,0) &= 1.\n\\end{aligned}\\]\nUse Fourier sine series:\n\\[w(x,t) = \\sum_{n=1}^\\infty b_n e^{-\\alpha (n\\pi)^2 t} \\sin(n\\pi x)\\]\nwhere the coefficients \\(b_n\\) come from:\n\\[b_n = 2 \\int_0^1 1 \\cdot \\sin(n\\pi x)\\, dx = \\frac{2}{n\\pi}(1 - \\cos(n\\pi)) = \\begin{cases}\n\\frac{4}{n\\pi}, & \\text{if } n \\text{ odd}, \\\\\n0, & \\text{if } n \\text{ even}.\n\\end{cases}\\]\nSo,\n\\[w(x,t) = \\sum_{\\substack{n=1 \\\\ n \\text{ odd}}}^\\infty \\frac{4}{n\\pi} e^{-\\alpha (n\\pi)^2 t} \\sin(n\\pi x)\\]\nStep 4: \\(~\\) Final answer for \\(u(x,t)\\)\n\\[\nu(x,t) = e^{\\frac{\\nu}{2\\alpha}x + \\frac{\\nu^2}{4\\alpha}t - \\beta t} \\sum_{\\substack{n=1\\\\n \\text{ odd}}}^{\\infty} \\frac{4}{n\\pi} e^{-\\alpha (n\\pi)^2 t} \\sin(n\\pi x)\n\\]\n\\(~\\)\n6. \\(~\\) Solve the problem\n\\[\n\\begin{aligned}\nu_t &= u_{xx}, \\;\\;0&lt;x&lt;1, \\;\\; 0&lt;t&lt;\\infty\\\\\n&u(0,t)=0\\\\\n&u(1,t)=\\cos t \\\\\n&u(x,0)=x\n\\end{aligned}\n\\]\nby\n\\((a)~\\) transforming it to one with zero BCs\n\\((b)~\\) solving the resulting problem\nSolution\n\\((a)\\)\nLet’s define:\n\\[u(x,t) = v(x,t) + w(x,t)\\]\nwhere \\(w(x,t)\\) is chosen so that it satisfies the boundary conditions, and thus the new function \\(v(x,t)\\) will satisfy homogeneous (zero) boundary conditions.\nWe want:\n\\[\\begin{cases}\nw(0,t) = 0 \\\\\nw(1,t) = \\cos t\n\\end{cases}\\]\nWe can try a linear function in \\(x\\):\n\\[w(x,t) = x \\cos t\\]\nThen we compute the equation for \\(v\\):\n\n\\(u_t = v_t + \\partial_t(x \\cos t) = v_t - x \\sin t\\)\n\\(u_{xx} = v_{xx} + \\partial_{xx}(x \\cos t) = v_{xx} + 0\\) (since \\(x \\cos t\\) is linear in \\(x\\))\n\nSo the equation becomes:\n\\[v_t - x \\sin t = v_{xx}\n\\; \\Rightarrow \\;\nv_t = v_{xx} + x \\sin t\\]\nAnd the transformed problem is:\n\\[\n\\begin{aligned}\nv_t &= v_{xx} + x \\sin t, &&0 &lt; x &lt; 1,\\; t &gt; 0 \\\\\nv(0,t) &= 0, \\\\\nv(1,t) &= 0, \\\\\nv(x,0) &= u(x,0) - x \\cos(0) = x - x = 0\n\\end{aligned}\\]\n\\((b)\\)\nStep 1: \\(~\\) Apply the Fourier sine transform\nLet’s define the sine transform:\n\\[\\mathcal{F}[v(x,t)] = \\hat{v}_n(t) = \\int_0^1 v(x,t) \\sin(n\\pi x)\\, dx\\]\nApply this transform to both sides of the PDE:\nLeft-hand side (time derivative):\n\\[\\mathcal{F}[v_t(x,t)] = \\frac{d}{dt} \\hat{v}_n(t)\\]\nRight-hand side:\n\\[\\mathcal{F}[v_{xx}(x,t)] = -n^2 \\pi^2 \\hat{v}_n(t)\\]\n\\[\\mathcal{F}[x\\sin t] = \\sin t \\cdot \\int_0^1 x \\sin(n\\pi x)\\, dx = \\sin t \\cdot \\frac{(-1)^{n+1}}{n\\pi}\\]\nStep 2: \\(~\\) Solve the ODE for \\(\\hat{v}_n(t)\\)\nLet’s write the equation again:\n\\[\\hat{v}_n’(t) + n^2 \\pi^2 \\hat{v}_n(t) = \\frac{(-1)^{n+1}}{n\\pi} \\sin t\\]\nUse integrating factor:\n\\[\\mu(t) = e^{n^2 \\pi^2 t}\n\\Rightarrow \\frac{d}{dt} \\left[ e^{n^2 \\pi^2 t} \\hat{v}_n(t) \\right] = \\frac{(-1)^{n+1}}{n\\pi} \\sin t \\cdot e^{n^2 \\pi^2 t}\\]\nNow integrate both sides from \\(0\\) to \\(t\\):\n\\[e^{n^2 \\pi^2 t} \\hat{v}_n(t)\n= \\frac{(-1)^{n+1}}{n\\pi} \\int_0^t \\sin s \\cdot e^{n^2 \\pi^2 s} \\, ds\\]\nSo the solution in the transform domain is:\n\\[\\hat{v}_n(t) = \\frac{(-1)^{n+1}}{n\\pi} \\int_0^t \\sin s \\cdot e^{n^2 \\pi^2 (s -t)} \\, ds\\]\nStep 3: \\(~\\) Invert the Fourier sine transform\nRecall:\n\\[v(x,t) = \\sum_{n=1}^\\infty \\hat{v}_n(t) \\sin(n\\pi x)\\]\nSubstitute the expression for \\(\\hat{v}_n(t)\\):\n\\[v(x,t) = \\sum_{n=1}^\\infty \\left[\n\\frac{(-1)^{n+1}}{n\\pi} \\int_0^t \\sin s \\cdot e^{n^2 \\pi^2 (s -t)} \\, ds\n\\right] \\sin(n\\pi x)\\]\nThis gives us the complete solution for \\(v(x,t)\\) using the Fourier sine transform method\nSo the full solution is:\n\\[u(x,t) =\n\\sum_{n=1}^\\infty \\left[\n\\frac{(-1)^{n+1}}{n\\pi} \\int_0^t \\sin s \\cdot e^{n^2 \\pi^2 (s -t)} \\, ds\n\\right] \\sin(n\\pi x)\n    +x \\cos t\\]\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy.integrate import quad\n\n# number of Fourier-terms\nN_terms = 20  \n\n# Define parameters\nL = 1\nx_vals = np.linspace(0, L, 200)\n\n# times at which to plot u(x, t)\nt_vals = [1e-2, 1.5, 3.0, 4.5, 6.0] \n\n# Compute u(x, t)\ndef compute_u(x, t, N=N_terms):\n    u = x *np.cos(t)  # add x cos t term\n    for n in range(1, N +1):\n        n_pi = n *np.pi\n        prefactor = (-1)**(n +1) /n_pi\n        \n        # Integrate I_n(t)\n        integrand = lambda s: np.sin(s) *np.exp(n_pi**2 *(s -t))\n        I_n, _ = quad(integrand, 0, t)\n        \n        A_n = prefactor *I_n\n        u += A_n *np.sin(n_pi *x)\n    return u\n\n# Plot\nplt.figure(figsize=(6, 4))\nfor t in t_vals:\n    u_vals = [compute_u(x, t) for x in x_vals]\n    plt.plot(x_vals, u_vals, label=f\"t = {t}\")\n\nplt.title(\"u(x,t) to the heat equation with nonhomogeneous BC\")\nplt.xlabel(\"x\")\nplt.ylabel(\"u(x,t)\")\nplt.legend()\nplt.grid(True)\nplt.show()\n\n\n\n\n\n\n\n\n\\(~\\)\n7. \\(~\\) Solve the problem\n\\[\n\\begin{aligned}\nu_t &= u_{xx} -u + x, \\;\\;0&lt;x&lt;1, \\;\\; 0&lt;t&lt;\\infty\\\\\n&u(0,t)=0\\\\\n&u(1,t)=1\\\\\n&u(x,0)=0\n\\end{aligned}\n\\]\nby\n\\((a)~\\) changing the nonhomogeneous BCs to homogeneous ones\n\\((b)~\\) transforming into a new equation without the term \\(-u\\)\n\\((c)~\\) solving the resulting problem\nSolution\n\\((a)~\\) Make the boundary conditions homogeneous\nLet’s define a new function:\n\\[u(x,t) = v(x,t) + w(x)\\]\nChoose \\(w(x)\\) to satisfy the boundary conditions:\n\\[\\begin{cases}\nw(0) = 0 \\\\\nw(1) = 1\n\\end{cases}\\]\nLet’s take the linear function:\n\\[w(x) = x\\]\nThen:\n\\[u(x,t) = v(x,t) + x \\; \\Rightarrow \\; v(x,t) = u(x,t) - x\\]\nNow we compute: * \\(u_t = v_t\\) * \\(u_{xx} = v_{xx} + w''(x) = v_{xx} + 0 = v_{xx}\\) (since \\(x'' = 0\\)) * \\(u = v + x\\)\nSubstitute into the original PDE:\n\\[u_t = u_{xx} - u + x\n\\Rightarrow v_t = v_{xx} - (v + x) + x = v_{xx} - v\\]\nSo the PDE for \\(v\\) is:\n\\[v_t = v_{xx} - v\\]\nAnd the initial and boundary conditions become:\n\n\\(v(0,t) = u(0,t) - 0 = 0\\)\n\\(v(1,t) = u(1,t) - 1 = 0\\)\n\\(v(x,0) = u(x,0) - x = -x\\)\n\nTransformed problem for \\(v(x,t)\\):\n\\[\n\\begin{aligned}\nv_t &= v_{xx} - v, \\quad 0 &lt; x &lt; 1,\\; t &gt; 0 \\\\\nv(0,t) &= v(1,t) = 0 \\\\\nv(x,0) &= -x\n\\end{aligned}\\]\n\\((b)~\\) Eliminate the reaction term \\(-v\\) by a second transformation\nLet:\n\\[v(x,t) = e^{-t} z(x,t)\\]\nThen:\n\n\\(v_t = -e^{-t} z + e^{-t} z_t = e^{-t}(z_t - z)\\)\n\\(v_{xx} = e^{-t} z_{xx}\\)\n\nSubstitute into \\(v_t = v_{xx} - v\\):\n\\[e^{-t}(z_t - z) = e^{-t} z_{xx} - e^{-t} z\n\\Rightarrow z_t - z = z_{xx} - z\n\\Rightarrow z_t = z_{xx}\\]\nSo now we have the heat equation!\nFrom \\(v = e^{-t} z\\,\\) at \\(t = 0\\):\n\\[z(x,0) = v(x,0) = -x\\]\nAnd since \\(v(0,t) = v(1,t) = 0\\), \\(\\,\\) we have:\n\\[z(0,t) = z(1,t) = 0\\]\nFinal simplified problem:\n\\[\n\\begin{aligned}\nz_t &= z_{xx}, \\quad 0 &lt; x &lt; 1,\\; t &gt; 0 \\\\\nz(0,t) &= z(1,t) = 0 \\\\\nz(x,0) &= -x\n\\end{aligned}\\]\n\\((c)~\\) Solve the simplified heat equation\nWe solve this using separation of variables / Fourier series\nStep 1: \\(~\\) Expand initial condition in sine series\n\\[z(x,0) = -x = \\sum_{n=1}^\\infty A_n \\sin(n\\pi x)\\]\nCompute coefficients:\n\\[A_n = 2 \\int_0^1 (-x) \\sin(n\\pi x) dx\n= -2 \\int_0^1 x \\sin(n\\pi x) dx\\]\nUse integration by parts:\n\\[\\int_0^1 x \\sin(n\\pi x) dx = \\frac{(-1)^{n+1}}{n\\pi}\n\\Rightarrow A_n = \\frac{2(-1)^{n}}{n\\pi}\\]\nStep 2: \\(~\\) General solution for \\(z(x,t)\\)\n\\[z(x,t) = \\sum_{n=1}^\\infty A_n e^{-n^2 \\pi^2 t} \\sin(n\\pi x)\n= \\sum_{n=1}^\\infty \\frac{2(-1)^n}{n\\pi} e^{-n^2 \\pi^2 t} \\sin(n\\pi x)\\]\nStep 3: \\(~\\) Back-substitute to get \\(u(x,t)\\)\nSo finally:\n\\[u(x,t) = x + e^{-t} \\sum_{n=1}^\\infty \\frac{2(-1)^n}{n\\pi} e^{-n^2 \\pi^2 t} \\sin(n\\pi x)\\]\nFinal Answer:\n\\[\n\\boxed{\nu(x,t) = x + \\sum_{n=1}^\\infty \\frac{2(-1)^n}{n\\pi} e^{-(n^2 \\pi^2 + 1)t} \\sin(n\\pi x)\n}\\]\n\n# Re-import necessary libraries after kernel reset\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport matplotlib.animation as animation\nfrom IPython.display import HTML\n\n# Spatial domain\nx = np.linspace(0, 1, 200)\n\n# Number of Fourier terms\nN = 50\n\n# Time parameters\nt_vals = np.linspace(0, 1, 100)  # from t=0 to t=1.5\n\n# Precompute sin(nπx) terms\nsin_terms = np.array([np.sin(n *np.pi *x) for n in range(1, N +1)])\n\n# Compute the solution u(x, t) at each time t\ndef compute_u_xt(t):\n    coeffs = np.array([\n        (2 *(-1)**n) /(n *np.pi) \n          *np.exp(-(n**2 *np.pi**2 +1) *t)\n        for n in range(1, N +1)\n    ])\n    v_xt = np.dot(coeffs, sin_terms)\n    return x +v_xt  # u = x +e^{-t} *z(x,t)\n\n# Set up the plot\nfig, ax = plt.subplots(figsize=(6, 4))\n\nline, = ax.plot([], [], lw=2)\nax.set_xlim(0, 1)\nax.set_ylim(-0.1, 1.1)\nax.set_xlabel('x')\nax.set_ylabel('u(x,t)')\ntitle = ax.set_title(\"\")\n\n# Initialization function\ndef init():\n    line.set_data([], [])\n    title.set_text(\"\")\n    return line, title\n\n# Animation update function\ndef update(frame):\n    t = t_vals[frame]\n    u = compute_u_xt(t)\n    line.set_data(x, u)\n    title.set_text(f\"u(x, t) at t = {t:.2f}\")\n    return line, title\n\n# Create animation\nani = animation.FuncAnimation(\n    fig, update, frames=len(t_vals),\n    init_func=init, blit=True, interval=80\n)\n\nplt.close()\nHTML('&lt;center&gt;' + ani.to_html5_video() + '&lt;/center&gt;')\n\n\n  \n  Your browser does not support the video tag.",
    "crumbs": [
      "**Second Semester**",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Parabolic Partial Differential Equations</span>"
    ]
  },
  {
    "objectID": "ch_x3_Elliptic_PDEs.html",
    "href": "ch_x3_Elliptic_PDEs.html",
    "title": "14  Elliptic Partial Differential Equations",
    "section": "",
    "text": "14.1 The Laplacian (an Intuitive Description)\n\\(~\\)\n\\(~\\)\n\\(~\\)\n\\(~\\)\nNOTES\n\\(~\\)\nExample \\(\\,\\) What is the wave equation \\(\\,u_{tt}=c^2\\nabla^2 u\\,\\) in polar coordinates if you know that the solution \\(\\,u\\,\\) depends only on \\(\\,r\\,\\) and \\(\\,t\\)\n\\(~\\)\nExample \\(\\,\\) What is Laplace’s equation in spherical coordinates if the solution \\(\\,u\\,\\) depends only on \\(\\,r\\)? \\(\\,\\)Can you find the solutions of this equation? These are the spherically symmetric potentials in three dimensions\n\\(~\\)",
    "crumbs": [
      "**Second Semester**",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Elliptic Partial Differential Equations</span>"
    ]
  },
  {
    "objectID": "ch_x3_Elliptic_PDEs.html#sec-x3-31",
    "href": "ch_x3_Elliptic_PDEs.html#sec-x3-31",
    "title": "14  Elliptic Partial Differential Equations",
    "section": "",
    "text": "The  Laplacian operator \n\\[ \\nabla^2 =\\frac{\\partial^2}{\\partial x^2} +\\frac{\\partial^2}{\\partial y^2} +\\frac{\\partial^2}{\\partial z^2}\\]\nis probably the most important operator in mathematical physics. \\(\\,\\)The question is, \\(\\,\\)what does it mean and why should the sum of three second derivatives have anything to do with the laws of nature?\nThe answer to this lies in the fact that the Laplacian of a function allows us to compare the function at a point with the function at neighboring points. \\(\\,\\)It does what the second derivative did in one dimension and might be thought of as a second derivative generalized to higher dimensions\n\n\n\n\n\n\n\n\n\nThe heat equation \\(\\,u_t=\\alpha\\nabla^2 u\\,\\) measures temperature \\(\\,u\\), and the equation can be interpreted to mean that the change in temperature with respect to time \\(\\,u_t\\) is proportional to \\(\\,\\nabla^2 u\\). \\(\\,\\)That is, \\(\\,\\)the temperature at a point is increasing if the temperature at that point is less than the average of the temperature on a circle around the point\nThe wave equation \\(\\,u_{tt}=c^2\\nabla^2 u\\,\\) measures the displacement of a drumhead and can be interpreted to mean that the acceleration (or force) of a point on the drumhead \\(\\,u_{tt}\\,\\) is proportional to \\(\\,\\nabla^2 u\\). \\(\\,\\)That is, \\(\\,\\)the drumhead at a point is accelerating upward if the drumhead at that point is less than the average of its neighbors\nLaplace’s equation \\(\\,\\nabla^2 u = 0\\,\\) says that the solution \\(\\,u\\,\\) is always equal to the average of its neighbors. \\(\\,\\)For example, \\(\\,\\)a steady-state, stretched rubber membrane satisfies Laplace’s equation, \\(\\,\\)hence, \\(\\,\\)the height of the membrane at any point is equal to the average height of the membrane on a circle around the point\nPoisson’s equation \\(\\,\\nabla^2 u=f\\), \\(\\,\\)where \\(\\,f\\,\\) is a function that depends only on the space variables\n\n\\(\\nabla^2 u=-\\rho\\,\\) describes the potential of an electrostatic field where \\(\\,\\rho\\,\\) represents a constant charge density\n\\(\\nabla^2 u =-g(x,y)\\,\\) describes the steady-state temperature \\(\\,u(x,y)\\,\\) due to a heat source \\(\\,g(x,y)\\)\n\n\\(\\nabla^2 u +\\lambda u =0\\,\\) is known as the Helmholtz equation which describes the fundamental shapes of a stretched membrane\n\n\n\nChanging Coordinates\nBefore we start, \\(\\,\\)however, \\(\\,\\)let’s review briefly the three major coordinate systems in two or three dimensions except for Cartesian system:\n\nPolar Coordinates\n\n\n\n\n\n\\[\\begin{matrix}\n  x = r\\cos\\theta \\\\\n  y = r\\sin\\theta \\, \\\\\n  \\end{matrix} \\;\\; \\Rightarrow \\;\\;\n  \\begin{matrix}\n    r^2 = x^2 + y^2 \\;\\;\\;\\\\\n    \\tan\\theta = y/x \\;\\;\\;\\;\\;\n\\end{matrix}\\]\nAs an illustration, \\(\\,\\)we see how the two-dimensional Laplacian is transformed into polar coordinates:\n\\[\\scriptsize\n\\begin{aligned}\n\\nabla^2u &= u_{xx} +u_{yy}\\\\\n&\\Updownarrow \\\\\nu_x = u_r r_x +u_\\theta \\theta_x &= u_r \\cos\\theta -u_\\theta \\frac{\\sin\\theta}{r}\\\\\nu_y = u_r r_y +u_\\theta \\theta_y &= u_r \\sin\\theta +u_\\theta \\frac{\\cos\\theta}{r}\\\\\n&\\Downarrow \\\\\nu_{xx} = (u_x)_r r_x +(u_x)_\\theta \\theta_x =&\n    {\\tiny\\left( u_{rr} \\cos\\theta -u_{\\theta r} \\frac{\\sin\\theta}{r} +u_\\theta\\frac{\\sin\\theta}{r^2} \\right) \\cos\\theta\n    -\\left( u_{r\\theta} \\cos\\theta -u_r\\sin\\theta -u_{\\theta\\theta} \\frac{\\sin\\theta}{r}\n    -u_\\theta \\frac{\\cos\\theta}{r} \\right) \\frac{\\sin\\theta}{r} }\\\\\nu_{yy} = (u_y)_r r_y +(u_y)_\\theta \\theta_y =&\n{\\tiny  \\left( u_{rr} \\sin\\theta +u_{\\theta r} \\frac{\\cos\\theta}{r} -u_\\theta\\frac{\\cos\\theta}{r^2} \\right) \\sin\\theta\n    +\\left( u_{r\\theta} \\sin\\theta +u_r\\cos\\theta +u_{\\theta\\theta} \\frac{\\cos\\theta}{r}\n    -u_\\theta \\frac{\\sin\\theta}{r} \\right) \\frac{\\cos\\theta}{r} } \\\\\n    &\\Updownarrow \\\\\n    \\color{red}{\\nabla^2 u = u_{rr}} &\\color{red}{+\\frac{1}{r} u_r +\\frac{1}{r^2} u_{\\theta\\theta}}\n\\end{aligned}\\]\nCylindrical Coordinates\n\n\n\n\n\n\\[\\begin{matrix}\n    x = r\\cos \\theta \\\\\n    y = r\\sin \\theta \\, \\\\\n    z = z \\quad\\;\\;\\;\n    \\end{matrix} \\;\\; \\Rightarrow \\;\\;\n    \\begin{matrix}\n    r^2 = x^2 + y^2 \\;\\;\\;\\\\\n    \\tan\\theta = y/x  \\quad\\; \\\\\n    z=z \\qquad\\;\\;\\;\\;\n\\end{matrix}\\]\nChanging the Laplacian \\(\\,\\nabla^2 u = u_{xx} +u_{yy} +u_{zz}\\,\\) to cylindrical coordinates, \\(\\,\\)we can show\n\\[ \\color{red}{\\nabla^2 u = u_{rr} +\\frac{1}{r} u_r +\\frac{1}{r^2} u_{\\theta\\theta} +u_{zz}}\\]\nSpherical Coordinates\n\n\n\n\n\n\\[\\begin{matrix}\nx = r\\sin\\phi\\cos \\theta \\\\\ny = r\\sin\\phi\\sin \\theta \\, \\\\\nz = r\\cos\\phi \\quad\\;\\;\\;\n\\end{matrix} \\;\\; \\Rightarrow \\;\\;\n\\begin{matrix}\nr^2 = x^2 + y^2 +z^2 \\;\\;\\;\\\\\n\\cos\\phi = z/r \\qquad\\;\\;\\;\\;\\\\\n\\tan\\theta=y/x \\qquad\\;\\;\\;\\;\n\\end{matrix}\\]\nFinally, \\(\\,\\)if we write the Laplacian in spherical coordinates, \\(\\,\\)we have\n\\[\\scriptsize \\begin{aligned}\n\\nabla^2u &= u_{xx} +u_{yy} +u_{zz}\\\\\n&\\Updownarrow \\\\\nu_x = u_r r_x +u_\\phi \\phi_x+u_\\theta \\theta_x &= u_r \\sin\\phi\\cos\\theta +u_\\phi \\frac{\\cos\\phi\\cos\\theta}{r}\n-u_\\theta \\frac{\\sin\\theta}{r\\sin\\phi}\\\\\nu_y = u_r r_y +u_\\phi \\phi_y+u_\\theta \\theta_y &= u_r \\sin\\phi\\sin\\theta +u_\\phi \\frac{\\cos\\phi\\sin\\theta}{r}\n+u_\\theta \\frac{\\cos\\theta}{r \\sin\\phi}\\\\\nu_z = u_r r_z +u_\\phi \\phi_z+u_\\theta \\theta_z &= u_r \\cos\\phi -u_\\phi\\frac{\\sin\\phi}{r} \\\\\n&\\Downarrow \\\\\n    u_{xx} = (u_x)_r r_x +(u_x)_\\phi \\phi_x+(u_x)_\\theta \\theta_x &=\n{\\tiny \\left( u_{rr} \\sin\\phi\\cos\\theta +u_{\\phi r} \\frac{\\cos\\phi\\cos\\theta}{r} -u_\\phi\\frac{\\cos\\phi\\cos\\theta}{r^2}\n    -u_{\\theta r} \\frac{\\sin\\theta}{r\\sin\\phi} +u_\\theta\\frac{\\sin\\theta}{r^2\\sin\\phi} \\right)\n    \\sin\\phi\\cos\\theta} \\\\\n{ \\tiny +\\left( u_{r\\phi}\\sin\\phi \\cos\\theta \\right. }&{\\tiny\\, \\left. +u_r\\cos\\phi\\cos\\theta +u_{\\phi\\phi} \\frac{\\cos\\phi\\cos\\theta}{r} -u_{\\phi} \\frac{\\sin\\phi\\cos\\theta}{r}\n    -u_{\\theta\\phi} \\frac{\\sin\\theta}{r\\sin\\phi} +u_\\theta \\frac{\\sin\\theta\\cot\\phi}{r\\sin\\phi}\n\\right)\\frac{\\cos\\phi \\cos\\theta}{r}} \\\\\n{\\tiny -\\left( u_{r\\theta}\\sin\\phi\\cos\\theta \\right. }&{\\tiny \\, \\left. -u_r \\sin\\phi\\sin\\theta +u_{\\phi\\theta} \\frac{\\cos\\phi\\cos\\theta}{r}\n-u_\\phi \\frac{\\cos\\phi\\sin\\theta}{r} -u_{\\theta\\theta}\\frac{\\sin\\theta}{r\\sin\\phi} -u_\\theta \\frac{\\cos\\theta}{r\\sin\\phi} \\right)\n\\frac{\\sin\\theta}{r\\sin\\phi} }\\qquad\\qquad\n\\end{aligned}\\]\n\\[\\scriptsize \\begin{aligned}\n    u_{yy} = (u_y)_r r_y +(u_y)_\\phi \\phi_y+(u_y)_\\theta \\theta_y &=\n{\\tiny \\left( u_{rr} \\sin\\phi\\sin\\theta +u_{\\phi r} \\frac{\\cos\\phi\\sin\\theta}{r}  -u_\\phi\\frac{\\cos\\phi\\sin\\theta}{r^2}\n    +u_{\\theta r} \\frac{\\cos\\theta}{r\\sin\\phi} -u_\\theta\\frac{\\cos\\theta}{r^2\\sin\\phi} \\right)\n    \\sin\\phi\\sin\\theta }\\\\\n{\\tiny+\\left( u_{r\\phi}\\sin\\phi \\sin\\theta \\right.} &{\\tiny\\, \\left. +u_r\\cos\\phi\\sin\\theta +u_{\\phi\\phi} \\frac{\\cos\\phi\\sin\\theta}{r} -u_{\\phi} \\frac{\\sin\\phi\\sin\\theta}{r}\n    +u_{\\theta\\phi} \\frac{\\cos\\theta}{r\\sin\\phi} -u_\\theta \\frac{\\cos\\theta\\cot\\phi}{r\\sin\\phi}\n\\right)\\frac{\\cos\\phi \\sin\\theta}{r}} \\\\\n{\\tiny+\\left( u_{r\\theta}\\sin\\phi\\sin\\theta \\right.} &{\\tiny \\, \\left.+u_r \\sin\\phi\\cos\\theta +u_{\\phi\\theta} \\frac{\\cos\\phi\\sin\\theta}{r}\n+u_\\phi \\frac{\\cos\\phi\\cos\\theta}{r} +u_{\\theta\\theta}\\frac{\\cos\\theta}{r\\sin\\phi} -u_\\theta \\frac{\\sin\\theta}{r\\sin\\phi} \\right)\n\\frac{\\cos\\theta}{r\\sin\\phi} }\\\\\n    u_{zz} = (u_z)_r r_z +(u_z)_\\phi \\phi_z+(u_z)_\\theta \\theta_z\n    &={\\tiny \\left( u_{rr}\\cos\\phi -u_{\\phi r}\\frac{\\sin\\phi}{r} +u_\\phi\\frac{\\sin\\phi}{r^2} \\right) \\cos\\phi}\n    {\\tiny +\\left( -u_{r\\phi}\\cos\\phi \\right. +}{\\tiny\\, \\left. u_{r}\\sin\\phi +u_{\\phi\\phi}\\frac{\\sin\\phi}{r} +u_\\phi\\frac{\\cos\\phi}{r} \\right) \\frac{\\sin\\phi}{r}}\\\\\n    &\\Downarrow \\\\\n    \\color{red}{\\nabla^2 u =u_{rr} +\\frac{2}{r} u_r }&\\color{red}{+\\frac{1}{r^2}u_{\\phi\\phi} +\\frac{\\cot\\phi}{r^2} u_{\\phi} +\\frac{1}{r^2\\sin^2\\phi}u_{\\theta\\theta}} \\\\\n    \\color{red}{= \\frac{1}{r^2}\\left(  r^2 u_r \\right)_r }& \\color{red}{+\\frac{1}{r^2 \\sin\\phi} \\left( \\sin\\phi\\, u_\\phi \\right)_\\phi +\\frac{1}{r^2\\sin^2\\phi}u_{\\theta\\theta}}\n\\end{aligned}\\]\n\n\n\n\n\nThe Laplacian in cartesian coordinates is the only one with constant coefficients. \\(\\,\\)This is one reason why problems in other coordinate systems are harder to solve. \\(\\,\\)It is still possible to use the separation of variables for these equations with variable coefficients; \\(\\,\\)it’s just that some of the resulting ordinary differential equations have variables coefficients\nWe arrive at a lot of fairly complicated equations, \\(\\,\\)such as Bessel’s equation, \\(\\,\\)Legendre equation, and other so-called classical equations of physics. \\(\\,\\)To solve these equations, \\(\\,\\)we must resort to infinite-series solutions and, \\(\\,\\)in particular, \\(\\,\\)the method of Frobenious",
    "crumbs": [
      "**Second Semester**",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Elliptic Partial Differential Equations</span>"
    ]
  },
  {
    "objectID": "ch_x3_Elliptic_PDEs.html#sec-x3-32",
    "href": "ch_x3_Elliptic_PDEs.html#sec-x3-32",
    "title": "14  Elliptic Partial Differential Equations",
    "section": "14.2 General Nature of Boundary-Value Problems",
    "text": "14.2 General Nature of Boundary-Value Problems\n\nMany important problems whose outcomes do not change with time are described by elliptic boundary-value problems. \\(\\,\\)There are two common situations in physical problems that give rise to PDEs that don’t involve time; \\(\\,\\)they are:\n1. \\(\\;\\)Steady-state problems\n2. \\(\\;\\)Problems where we factor out the time component in the solution\nWhen studying boundary-value problems (BVPs), \\(\\,\\)there are three types of BCs that are most common; \\(\\,\\)we discuss these three types now\nDirichlet Problems\nHere, \\(\\,\\)the PDE holds over a given region of space, \\(\\,\\)and the solution is specified on the boundary of the region. \\(\\,\\)An example would be to find the steady-state temperature inside a circle with the temperature given on the boundary\n\\[ u_{rr} +\\frac{1}{r}u_r + \\frac{1}{r^2} u_{\\theta\\theta}=0, \\] \\[\\;\\; u(1,\\theta) = \\sin\\theta, \\;\\;\\;0 &lt; r &lt; 1,\\;\\;\\; 0 \\leq \\theta &lt; 2\\pi\\]\n\n\n\n\n\nAnother example would be an exterior Dirichlet problem in which we are looking for the solution of Laplace’s equation outside the unit circle, \\(\\,\\)and the boudary condition is given on the circle\n\n\n\n\n\nNeumann Problems\nHere, \\(\\,\\)the PDE holds in some region of space, \\(\\,\\)but now the outward normal derivative\n\\[\\frac{\\partial u}{\\partial n}\\]\n(which is proportional to the inward flux) is specified on the boundary. \\(\\,\\)For example, \\(\\,\\)suppose the inward flow of heat varies around the circle according to\n\\[ \\frac{\\partial u}{\\partial r}=\\sin\\theta\\]\nThe steady-state temperature inside the circle would then be given by the solution of the BVP\n\\[ \\nabla^2 u = 0, \\quad 0 &lt; r &lt; 1\\]\n\\[ \\frac{\\partial u}{\\partial r} =\\sin\\theta, \\quad r=1, \\;\\; 0 \\leq \\theta &lt; 2\\pi\\]\nHere, \\(\\,\\)we can see that the flux of heat across the boundary is inward for \\(\\,0 \\leq \\theta \\leq \\pi\\,\\) and outward for \\(\\,\\pi \\leq \\theta &lt; 2\\pi\\)\n\n\n\n\n\nHowever, \\(\\,\\)since the total flux\n\\[ \\int_0^{2\\pi} \\frac{\\partial u}{\\partial r}\\, d\\theta\\;=\\int_0^{2\\pi} \\sin\\theta \\,d\\theta = 0\\]\n(a condition that must be true for Neumann problems), \\(\\,\\)we can say that the temperature at each point inside circle does not change with respect to time. \\(\\,\\)In other words, \\(\\,\\)Neumann problems make sense only if the net gain in heat across the boundary is zero\nThe Neumann problem is somewhat different from other boundary conditions, \\(\\,\\)in that solutions are not unique. \\(\\,\\)In other words, \\(\\,\\)the above Neumann problem has an infinite number of solutions \\(\\,u(r,\\theta)\\). \\(\\,\\)Once we have one solution, \\(\\,\\)we can get the others just by adding a constant. \\(\\,\\)For example, \\(\\,\\)one solution to our Neumann problem is\n\\[ u(r,\\theta)=r\\sin\\theta\\]\nand it is obvious that if we add a constant to this solution, \\(\\,\\)another one is obtained. \\(\\,\\)For this reason, \\(\\,\\)if we want to find one solution to the Neumann problem, \\(\\,\\)we must have some additional information (like knowing the solution at one point)\nRobin Problems\nThese problems correspond to the PDEs being given in some region of space, \\(\\,\\)but now the condition on the boundary is a mixture of the first two kinds\n\\[ \\frac{\\partial u}{\\partial n} = -h(u -g)\\]\nwhere \\(\\,h\\,\\) is a constant and \\(\\,g\\,\\) is a given function that can vary over the boundary. \\(\\,\\)This BC says the inward flux across the boundary is proportional to the difference between the boundary value of \\(\\,u\\,\\) and specified environment value \\(\\,g\\,\\)\nIn heat transfer, \\(\\,\\)this, \\(\\,\\)of course, \\(\\,\\)is just Newton’s law of cooling. \\(\\,\\)The constant \\(\\,h\\,\\) is a physical parameter that measures the amount of flux across the boundary per difference between \\(\\,u\\,\\) and \\(\\,g\\). \\(\\,\\)If \\(\\,h\\,\\) is large, \\(\\,\\)and so the solution looks very much like the solution of the Dirichlet problem \\(\\,u=g\\). \\(\\,\\)On the order hand, \\(\\,\\)if \\(\\,h=0\\), then the BC is reduced to the insulated BC\n\\[ \\color{blue}{\\frac{\\partial u}{\\partial r}=0} \\]\n\n\\(~\\)\nExample \\(\\,\\) Does the following Neumann have a solution inside the circle?\n\\[ \\nabla^2 u = 0, \\quad 0 &lt; r &lt; 1\\]\n\\[ \\frac{\\partial u}{\\partial r} =\\sin^2\\theta, \\quad r=1, \\;\\; 0 \\leq \\theta &lt; 2\\pi\\]\n\\(~\\)\nExample \\(\\,\\) For different values of \\(h\\), \\(~\\)imagine the solution \\(u(r,\\theta)\\,\\) to\n\\[ \\nabla^2 u = 0, \\quad 0 &lt; r &lt; 1\\]\n\\[ \\frac{\\partial u}{\\partial r} +h(u -\\sin\\theta)=0, \\quad r=1, \\;\\; 0 \\leq \\theta &lt; 2\\pi\\]\n\\(~\\)",
    "crumbs": [
      "**Second Semester**",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Elliptic Partial Differential Equations</span>"
    ]
  },
  {
    "objectID": "ch_x3_Elliptic_PDEs.html#sec-x3-33",
    "href": "ch_x3_Elliptic_PDEs.html#sec-x3-33",
    "title": "14  Elliptic Partial Differential Equations",
    "section": "14.3 Interior Dirichlet Problem for a Circle",
    "text": "14.3 Interior Dirichlet Problem for a Circle\n\nThis section presents a number of new ideas to solve the interior Dirichlet problem for the circle\n\\[ \\color{red}{u_{rr} +\\frac{1}{r}u_r + \\frac{1}{r^2} u_{\\theta\\theta}=0, \\;\\;\\;0 &lt; r &lt; 1}\\]\n\\[ \\color{red}{u(1,\\theta) = g(\\theta), \\;\\;\\; 0 \\leq \\theta &lt; 2\\pi}\\]\nThe method of separation of variables will be the usual procedure\n\\[\\scriptsize\\begin{aligned}\n  u_{rr} +\\frac{1}{r}u_r\n  &+\\frac{1}{r^2}u_{\\theta\\theta}= 0 \\\\\n  u(1,\\theta) &= g(\\theta) \\\\\n  &\\Downarrow u(r,\\theta)=R(r)\\Theta(\\theta)\\\\\n\\end{aligned}\\]\n\\[\\scriptsize\\begin{aligned}\n  -\\frac{r^2R'' +rR'}{R}\n   &= \\frac{\\;\\Theta''}{\\Theta} =-\\lambda \\leq 0 \\\\\n  &\\Downarrow \\\\\n  \\Theta'' +\\lambda \\Theta =0\n   \\;\\; \\xrightarrow[]\n    {\\;\\;\\Theta(0)=\\Theta(2\\pi),\\;\\Theta'(0)\n    =\\Theta'(2\\pi)\\;\\;}\n      &\\;\\;\\Theta_n(\\theta)=a_n \\cos n\\theta\n      +b_n \\sin n\\theta, \\;\\; \\lambda_n =n^2,\n      \\;\\; n=0,1,2,\\cdots \\\\\n  &\\Downarrow \\\\\n  r^2R_0'' +rR_0'=0\\;\n   &\\xrightarrow[]{\\;\\;| R_0(0)| \\,&lt;\\,\\infty  \\;\\;}\n    \\;\\; R_0(r)=1 \\\\\n  r^2R_n''+rR_n' -n^2 R_n=0,\\;\\;n =1,2,\\cdots\\;\n   &\\xrightarrow[]{\\;\\; | R_n(0) | \\,&lt;\\,\\infty \\;\\;}\n    \\;\\;R_n(r)= r^n \\\\\n  &\\Downarrow \\\\\n  \\color{red}{u(r,\\theta) =\\frac{a_0}{2}\n   +\\sum_{n=1}^\\infty }\n   &\\color{red}{ r^n \\left( a_n \\cos n\\theta\n    +b_n \\sin n\\theta \\right)} \\\\\n  a_n = \\frac{1}{\\pi} \\int_0^{2\\pi} &g(\\theta)\n   \\cos n\\theta \\,d\\theta \\\\\n  b_n = \\frac{1}{\\pi} \\int_0^{2\\pi} &g(\\theta)\n   \\sin n\\theta \\,d\\theta\n\\end{aligned}\\]\n\nThe solution can be interpreted as expanding the boundary function \\(\\,g(\\theta)\\,\\) as a Fourier series\n\\[ g(\\theta)=\\frac{a_0}{2} +\\sum_{n=1}^\\infty \\left(a_n \\cos n\\theta +b_n \\sin n\\theta \\right)\\]\nand solve the problem for \\(\\,\\cos n\\theta\\,\\) and \\(\\,\\sin n\\theta\\,\\) in the series. \\(\\,\\)Since each of these terms will give rise to solutions \\(\\,r^n\\cos n\\theta\\,\\) and \\(\\,r^n \\sin n\\theta\\), \\(\\,\\)we can then say (by superposition) that\n\\[ u(r,\\theta)=\\frac{a_0}{2} +\\sum_{n=1}^\\infty {\\color{red}{r^n}} \\left(a_n \\cos n\\theta +b_n \\sin n\\theta \\right)\\]\nNote that the constant term \\(\\,\\displaystyle\\frac{a_0}{2}\\,\\) in the solution represents the average of \\(\\,g(\\theta)\\)\n\\[ \\frac{a_0}{2} = \\frac{1}{2\\pi} \\int_0^{2\\pi} g(\\theta)\\,d\\theta\\]\nIf the radius of the circle was arbitrary (say \\(R\\)), \\(\\,\\)then the solution would be\n\\[ u(r,\\theta)=\\frac{a_0}{2} +\\sum_{n=1}^\\infty \\left(\\frac{r}{R}\\right)^n \\left(a_n \\cos n\\theta +b_n \\sin n\\theta \\right)\\]\nThis completes our discussion of the separation of variables solution. \\(\\,\\)We now go to the interesting Poisson integral formula\n\n\n\\(~\\)\nExample \\(\\,\\)The solution of\n\\[ \\quad\\;\\;\\;\\nabla^2 u = 0, \\quad 0 &lt; r &lt; 1\\]\n\\[\\begin{aligned}\nu(1, \\theta)&=1 + \\sin\\theta +\\frac{1}{2}\\sin3\\theta +\\cos 4\\theta \\\\\n\\end{aligned}\\]\nwould be\n\\[\\begin{aligned}\nu(r, \\theta)&=1 + r\\sin\\theta +\\frac{r^3}{2}\\sin3\\theta +r^4\\cos 4\\theta \\\\\n\\end{aligned}\\]\n\\(~\\)\n\nPoisson Integral Formula\nWe start with the separation of varaiables solution\n\\[ u(r,\\theta)\n=\\frac{a_0}{2} +\\sum_{n=1}^\\infty\n  \\left(\\frac{r}{R}\\right)^n \\left(a_n \\cos n\\theta\n   +b_n \\sin n\\theta \\right)\\]\nand substitute the coefficients \\(\\,a_n\\,\\) and \\(\\,b_n\\). \\(\\,\\)After a few manipulations, \\(\\,\\)we have\n\\[\\scriptsize\\begin{aligned}\nu(r,\\theta)\n  &= \\frac{1}{2\\pi} \\int_0^{2\\pi}\n   g(\\alpha)\\,d\\alpha +\\frac{1}{\\pi}\n  \\sum_{n=1}^\\infty \\left(\\frac{r}{R} \\right)^n\n   \\int_0^{2\\pi} g(\\alpha)\n    \\left( \\cos n\\alpha\\cos n\\theta\n    +\\sin n\\alpha \\sin n\\theta \\right)\\,d\\alpha \\\\\n  &= \\frac{1}{2\\pi} \\int_0^{2\\pi}  \n   \\left\\{ 1 +2\\sum_{n=1}^\\infty\n   \\left(\\frac{r}{R} \\right)^n\n    \\cos n(\\theta -\\alpha) \\right\\}\n     \\,g(\\alpha)\\,d\\alpha \\\\\n\\end{aligned}\\]\n\\[\\scriptsize\\begin{aligned}\n&= \\frac{1}{2\\pi} \\int_0^{2\\pi}  \\left\\{  1 +\\sum_{n=1}^\\infty \\left(\\frac{r}{R} \\right)^n \\left[e^{in(\\theta -\\alpha)} +e^{-in(\\theta -\\alpha)}  \\right] \\right\\}\\,g(\\alpha)\\,d\\alpha \\\\\n&= \\frac{1}{2\\pi} \\int_0^{2\\pi}  \\left\\{  1 +\\frac{re^{i(\\theta -\\alpha)}}{R -re^{i(\\theta -\\alpha)}}\n+\\frac{re^{-i(\\theta -\\alpha)}}{R -re^{-i(\\theta -\\alpha)}} \\right\\}\\,g(\\alpha)\\,d\\alpha \\\\\n&= \\color{red}{\\frac{1}{2\\pi} \\int_0^{2\\pi}  \\left\\{ \\frac{R^2 -r^2}{R^2 +r^2 -2rR\\cos(\\theta -\\alpha)} \\right\\}\\,g(\\alpha)\\,d\\alpha}\n\\end{aligned}\\]\nThis last equation is what we were looking for; \\(\\,\\)it’s the Poisson Integral Formula\n\nWe can interpret the Poisson integral solution as finding the potential \\(\\,u\\,\\) at \\(\\,(r,\\theta)\\,\\) as a weighted average of the boundary potentials \\(\\,g(\\theta)\\,\\) weighted by the Poisson kernel\n\\[\\text{Poisson kernel}=\\frac{R^2 -r^2}{R^2 +r^2 -2rR\\cos(\\theta -\\alpha)}\\]\n\n\n\n\n\nFor boundary values \\(\\,g(\\alpha)\\,\\) close to \\(\\,(r,\\theta)\\,\\), \\(\\,\\)the Poisson kernel gets larger, \\(\\,\\)since the denominator of the Poisson kernel is the square of the distance from \\(\\,(r,\\theta)\\,\\) to \\(\\,(R,\\alpha)\\)\nIf we evaluate the potential at the center of the circle by the Poisson integral, \\(\\,\\)we find\n\\[ u(0,0) = \\frac{1}{2\\pi} \\int_0^{2\\pi} g(\\theta) \\,d\\theta\\]\nIn other words, \\(\\,\\)the potential at the center of the circle is the average of the boundary potential\n\n\n\\(~\\)",
    "crumbs": [
      "**Second Semester**",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Elliptic Partial Differential Equations</span>"
    ]
  },
  {
    "objectID": "ch_x3_Elliptic_PDEs.html#sec-x3-34",
    "href": "ch_x3_Elliptic_PDEs.html#sec-x3-34",
    "title": "14  Elliptic Partial Differential Equations",
    "section": "14.4 The Dirichlet Problem in an Annulus and in an Exterior",
    "text": "14.4 The Dirichlet Problem in an Annulus and in an Exterior\n\nThe Dirichlet problem between two circles (annulus) is\n\\[ \\color{red}{u_{rr} +\\frac{1}{r}u_r + \\frac{1}{r^2} u_{\\theta\\theta}=0, \\;\\;R_1 &lt; r &lt; R_2}\\] \\[\\;\\; \\color{red}{\\begin{aligned} u(R_1,\\theta) &= g_1(\\theta) \\\\ u(R_2,\\theta) &= g_2(\\theta) \\end{aligned}, \\;\\;\\; 0 \\leq \\theta &lt; 2\\pi}\\]\n\n\n\n\n\nWe begin by looking for solutions of the form\n\\[ u(r,\\theta)=R(r)\\Theta(\\theta)\\]\nSubstituting this into Laplace’s equation, \\(\\,\\)we get the two following ODEs in \\(\\,R(r)\\,\\) and \\(\\,\\Theta(\\theta)\\):\n\\[\\begin{aligned}\n&r^2 R'' +rR' -\\lambda R = 0 \\quad (\\text{Euler's equation}) \\\\\n&\\Theta'' +\\lambda \\Theta = 0\n\\end{aligned}\\]\nNote that in the two equations, \\(\\,\\)we required the separation constant \\(\\,\\lambda\\,\\) to be greater than, \\(\\,\\)or equal to zero, \\(\\,\\)or else the solution for \\(\\,\\Theta(\\theta)\\,\\) would not be periodic\nSolving these two ODEs, \\(\\,\\)we now have\n\\[\\begin{aligned}\n\\lambda = 0 \\;\\;\n&\\begin{cases}\n\\Theta(\\theta)= \\alpha + \\beta\\theta \\\\\nR(r) = \\gamma + \\delta\\ln r\n\\end{cases} \\\\\n\\lambda &gt; 0 \\;\\;\n&\\begin{cases}\n\\Theta(\\theta)=  a\\cos\\sqrt{\\lambda}\\theta +b\\sin \\sqrt{\\lambda}\\theta\\\\\nR(r) = cr^{\\sqrt{\\lambda}} +dr^{-\\sqrt{\\lambda}}   \n\\end{cases}\n\\end{aligned}\\]\nand using the requirement that \\(\\,\\Theta(\\theta)\\,\\) must be periodic with period \\(\\,2\\pi\\), \\(\\,\\)we have that \\(\\,\\beta=0 \\,\\text{ at }\\, \\lambda=0\\,\\) and \\(\\,\\lambda (&gt;0)\\,\\) must be \\(\\,n^2\\), \\(\\,n= 0,1,2,\\cdots\\)\nHence, \\(\\,\\)we arrive at the following solutions to Laplace’s equation\n\\[\\begin{aligned}\n  &1 \\;\\;\\;(\\text{constants}) \\\\\n  &\\ln r \\\\\n  &r^n \\cos n\\theta \\\\\n  &r^n \\sin n\\theta \\\\\n  &r^{-n} \\cos n\\theta \\\\\n  &r^{-n} \\sin n\\theta\n\\end{aligned}\\]\nSince any sum of these solutions is also a solution, \\(\\,\\)we arrive at our general solution\n\\[ \\color{red}{u(r,\\theta) =\n\\frac{a_0}{2} +\\frac{\\tilde{a_0}}{2} \\ln r\n  +\\sum_{n=1}^\\infty \\left[ \\left(a_n r^n\n   +\\tilde{a_n} r^{-n}\\right) \\cos n\\theta +\n\\left(b_n r^n +\\tilde{b_n} r^{-n}\\right)\n\\sin n\\theta \\right]}\\]\nThe only task left is to determine the constants in the sum so that \\(\\,u(r,\\theta)\\,\\) satisfies the BCs\n\\[ \\begin{aligned} u(R_1,\\theta) &= g_1(\\theta) \\\\ u(R_2,\\theta) &= g_2(\\theta) \\end{aligned}, \\;\\;\\; 0 \\leq \\theta &lt; 2\\pi\\]\nSubstituting the general solution into these BCs and integrating gives the following equations\n\\[\\scriptsize\\begin{aligned}\n&\\begin{cases}\n  a_0 +\\tilde{a_0} \\ln R_1\n   =\\displaystyle \\frac{1}{\\pi} \\int_0^{2\\pi}\n    g_1(\\alpha)\\,d\\alpha \\\\\n  a_0 +\\tilde{a_0} \\ln R_2\n   =\\displaystyle \\frac{1}{\\pi} \\int_0^{2\\pi}\n    g_2(\\alpha)\\,d\\alpha\n\\end{cases} &&\\text{Solve for } a_0, \\tilde{a_0} \\\\\n&\\begin{cases}\n  a_n R_1^n +\\tilde{a_n} R_1^{-n}\n   =\\displaystyle \\frac{1}{\\pi} \\int_0^{2\\pi}\n    g_1(\\alpha) \\cos n\\alpha \\,d\\alpha \\\\\n  a_n R_2^n +\\tilde{a_n} R_2^{-n}\n   =\\displaystyle \\frac{1}{\\pi} \\int_0^{2\\pi}\n    g_2(\\alpha) \\cos n\\alpha \\,d\\alpha\n\\end{cases} &&\\text{Solve for } a_n, \\tilde{a_n} \\\\\n&\\begin{cases}\n  b_n R_1^n +\\tilde{b_n} R_1^{-n}\n   =\\displaystyle \\frac{1}{\\pi} \\int_0^{2\\pi}\n    g_1(\\alpha) \\sin n\\alpha \\,d\\alpha \\\\\n  b_n R_2^n +\\tilde{b_n} R_2^{-n}\n   =\\displaystyle \\frac{1}{\\pi} \\int_0^{2\\pi}\n    g_2(\\alpha) \\sin n\\alpha \\,d\\alpha\n\\end{cases} &&\\text{Solve for } b_n, \\tilde{b_n}\n\\end{aligned}\\]\n\n\\(~\\)\nExample \\(\\,\\) Suppose the potential on the inside circle is zero, \\(\\,\\)while the outside potential is \\(\\,\\sin \\theta\\)\n\\[ u_{rr} +\\frac{1}{r}u_r + \\frac{1}{r^2} u_{\\theta\\theta}=0, \\;\\;\\;1 &lt; r &lt; 2\\]\n\\[ \\begin{aligned} u(1,\\theta) &= 0 \\\\ u(2,\\theta) &= \\sin\\theta \\end{aligned}, \\;\\;\\; 0 \\leq \\theta &lt; 2\\pi\\]\nSolving the necessary equations for \\(\\,a_0, \\tilde{a_0}\\), \\(a_n, \\tilde{a_n}\\), \\(b_n\\), and \\(\\tilde{b_n}\\,\\) yields\n\\[ u(r,\\theta)=\\frac{2}{3} \\left(r -\\frac{1}{r} \\right) \\sin \\theta\\]\n\\(~\\)\nExample \\(\\,\\) Consider the problem with constant potentials on the boundaries\n\\[ u_{rr} +\\frac{1}{r}u_r + \\frac{1}{r^2} u_{\\theta\\theta}=0, \\;\\;\\;1 &lt; r &lt; 2 \\]\n\\[ \\begin{aligned} u(1,\\theta) &= 3 \\\\ u(2,\\theta) &= 5 \\end{aligned}, \\;\\;\\; 0 \\leq \\theta &lt; 2\\pi \\]\nIn this case, \\(\\,\\)since it’s obvious that the solution is independent of \\(\\,\\theta\\), \\(\\,\\)we know our solution must be of the form \\(\\,a_0 +\\tilde{a_0} \\ln r\\). \\(\\,\\)Using our two equations for \\(\\,a_0\\,\\) and \\(\\,\\tilde{a_0}\\), \\(\\,\\)we obtain\n\\[ u(r,\\theta)=3 +\\frac{2}{\\ln 2} \\ln r\\]\nThe only solutions of the two dimensional Laplace equation that depend only on \\(\\,r\\,\\) are constant and \\(\\,\\ln r\\). \\(\\,\\)The potential \\(\\,\\ln r\\,\\) is very important and is called the logarithmic potential\n\\(~\\)\nExample \\(\\,\\) Another interesting problem is\n\\[ u_{rr} +\\frac{1}{r}u_r + \\frac{1}{r^2} u_{\\theta\\theta}=0, \\;\\;\\;1 &lt; r &lt; 2\\]\n\\[ \\begin{aligned} u(1,\\theta) &= \\sin\\theta \\\\ u(2,\\theta) &= \\sin\\theta \\end{aligned}, \\;\\;\\; 0 \\leq \\theta &lt; 2\\pi\\]\nA quick check of the coefficients reveals that they are all zero except for \\(\\,b_1\\,\\) and \\(\\,\\tilde{b}_1\\). \\(\\,\\)Solving for \\(\\,b_1\\,\\) and \\(\\,\\tilde{b_1}\\,\\) gives the solution\n\\[ u(r,\\theta)=\\left(\\frac{r}{3} +\\frac{2}{3r}\\right) \\sin\\theta\\]\n\\(~\\)\n\nExterior Dirichlet Problem\nThe exterior Dirichlet problem\n\\[ u_{rr} +\\frac{1}{r}u_r + \\frac{1}{r^2} u_{\\theta\\theta}=0, \\;\\;\\;\\color{red}{R_1 &lt; r &lt; \\infty}\\]\n\\[ u(R_1,\\theta)=g(\\theta), \\;\\;\\; 0 \\leq \\theta &lt; 2\\pi\\]\nis solved exactly like the interior Dirichlet problem except that now we throw out the solutions that are unbounded as \\(\\,r\\,\\) goes to infinity\nHence, \\(\\,\\)we left with the solution\n\\[ \\color{red}{u(r,\\theta)= \\frac{a_0}{2} +\\sum_{n=1}^{\\infty} \\left(\\frac{r}{R_1}\\right)^{-n}\\left( \\tilde{a_n} \\cos n\\theta +\\tilde{b_n} \\sin n \\theta \\right)}\\]\nwhere \\(\\,a_0\\), \\(\\,\\tilde{a_n}\\,\\) and \\(\\,\\tilde{b_n}\\,\\) are exactly as Fourier series. \\(\\,\\)In other words, \\(\\,\\)we merely expand \\(\\,u(R_1,\\theta)=g(\\theta)\\,\\) as a Fourier series\n\\[ g(\\theta)=\\frac{a_0}{2} +\\sum_{n=1}^\\infty \\left( \\tilde{a_n} \\cos n\\theta +\\tilde{b_n} \\sin n\\theta \\right)\\]\nand then insert the factor \\(\\,\\displaystyle\\left(\\frac{r}{R_1}\\right)^{-n}\\,\\) in each term to get the solution\n\n\\(~\\)\nExample \\(\\,\\) The exterior problem\n\\[ u_{rr} +\\frac{1}{r}u_r + \\frac{1}{r^2} u_{\\theta\\theta}=0, \\;\\;\\;1 &lt; r &lt; \\infty\\]\n\\[ u(1,\\theta)=1 +\\sin\\theta +\\cos 3\\theta, \\;\\;\\; 0 \\leq \\theta &lt; 2\\pi\\]\nhas the solution\n\\[ u(r,\\theta)=1 +\\frac{1}{r}\\sin\\theta +\\frac{1}{r^3}\\cos 3\\theta, \\;\\;\\; 0 \\leq \\theta &lt; 2\\pi\\]\n\\(~\\)\nExample \\(\\,\\) The exterior Neumann problem\n\\[ u_{rr} +\\frac{1}{r}u_r + \\frac{1}{r^2} u_{\\theta\\theta}=0, \\;\\;\\;1 &lt; r &lt; \\infty\\]\n\\[ \\frac{\\partial u}{\\partial r}(1,\\theta)=g(\\theta), \\;\\;\\; 0 \\leq \\theta &lt; 2\\pi\\]\nhas a solution that is the same form as the Dirichlet problem\n\\[ u(r,\\theta)= \\frac{a_0}{2} +\\sum_{n=1}^{\\infty} r^{-n}\\left( \\tilde{a_n} \\cos n\\theta +\\tilde{b_n} \\sin n \\theta \\right)\\]\nbut now the coefficients \\(\\,a_0\\), \\(\\,\\tilde{a_n}\\) and \\(\\,\\tilde{b_n}\\,\\) must satisfy the new BC. Of course, once you have this solution, \\(\\,\\)any constant plus this solution is also a solution\n\\(~\\)",
    "crumbs": [
      "**Second Semester**",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Elliptic Partial Differential Equations</span>"
    ]
  },
  {
    "objectID": "ch_x3_Elliptic_PDEs.html#sec-x3-35",
    "href": "ch_x3_Elliptic_PDEs.html#sec-x3-35",
    "title": "14  Elliptic Partial Differential Equations",
    "section": "14.5 Laplace’s Equation in Spherical Coordinates (Spherical Harmonics)",
    "text": "14.5 Laplace’s Equation in Spherical Coordinates (Spherical Harmonics)\n\nAn important problem in physics is to find the potential inside or outside a sphere when the potential is given on the boundary\nFor the interior problem, \\(\\,\\)we must find the solution \\(\\,u(r,\\phi,\\theta)\\,\\) that satisfies\n\\[ \\left( r^2 u_r \\right)_r +\\frac{1}{\\sin\\phi} \\left( \\sin\\phi\\, u_\\phi \\right)_\\phi +\\frac{1}{\\sin^2\\phi} u_{\\theta\\theta} = 0 \\]\n \\[ u(1,\\phi,\\theta)=g(\\phi,\\theta), \\;0\\leq \\theta &lt;2\\pi,\\;0 \\leq \\phi &lt; \\pi \\] \nNote that this spherical Laplacian is written in a different form than those we’ve seen before. \\(\\,\\)This form is slightly more compact and easier to use\nQuite often \\(\\,g(\\phi,\\theta)\\,\\) has a specific form, \\(\\,\\)so that it isn’t necessary to solve the problem in its most general form. \\(\\,\\)Two important special cases are considered in this section. \\(\\,\\)One is the case when \\(\\,g(\\phi,\\theta)\\,\\) is constant, \\(\\,\\)and the other is when it depends only on the angle \\(\\,\\phi\\,\\) (the angle from the north pole)\n\n\n\n\n\nSpecial Case 1 \\(\\,\\) \\(-\\;\\;\\color{red}{g(\\phi,\\theta) = \\text{constant}}\\)\nIn this case, \\(\\,\\)it is clear that the solution is independent of \\(\\,\\phi\\,\\) and \\(\\,\\theta\\), \\(\\,\\)and so Laplace’s equation reduces to the ODE\n\\[ \\left( r^2 u_r \\right)_r = 0 \\]\nThis is a simple ODE that the student can easily solve; \\(\\,\\)the general solution is\n\\[ u(r)=\\frac{a}{r} +b \\]\nIn other words, \\(\\,\\)constant and \\(\\,\\displaystyle\\frac{a}{r}\\,\\) are the only potential that depend only on the radial distance from the origin. \\(\\,\\)The potential \\(\\,\\displaystyle\\frac{1}{r}\\,\\) is very important in physics and is called the Newtonian potential\n\n\\(~\\)\nExample \\(\\,\\) (Potential interior to a sphere)\n\\[ \\nabla^2 u = 0, \\;\\;\\; 0 &lt; r &lt; 1 \\]\n\\[ u(1,\\phi,\\theta) = 3 \\]\nHere solution must be \\(\\,u(r,\\phi,\\theta) = 3\\,\\) in order to be bounded\n\\(~\\)\nExample \\(\\,\\) (Potential between two spheres each at constant potential)\nSuppose we want to find the steady-state temperature between two spheres held at different temperatures\n\\[ \\nabla^2 u = 0, \\;\\;\\; R_1 &lt; r &lt; R_2 \\]\n\\[\\begin{aligned}\nu(R_1,\\phi,\\theta) &= A \\\\\nu(R_2,\\phi,\\theta) &= B\n\\end{aligned}\\]\nSince we know the potential has the general solution\n\\[ u(r)=\\frac{a}{r} +b \\]\nwe substitute it in the BCs and solve for \\(\\,a\\,\\) and \\(\\,b\\,\\); \\(\\,\\)doing this gives\n\\[ u(r)= (A -B) \\frac{R_1 R_2}{R_2 -R_1} \\frac{1}{r} + \\frac{R_2 B -R_1 A}{R_2 -R_1} \\]\n\n\n\n\n\n\\(~\\)\n\nSpecial Case 2 \\(\\,\\) \\(-\\;\\;\\color{red}{g(\\phi,\\theta) \\text{ depends only on } \\phi}\\)\nHere, the Dirichlet problem takes the form\n\\[ \\left( r^2 u_r \\right)_r +\\frac{1}{\\sin\\phi}\n\\left( \\sin\\phi\\, u_\\phi \\right)_\\phi = 0,\n\\;\\;\\; 0 &lt; r &lt; 1 \\]\n\\[ u(1,\\phi)=g(\\phi),\\;\\;\\; 0 \\leq \\phi \\leq \\pi \\]\nUsing separation of variables, \\(\\,\\)we look for solutions of the form\n\\[ u(r,\\phi)=R(r)\\Phi(\\phi) \\]\nand arrive at the two ODEs\n\\[ \\begin{aligned}\n  &\\left(\\sin\\phi\\, \\Phi' \\right)' +\\lambda \\sin\\phi \\Phi= 0 &&\\text{Legendre's equation}\\\\\n  &r^2R'' +2rR' -\\lambda R = 0 && \\text{Euler's equation}\n  \\end{aligned} \\]\nLegendre’s equation isn’t easy; \\(\\,\\)the general strategy in solving this equation is to make the substitution\n\\[x=\\cos\\phi\\]\nMaking this change of variable gives rise to the new Legendre’s equation\n\\[\\scriptsize\n\\begin{aligned}\n  \\left(\\sin\\phi\\, \\Phi' \\right)'\n  &+\\lambda \\sin\\phi \\Phi= 0\\\\\n  &\\Downarrow\n  \\;\\; x=\\cos\\phi,\n    \\;\\; \\Phi'=\\frac{d\\Phi}{dx} \\frac{dx}{d\\phi}\n    =-\\sin\\phi\\frac{d\\Phi}{dx} \\\\\n  \\left(\\sin\\phi\\, \\Phi' \\right)'\n  &=-\\frac{d}{dx}\n    \\left(\\sin^2\\phi \\frac{d\\Phi}{dx} \\right)\n    \\frac{dx}{d\\phi}\n  =\\left(\\sin^2\\phi \\frac{d^2\\Phi}{dx^2}\n  +2\\sin\\phi\\cos\\phi\n  \\frac{d\\phi}{dx}\\frac{d\\Phi}{dx} \\right)\\sin\\phi \\\\\n  &=\\left[ (1 -x^2) \\frac{d^2\\Phi}{dx^2}\n  -2x\\frac{d\\Phi}{dx} \\right] \\sin\\phi \\\\\n  &\\Downarrow \\\\\n    \\color{red}{(1 -x^2) \\frac{d^2\\Phi}{dx^2}}\n  &\\color{red}{-2x\\frac{d\\Phi}{dx} +\\lambda \\Phi = 0,\n  \\;\\;\\; -1 \\leq x \\leq 1} \\\\\n  &\\Downarrow \\\\\n  \\frac{d}{dx} \\left[ (1 -x^2)\n  \\frac{d\\Phi}{dx} \\right]\n  &+\\lambda \\Phi = 0\n\\end{aligned}\\]\n\nOne of the difficulties in this equation is that\nthe coefficient \\(\\,(1-x^2)\\) of \\(\\displaystyle \\,\\frac{d^2\\Phi}{dx^2}\\) is zero at the ends of the domain \\(-1 \\leq x \\leq 1\\) \nEquations like this are called singular differential equations. \\(\\,\\)We arrive at a very interesting conclusion\nThe only bounded solutions of Legendre’s equation occur when \\(\\,\\lambda=n(n +1)\\), \\(\\,n=0,1,2,\\cdots\\,\\) and these solutions are Legendre polynomials \\(\\,P_n(x)\\)\n\\[\\begin{aligned}\n  &P_0(x) = 1 \\\\\n  &P_1(x) = x \\\\\n  &P_2(x) = \\frac{1}{2}(3x^2 -1) \\\\\n  &P_3(x) = \\frac{1}{2}(5x^3 -3x) \\\\\n  &\\qquad \\vdots \\\\\n  &P_n(x) = \\frac{1}{2^n n!} \\frac{d^n}{dx^n}\n   \\left[ (x^2 -1)^n \\right]\n\\end{aligned}\\]\nThe graphs of a few Legendre polynomials are shown in\n\n\n\n\n\nWe now solve Euler’s equation by substituting \\(\\,\\lambda = n(n+1)\\,\\) and \\(\\,R(r)=r^\\alpha\\,\\) in the equation and solving for \\(\\,\\alpha\\). \\(\\,\\)Doing this, \\(\\,\\)we get two values\n\\[\\alpha = \\begin{cases}\n  \\phantom{-(n \\,} n \\\\\n-(n +1)\n\\end{cases}\\]\nand, \\(\\,\\)hence, \\(\\,\\)Euler’s equation has the general solution\n\\[R_n(r)= a_n r^n + b_n r^{-(n +1)} \\;\\;\\xrightarrow[]{\\text{ bounded solution }}\\;\\;a_n r^n\\]\nThe final step is to form the sum\n\\[ \\color{red}{u(r,\\phi) = \\sum_{n=0}^\\infty a_n r^n P_n(\\cos\\phi)}\\;\\;\\;\\]\nin such a way that it agrees with the BC \\(\\,u(1,\\phi)=g(\\phi)\\). \\(\\,\\)Substituting the above solution into the BC gives\n\\[ g(\\phi) = \\sum_{n=0}^\\infty a_n P_n(\\cos\\phi)\\]\nIf we multiply each side of this equation by \\(\\,P_m(\\cos\\phi)\\sin\\phi\\,\\) and integrate \\(\\,\\phi\\,\\) from \\(\\,0\\,\\) to \\(\\,\\pi\\), \\(\\,\\)we get\n\\[\\begin{aligned}\n\\int_0^\\pi g(\\phi) P_m(\\cos\\phi)\\,\\sin\\phi \\,d\\phi\n&= \\sum_{n=0}^\\infty\n    a_n \\int_0^{\\pi} P_n(\\cos\\phi)\n    P_m(\\cos\\phi)\\,\\sin\\phi \\, d\\phi \\\\\n    &= \\sum_{n=0}^\\infty a_n\n    \\int_{-1}^1 P_n(x) P_m(x) \\,dx \\\\\n    &=\n\\begin{cases}\n    \\quad\\: 0  & n \\neq m \\\\\n    \\displaystyle \\frac{2}{2m +1}\\, a_m & n = m\n\\end{cases} \\\\\n&\\Downarrow \\\\\na_n &=\\frac{2n +1}{2} \\int_0^{\\pi} g(\\phi)\n    P_n(\\cos\\phi)\\, \\sin \\phi \\,d\\phi\n\\end{aligned}\\]\n\n\n\\(~\\)\nExample \\(\\,\\) Suppose the temperature on the surface of the sphere is given by\n\\[ g(\\phi) = 1 -\\cos 2\\phi, \\quad 0 \\leq \\phi \\leq \\pi \\]\nand suppose we would like to find the temperature inside the sphere. \\(\\,\\)In this problem, \\(\\,\\)the temperature is constant on circles of constant latitude. \\(\\,\\)To find \\(\\,u\\), \\(\\,\\)we must solve\n\\[ \\left( r^2 u_r \\right)_r +\\frac{1}{\\sin\\phi} \\left( \\sin\\phi\\, u_\\phi \\right)_\\phi = 0, \\;\\;\\; 0 &lt; r &lt; 1 \\]\n\\[u(1,\\phi)=1 -\\cos 2\\phi,\\;\\;\\; 0 \\leq \\phi \\leq \\pi \\]\nOur goal now is to expand \\(\\,g(\\phi)\\,\\) as a series of Legendre polynomials;\n\\[\\begin{aligned}\n1 -\\cos 2\\phi &= 1 - \\left( 2\\cos^2\\phi -1 \\right) \\\\\n&= \\frac{4}{3} - \\frac{4}{3}\\left[ \\frac{1}{2} (3\\cos^2\\phi -1) \\right] \\\\\n&= \\frac{4}{3} P_0(\\cos\\phi) -\\frac{4}{3}P_2(\\cos\\phi)\\\\\n&\\Downarrow \\\\\na_0 = \\frac{4}{3},\\;\\; a_2&=-\\frac{4}{3},\\;\\;a_1=a_3=a_4=a_5=\\cdots =0\n\\end{aligned}\\]\nHence, \\(\\,\\)the solution to the problem is\n\\[\\begin{aligned}\nu(r,\\phi) &= \\frac{4}{3} P_0(\\cos\\phi) -\\frac{4}{3} r^2 P_2(\\cos\\phi)\n= \\frac{4}{3} - \\frac{2}{3} r^2 (3\\cos^2\\phi -1)\n\\end{aligned}\\]\n\\(~\\)\nNOTE \\(\\,\\) The solution of the exterior Dirichlet problem\n\\[\\;\\;\\displaystyle \\left( r^2 u_r \\right)_r +\\frac{1}{\\sin\\phi} \\left( \\sin\\phi\\, u_\\phi \\right)_\\phi = 0, \\;\\; 1 &lt; r &lt; \\infty, \\;\\;u(1,\\phi)=g(\\phi),\\;\\;\\; 0 \\leq \\phi \\leq \\pi \\]\nis\n\\[ u(r,\\phi)=\\sum_{n=0}^\\infty \\frac{b_n}{r^{n+1}} P_n(\\cos\\phi)\\]\nwhere \\(\\;\\displaystyle b_n = \\frac{2n +1}{2} \\int_0^\\pi g(\\phi) P_n(\\cos\\phi)\\, \\sin\\phi \\, d\\phi\\)\nFor example, \\(\\,\\)the BC \\(\\,g(\\phi)=3\\,\\) would yield the solution \\(\\,u(r,\\theta)=3/r\\). \\(\\,\\)Note that in this problem, \\(\\,\\)the solution goes to zero, \\(\\,\\)while in two dimensions, \\(\\,\\)the exterior solution with constant BC was itself a constant\n\\(~\\)\n\nGeneral Case \\(\\,\\) \\(-\\;\\;\\color{red}{g(\\phi,\\theta)}\\)\nWe consider the boundary value problem\n\\[\\color{red}{\\begin{aligned}\nu_{rr} +\\frac{2}{r} u_r &+\\frac{1}{r^2}u_{\\phi\\phi} +\\frac{\\cot\\phi}{r^2} u_{\\phi} +\\frac{1}{r^2\\sin^2\\phi}u_{\\theta\\theta} = 0, \\;\\; r &lt; R \\\\\nu(R,\\phi,\\theta) &= g(\\phi,\\theta)\\\\\n\\end{aligned}}\\]\nin a sphere of radius \\(\\,R\\)\n1. \\(~\\) Solve this problem by separation of variables,\n2. \\(~\\) Derive \\(\\,K(r,\\phi,\\theta; R,\\varphi,\\vartheta)\\,\\) in the equivalent integral formula:\n\\[ u(r,\\phi,\\theta) = \\int_0^{2\\pi} \\int_0^\\pi K(r,\\phi,\\theta; R,\\varphi,\\vartheta)\\, g(\\varphi,\\vartheta) \\sin\\varphi \\,d\\varphi \\, d\\vartheta\\]\n1. \\(\\,\\)Applying separation of variables, \\(\\,\\)we find that the equation has the solution of the form \\(\\,R(r) \\,\\Phi(\\phi) \\,\\Theta(\\theta)\\), \\(\\,\\)provided\n\\[ -r^2 \\sin^2\\phi \\, \\frac{R'' +\\frac{2}{r}R'}{R} -\\sin\\phi\\frac{(\\sin\\phi\\, \\Phi')'}{\\Phi} = \\frac{\\;\\Theta''}{\\Theta}= -\\mu &lt; 0\\]\nSince \\(\\Theta\\) must be periodic of period \\(\\,2\\pi\\), \\(\\,\\)we have \\(\\,\\color{blue}{\\Theta=\\cos m\\theta}\\,\\) or \\(\\,\\color{blue}{\\sin m\\theta}\\), \\(\\,\\)where \\(\\,\\color{blue}{\\mu = m^2, \\; m = 0, 1, 2,\\cdots}\\). \\(\\,\\)Then\n\\[\\begin{aligned}\n  -\\frac{r^2 R'' +2rR'}{R}\n  = \\frac{(\\sin\\phi\\, \\Phi')'}{\\sin\\phi\\,\\Phi}\n  &-\\frac{m^2}{\\sin^2\\phi} = -\\lambda &lt; 0 \\\\\n  &\\Downarrow \\\\\n  \\color{blue}{(\\sin\\phi\\, \\Phi')'\n  +\\left(\\lambda \\sin\\phi \\,\n  {\\color{red}{-\\frac{m^2}{\\sin\\phi}}} \\right)\n  \\Phi } \\;& {\\color{blue}{= 0}}\\\\\n  \\color{blue}{r^2 R'' +2r R' -\\lambda R}\n   \\;&\\color{blue}{= 0}\n\\end{aligned}\\]\n\nThe equation for \\(\\,\\Phi(\\phi)\\,\\) is singular at its two endpoints \\(\\,\\phi=0\\,\\) and \\(\\,\\phi=\\pi\\). \\(\\,\\)In lieu of boundary conditions, \\(\\,\\)we impose the condition that \\(\\,\\Phi\\,\\) and \\(\\,\\Phi'\\,\\) remain bounded at both ends. \\(\\,\\)This gives an eigenvalue problem with two singular endpoints. \\(\\,\\)We introduce the new variable \\(\\,x=\\cos\\phi\\,\\) and let \\(\\,\\Phi(\\phi)=P(\\cos\\phi)\\). \\(\\,\\)Then the equation for \\(\\,\\Phi(\\phi)\\,\\) becomes\n\\[\\color{red}{\\frac{d}{dx}\n\\left[\\left(1 -x^2\\right) \\frac{dP}{dx} \\right]\n+\\left[\\lambda -\\frac{m^2}{1 -x^2 } \\right] P = 0}\n\\tag{AL}\\label{eq:AL}\\]\nIn the case of \\(\\,m=0\\):\n\\[\\frac{d}{dx} \\left[\\left(1 -x^2\\right)\n\\frac{dP}{dx} \\right] +\\lambda P = 0\n\\tag{LG}\\label{eq:LG}\\]\nwe obtain a function which is bounded for \\(\\,-1 \\leq x \\leq 1\\,\\) if and only if\n\\[\\lambda=n(n+1), \\;\\; n=0,1,2,\\cdots\\]\nThese, \\(\\,\\)then, \\(\\,\\)are the eigenvalues. \\(\\,\\)Setting \\(P_n(1)=1\\), \\(\\,\\)we obtain the eigenfunction \\(\\,P_n(x)\\,\\) corresponding to the eigenvalue \\(\\,\\lambda_n=n(n+1):\\)\n\\[ P_n(x)= \\sum_{k=0}^n \\frac{(n +k)!}{2^k (k!)^2 (n -k)!}(x - 1)^k\\;\\;\\]\nIt is called a Legendre polynomial\nSince \\(\\,P_n(x)\\,\\) is of exactly degree \\(\\,n\\,\\) in \\(\\,x\\), \\(\\,\\)any polynomial of degree \\(\\,k\\,\\) can be expressed as a linear combination of \\(\\,P_n(x)\\,\\) with \\(\\,n=0,1,\\cdots,k\\). \\(\\,\\)Since \\(\\,P_n(x)\\,\\) are orthogonal, \\(\\,\\)each \\(\\,P_n(x)\\,\\) must therefore be orthogonal to all powers of \\(\\,x\\,\\) less than \\(\\,n:\\)\n\\[ \\int_{-1}^1 x^k P_n(x)\\, dx=0, \\;\\;k=0,1,\\cdots, n-1\\]\nThese \\(\\,n\\,\\) linear conditions, \\(\\,\\)together with the fact that \\(\\,P_n(x)\\,\\) is of degree \\(\\,n\\), \\(\\,\\)determine \\(\\,P_n(x)\\,\\) uniquely\nWe can verify the identity\n\\[ P_n(x)=\\frac{1}{2^n n!}\\frac{d^n}{dx^n} \\left(x^2 -1\\right)^n\\]\nwhich is called the Rodrigues formula. \\(\\,\\)It is clear from this formula that \\(\\,P_n(x)\\,\\) is even in \\(\\,x\\,\\) if \\(\\,n\\,\\) is even, \\(\\,\\)and odd if \\(\\,n\\,\\) is odd\nThe Legendre polynomials can also be defined as the coefficients in a formal expansion in powers of \\(\\,t\\,\\) of the generating function\n\\[ \\frac{1}{\\sqrt{1 -2xt +t^2}} = \\sum_{n=0}^\\infty P_n(x) t^n \\]\nWe now wish to consider the equation \\(\\eqref{eq:AL}\\) with a positive \\(m\\). \\(\\,\\)For this purpose, \\(\\,\\)we first differentiate the equation \\(\\eqref{eq:LG}\\) \\(\\,m\\,\\) times with respect to \\(\\,x\\). \\(\\,\\)This gives\n\\[ \\left(1 -x^2\\right) \\frac{d^{m+2}}{dx^{m+2}}P -2(m +1)x \\frac{d^{m+1}}{dx^{m+1}}P +\\left[ \\lambda -m(m +1)\\right] \\frac{d^{m}}{dx^{m}}P=0\\]\nWe now introduce the new dependent variable\n\\[ \\color{red}{Q(x)=\\left(1 -x^2\\right)^{m/2} \\frac{d^{m}}{dx^{m}}P(x)}\\]\nThe above equation becomes\n\\[\\frac{d}{dx} \\left[\\left(1 -x^2\\right) \\frac{dQ}{dx} \\right] +\\left[\\lambda -\\frac{m^2}{1 -x^2 } \\right] Q = 0\\;\\;\\]\nThus any solution \\(\\,P(x)\\,\\) of \\(\\eqref{eq:LG}\\) leads to a solution \\(\\,Q(x)\\,\\) of \\(\\eqref{eq:AL}\\), \\(\\,\\)unless \\(\\,P(x)\\,\\) is a polynomial of degree less than \\(\\,m\\)\nSince \\(\\,P_n(x)\\,\\) is a polynomial of degree \\(\\,n\\),\n\\[\\frac{d^m}{dx^m} P_n = 0 \\, \\text{ for }\\, n &lt; m,\\]\nand we get no eigenfunction. \\(\\,\\)Therefore, \\(\\,\\)the eigenvalues are precisely \\(\\,n(n+1)\\,\\) with \\(\\,n=m, m+1, \\cdots.\\) \\(\\,\\)The eigenfunctions\n\\[ \\color{red}{P_n^m(x)=\\left(1 -x^2 \\right)^{m/2} \\frac{d^m}{dx^m} P_n(x)=\\frac{1}{2^n n!} \\left(1 -x^2 \\right)^{m/2} \\frac{d^{m +n}}{dx^{m +n}} \\left(x^2 -1 \\right)^n}\\]\nare called the associated Legendre functions. \\(\\,P_n^m(x)\\,\\) corresponds to the eigenvalue \\(\\,n(n +1)\\), \\(\\,\\)and \\(\\,n \\geq m\\)\nIn Fourier expressions, \\(\\,\\)it is useful to know the integrals of \\(\\,\\left[P_n^m(x)\\right]^2\\). \\(\\,\\)It follows from the Rodrigues formula and \\(\\,n\\,\\) integrations by parts that\n\\[\\tiny \\begin{aligned}\n  \\int_0^\\pi \\left[ P_n(\\cos\\phi)\\right]^2 \\,\\sin\\phi  \\,d\\phi &= \\int_{-1}^1 \\left[P_n(x) \\right]^2 \\,dx\\\\\n  &= \\frac{1}{2^{2n} n!^2} \\int_{-1}^1 \\left[ \\frac{d^n}{dx^n} \\left(1- x^2 \\right)^n\\right]^2 \\, dx\\\\\n  &= \\frac{1}{2^{2n} n!^2} \\left[ \\underbrace{\\left.\\frac{d^{n-1}}{dx^{n-1}} \\left(1- x^2 \\right)^n\n  \\frac{d^{n}}{dx^{n}} \\left(1- x^2 \\right)^n \\right|_{-1}^{1}}_{=0}\n  -\\int_{-1}^1 \\frac{d^{n-1}}{dx^{n-1}} \\left(1- x^2 \\right)^n\n  \\frac{d^{n+1}}{dx^{n+1}} \\left(1- x^2 \\right)^n \\,dx\\right]\\\\\n  &\\,\\vdots \\\\\n  &=\\frac{(-1)^n}{2^{2n} n!^2} \\int_{-1}^1 \\left(1- x^2 \\right)^n\n  \\underbrace{\\frac{d^{2n}}{dx^{2n}} \\left(1- x^2 \\right)^n}_{(-1)^n (2n)!} \\,dx \\\\\n  &=\\frac{(2n)!}{2^{2n} n!^2} \\int_{-1}^1 \\left(1- x^2 \\right)^n \\,dx = \\frac{(2n)!}{2^{2n} n!^2} 2^{2n +1} \\frac{n!^2}{(2n+1)!}=\\frac{2}{2n +1}\n\\end{aligned}\\]\nSimilarly,\n\\[\\tiny \\begin{aligned}\n  \\color{blue}{\\int_0^\\pi \\left[ P_n^m(\\cos\\phi)\\right]^2 \\,\\sin\\phi  \\,d\\phi} &= \\int_{-1}^1 \\left[P_n^m(x) \\right]^2 \\,dx\n  = \\int_{-1}^1 \\left[ \\left(1 -x^2 \\right)^{m/2} \\frac{d^m}{dx^m} P_n(x) \\right]^2 \\, dx\\\\\n  &= \\underbrace{\\left. \\frac{d^{m-1}}{dx^{m-1}} P_n(x)\n  \\left\\{ \\left(1- x^2 \\right)^m  \\frac{d^m}{dx^m} P_n(x) \\right\\} \\right|_{-1}^{\\phantom{-}1}}_{=0}\n  -\\int_{-1}^1 \\frac{d^{m-1}}{dx^{m-1}} P_n(x)\n  \\frac{d}{dx} \\left\\{\\left(1- x^2 \\right)^m  \\frac{d^m}{dx^m} P_n(x)  \\right\\} \\,dx \\\\\n  &\\;\\Downarrow \\;\\;\\left(1- x^2 \\right)^m = (-1)^m x^{2m}+\\cdots, \\;\\;\\frac{d^m}{dx^m} P_n(x)=c_n \\frac{n!}{(n -m)!}x^{n -m}+\\cdots\\\\\n  &= (-1)^m \\int_{-1}^1 P_n(x)\n  \\underbrace{\\frac{d^m}{dx^m} \\left\\{\\left(1- x^2 \\right)^m  \\frac{d^m}{dx^m} P_n(x) \\right\\}}_{(-1)^m \\,c_n\\, \\frac{n!}{(n-m)!\\;} \\,\\frac{(n +m)!\\;}{n!} \\,x^n +\\,\\cdots} \\,dx \\\\\n  &=\\frac{(n+m)!}{(n-m)!} \\int_{-1}^1 P_n(x) c_n x^n \\,dx \\\\\n  &=\\frac{(n+m)!}{(n-m)!} \\int_{-1}^1 \\left[ P_n(x) \\right ]^2 \\,dx \\\\\n  &=\\color{blue}{\\frac{2}{2n +1}\\frac{(n+m)!}{(n-m)!}}\n\\end{aligned}\\]\nin which \\(\\,c_n\\,\\) is the coefficient of \\(\\,x^n\\,\\) in \\(\\,P_n(x)\\)\nPutting \\(\\,\\lambda=n(n+1)\\,\\) in the equation for \\(\\,R(r)\\), \\(\\,\\)we obtain the two solutions \\(\\,\\color{blue}{r^n}\\,\\) and \\(\\,r^{-(n +1)}\\). \\(\\,\\)Only the first of these is bounded at \\(\\,r=0\\)\nThe method of separation of variables thus gives the harmonic functions\n\n\\[\\begin{aligned}\n  &r^n P_n^m(\\cos\\phi) \\cos m\\theta\\\\\n  &r^n P_n^m(\\cos\\phi) \\sin m\\theta\n\\end{aligned}\\]\n\nWe note that \\(\\,P_n^m(\\cos\\phi)\\,\\) is \\(\\,\\sin^m\\phi\\,\\) times a polynomial of degree \\(\\,(n -m)\\,\\) in \\(\\,\\cos\\phi\\,\\) which is even or odd according as \\(\\,(n -m)\\,\\) is even or odd\nSince \\(\\,r\\cos\\phi=z\\), \\(\\,\\)it follows that \\(\\,r^{n-m}\\,\\) times this polynomial is a polynomial in \\(\\,r^2\\,\\) and \\(\\,z\\), \\(\\,\\)and hence in \\(\\,x\\), \\(\\,y\\), and \\(\\,z\\)\nAlso, \\(\\,r\\sin\\phi=\\sqrt{x^2 +y^2}\\,\\). \\(\\,\\)Thus \\(\\,r^m\\sin^m\\phi \\cos m\\theta\\,\\) and \\(\\,r^m\\sin^m\\phi \\sin m\\theta\\,\\) are the polynomial solutions in \\(\\,x\\,\\) and \\(\\,y\\,\\) obtained by separating the two-dimensional Laplace’s equation in polar coordinates\nWe see then that the functions \\(\\,r^n P_n^m(\\cos\\phi) \\cos m\\theta\\,\\) and \\(\\,r^n P_n^m(\\cos\\phi) \\sin m\\theta\\,\\) are homogeneous polynomials of degree \\(\\,n\\,\\) in \\(\\,x\\), \\(\\,y\\), and \\(\\,z\\). \\(\\,\\)They are of degree \\(\\,(n -m)\\,\\) in \\(\\,z\\). \\(\\,\\)These polynomials are called spherical harmonics\n\nThe formal solution of the problem is then\n\\[ \\color{red}{u(r,\\phi,\\theta) = \\sum_{n=0}^\\infty \\left( \\frac{r}{R} \\right)^n \\left[ \\frac{1}{2} a_{n0} P_n(\\cos\\phi) +\\sum_{m=1}^n \\left(a_{nm} \\cos m\\theta +b_{nm} \\sin m\\theta \\right) P_n^m(\\cos\\phi) \\right]} \\]\nwhere\n\\[\\begin{aligned}\n  a_{nm} &= \\frac{(2n +1)}{2\\pi}\n  \\frac{(n -m)!}{(n +m)!}\n\\int_{0}^{2\\pi} \\int_{0}^\\pi g(\\phi,\\theta)\n    \\,P_n^m(\\cos\\phi) \\,\\cos m\\theta \\,\n    \\sin\\phi \\,d\\phi\n      \\,d\\theta \\\\\n  b_{nm} &= \\frac{(2n +1)}{2\\pi}\n  \\frac{(n -m)!}{(n +m)!}\n  \\int_{0}^{2\\pi} \\int_{0}^\\pi g(\\phi,\\theta)\n      \\,P_n^m(\\cos\\phi) \\,\\sin m\\theta \\,\n      \\sin\\phi \\,d\\phi\n      \\,d\\theta\n\\end{aligned} \\]\nare the \\(\\,n\\,\\)-th Fourier coefficients of \\(\\,g(\\phi,\\theta)=u(R,\\phi,\\theta)\\)\n\\(~\\)\n2. \\(\\,\\)We can write this solution as\n\\[ u(r,\\phi,\\theta) = \\int_0^{2\\pi} \\int_0^\\pi K(r,\\phi,\\theta; R,\\varphi,\\vartheta)\\, g(\\varphi,\\vartheta) \\sin\\varphi \\,d\\varphi \\, d\\vartheta \\]\nwhere \\(\\,K(r,\\phi,\\theta; R,\\varphi,\\vartheta)\\,\\) is\n\\[ \\scriptsize\\displaystyle\n   \\sum_{n=0}^\\infty \\left( \\frac{r}{R} \\right)^n\n   \\underbrace{\\left[ \\frac{2n +1}{4\\pi}\n    P_n(\\cos\\phi) P_n(\\cos\\varphi) +\\sum_{m=1}^n\n     \\frac{(2n +1)}{2\\pi}\\frac{(n -m)!}{(n +m)!}\n      P_n^m(\\cos\\phi) P_n^m(\\cos\\varphi)\n      \\cos m(\\theta -\\vartheta)\n       \\right]}_{K_n(\\phi,\\,\\theta;\n        \\,\\varphi,\\,\\vartheta)}\n\\tag{KL}\\label{eq:KL}\\]\n\nFor a fixed \\(\\,n\\),\n\\[\\begin{aligned}\n\\int_{0}^{2\\pi} \\int_{0}^\\pi &\n  \\,K_n(\\phi,\\theta;\\varphi,\\vartheta)\\, r^l\n   \\,P_l^m(\\cos\\varphi) \\,(a\\cos m\\vartheta\n   +b\\sin m\\vartheta) \\,\\sin\\varphi \\,d\\varphi\n    \\,d\\vartheta \\\\\n&=\n\\begin{cases}\n   r^n P_n^m(\\cos\\phi)\n   (a\\cos m\\theta +b\\sin m\\theta)\n   & \\text{ for }\\; l=n \\\\\n   0 & \\text{ for }\\; l \\neq n\n\\end{cases}\n\\end{aligned}\\]\nThis means that any homogeneous polynomial of degree \\(\\,l\\,\\) which is a solution of Laplace’s equation can be written as a linear combination of the \\(\\,2l+1\\,\\) spherical harmonics \\(\\,r^l \\,P_l^m(\\cos\\phi)\\cos m\\theta\\,\\) and \\(\\,r^l \\,P_l^m(\\cos\\phi)\\sin m\\theta\\). \\(\\,\\)The kernel \\(\\,K_n(\\phi,\\theta;\\varphi,\\vartheta)\\,\\) has the property that it reproduces all homogeneous harmonic polynomials of degree \\(\\,n,\\) \\(\\,\\)and reduces those of other degrees to zero\nSince rotation of the coordinate axes takes a homogeneous harmonic polynomial of degree \\(\\,l\\,\\) into another such polynomial in the new variable, \\(\\,\\)the kernel \\(\\,K_n\\,\\) must be unchanged by such a rotation. \\(\\,\\)That is, \\(\\,\\)if the new polar coordinates are \\(\\,(r',\\phi',\\theta'),\\) \\(\\,\\)we have \\(\\,K_n(\\phi,\\theta;\\varphi,\\vartheta)=K_n(\\phi',\\theta';\\varphi',\\vartheta')\\). \\(\\,\\)In particular, \\(\\,\\)we can rotate our coordinates so that the new \\(\\,z\\)-axis passes through \\(\\,(r,\\phi,\\theta)\\); \\(\\,\\)that is, so that \\(\\,\\phi'=0\\)\nBy definition, \\(\\,P_n(1)=1\\). \\(\\,\\)On the other hand, \\(\\,P_n^m(x)\\,\\) with \\(\\,m \\geq 1\\,\\) has a factor \\(\\,\\left(1 -x^2\\right)^{m/2}\\), \\(\\,\\)so that \\(\\,P_n^m(1)=0\\), \\(\\,\\)for \\(\\,m \\geq 1\\). \\(\\,\\)Therefore\n\\[ K_n(\\phi,\\theta;\\varphi,\\vartheta) =K_n(0,\\theta';\\varphi',\\vartheta')= \\frac{2n+1}{4\\pi} P_n(\\cos\\varphi') \\tag{KS}\\label{eq:KS}\\]\nwhere \\(\\,\\varphi'\\,\\) is the angle between the direction \\(\\,(\\phi,\\theta)\\,\\) and \\(\\,(\\varphi,\\vartheta):\\)\n\\[ \\cos\\varphi'=\\cos\\phi\\cos\\varphi +\\sin\\phi\\sin\\varphi \\cos(\\theta-\\vartheta) \\]\nSubstituting the identity \\(\\eqref{eq:KS}\\) in \\(\\eqref{eq:KL}\\), \\(\\,\\)we find\n\\[\\begin{aligned}\n    K(r,\\phi,\\theta;R,\\varphi,\\vartheta) &= \\frac{1}{4\\pi} \\sum_{n=0}^\\infty (2n +1) \\left( \\frac{r}{R} \\right)^n P_n(\\cos\\varphi')\\\\\n    &= \\frac{1}{4\\pi} \\left( 2r\\frac{\\partial }{\\partial r} +1 \\right) \\sum_{n=0}^\\infty \\left( \\frac{r}{R} \\right)^n P_n(\\cos\\varphi')\n\\end{aligned}\\]\nEvaluating the generating function at \\(\\,x=\\cos\\varphi'\\,\\) and \\(\\,t=\\frac{r}{R}\\), \\(\\,\\)we have the identity\n\\[ \\frac{R}{\\sqrt{r^2 +R^2 -2rR\\cos\\varphi'}} = \\sum_{n=0}^\\infty \\left( \\frac{r}{R}\\right)^n P_n(\\cos\\varphi') \\]\nSince\n\\[ \\scriptsize\n\\begin{aligned}\n\\left( 2r\\frac{\\partial}{\\partial r} +1\\right)\n&\\frac{R}{\\sqrt{r^2 +R^2 -2rR\\cos\\varphi'}}\n= \\frac{R\\left(R^2 -r^2\\right)}{\\left(r^2 +R^2 -2rR\\cos\\varphi'\\right)^{3/2}} \\\\\n&= \\frac{\\displaystyle 1 -\\left(\\frac{r}{R}\\right)^2}{\\displaystyle\\left(1 +\\left(\\frac{r}{R}\\right)^2 -2\\frac{r}{R}\\cos\\varphi'\\right)^{3/2}} = \\mathcal{P}(r, R, \\varphi')\n\\end{aligned}\\]\nwhere \\(\\,\\left(r^2 +R^2 -2rR\\cos\\varphi'\\right)^{1/2}\\) is the distance from the point \\(\\,(r,\\phi,\\theta)\\,\\) to the point \\(\\,(R,\\varphi,\\vartheta)\\), \\(\\,\\)thus, \\(\\,\\)the solution formula is equivalent to the integral formula\n\\[\n\\color{red}{u(r,\\phi,\\theta)\n=\\frac{1}{4\\pi} \\int_0^{2\\pi} \\int_0^\\pi\n  \\mathcal{P}(r, R, \\varphi') \\,\n    g(\\varphi,\\vartheta) \\sin\\varphi \\,d\\varphi \\,\n    d\\vartheta} \\]\nThis is Poisson’s integral formula in three dimensions\n\n\n\\(~\\)",
    "crumbs": [
      "**Second Semester**",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Elliptic Partial Differential Equations</span>"
    ]
  },
  {
    "objectID": "ch_x3_Elliptic_PDEs.html#sec-x3-36",
    "href": "ch_x3_Elliptic_PDEs.html#sec-x3-36",
    "title": "14  Elliptic Partial Differential Equations",
    "section": "14.6 A Nonhomogeneous Dirichlet Problem (Green’s Function)",
    "text": "14.6 A Nonhomogeneous Dirichlet Problem (Green’s Function)\n\nA common problem in applied mathematics is to find the potential in some region of space in response to a forcing term \\(\\,f\\). \\(\\,\\)A typical example would be to find the potential inside a circle in two dimensions that satisfies\n\\[ u_{rr} +\\frac{1}{r}u_r + \\frac{1}{r^2} u_{\\theta\\theta}=-f(r,\\theta), \\;\\;\\;0 &lt; r &lt; 1 \\]\n\\[ u(1,\\theta) = 0, \\;\\;\\; 0 \\leq \\theta &lt; 2\\pi \\]\nNote that we have chosen the boundary values to be zero. \\(\\,\\)If we wanted to solve the general case, where both the equation and BC were nonhomogeneous, \\(\\,\\)we could add the Poisson integral formula from Section 14.3 to the solution from this section\nIn order to gain a little intuition about nonhomogeneous differential equations, \\(\\,\\)let’s consider graphing the solution to the following Poisson’s equation:\n\\[ u_{rr} +\\frac{1}{r}u_r + \\frac{1}{r^2} u_{\\theta\\theta}=-q, \\quad 0 &lt; r &lt; 1 \\]\n\\[ u(1,\\theta) = 0, \\quad 0 \\leq \\theta &lt; 2\\pi \\]\nHere, \\(\\,\\)the potential is fixed at zero on the boundary, \\(\\,\\)and the Laplacian of \\(\\,u\\,\\) is always equal to \\(-q\\,\\) inside the circle. \\(\\,\\)Since \\(\\,\\nabla^2 u(p)\\,\\) measures the difference between \\(\\,u(p)\\,\\) and the average of its neighbors, \\(\\,\\)it will look like a thin membrane fixed at the boundary that was continuously being pushed up by a stream of air from below\n\n\\(~\\)\n\nPotentials from Point Sources and Sinks\n\nIn solving a nonhomogeneous linear equation, \\(\\,\\)it is sufficient to solve the equation with a point source since we can find the solution to the general problem by summing the responses to point sources. \\(\\,\\)Our goal here is to find the potential in some region of space due to a point source (or sink)\nSuppose we have a single point source of magnitude \\(\\,q\\,\\) located at the origin. \\(\\,\\)It is clear that the heat will flow outward along radial lines, \\(\\,\\)and, \\(\\,\\)hence, \\(\\,\\)if we compute the total outward flux across a circle of radius \\(\\,r\\), \\(\\,\\)we have\n\n\\[ \\text{Total outward flux across the circle }=-\\int_0^{2\\pi} \\underbrace{u_r(r)}_{&lt;\\,0}\\, r\\,d\\theta = -2\\pi r\\, u_r(r)\\]\n\nBut the outward flux must be equal to the heat generated within the circle (conservation of energy), \\(\\,\\)so we have\n\\[-2\\pi r u_r(r) = q\\]\nSolving this simple differential equation for \\(\\,u_r(r)\\), \\(\\,\\)we get\n\\[ u(r)=\\frac{q}{2\\pi} \\ln \\frac{1}{r} \\]\nA sink, \\(\\,\\)on the other hand, \\(\\,\\)is represented by a negative source, \\(\\,\\)and so a sink with magnitude \\(-q\\,\\) would give rise to a potential field\n\\[ u(r)=-\\frac{q}{2\\pi} \\ln \\frac{1}{r} \\]\n\n\n\\(~\\)\n\nPoisson’s Equation Inside a Circle\n\nWe will now solve the important problem\n\\[ u_{rr} +\\frac{1}{r}u_r + \\frac{1}{r^2} u_{\\theta\\theta}=-f(r,\\theta), \\quad 0 &lt; r &lt; 1 \\]\n\\[ u(1,\\theta) = 0, \\quad 0 \\leq \\theta &lt; 2\\pi \\]\nThe Green function technique (impulse-response method) consists of two steps:\n\nFinding the potential \\(\\,G(r,\\theta,\\rho,\\phi)\\,\\) at \\(\\,(r,\\theta)\\), \\(\\,\\)which we force to be zero on the boundary and which is due to a single charge (magnitude 1) at \\(\\,(\\rho,\\phi)\\)\nSumming the individual responses \\(\\,G(r,\\theta,\\rho,\\phi)\\,\\) weighted by \\(\\,f(\\rho,\\phi)\\,\\) over all \\(\\,(\\rho,\\phi)\\,\\) in the circle to get the solution\n\n\\[ u(r,\\theta)=\\int_0^{2\\pi} \\int_0^1 G(r,\\theta,\\rho,\\phi)\\, f(\\rho,\\phi)\\,\\rho\\, d\\rho d\\phi\\]\n\n\nSTEP 1 \\(\\,\\) Since the function\n\\[ \\frac{1}{2\\pi} \\ln \\frac{1}{R} \\]\nis the potential at \\(\\,(r,\\theta)\\,\\) due to a single unit charge at \\(\\,(\\rho,\\phi)\\), \\(\\,\\)where \\(\\,R\\,\\) is the distance from \\(\\,(r,\\theta)\\,\\) to \\(\\,(\\rho,\\phi)\\,\\), \\(\\,\\)the only thing left to do is to modify the function so that it is zero on the boundary\nSTEP 2 \\(\\,\\) The strategy is to place a sink outside the circle at such a point that the potential due to both is constant on the circle at \\(\\,r=1\\). \\(\\,\\)We can then subtract this constant value to obtain a zero potential on the boundary. \\(\\,\\)The big question is, \\(\\,\\)of course, \\(\\,\\)where do we place the sink outside circle, \\(\\,\\)so that the potential on the boundary is constant?\nWithout going into the details, \\(\\,\\)we can show rather easily that if the sink is placed at \\(\\,(1/\\rho,\\phi)\\), \\(\\,\\)then the potential due to the source and the sink will be constant on the circle \\(\\,r=1\\)\n\\[\\begin{aligned}\n\\displaystyle u^*(r,\\theta)\n   &= \\frac{1}{2\\pi} \\ln \\frac{1}{R}\n   -\\frac{1}{2\\pi} \\ln \\frac{1}{\\bar{R}}\n   = \\frac{1}{2\\pi} \\ln\\frac{\\bar{R}}{R} \\\\[2pt]\n   &=\\frac{1}{2\\pi}\n   \\ln \\frac{\\sqrt{r^2 +1/\\rho^2\n    -2r/\\rho \\cos(\\theta-\\phi)}}{\\sqrt{r^2 +\\rho^2\n     -2r\\rho \\cos(\\theta -\\phi)}} \\\\[2pt]\n   &\\Downarrow \\;\\; r=1 \\\\\n   u^*(1,\\theta) &= -\\frac{1}{2\\pi} \\ln \\rho\n\\end{aligned}\\]\nWith these steps in mind, \\(\\,\\)we construct Green’s function\n\\[ G(r,\\theta,\\rho,\\phi)=\\frac{1}{2\\pi}\\ln \\frac{1}{R} -\\frac{1}{2\\pi}\\ln \\frac{1}{\\bar{R}} + \\frac{1}{2\\pi}\\ln \\rho \\]\n\n\n\n\n\nTo find the solution to our orginal problem, \\(\\,\\)we merely superimpose the impulse functions; \\(\\,\\)this brings us to the final solution\n\\[ \\color{red}{u(r,\\theta) = \\frac{1}{2\\pi} \\int_0^{2\\pi} \\int_0^1 \\ln \\left(\\rho\\frac{\\bar{R}}{R}\\right) \\, f(\\rho,\\phi)\\, \\rho\\,d\\rho\\,d\\phi}\n\\tag{CG}\\label{eq:CG} \\]\nThis is Green’s function solution of Poisson’s equation inside a circle\nNOTES\n1. It is also possible to solve\n\\[ u_{rr} +\\frac{1}{r}u_r + \\frac{1}{r^2} u_{\\theta\\theta}=0, \\;\\;\\;0 &lt; r &lt; 1 \\]\n\\[ u(1,\\theta) = g(\\theta), \\;\\;\\; 0 \\leq \\theta &lt; 2\\pi \\]\nby means of the Green’s function approach\n\nChoose any point \\(\\,(r_0,\\phi_0)\\,\\) inside a circle. \\(\\,\\)Let \\(\\,v(r,\\phi)\\,\\) be any twice continuously differentiable function in a circle such that \\(\\,v=g\\,\\) on the boundary, \\(\\,\\)and \\(\\,v=0\\,\\) near \\(\\,(r_0,\\phi_0)\\). \\(\\,\\)Let \\(w = u -v\\). \\(\\,\\)Then \\(\\,w\\,\\) satisfies\n\\[ \\nabla^2w = -\\nabla^2v, \\;\\;\\;0 &lt; r &lt; 1 \\]\n\\[ w(1,\\theta) = 0, \\;\\;\\; 0 \\leq \\theta &lt; 2\\pi \\]\nTherefore\n\\[ w(r_0,\\theta_0)=-\\int_0^{2\\pi} \\int_0^1 G(r_0,\\theta_0,\\rho,\\phi)\\, \\nabla^2 v\\,\\rho\\, d\\rho d\\phi \\]\nSince \\(\\,v=0\\,\\) near the singularity \\(\\,(r_0,\\theta_0)\\,\\) of \\(\\,G\\), \\(\\,\\)we can apply the divergence theorem to the identity\n\\[ G\\nabla^2 v = G \\nabla^2 v -v\\nabla^2 G = \\nabla \\cdot \\left( G\\nabla v -v\\nabla G\\right)\\]\nto find that\n\\[ \\begin{aligned} w(r_0,\\theta_0)&=-\\int_0^{2\\pi} \\int_0^1 G(r_0,\\theta_0,\\rho,\\phi)\\, \\nabla^2 v\\,\\rho\\, d\\rho d\\phi \\\\\n&\\Downarrow \\;\\;{\\scriptstyle \\text{Divergence Theorem}} \\\\\n&=-\\int_0^{2\\pi} \\left[ G(r_0,\\theta_0,1,\\phi) \\frac{\\partial v}{\\partial r} -v\\frac{\\partial G}{\\partial r}(r_0,\\theta_0,1,\\phi)\\right]\\,d\\phi \\\\\n&\\Downarrow \\;\\;{\\scriptstyle \\text{ On the boundary, } G\\, =\\, 0, \\; v\\, = g} \\\\\n&= \\int_0^{2\\pi} \\frac{\\partial G}{\\partial r}(r_0,\\theta_0,1,\\phi) \\,g(\\phi)\\,d\\phi\n\\end{aligned} \\]\nBut \\(\\,v(r_0,\\theta_0)=0\\), \\(\\,\\)so that \\(\\,w(r_0,\\theta_0)=u(r_0,\\theta_0).\\) \\(\\,\\)Since \\(\\,(r_0,\\theta_0)\\,\\) was any point in a circle, \\(\\,\\)the solution is\n\\[ u(r,\\theta) = \\int_0^{2\\pi} \\frac{\\partial G}{\\partial r}(r,\\theta,1,\\phi)\\,g(\\phi)\\,d\\phi \\]\nwhich, \\(\\,\\)if we compute \\(\\,\\displaystyle\\frac{\\partial G}{\\partial r}\\), \\(\\,\\)gives\n\\[ u(r,\\theta) = \\frac{1}{2\\pi} \\int_0^{2\\pi} \\left[ \\frac{1 - r^2}{1 +r^2 -2r\\cos(\\theta -\\phi)} \\right]\\,g(\\phi)\\,d\\phi\n\\tag{PI}\\label{eq:PI}\\]\nwhich is the Poisson integral formula\n\n2. The solution to the general Dirichlet problem\n\\[ u_{rr} +\\frac{1}{r}u_r + \\frac{1}{r^2} u_{\\theta\\theta}=-f(r,\\theta), \\;\\;\\;0 &lt; r &lt; 1 \\]\n\\[ u(1,\\theta) = g(\\theta), \\;\\;\\; 0 \\leq \\theta &lt; 2\\pi \\]\nwould be the sum of solutions \\(\\eqref{eq:CG}\\) and \\(\\eqref{eq:PI}\\)\n\\(~\\)\n\nPoisson’s Equation Inside a Sphere\n\nWe consider the problem\n\\[\\begin{aligned}\nu_{rr} &+\\frac{2}{r} u_r\n+\\frac{1}{r^2}u_{\\phi\\phi}\n+\\frac{\\cot\\phi}{r^2} u_{\\phi}\n+\\frac{1}{r^2\\sin^2\\phi}u_{\\theta\\theta}\n  = -F(r,\\phi,\\theta), \\;\\; r &lt; R \\\\\n&u(R,\\phi,\\theta) = 0\n\\end{aligned}\\]\nThe solution procedure consists of two steps:\n\nSolve by taking the finite Fourier Transforms\n\\[\\scriptsize\n\\begin{aligned}\n\\mathcal{F}_{ac,nm}\n  \\left[f(r,\\phi,\\theta)\\right]\n   &= \\frac{2n +1}{2\\pi} \\frac{(n -m)!}{(n +m)!}\n   \\int_{0}^{2\\pi} \\int_{0}^\\pi f(r,\\phi,\\theta)\n   \\,P_n^m(\\cos\\phi) \\,\\cos m\\theta\n   \\,\\sin\\phi \\,d\\phi \\,d\\theta \\\\\n\\mathcal{F}_{as,nm}\n  \\left[f(r,\\phi,\\theta)\\right]\n   &= \\frac{2n +1}{2\\pi} \\frac{(n -m)!}{(n +m)!}\n   \\int_{0}^{2\\pi} \\int_{0}^\\pi f(r,\\phi,\\theta)\n   \\,P_n^m(\\cos\\phi) \\,\\sin m\\theta\n   \\,\\sin\\phi \\,d\\phi \\,d\\theta\n\\end{aligned}\\]\nDerive \\(\\,G(r,\\phi,\\theta; \\rho, \\varphi, \\vartheta)\\,\\) in the equivalent integral formula\n\\[\\scriptsize u(r,\\phi,\\theta)\n= \\int_0^{2\\pi} \\int_0^\\pi \\int_0^R\nG(r,\\phi,\\theta; \\rho,\\varphi,\\vartheta)\\,\nF(\\rho, \\varphi,\\vartheta)\n\\,\\rho^2 \\sin\\varphi\n\\,d\\rho \\,d\\varphi \\, d\\vartheta\\]\n\n\n\nSTEP 1 \\(\\,\\)Using the finite Fourier transforms of \\(\\,u\\,\\) and \\(\\,F\\):\n\\[\\begin{aligned}\na_{nm}(r) &= \\mathcal{F}_{ac,nm}\n  \\left[u(r,\\phi,\\theta)\\right]\\\\\nb_{nm}(r) &= \\mathcal{F}_{as,nm}\n  \\left[u(r,\\phi,\\theta)\\right]\\\\\nA_{nm}(r) &= \\mathcal{F}_{ac,nm}\n  \\left[F(r,\\phi,\\theta)\\right]\\\\\nB_{nm}(r) &= \\mathcal{F}_{as,nm}\n  \\left[F(r,\\phi,\\theta)\\right]\n\\end{aligned}\\]\nwe obtain the finite Fourier transforms of the differential equation and integrating by parts leads to the system\n\\[\\begin{aligned}\nr^2 a_{nm}'' +2r a_{nm}' &-n(n +1) a_{nm} = -r^2 A_{nm}, \\;\na_{nm}(R)= 0, \\; \\left|a_{nm}\\right| &lt; \\infty\\\\\nr^2 b_{nm}'' +2r b_{nm}' &-n(n +1) b_{nm} = -r^2 B_{nm}, \\;\nb_{nm}(R)= 0, \\; \\left|b_{nm}\\right| &lt; \\infty\n\\end{aligned}\\]\nThese problems have the solutions\n\\[\\begin{aligned}\na_{nm}(r)\n  &=\\int_0^R G_n(r,\\rho)\\,A_{nm}(\\rho)\n   \\,\\rho^2 \\,d\\rho \\\\\nb_{nm}(r)\n  &=\\int_0^R G_n(r,\\rho)\\,B_{nm}(\\rho)\n   \\,\\rho^2 \\,d\\rho\n\\end{aligned}\n\\tag{SG}\\label{eq:SG}\\]\nwhere (See Section 3.10)\n\\[ \\scriptsize G_n(r,\\rho) =\n\\begin{cases}\n\\displaystyle\\frac{1}{(2n +1)R}\\left(\\frac{r}{R} \\right)^n\\left[\\left(\\frac{\\rho}{R} \\right)^{-(n+1)} -\\left(\\frac{\\rho}{R} \\right)^{n} \\right] & \\text{ for } r \\leq \\rho \\\\\n\\displaystyle\\frac{1}{(2n +1)R}\\left(\\frac{\\rho}{R} \\right)^n\\left[\\left(\\frac{r}{R} \\right)^{-(n+1)} -\\left(\\frac{r}{R} \\right)^{n} \\right]& \\text{ for } r \\geq \\rho\n\\end{cases}\\]\nThus the problem \\(\\eqref{eq:SG}\\) has a solution\n\\[ u(r,\\phi,\\theta) = \\sum_{n=0}^\\infty \\left[ \\frac{1}{2} a_{n0}(r) P_n(\\cos\\phi) +\\sum_{m=1}^n \\left(a_{nm}(r) \\cos m\\theta +b_{nm}(r) \\sin m\\theta \\right) P_n^m(\\cos\\phi) \\right]\n\\tag{SS}\\label{eq:SS}\\]\nSTEP 2 \\(\\,\\)We substitute the definitions of \\(\\,A_{nm}(r)\\,\\) and \\(\\,B_{nm}(r)\\,\\) in \\(\\eqref{eq:SG}\\), \\(\\,\\)and formally interchange integration and summation in \\(\\eqref{eq:SS}\\). \\(\\,\\)This process gives the formal solution\n\\[ u(r,\\phi,\\theta) = \\int_0^{2\\pi} \\int_0^\\pi \\int_0^R G(r,\\phi,\\theta; \\rho,\\varphi,\\vartheta)\\, F(\\rho, \\varphi,\\vartheta) \\,\\rho^2 \\sin\\varphi \\,d\\rho \\,d\\varphi \\, d\\vartheta \\]\nwhere for \\(\\,r &gt; \\rho\\)\n\\[\\scriptsize \\begin{aligned}\nG(r,\\phi,\\theta;\\rho,\\varphi,\\vartheta) &=\\sum_{n=0}^\\infty \\frac{1}{(2n +1)R}\n  \\left(\\frac{\\rho}{R} \\right)^n\n  \\left[ \\left(\\frac{r}{R} \\right)^{-(n +1)} -\\left(\\frac{r}{R} \\right)^{n} \\right]\n  K_n(\\phi,\\theta;\\varphi,\\vartheta)\\\\\n&= \\frac{1}{4\\pi R} \\sum_{n=0}^\\infty \\left(\\frac{\\rho}{R} \\right)^n\n  \\left[ \\left(\\frac{r}{R} \\right)^{-(n +1)} -\\left(\\frac{r}{R} \\right)^{n} \\right] P_n(\\cos\\varphi')\n\\end{aligned}\\]\nFor \\(\\,r &lt; \\rho\\), \\(\\,\\)we must only interchange \\(\\,r\\,\\) and \\(\\,\\rho\\,\\) in this formula\nTo evaluate the series in the above equation, \\(\\,\\)we use the generating function:\n\\[\\begin{aligned}\nG(r,\\phi,\\theta;\\rho,\\varphi,\\vartheta) &=\\frac{1}{4\\pi} \\frac{1}{\\sqrt{r^2+\\rho^2-2r\\rho\\cos\\varphi'}}\n-\\frac{1}{4\\pi} \\frac{1}{\\sqrt{R^2 +\\frac{r^2\\rho^2}{R^2} -2r\\rho\\cos\\varphi'}}\n\\end{aligned}\n\\]\nThis, \\(\\,\\)then, \\(\\,\\)is the Green’s function for \\(\\,r &gt; \\rho\\). \\(\\,\\)It is already symmetric in \\(\\,r\\,\\) and \\(\\,\\rho\\), \\(\\,\\)so that the same formula holds for \\(\\,r &lt; \\rho\\), and even for \\(\\,r=\\rho\\). \\(\\,\\)We note that \\(\\,G=0\\,\\) for \\(\\,r=R\\)\n\\(~\\)",
    "crumbs": [
      "**Second Semester**",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Elliptic Partial Differential Equations</span>"
    ]
  },
  {
    "objectID": "ch_x3_Elliptic_PDEs.html#sec-x3-37",
    "href": "ch_x3_Elliptic_PDEs.html#sec-x3-37",
    "title": "14  Elliptic Partial Differential Equations",
    "section": "14.7 Hydrogen Atom",
    "text": "14.7 Hydrogen Atom\n\nAs the electron circles around the nucleus, \\(\\,\\)the system (the proton and the electron) rotates around the center of gravity. \\(\\,\\)For the rotating system, \\(\\,\\)we can write the reduced mass, \\(\\,\\mu\\), \\(\\,\\)as\n\\[ \\frac{1}{\\mu} = \\frac{1}{m_e} + \\frac{1}{m_p} \\]\n\n\\(~\\)or\n\n\\[ \\mu = \\frac{m_em_p}{m_e +m_p} \\]\nSince the mass of the electron is so much less than that of the proton, \\(\\,m_e +m_p \\approx m_p\\,\\) and \\(\\,\\mu = m_e\\). \\(\\,\\)Assuming that the nucleus is stationary and that the electron does all of the moving (known as the Born-Oppenheimer approximation) leads to the same result\nThe Hamiltonian can be written as the sum of the kinetic and potential energies\n\\[H = T + V\\]\nin which \\(\\,T = p^2/2\\mu\\,\\) and the potential energy \\(\\,V\\) \\(\\,\\)for the interaction of the electron with the proton is \\(-e^2/r\\). \\(\\,\\)Therefore, \\(\\,\\)the Hamiltonian operator \\(\\,H\\,\\) is\n\\[ H=-\\frac{\\hbar^2}{2\\mu} \\nabla^2 -\\frac{e^2}{r} \\]\nTherefore, \\(\\,\\)the time-independent wave function \\(\\psi\\) gives\n\\[\\begin{aligned}\nH \\psi &= E \\psi \\\\\n&\\Downarrow \\\\\n\\nabla^2 \\psi +\\color{red}{\\frac{2\\mu}{\\hbar^2}} &\\color{red}{\\left( \\frac{e^2}{r} +E \\right) \\psi} =0 \\\\\n&\\Downarrow \\\\\n\\frac{1}{r^2}\\left( r^2 \\psi_r \\right)_r +\\frac{1}{r^2\\sin\\phi} \\left( \\sin\\phi\\, \\psi_\\phi \\right)_\\phi\n  &+\\frac{1}{r^2\\sin^2\\phi} \\psi_{\\theta\\theta} +\\frac{2\\mu}{\\hbar^2} \\left( \\frac{e^2}{r} +E \\right) \\psi =0\n\\end{aligned}\\]\nWe assume that \\(\\,\\psi(r,\\phi,\\theta)=R(r)\\,\\Phi(\\phi)\\,\\Theta(\\theta)\\,\\) and make the substitution. \\(\\,\\)For simplicity, \\(\\,\\)we will write the partial solutions as \\(\\,R\\), \\(\\,\\Phi\\), and \\(\\,\\Theta\\,\\) without showing the functionality. \\(\\,\\)We now divide both sides by \\(\\,R\\,\\Phi\\,\\Theta\\,\\) and multiply by \\(\\,r^2\\sin^2\\phi\\):\n\\[\\frac{\\sin^2\\phi}{R}\\left( r^2 R_r \\right)_r +\\frac{\\sin\\phi}{\\Phi} \\left( \\sin\\phi\\, \\Phi_\\phi \\right)_\\phi\n+\\frac{1}{\\Theta} \\Theta_{\\theta\\theta} +\\frac{2\\mu r^2\\sin^2\\phi}{\\hbar^2} \\left( \\frac{e^2}{r} +E \\right) =0 \\]\nInspection shows that, \\(\\,\\)of the four terms on the left side of the equation, \\(\\,\\)there is no functional dependence on \\(\\,\\theta\\,\\) except in the third term. \\(\\,\\)Therefore, \\(\\,\\)with respect to the other variables, \\(\\,\\)the third term can be treated as a constant:\n\\[\\begin{aligned}\n  \\scriptsize\n  -\\frac{\\sin^2\\phi}{R}\\left( r^2 R_r \\right)_r -\\frac{\\sin\\phi}{\\Phi} \\left( \\sin\\phi\\, \\Phi_\\phi \\right)_\\phi\n    &\\scriptsize\n    -\\frac{2\\mu r^2\\sin^2\\phi}{\\hbar^2} \\left( \\frac{e^2}{r} +E \\right)\n    = \\frac{1}{\\Theta} \\Theta_{\\theta\\theta} = -\\lambda &lt;0 \\\\\n  &\\Downarrow \\\\\n  \\Theta_{\\theta\\theta} &+\\lambda \\Theta = 0, \\;\\;\\Theta(0)=\\Theta(2\\pi)\\\\\n    &\\Downarrow \\;\\;e^{\\pm i2\\pi \\sqrt{\\lambda}} =1\\\\\n    \\lambda_m = m^2, &\\;m=0,\\pm 1, \\pm 2,\\cdots \\\\\n    \\Theta_m(\\theta) &= \\frac{1}{\\sqrt{2\\pi}} e^{im\\theta}\n\\end{aligned}\\]\nActually, \\(\\,\\)there are two solutions: \\(\\,e^{im\\theta}\\,\\) and \\(\\,e^{-im\\theta}\\), \\(\\,\\)but we cover the latter by allowing \\(\\,m\\,\\) to run negative\nWe can now write the wave equation as\n\\[\\scriptsize \\frac{\\sin^2\\phi}{R}\\left( r^2 R_r \\right)_r +\\frac{\\sin\\phi}{\\Phi} \\left( \\sin\\phi\\, \\Phi_\\phi \\right)_\\phi\n  -m^2 +\\frac{2\\mu r^2\\sin^2\\phi}{\\hbar^2} \\left( \\frac{e^2}{r} +E \\right)\n  = 0\\]\nIf we divide the equation by \\(\\sin^2\\phi\\) and rearrange, \\(~\\)we obtain\n\\[\\scriptsize \\begin{aligned}\n\\frac{1}{R}\\left( r^2 R_r \\right)_r +\\frac{2\\mu r^2}{\\hbar^2} \\left( \\frac{e^2}{r} +E \\right) &+\\frac{1}{\\Phi\\sin\\phi}\n  \\left( \\sin\\phi\\, \\Phi_\\phi \\right)_\\phi -\\frac{m^2}{\\sin^2\\phi} = 0 \\\\\n&\\Downarrow \\\\\n  \\frac{1}{\\Phi\\sin\\phi} \\left( \\sin\\phi\\, \\Phi_\\phi \\right)_\\phi &-\\frac{m^2}{\\sin^2\\phi} = -\\beta \\\\\n  \\frac{1}{R}\\left( r^2 R_r \\right)_r +\\frac{2\\mu r^2}{\\hbar^2} &\\left( \\frac{e^2}{r} +E \\right) = \\beta \\\\[2pt]\n&\\Downarrow \\;\\;\n\\frac{1}{\\sin\\phi} \\left( \\sin\\phi\\, \\Phi_\\phi \\right)_\\phi +\\left( \\beta-\\frac{m^2}{\\sin^2\\phi}\\right)\\Phi = 0 \\\\[2pt]\n\\frac{1}{r^2}\\left( r^2 R_r \\right)_r +\\color{red}{\\frac{2\\mu}{\\hbar^2}} &\\color{red}{\\left( \\frac{e^2}{r} +E \\right)R -\\frac{\\beta}{r^2}R}=0\n\\end{aligned}\\]\nWe will now turn our attention to the equation involving \\(\\,\\phi\\). \\(\\,\\)The standard method for solving this equation is to make the transformation \\(\\,x=\\cos\\phi\\)\n\\[\\scriptsize\\begin{aligned}\n  \\frac{1}{\\sin\\phi} \\frac{d}{d\\phi} \\left( \\frac{\\sin^2\\phi}{\\sin\\phi}\\, \\frac{d\\Phi}{d\\phi} \\right) &+\\left( \\beta-\\frac{m^2}{\\sin^2\\phi}\\right)\\Phi = 0 \\\\\n  &\\Downarrow\\;\\;x=\\cos\\phi,\\;dx=-\\sin\\phi\\,d\\phi \\\\\n  \\frac{d}{dx} \\left[ \\left(1 -x^2\\right) \\frac{d\\Phi}{dx} \\right] &+ \\left( \\beta-\\frac{m^2}{1 -x^2}\\right)\\Phi = 0 \\\\\n  &\\Downarrow \\\\\n  \\left(1 -x^2\\right) \\frac{d^2\\Phi}{dx^2} -2x \\frac{d\\Phi}{dx} &+ \\left( \\beta-\\frac{m^2}{1 -x^2}\\right)\\Phi = 0\n\\end{aligned}\\]\nThis is the associated Legendre equation. \\(\\,\\)If you start with the Legendre equation and differentiate it \\(\\,|m|\\,\\) times, \\(\\,\\)you end up with a new differential equation\n\\[\\scriptsize\\begin{aligned}\n  \\left(1 -x^2\\right)P_l''(x) -\\,&\\,2xP_l'(x) +l(l+1)P_l(x) = 0,\\;\\;\\; l=0,1,2,\\cdots \\\\[5pt]\n  &\\Downarrow\\;\\; {\\scriptstyle \\text{differentiate it } |m| \\text{ times}, \\;\\;y = \\left(\\frac{d}{dx}\\right)^{|m|} P_l(x),\\; \\; |m|\\, \\leq\\, l} \\\\[5pt]\n  \\left(1 -x^2\\right)y'' -2 |m| x y'&-|m|(|m| -1)y -2x y' -2|m|y +l(l +1)y = 0\\\\\n  &\\Downarrow\\\\\n  \\left(1 -x^2\\right)y'' -2x(|m| +1)&y' +\\left[l(l +1) -|m|(|m| +1)\\right] y = 0\n\\end{aligned}\\]\nAnd the substitution\n\\[\\scriptsize\\begin{aligned}\ny&= \\left(1 -x^2\\right)^{-|m|/2} \\Phi \\\\\ny'&= \\left(1 -x^2\\right)^{-|m|/2} \\left[\\frac{d\\Phi}{dx} +\\frac{|m|x}{1 -x^2}\\Phi \\right] \\\\\ny''&= \\left(1 -x^2\\right)^{-|m|/2} \\left[\\frac{d^2\\Phi}{dx^2} +\\frac{2|m|x}{1 -x^2}\\frac{d\\Phi}{dx}\n  +\\frac{|m|+|m|x^2 +|m|^2x^2}{(1-x^2)^2} \\Phi \\right]\n\\end{aligned}\\]\nputs the equation into standard form with respect to \\(\\,\\Phi\\). \\(\\,\\)Then the standard solutions of the associated Legendre equation were found to be\n\\[\\scriptsize \\Phi(x) = P_l^{|m|}(x) = \\left(1 -x^2\\right)^{|m|/2} \\left(\\frac{d}{dx}\\right)^{|m|} P_l(x) = \\left(1 -x^2\\right)^{|m|/2} \\frac{1}{2^l l!} \\left(\\frac{d}{dx}\\right)^{|m|+l} \\left(x^2 -1\\right)^l \\]\nand \\(\\,P_l^{|m|}(x)\\,\\) are orthogonal with the weight function \\(1\\)\n\\[ \\scriptsize \\int_{-1}^1 P_l^{|m|}(x)\\, P_{l'}^{|m|}(x)\\,dx = \\frac{2}{2l +1} \\frac{(l +|m|)!}{(l -|m|)!} \\,\\delta_{ll'} \\]\nNotice that \\(\\,l\\,\\) must be a nonnegative integer. \\(\\,\\)For any given \\(\\,l,\\) \\(\\,\\)then, \\(\\,\\)there are \\(\\,(2l +1)\\,\\) possible values of \\(\\,m\\):\n\\[l=0,1,2,\\cdots; \\;\\;m =-l, -1 +1, \\cdots, -1, 0, 1, \\cdots, l -1, l \\]\nFor historical reasons, \\(\\,l\\,\\) is called the azimuthal quantum number, \\(\\,\\)and \\(\\,m\\,\\) the magnetic quantum number\nThe normalized angular wave functions are called spherical harmonics:\n\\[ \\color{red}{Y_l^{m}(\\phi,\\theta)=\\epsilon \\sqrt{\\frac{(2l +1)}{4\\pi} \\frac{(l -|m|)!}{(l +|m|)!}} P_l^{|m|}(\\cos\\phi) e^{im\\theta}} \\]\nwhere \\(\\,\\epsilon=(-1)^m\\,\\) for \\(\\,m \\geq 0\\,\\) and \\(\\,\\epsilon=1\\,\\) for \\(\\,m &lt; 0\\)\nNotice that the angular part of the wave function, \\(\\,Y(\\phi,\\theta)\\), \\(\\,\\)is the same for all spherically symmetric potentials; \\(\\,\\)the actual shape of the potential \\(\\,V(r)\\) \\(\\,\\)affects only the radial part of the wave function, \\(\\,R(r)\\), \\(\\,\\)which is determined by\n\\[ \\frac{1}{r^2}\\left( r^2 R_r \\right)_r +\\frac{2\\mu}{\\hbar^2} \\left( \\frac{e^2}{r} +E \\right)R -\\frac{l(l +1)}{r^2}R=0\\]\nThis equation simplifies if we change variables:\n\\[\\scriptsize \\begin{aligned}\n\\text{Let } u(r) &= rR(r) \\\\\n&\\Downarrow \\;{\\tiny\\text{so that}} \\\\\nR = \\frac{u}{r}, \\;\\; \\frac{dR}{dr} = \\frac{1}{r^2}\n  &\\left( r\\frac{du}{dr} -u \\right),\n  \\;\\; \\frac{d}{dr}\n  \\left[ r^2 \\frac{dR}{dr} \\right]\n  = r\\frac{d^2 u}{dr^2} \\\\\n&\\Downarrow \\;\n{\\tiny \\kappa = \\frac{\\sqrt{-2\\mu E}}{\\hbar}\n  \\;\\;\\text{ and }\n  \\;\\rho_0 = \\frac{2\\mu e^2}{\\hbar^2 \\kappa}} \\\\\n  \\frac{d^2u}{dr^2}\n  =\\left[-\\frac{\\rho_0 \\kappa}{ r}\n  +\\kappa^2 +\\frac{l(l +1)}{r^2} \\right] u \\;\\;\n    &\\overset{\\rho=\\kappa r}{\\rightarrow}\n    \\;\\;\\frac{d^2u}{d\\rho^2}\n    =\\left[1 -\\frac{\\rho_0}{\\rho}\\right.\n      +\\left.\\frac{l(l +1)}{\\rho^2} \\right] u\n\\end{aligned}\\]\nThe next step is to peel off the asymptotic behavior, \\(\\,\\)introducing the new function \\(\\,v(\\rho)\\)\n\\[u(\\rho)=\\rho^{l +1} e^{-\\rho} v(\\rho)\\]\nin the hope that \\(\\,v(\\rho)\\,\\) will turn out to be simpler than \\(\\,u(\\rho)\\). In terms of \\(\\,v(\\rho)\\), \\(\\,\\)then, \\(\\,\\)the radial equation reads\n\\[ \\rho \\frac{d^2v}{d\\rho^2} +2(l +1 -\\rho) \\frac{dv}{d\\rho} +\\left[ \\rho_0 -2(l +1) \\right]v=0 \\]\nFinally, \\(\\,\\)we assume the solution can be expressed as a power series in \\(\\,\\rho\\):\n\\[ v(\\rho) = \\sum_{k = 0}^\\infty c_k \\rho^k \\]\nInserting the power series into the equation and equating the coefficients of like powers yields\n\\[ c_{k +1} = \\frac{2(k +l +1) -\\rho_0}{(k +1)(k +2l +2)} c_k\\]\nNow let’s see what the coefficients look like for large \\(\\,k\\). \\(\\,\\)In this regime the recursion formula says\n\\[c_{k +1} \\approx \\frac{2}{k +1} c_k\\]\nSuppose for a moment that this were exact. \\(\\,\\)Then\n\\[c_k = \\frac{2^k}{k!} c_0\\]\nso \\[v(\\rho) = c_0 \\sum_{k=0}^\\infty \\frac{2^k}{k!} \\rho^k = c_0 e^{2\\rho} \\;\\;\n\\Rightarrow \\;\\; u(\\rho)=c_0\\rho^{l +1} e^\\rho \\]\nwhich blows up at large \\(\\,\\rho\\). \\(\\,\\)The positive exponential is precisely the asymptotic behavior we didn’t want\nThere is only one way out of this dilemma: The series must terminate. \\(\\,\\)There must occur some maximal integer, \\(\\,k_{max}\\), \\(\\,\\)such that\n\\[ c_{k_{max}\\, +1} = 0 \\]\nand beyond which all coefficient vanish automatically. \\(\\,\\)Evidently\n\\[ 2(k_{max} +l +1) -\\rho_0 = 0 \\]\nDefining the so-called principle quantum number\n\\[ n = k_{max} +l +1 \\]\nwe have\n\\[ \\rho_0 = 2n \\]\nBut \\(\\,\\rho_0\\,\\) determine \\(\\,E\\)\n\\[ E = -\\frac{\\hbar^2 \\kappa^2}{2 \\mu} = -\\frac{2\\mu e^4}{\\hbar^2 \\rho_0^2} \\]\nso the allowed energies are\n\\[ E_n = - \\left[ \\frac{\\mu e^4}{2\\hbar^2} \\right] \\frac{1}{n^2}, \\;\\;\\;n=1,2,3,\\cdots \\]\nThis is the famous Bohr formula. \\(\\,\\)The ground state (that is, \\(\\,\\)the state of lowest energy) is the case \\(\\,n=1\\); \\(\\,\\)putting in the accepted values for the physical constants, \\(\\,\\)we get\n\\[ E_1 = - \\left[ \\frac{\\mu e^4}{2\\hbar^2} \\right] =-13.6\\,eV \\]\nand we find that\n\\[ \\kappa = \\frac{\\mu e^2}{\\hbar^2} \\frac{1}{n} = \\frac{1}{a_0 n},\\;\\;\\rho = \\frac{r}{a_0 n} \\]\nwhere\n\\[ a_0 = \\frac{\\hbar^2}{\\mu e^2} = 0.529 \\times 10^{-10} m \\]\nis the so-called Bohr radius\nAnyway the radial equation gives\n\\[ \\color{red}{R_{nl}(r) = \\frac{1}{r} \\rho^{l +1} e^{-\\rho} v(\\rho)} \\]\nin which \\(\\,v(\\rho)\\,\\) is a polynomial of degree \\(\\,k_{max} = n -l -1\\,\\) in \\(\\,\\rho,\\) \\(\\,\\)whose coefficients are determined by the recursion formula\n\\[ c_{k +1} = \\frac{2(k +l +1 -n)}{(k +1)(k +2l +2)} c_k \\]\nFor arbitrary \\(\\,n\\), \\(\\,\\)the possible values of \\(\\,l\\,\\) are\n\\[ l=0,1,2,\\cdots,n-1 \\]\nand for each \\(\\,l\\), \\(\\,\\)there are \\(\\,(2l +1)\\,\\) possible values of \\(\\,m\\), \\(\\,\\)so the total degeneracy of the energy level \\(\\,E_n\\,\\) is\n\\[ d(n)=\\sum_{l=0}^{n-1}(2l +1)=n^2 \\]\nThe polynomial \\(\\,v(\\rho)\\,\\) is a function well known to applied mathematicians; \\(\\,\\)it can be written as\n\\[ \\color{red}{v(\\rho) = L_{n -l -1}^{2l +1}(2\\rho)} \\]\nwhere\n\\[ L_{q-p}^p(x)=(-1)^p \\left( \\frac{d}{dx} \\right)^p L_q(x) =\\sum_{k=0}^{q-p} \\frac{(-1)^k}{k!} \\binom{q}{q-p-k} x^k \\]\nis a generalized Laguerre polynomial, \\(\\,\\)and\n\\[ L_q(x) =\\frac{e^x}{q!} \\left( \\frac{d}{dx} \\right)^q (e^{-x} x^q) = \\sum_{k=0}^q \\frac{(-1)^k}{k!} \\binom{q}{k} x^k  \\]\nis the \\(\\,q\\)-th Laguerre polynomial\nThe radial wave functions are given by\n\\[ \\color{red}{R_{nl}(r) = \\sqrt{\\left( \\frac{2}{na_0}\\right)^3 \\frac{(n -l -1)!}{2n (n +l)!}} e^{-r/na_0}\n\\left( \\frac{2r}{na_0} \\right)^l \\, L_{n -l -1}^{2l +1} \\left( \\frac{2r}{na_0} \\right)} \\]\nThe combinatorial factor in front assures that \\(\\,R_{nl}(r)\\,\\) are normalized with respect to an integration over \\(\\,r\\):\n\\[ \\int_0^\\infty R^2_{nl}(r) \\, r^2 dr = 1\\]\nNote that the volume element here is \\(\\,r^2 dr\\), which is the \\(\\,r\\,\\) part of the spherical coordinate volume element \\(\\,r^2 \\sin \\phi \\,dr \\,d\\phi \\,d\\theta\\)\nThe complete hydrogen atomic wave functions are\n\\[ \\color{red}{\\psi_{nlm}(r, \\phi, \\theta) = R_{nl}(r)\\,Y_l^m(\\phi,\\theta)} \\]\nThe normalization condition is\n\\[ \\scriptsize\\int_0^{2\\pi} \\int_0^{\\pi} \\int_0^\\infty \\psi_{nlm}^2(r,\\phi,\\theta) \\,r^2 \\sin\\phi \\,dr \\,d\\phi \\, d\\theta = 1 \\]\nThe functions \\(\\,\\psi_{nml}\\,\\) must also be orthogonal. \\(\\,\\)This orthonomality relationship is given by\n\\[ \\scriptsize\\int_0^{2\\pi} \\int_0^{\\pi} \\int_0^\\infty \\psi^*_{nlm}(r',\\phi',\\theta')\\,\\psi_{nlm}(r,\\phi,\\theta) \\,r^2 \\sin\\phi \\,dr \\,d\\phi \\, d\\theta = \\delta_{nn'} \\delta_{ll'}\\delta_{mm'}\\]\n\n\\(~\\)",
    "crumbs": [
      "**Second Semester**",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Elliptic Partial Differential Equations</span>"
    ]
  },
  {
    "objectID": "ch_x3_Elliptic_PDEs.html#worked-exercises",
    "href": "ch_x3_Elliptic_PDEs.html#worked-exercises",
    "title": "14  Elliptic Partial Differential Equations",
    "section": "Worked Exercises",
    "text": "Worked Exercises\n1. \\(~\\) Solve the exterior Dirichlet problem in a circle: \\[\n\\begin{align*}\n\\nabla^2 u &= 0, \\;\\;1 &lt; r &lt; \\infty \\\\\n&u(1,\\theta) =  \\sin\\theta + \\cos 3\\theta\n\\end{align*}\n\\]\nSolution\nStep 1: \\(~\\) General Solution in Polar Coordinates\nThe general solution to Laplace’s equation in polar coordinates for \\(r &gt; 1\\) (the exterior region) is:\n\\[\\scriptsize u(r, \\theta) = a_0 + b_0 \\ln r + \\sum_{n=1}^\\infty \\left( a_n r^n + b_n r^{-n} \\right) \\cos n\\theta + \\sum_{n=1}^\\infty \\left( c_n r^n + d_n r^{-n} \\right) \\sin n\\theta\\]\nStep 2: \\(~\\) Apply the decay condition \\(u(r,\\theta) \\to 0\\) as \\(r \\to \\infty\\)\nThis requires:\n\n\\(a_0 = 0\\)\n\\(b_0 = 0\\)\n\\(a_n = 0\\), \\(c_n = 0\\) for all \\(n \\ge 1\\)\n\nHence, the solution becomes:\n\\[u(r, \\theta) = \\sum_{n=1}^\\infty b_n r^{-n} \\cos n\\theta + \\sum_{n=1}^\\infty d_n r^{-n} \\sin n\\theta\\]\nStep 3: \\(~\\) Use Boundary Condition \\(u(1, \\theta) = \\sin\\theta + \\cos 3\\theta\\)\nAt \\(r = 1\\), we get:\n\\[u(1, \\theta) = \\sum_{n=1}^\\infty b_n \\cos n\\theta + \\sum_{n=1}^\\infty d_n \\sin n\\theta = \\sin\\theta + \\cos 3\\theta\\]\nSo:\n\n\\(b_3 = 1\\), \\(d_1 = 1\\)\nAll other \\(b_n\\), \\(d_n\\) are \\(0\\)\n\nFinal Solution\n\\[\nu(r, \\theta) = \\frac{\\cos 3\\theta}{r^3} + \\frac{\\sin\\theta}{r}\n\\]\nThis satisfies Laplace’s equation in \\(r &gt; 1\\), the boundary condition at \\(r = 1\\), and decays at infinity\n\\(~\\)\n2. \\(~\\) Solve the interior Dirichlet problem in a sphere \\[\n\\begin{align*}\n\\nabla^2 u &= 0, \\;r &lt;1\\\\\n&u(1,\\phi) = 1 + \\cos\\phi\n\\end{align*}\n\\]\nSolution\nWe are solving the interior Dirichlet problem for Laplace’s equation inside the unit sphere. The boundary condition is given only in terms of \\(\\phi\\), which suggests axial symmetry (i.e., the function does not depend on the azimuthal angle \\(\\theta\\)). Hence, we can solve the axisymmetric Laplace equation in spherical coordinates\nStep 1: \\(~\\) Axisymmetric Laplace Equation\nIn spherical coordinates with axial symmetry (no dependence), Laplace’s equation becomes:\n\\[\\frac{1}{r^2} \\frac{\\partial}{\\partial r} \\left( r^2 \\frac{\\partial u}{\\partial r} \\right) +\\frac{1}{r^2 \\sin \\phi} \\frac{\\partial}{\\partial \\phi} \\left( \\sin \\phi \\frac{\\partial u}{\\partial \\phi} \\right) = 0\\]\nStep 2: \\(~\\) Separation of Variables\nAssume \\(u(r, \\phi) = R(r) P(\\phi)\\). Substituting and separating variables gives:\n\\[\\frac{1}{R} \\frac{d}{dr} \\left( r^2 \\frac{dR}{dr} \\right)\n    +\\frac{1}{P \\sin \\phi} \\frac{d}{d\\phi} \\left( \\sin \\phi \\frac{dP}{d\\phi} \\right) = 0\\]\nSet both parts equal to \\(\\ell(\\ell+1)\\). Then:\n\n\\(R(r) = A_\\ell r^\\ell\\) (we discard \\(r^{-(\\ell+1)}\\) since \\(u\\) must be finite at \\(r = 0\\)),\n\\(P(\\phi)\\) satisfies Legendre’s equation:\n\n\\[\\frac{1}{\\sin \\phi} \\frac{d}{d\\phi} \\left( \\sin \\phi \\frac{dP}{d\\phi} \\right) + \\ell(\\ell+1) P = 0 \\Rightarrow P(\\phi) = P_\\ell(\\cos\\phi)\\]\nStep 3: \\(~\\) General Solution\nThe general solution is:\n\\[u(r, \\phi) = \\sum_{\\ell=0}^\\infty A_\\ell r^\\ell P_\\ell(\\cos\\phi)\\]\nStep 4: \\(~\\) Apply Boundary Condition\nAt \\(r = 1\\), the boundary condition is:\n\\[u(1, \\phi) = \\sum_{\\ell=0}^\\infty A_\\ell P_\\ell(\\cos\\phi) = 1 + \\cos\\phi\\]\nWe expand \\(1 + \\cos\\phi\\) in terms of Legendre polynomials\n\n\\(P_0(\\cos\\phi) = 1\\)\n\\(P_1(\\cos\\phi) = \\cos\\phi\\)\n\nSo:\n\\[1 + \\cos\\phi = P_0(\\cos\\phi) + P_1(\\cos\\phi)\n\\Rightarrow A_0 = 1, \\; A_1 = 1, \\; A_\\ell = 0 \\text{ for } \\ell \\geq 2\\]\nFinal Solution\n\\[u(r, \\phi) = P_0(\\cos\\phi) + r P_1(\\cos\\phi)\n= 1 + r \\cos\\phi\\]\nThis satisfies Laplace’s equation inside the unit ball and matches the boundary condition\n\\(~\\)\n3. \\(~\\) Solve the exterior Dirichlet problem in a sphere \\[\n\\begin{align*}\n\\nabla^2 u &= 0, \\;1 &lt; r \\\\\n&u(1,\\phi) = 1 + \\cos2\\phi\n\\end{align*}\n\\]\nSolution\nWe are solving the exterior Dirichlet problem for Laplace’s equation in a spherically symmetric setting. The boundary condition depends only on the polar angle \\(\\phi\\), so we assume axial symmetry (i.e., no \\(\\theta\\)-dependence)\nStep 1: \\(~\\) General Axisymmetric Solution to Laplace’s Equation (Exterior)\nThe axisymmetric solution to Laplace’s equation in spherical coordinates (no \\(\\theta\\) dependence) is:\n\\[u(r, \\phi) = \\sum_{\\ell=0}^{\\infty} \\left( A_\\ell r^{-\\ell-1} + B_\\ell r^\\ell \\right) P_\\ell(\\cos\\phi)\\]\nTo ensure the solution decays at infinity:\n\nSet \\(B_\\ell = 0\\)\n\nSo the solution becomes:\n\\[u(r, \\phi) = \\sum_{\\ell=0}^{\\infty} A_\\ell r^{-\\ell - 1} P_\\ell(\\cos\\phi)\\]\nStep 2: \\(~\\) Apply Boundary Condition\nAt \\(r = 1\\), the boundary condition is:\n\\[u(1, \\phi) = \\sum_{\\ell=0}^\\infty A_\\ell P_\\ell(\\cos\\phi) = 1 + \\cos 2\\phi\\]\nRecall:\n\n\\(P_0(\\cos\\phi) = 1\\)\n\\(P_2(\\cos\\phi) = \\frac{1}{2}(3\\cos^2\\phi - 1) = \\cos 2\\phi\\) (identity on the sphere)\n\nSo this corresponds to:\n\\[A_0 = 1, \\quad A_2 = 1, \\quad A_\\ell = 0 \\text{ for } \\ell \\ne 0,2\\]\nFinal Solution\n\\[u(r, \\phi) = \\frac{1}{r} P_0(\\cos\\phi) + \\frac{1}{r^3} P_2(\\cos\\phi)\n= \\frac{1}{r} + \\frac{1}{r^3} \\cos 2\\phi\\]\nThis is harmonic for \\(r &gt; 1\\), satisfies the boundary condition at \\(r = 1\\), and decays as \\(r \\to \\infty\\)\n\\(~\\)\n4. \\(~\\) Solve the Dirichlet problem in a circle:\n\\[\n\\begin{aligned}\n\\nabla^2 u &= 0, \\;\\;1 &lt; r &lt; 2 \\\\\nu(1,\\theta) &=  \\frac{1}{2} + \\cos\\theta\\\\\nu(2,\\theta) &= 1 + \\sin\\theta\n\\end{aligned}\n\\]\nSolution\nWe solve this using separation of variables in polar coordinates \\((r, \\theta)\\), where the Laplacian is:\n\\[\\nabla^2 u = \\frac{\\partial^2 u}{\\partial r^2} + \\frac{1}{r} \\frac{\\partial u}{\\partial r} + \\frac{1}{r^2} \\frac{\\partial^2 u}{\\partial \\theta^2}\\]\n🧩 General Solution to Laplace’s Equation in Annulus\nWe use the standard separation-of-variables solution:\n\\[\\scriptsize u(r, \\theta) = a_0 + b_0 \\log r + \\sum_{n=1}^{\\infty} \\left( A_n r^n + B_n r^{-n} \\right)\\cos(n\\theta) + \\left( C_n r^n + D_n r^{-n} \\right)\\sin(n\\theta)\\]\nSince \\(r \\in (1, 2)\\), both \\(r^n\\) and \\(r^{-n}\\) are bounded, so we retain all terms\nStep 1: \\(~\\) Fourier Series of Boundary Data\nWe write the boundary functions as Fourier series\n\\[u(r,\\theta) = a_0 + b_0 \\log r + (A_1 r + B_1 r^{-1}) \\cos\\theta + (C_1 r + D_1 r^{-1}) \\sin\\theta\\]\nWe’ll determine the constants \\(a_0\\), \\(b_0\\), \\(A_1\\), \\(B_1\\), \\(C_1\\), \\(D_1\\) by applying the boundary conditions at \\(r = 1\\) and \\(r = 2\\)\nAt \\(r = 1\\):\n\\[u(1, \\theta) = a_0 + (A_1 + B_1)\\cos\\theta + (C_1 + D_1)\\sin\\theta\n= \\frac{1}{2} + \\cos\\theta\\]\nthis gives:\n\n\\(a_0 = \\frac{1}{2}\\)\n\\(A_1 + B_1 = 1\\)\n\\(C_1 + D_1 = 0\\)\n\nAt \\(r = 2\\):\n\\[u(2, \\theta) = a_0 + b_0 \\log 2 + (2A_1 + \\frac{1}{2}B_1)\\cos\\theta + (2C_1 + \\frac{1}{2}D_1)\\sin\\theta = 1 + \\sin\\theta\\]\nMatch terms:\n\n\\(a_0 + b_0 \\log 2 = 1\\)\n\\(2A_1 + \\frac{1}{2}B_1 = 0\\)\n\\(2C_1 + \\frac{1}{2}D_1 = 1\\)\n\nNow solve the system\nStep 2: \\(~\\) Solve the System\nWe already have:\n\n\\(a_0 = \\frac{1}{2}\\)\n\\(a_0 + b_0 \\log 2 = 1 \\Rightarrow \\frac{1}{2} + b_0 \\log 2 = 1 \\Rightarrow b_0 = \\frac{1}{2 \\log 2}\\)\n\nSolve for \\(A_1\\), \\(B_1\\):\n\n\\(A_1 + B_1 = 1\\)\n\\(2A_1 + \\frac{1}{2}B_1 = 0\\)\n\nMultiply first by 2 and subtract by second:\n\\[(2A_1 + 2B_1) - (2A_1 + \\tfrac{1}{2}B_1) = 2 - 0 \\Rightarrow \\tfrac{3}{2}B_1 = 2 \\Rightarrow B_1 = \\frac{4}{3},\\; A_1 = 1 - \\frac{4}{3} = -\\frac{1}{3}\\]\nSolve for \\(C_1\\), \\(D_1\\):\n\n\\(C_1 + D_1 = 0\\)\n\\(2C_1 + \\frac{1}{2}D_1 = 1\\)\n\nSubstitute \\(D_1 = -C_1\\) into second:\n\\[2C_1 - \\frac{1}{2}C_1 = 1 \\Rightarrow \\frac{3}{2}C_1 = 1 \\Rightarrow C_1 = \\frac{2}{3},\\; D_1 = -\\frac{2}{3}\\]\nFinal Solution\n\\[\\boxed{\n\\begin{aligned}\nu(r, \\theta) &= \\frac{1}{2} + \\frac{1}{2 \\log 2} \\log r + \\left( -\\frac{1}{3}r + \\frac{4}{3}r^{-1} \\right)\\cos \\theta \\\\\n&\\quad + \\left( \\frac{2}{3}r - \\frac{2}{3}r^{-1} \\right)\\sin \\theta\n\\end{aligned}\n}\\]\nThis is the harmonic function in the annulus \\(1 &lt; r &lt; 2\\) satisfying the given Dirichlet boundary conditions",
    "crumbs": [
      "**Second Semester**",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Elliptic Partial Differential Equations</span>"
    ]
  },
  {
    "objectID": "x_numpy_vectors_matrices_and_multidimensional_arrays.html",
    "href": "x_numpy_vectors_matrices_and_multidimensional_arrays.html",
    "title": "Appendix A — Numpy: Vectors, Matrices, and Multidimensional Arrays",
    "section": "",
    "text": "A.1 Importing numpy\n\\(~\\)\n\\(~\\)\nimport numpy as np\n\nprint(\"numpy: \", np.__version__)\n\nnumpy:  2.3.1",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>Numpy: Vectors, Matrices, and Multidimensional Arrays</span>"
    ]
  },
  {
    "objectID": "x_numpy_vectors_matrices_and_multidimensional_arrays.html#importing-numpy",
    "href": "x_numpy_vectors_matrices_and_multidimensional_arrays.html#importing-numpy",
    "title": "Appendix A — Numpy: Vectors, Matrices, and Multidimensional Arrays",
    "section": "",
    "text": "_show_array.py\nshow_array()\nshow_array_aggregation()",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>Numpy: Vectors, Matrices, and Multidimensional Arrays</span>"
    ]
  },
  {
    "objectID": "x_numpy_vectors_matrices_and_multidimensional_arrays.html#the-numpy-array-object",
    "href": "x_numpy_vectors_matrices_and_multidimensional_arrays.html#the-numpy-array-object",
    "title": "Appendix A — Numpy: Vectors, Matrices, and Multidimensional Arrays",
    "section": "A.2 The Numpy array object",
    "text": "A.2 The Numpy array object\n\ndata = np.array([[1, 2], [3, 4], [5, 6]])\ndata\n\narray([[1, 2],\n       [3, 4],\n       [5, 6]])\n\n\n\ntype(data)\n\nnumpy.ndarray\n\n\n\ndata.ndim\n\n2\n\n\n\ndata.shape\n\n(3, 2)\n\n\n\ndata.size\n\n6\n\n\n\ndata.dtype\n\ndtype('int64')\n\n\n\ndata.nbytes\n\n48",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>Numpy: Vectors, Matrices, and Multidimensional Arrays</span>"
    ]
  },
  {
    "objectID": "x_numpy_vectors_matrices_and_multidimensional_arrays.html#data-types",
    "href": "x_numpy_vectors_matrices_and_multidimensional_arrays.html#data-types",
    "title": "Appendix A — Numpy: Vectors, Matrices, and Multidimensional Arrays",
    "section": "A.3 Data types",
    "text": "A.3 Data types\n\nd0 = np.array([1, 2, 3], dtype=int)\nd0\n\narray([1, 2, 3])\n\n\n\nd1 = np.array([1, 2, 3], dtype=float)\nd1\n\narray([1., 2., 3.])\n\n\n\nd2 = np.array([1, 2, 3], dtype=complex)\nd2\n\narray([1.+0.j, 2.+0.j, 3.+0.j])\n\n\n\nA.3.1 Type casting\n\ndata = np.array([1, 2, 3], dtype=float)\ndata\n\narray([1., 2., 3.])\n\n\n\ndata = np.array(data, dtype=int)\ndata\n\narray([1, 2, 3])\n\n\n\ndata = np.array([1.6, 2, 3], dtype=float)\ndata.astype(int)\n\narray([1, 2, 3])\n\n\n\n\nA.3.2 Type promotion\n\nd1 = np.array([1, 2, 3], dtype=float)\nd2 = np.array([1, 2, 3], dtype=complex)\n\n\nd1 + d2\n\narray([2.+0.j, 4.+0.j, 6.+0.j])\n\n\n\n(d1 + d2).dtype\n\ndtype('complex128')\n\n\n\n\nA.3.3 Type-depending operation\n\nnp.sqrt(np.array([-1, 0, 1]))\n\n/var/folders/4x/8kn2nym12cn7x7qmg_6s4b8h0000gn/T/ipykernel_76845/208196152.py:1: RuntimeWarning: invalid value encountered in sqrt\n  np.sqrt(np.array([-1, 0, 1]))\n\n\narray([nan,  0.,  1.])\n\n\n\nnp.sqrt(np.array([-1, 0, 1], dtype=complex))\n\narray([0.+1.j, 0.+0.j, 1.+0.j])\n\n\n\n\nA.3.4 Real and imaginary parts\n\ndata = np.array([1, 2, 3], dtype=complex)\ndata\n\narray([1.+0.j, 2.+0.j, 3.+0.j])\n\n\n\ndata.real\n\narray([1., 2., 3.])\n\n\n\ndata.imag\n\narray([0., 0., 0.])\n\n\n\nnp.real(data)\n\narray([1., 2., 3.])\n\n\n\nnp.imag(data)\n\narray([0., 0., 0.])\n\n\n\n\nA.3.5 Order of array data in memory\n\nMultidimensional arrays are stored as contiguous data in memory. \\(~\\)Consider the case of a two-dimensional array, \\(~\\)containing rows and columns: \\(~\\)One possible way to store this array as a consecutive sequence of values is to store the rows after each other, and another equally valid approach is to store the columns one after another\nThe former is called row-major format and the latter is column-major format. Whether to use row-major or column-major is a matter of conventions, and the row-major format is used for example in the C programming language, and Fortran uses the column-major format\nA numpy array can be specified to be stored in row-major format, using the keyword argument order='C', and column-major format, using the keyword argument order='F', when the array is created or reshaped. The default format is row-major\nIn general, the numpy array attribute ndarray.strides defines exactly how this mapping is done. The strides attribute is a tuple of the same length as the number of axes (dimensions) of the array. Each value in strides is the factor by which the index for the corresponding axis is multiplied when calculating the memory offset (in bytes) for a given index expression\n\n\ndata = np.array([[1, 2, 3], [4, 5, 6]], dtype=np.int32)\ndata\n\narray([[1, 2, 3],\n       [4, 5, 6]], dtype=int32)\n\n\n\ndata.strides\n\n(12, 4)\n\n\n\ndata = np.array([[1, 2, 3], [4, 5, 6]], dtype=np.int32, order='F')\ndata\n\narray([[1, 2, 3],\n       [4, 5, 6]], dtype=int32)\n\n\n\ndata.strides\n\n(4, 8)",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>Numpy: Vectors, Matrices, and Multidimensional Arrays</span>"
    ]
  },
  {
    "objectID": "x_numpy_vectors_matrices_and_multidimensional_arrays.html#creating-arrays",
    "href": "x_numpy_vectors_matrices_and_multidimensional_arrays.html#creating-arrays",
    "title": "Appendix A — Numpy: Vectors, Matrices, and Multidimensional Arrays",
    "section": "A.4 Creating arrays",
    "text": "A.4 Creating arrays\n\nA.4.1 Arrays created from lists and other array-like objects\n\ndata = np.array([1, 2, 3, 4])\ndata.ndim, data.shape\n\n(1, (4,))\n\n\n\ndata = np.array(((1, 2), (3, 4)))\ndata.ndim, data.shape\n\n(2, (2, 2))\n\n\n\n\nA.4.2 Arrays filled with constant values\n\nnp.zeros((2, 3))\n\narray([[0., 0., 0.],\n       [0., 0., 0.]])\n\n\n\ndata = np.ones(4)\ndata, data.dtype\n\n(array([1., 1., 1., 1.]), dtype('float64'))\n\n\n\n5.4 * np.ones(10)\n\narray([5.4, 5.4, 5.4, 5.4, 5.4, 5.4, 5.4, 5.4, 5.4, 5.4])\n\n\n\nnp.full(10, 5.4) # slightly more efficient\n\narray([5.4, 5.4, 5.4, 5.4, 5.4, 5.4, 5.4, 5.4, 5.4, 5.4])\n\n\n\nx1 = np.empty(5)\nx1.fill(3.0)\nx1\n\narray([3., 3., 3., 3., 3.])\n\n\n\n\nA.4.3 Arrays filled with incremental sequences\n\nnp.arange(0, 11, 1)\n\narray([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10])\n\n\n\nnp.linspace(0, 10, 11)  # generally recommended\n\narray([ 0.,  1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10.])\n\n\n\n\nA.4.4 Arrays filled with logarithmic sequences\n\nnp.logspace(0, 2, 10)  # 5 data points between 10**0=1 to 10**2=100\n\narray([  1.        ,   1.66810054,   2.7825594 ,   4.64158883,\n         7.74263683,  12.91549665,  21.5443469 ,  35.93813664,\n        59.94842503, 100.        ])\n\n\n\n\nA.4.5 Mesh grid arrays\n\nx = np.array([-1, 0, 1])\ny = np.array([-2, 0, 2])\n\nX, Y = np.meshgrid(x, y)\n\n\nX\n\narray([[-1,  0,  1],\n       [-1,  0,  1],\n       [-1,  0,  1]])\n\n\n\nY\n\narray([[-2, -2, -2],\n       [ 0,  0,  0],\n       [ 2,  2,  2]])\n\n\n\nZ = (X + Y)**2\nZ\n\narray([[9, 4, 1],\n       [1, 0, 1],\n       [1, 4, 9]])\n\n\n\n\nA.4.6 Creating uninitialized arrays\n\nnp.empty(3, dtype=float)\n\narray([0., 0., 0.])\n\n\n\n\nA.4.7 Creating arrays with properties of other arrays\n\ndef f(x):    \n    y = np.ones_like(x)    # compute with x and y    \n    return y\n\nx = np.array([[1, 2, 3], [4, 5, 6]])\ny = f(x)\ny\n\narray([[1, 1, 1],\n       [1, 1, 1]])\n\n\n\n\nA.4.8 Creating matrix arrays\n\nnp.identity(4)\n\narray([[1., 0., 0., 0.],\n       [0., 1., 0., 0.],\n       [0., 0., 1., 0.],\n       [0., 0., 0., 1.]])\n\n\n\nnp.eye(4, k=1)\n\narray([[0., 1., 0., 0.],\n       [0., 0., 1., 0.],\n       [0., 0., 0., 1.],\n       [0., 0., 0., 0.]])\n\n\n\nnp.eye(4, k=-1)\n\narray([[0., 0., 0., 0.],\n       [1., 0., 0., 0.],\n       [0., 1., 0., 0.],\n       [0., 0., 1., 0.]])\n\n\n\nnp.diag(np.arange(0, 20, 5))\n\narray([[ 0,  0,  0,  0],\n       [ 0,  5,  0,  0],\n       [ 0,  0, 10,  0],\n       [ 0,  0,  0, 15]])",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>Numpy: Vectors, Matrices, and Multidimensional Arrays</span>"
    ]
  },
  {
    "objectID": "x_numpy_vectors_matrices_and_multidimensional_arrays.html#indexing-and-slicing",
    "href": "x_numpy_vectors_matrices_and_multidimensional_arrays.html#indexing-and-slicing",
    "title": "Appendix A — Numpy: Vectors, Matrices, and Multidimensional Arrays",
    "section": "A.5 Indexing and slicing",
    "text": "A.5 Indexing and slicing\n\nA.5.1 One-dimensional arrays\n\na = np.arange(0, 11)\na\n\narray([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10])\n\n\n\na[0]\n\nnp.int64(0)\n\n\n\na[-1]\n\nnp.int64(10)\n\n\n\na[4]\n\nnp.int64(4)\n\n\n\n\na[1:-1]\n\narray([1, 2, 3, 4, 5, 6, 7, 8, 9])\n\n\n\na[1:-1:2]\n\narray([1, 3, 5, 7, 9])\n\n\n\n\na[:5]\n\narray([0, 1, 2, 3, 4])\n\n\n\na[-5:]\n\narray([ 6,  7,  8,  9, 10])\n\n\n\na[::-2]\n\narray([10,  8,  6,  4,  2,  0])\n\n\n\n\nA.5.2 Multidimensional arrays\n\nf = lambda m, n: n + 10*m\n\n\n# please search for numpy.fromfunction at google\nA = np.fromfunction(f, (6, 6), dtype=int)\nA  \n\narray([[ 0,  1,  2,  3,  4,  5],\n       [10, 11, 12, 13, 14, 15],\n       [20, 21, 22, 23, 24, 25],\n       [30, 31, 32, 33, 34, 35],\n       [40, 41, 42, 43, 44, 45],\n       [50, 51, 52, 53, 54, 55]])\n\n\n\nA[:, 1]  # the second column\n\narray([ 1, 11, 21, 31, 41, 51])\n\n\n\nA[1, :]  # the second row\n\narray([10, 11, 12, 13, 14, 15])\n\n\n\nA[:3, :3]\n\narray([[ 0,  1,  2],\n       [10, 11, 12],\n       [20, 21, 22]])\n\n\n\nA[3:, :3]\n\narray([[30, 31, 32],\n       [40, 41, 42],\n       [50, 51, 52]])\n\n\n\nA[::2, ::2]\n\narray([[ 0,  2,  4],\n       [20, 22, 24],\n       [40, 42, 44]])\n\n\n\nA[1::2, 1::3]\n\narray([[11, 14],\n       [31, 34],\n       [51, 54]])\n\n\n\n\nA.5.3 Views\n\nSubarrays that are extracted from arrays using slice operations are alternative views of the same underlying array data. That is, \\(~\\)they are arrays that refer to the same data in memory as the original array, \\(~\\)but with a different strides configuration\nWhen elements in a view are assigned new values, \\(~\\)the values of the original array are therefore also updated. For example,\n\n\nB = A[1:5, 1:5]\nB\n\narray([[11, 12, 13, 14],\n       [21, 22, 23, 24],\n       [31, 32, 33, 34],\n       [41, 42, 43, 44]])\n\n\n\nB[:, :] = 0\nA\n\narray([[ 0,  1,  2,  3,  4,  5],\n       [10,  0,  0,  0,  0, 15],\n       [20,  0,  0,  0,  0, 25],\n       [30,  0,  0,  0,  0, 35],\n       [40,  0,  0,  0,  0, 45],\n       [50, 51, 52, 53, 54, 55]])\n\n\n\nWhen a copy rather than a view is needed, the view can be copied explicitly by using the copy method of the ndarray instance\n\n\nC = B[1:3, 1:3].copy()\nC\n\narray([[0, 0],\n       [0, 0]])\n\n\n\nC[:, :] = 1\nC\n\narray([[1, 1],\n       [1, 1]])\n\n\n\nB\n\narray([[0, 0, 0, 0],\n       [0, 0, 0, 0],\n       [0, 0, 0, 0],\n       [0, 0, 0, 0]])\n\n\n\n\nA.5.4 Fancy indexing and boolean-valued indexing\n\nA = np.linspace(0, 1, 11)\nA\n\narray([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ])\n\n\n\nA[np.array([0, 2, 4])]\n\narray([0. , 0.2, 0.4])\n\n\n\nA[[0, 2, 4]]\n\narray([0. , 0.2, 0.4])\n\n\n\n\nA &gt; 0.5\n\narray([False, False, False, False, False, False,  True,  True,  True,\n        True,  True])\n\n\n\nA[A &gt; 0.5]\n\narray([0.6, 0.7, 0.8, 0.9, 1. ])\n\n\n\nUnlike arrays created by using slices, \\(~\\)the arrays returned using fancy indexing and Boolean-valued indexing are not views, \\(~\\)but rather new independent arrays\n\n\nA = np.arange(10)\nindices = [2, 4, 6]\n\n\nB = A[indices]\n\n\nB[0] = -1\nA\n\narray([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])\n\n\n\nA[indices] = -1\nA\n\narray([ 0,  1, -1,  3, -1,  5, -1,  7,  8,  9])\n\n\n\n\nA = np.arange(10)\n\n\nB = A[A &gt; 5]\n\n\nB[0] = -1\nA\n\narray([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])\n\n\n\nA[A &gt; 5] = -1\nA\n\narray([ 0,  1,  2,  3,  4,  5, -1, -1, -1, -1])\n\n\n\n\nA.5.5 Summery\n\nshow_array((4, 4), ':, :')\n\n\n\n\n\n\n\n\n\nshow_array((4, 4), '0')\n\n\n\n\n\n\n\n\n\nshow_array((4, 4), '1, :')\n\n\n\n\n\n\n\n\n\nshow_array((4, 4), ':, 2')\n\n\n\n\n\n\n\n\n\n\nshow_array((4, 4), '0:2, 0:2')\n\n\n\n\n\n\n\n\n\nshow_array((4, 4), '0:2, 2:4')\n\n\n\n\n\n\n\n\n\nshow_array((4, 4), '::2, ::2')\n\n\n\n\n\n\n\n\n\nshow_array((4, 4), '1::2, 1::2')\n\n\n\n\n\n\n\n\n\nshow_array((4, 4), ':, [0, 3]')\n\n\n\n\n\n\n\n\n\nshow_array((4, 4), '[1, 3], [0, 3]')\n\n\n\n\n\n\n\n\n\nshow_array((4, 4), ':, [False, True, True, False]')\n\n\n\n\n\n\n\n\n\nshow_array((4, 4), '1:3, [False, True, True, False]')",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>Numpy: Vectors, Matrices, and Multidimensional Arrays</span>"
    ]
  },
  {
    "objectID": "x_numpy_vectors_matrices_and_multidimensional_arrays.html#reshaping-and-resizing",
    "href": "x_numpy_vectors_matrices_and_multidimensional_arrays.html#reshaping-and-resizing",
    "title": "Appendix A — Numpy: Vectors, Matrices, and Multidimensional Arrays",
    "section": "A.6 Reshaping and resizing",
    "text": "A.6 Reshaping and resizing\n\nReshaping an array does not require modifying the underlying array data; it only changes in how the data is interpreted, by redefining the array’s strides attribute\n\n\ndata = np.array([[1, 2], [3, 4]])\ndata1 = np.reshape(data, (1, 4))\ndata1\n\narray([[1, 2, 3, 4]])\n\n\n\ndata1[0, 1] = -1\ndata\n\narray([[ 1, -1],\n       [ 3,  4]])\n\n\n\ndata2 = data.reshape(4)\ndata2\n\narray([ 1, -1,  3,  4])\n\n\n\ndata2[1] = -2\ndata\n\narray([[ 1, -2],\n       [ 3,  4]])\n\n\n\n\ndata = np.array([[1, 2], [3, 4]])\ndata1 = np.ravel(data)\ndata1\n\narray([1, 2, 3, 4])\n\n\n\ndata1[0] = -1\ndata\n\narray([[-1,  2],\n       [ 3,  4]])\n\n\n\nThe ndarray method flatten perform the same function, \\(~\\)but returns a copy instead of a view\n\n\ndata2 = data.flatten()\ndata2\n\narray([-1,  2,  3,  4])\n\n\n\ndata2[0] = -2\ndata\n\narray([[-1,  2],\n       [ 3,  4]])\n\n\n\n\ndata = np.arange(0, 5)\ndata.shape\n\n(5,)\n\n\n\ncolumn = data[:, np.newaxis]\ncolumn\n\narray([[0],\n       [1],\n       [2],\n       [3],\n       [4]])\n\n\n\ncolumn.shape\n\n(5, 1)\n\n\n\nrow = data[np.newaxis, :]\nrow\n\narray([[0, 1, 2, 3, 4]])\n\n\n\nrow.shape\n\n(1, 5)\n\n\n\nrow[0, 0] = -1\ndata\n\narray([-1,  1,  2,  3,  4])\n\n\n\n\nnp.expand_dims(data, axis=1) \n\narray([[-1],\n       [ 1],\n       [ 2],\n       [ 3],\n       [ 4]])\n\n\n\nrow = np.expand_dims(data, axis=0)\nrow\n\narray([[-1,  1,  2,  3,  4]])\n\n\n\nrow[0, 0] = 0\ndata\n\narray([0, 1, 2, 3, 4])\n\n\n\n\ndata = np.arange(5)\ndata\n\narray([0, 1, 2, 3, 4])\n\n\n\nnp.vstack((data, data, data))\n\narray([[0, 1, 2, 3, 4],\n       [0, 1, 2, 3, 4],\n       [0, 1, 2, 3, 4]])\n\n\n\nnp.hstack((data, data, data))\n\narray([0, 1, 2, 3, 4, 0, 1, 2, 3, 4, 0, 1, 2, 3, 4])\n\n\n\ndata = data[:, np.newaxis]\ndata.shape\n\n(5, 1)\n\n\n\nnp.hstack((data, data, data))\n\narray([[0, 0, 0],\n       [1, 1, 1],\n       [2, 2, 2],\n       [3, 3, 3],\n       [4, 4, 4]])\n\n\n\ndata1 = np.array([[1, 2], [3, 4]])\ndata2 = np.array([[5, 6]])\n\n\nnp.concatenate((data1, data2), axis=0)\n\narray([[1, 2],\n       [3, 4],\n       [5, 6]])\n\n\n\nnp.concatenate((data1, data2.T), axis=1)\n\narray([[1, 2, 5],\n       [3, 4, 6]])",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>Numpy: Vectors, Matrices, and Multidimensional Arrays</span>"
    ]
  },
  {
    "objectID": "x_numpy_vectors_matrices_and_multidimensional_arrays.html#vectorized-expressions",
    "href": "x_numpy_vectors_matrices_and_multidimensional_arrays.html#vectorized-expressions",
    "title": "Appendix A — Numpy: Vectors, Matrices, and Multidimensional Arrays",
    "section": "A.7 Vectorized expressions",
    "text": "A.7 Vectorized expressions\n\nA.7.1 Arithmetic operations\n\nx = np.array([[1, 2], [3, 4]])\ny = np.array([[5, 6], [7, 8]])\n\n\nx + y\n\narray([[ 6,  8],\n       [10, 12]])\n\n\n\ny - x\n\narray([[4, 4],\n       [4, 4]])\n\n\n\nx * y\n\narray([[ 5, 12],\n       [21, 32]])\n\n\n\ny / x\n\narray([[5.        , 3.        ],\n       [2.33333333, 2.        ]])\n\n\n\n\nx * 2\n\narray([[2, 4],\n       [6, 8]])\n\n\n\n2**x\n\narray([[ 2,  4],\n       [ 8, 16]])\n\n\n\ny / 2\n\narray([[2.5, 3. ],\n       [3.5, 4. ]])\n\n\n\n(y / 2).dtype\n\ndtype('float64')\n\n\n\n\nA.7.2 Broadcasting\n\na = np.array([[11, 12, 13], [21, 22, 23], [31, 32, 33]])\nb = np.array([[1, 2, 3]])\n\n\na + b\n\narray([[12, 14, 16],\n       [22, 24, 26],\n       [32, 34, 36]])\n\n\n\nshow_array_broadcasting(a, b)\n\n\n\n\n\n\n\n\n\na + b.T\n\narray([[12, 13, 14],\n       [23, 24, 25],\n       [34, 35, 36]])\n\n\n\nshow_array_broadcasting(a, b.T)\n\n\n\n\n\n\n\n\n\n\nx = np.array([1, 2, 3, 4]).reshape(2, 2)\nx.shape\n\n(2, 2)\n\n\n\nz = np.array([[2, 4]])\nz.shape\n\n(1, 2)\n\n\n\nx / z\n\narray([[0.5, 0.5],\n       [1.5, 1. ]])\n\n\n\nzz = np.vstack((z, z))\nzz\n\narray([[2, 4],\n       [2, 4]])\n\n\n\nx / zz\n\narray([[0.5, 0.5],\n       [1.5, 1. ]])\n\n\n\n\nz = np.array([[2], [4]])\nz.shape\n\n(2, 1)\n\n\n\nx / z\n\narray([[0.5 , 1.  ],\n       [0.75, 1.  ]])\n\n\n\nzz = np.concatenate([z, z], axis=1)\nzz\n\narray([[2, 2],\n       [4, 4]])\n\n\n\nx / zz\n\narray([[0.5 , 1.  ],\n       [0.75, 1.  ]])\n\n\n\n\nx = z = np.array([1, 2, 3, 4])\ny = np.array([5, 6, 7, 8])\nx = x + y  # x is reassigned to a new array\n\n\nx, z\n\n(array([ 6,  8, 10, 12]), array([1, 2, 3, 4]))\n\n\n\nx = z = np.array([1, 2, 3, 4])\ny = np.array([5, 6, 7, 8])\nx += y  # the values of array x are updated in place\n\n\nx, z\n\n(array([ 6,  8, 10, 12]), array([ 6,  8, 10, 12]))\n\n\n\n\nA.7.3 Elementwise functions\n\nx = np.linspace(-1, 1, 11)\nx\n\narray([-1. , -0.8, -0.6, -0.4, -0.2,  0. ,  0.2,  0.4,  0.6,  0.8,  1. ])\n\n\n\ny = np.sin(np.pi * x)\n\n\nnp.round(y, decimals=4)\n\narray([-0.    , -0.5878, -0.9511, -0.9511, -0.5878,  0.    ,  0.5878,\n        0.9511,  0.9511,  0.5878,  0.    ])\n\n\n\nnp.add(np.sin(x)**2, np.cos(x)**2)\n\narray([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.])\n\n\n\nnp.sin(x)**2 + np.cos(x)**2\n\narray([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.])\n\n\n\n\ndef heaviside(x):\n    return 1 if x &gt; 0 else 0\n\n\nheaviside(-1)\n\n0\n\n\n\nheaviside(1.5)\n\n1\n\n\n```{python}\nx = np.linspace(-5, 5, 11)\nheaviside(x)\n```\n\nValueError \n      1 x = np.linspace(-5, 5, 11)\n----&gt; 2 heaviside(x)\n\n      1 def heaviside(x):\n----&gt; 2     return 1 if x &gt; 0 else 0\n\nValueError: The truth value of an array with more than \none element is ambiguous. Use a.any() or a.all()\n\nheaviside = np.vectorize(heaviside)\nheaviside(x)\n\narray([0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1])\n\n\n\ndef heaviside(x):  # much better way\n    return 1 * (x &gt; 0)\n\nheaviside(x)\n\narray([0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1])\n\n\n\n\nA.7.4 Aggregation\n\ndata = np.random.normal(size=(15, 15)) \n\n\nnp.mean(data)\n\nnp.float64(-0.041673140354896374)\n\n\n\ndata.mean()\n\nnp.float64(-0.041673140354896374)\n\n\n\n\ndata = np.random.normal(size=(5, 10, 15))\n\n\ndata.sum(axis=0).shape\n\n(10, 15)\n\n\n\ndata.sum(axis=(0, 2)).shape\n\n(10,)\n\n\n\ndata.sum()\n\nnp.float64(18.68747786875952)\n\n\n\n\ndata = np.arange(9).reshape(3, 3)\ndata\n\narray([[0, 1, 2],\n       [3, 4, 5],\n       [6, 7, 8]])\n\n\n\ndata.sum()\n\nnp.int64(36)\n\n\n\nshow_array_aggregation(data, None)\n\n\n\n\n\n\n\n\n\ndata.sum(axis=0)\n\narray([ 9, 12, 15])\n\n\n\nshow_array_aggregation(data, 0)\n\n\n\n\n\n\n\n\n\ndata.sum(axis=1)\n\narray([ 3, 12, 21])\n\n\n\nshow_array_aggregation(data, 1)\n\n\n\n\n\n\n\n\n\n\nA.7.5 Boolean arrays and conditional expressions\n\na = np.array([1, 2, 3, 4])\nb = np.array([4, 3, 2, 1])\n\n\na &lt; b\n\narray([ True,  True, False, False])\n\n\n\nnp.all(a &lt; b)\n\nnp.False_\n\n\n\nnp.any(a &lt; b)\n\nnp.True_\n\n\n\n\nx = np.array([-2, -1, 0, 1, 2])\n\n\nx &gt; 0\n\narray([False, False, False,  True,  True])\n\n\n\n1 * (x &gt; 0)\n\narray([0, 0, 0, 1, 1])\n\n\n\nx * (x &gt; 0)\n\narray([0, 0, 0, 1, 2])\n\n\n\n\ndef pulse(x, position, height, width):\n    return height * (x &gt;= position) * (x &lt;= (position + width))  \n\n\nx = np.linspace(-5, 5, 31)\n\n\npulse(x, position=-2, height=1, width=5)\n\narray([0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n       1, 1, 1, 0, 0, 0, 0, 0, 0])\n\n\n\npulse(x, position=1, height=2, width=2)\n\narray([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 2, 2,\n       2, 2, 2, 0, 0, 0, 0, 0, 0])\n\n\n\n\nx = np.linspace(-4, 4, 9)\nx\n\narray([-4., -3., -2., -1.,  0.,  1.,  2.,  3.,  4.])\n\n\n\nnp.where(x &lt; 0, x**2, x**3)\n\narray([16.,  9.,  4.,  1.,  0.,  1.,  8., 27., 64.])\n\n\n\nnp.select([x &lt; -1, x &lt; 2, x&gt;= 2], [x**2, x**3, x**4])\n\narray([ 16.,   9.,   4.,  -1.,   0.,   1.,  16.,  81., 256.])\n\n\n\nnp.choose([0, 0, 0, 1, 1, 1, 2, 2, 2], [x**2, x**3, x**4])\n\narray([ 16.,   9.,   4.,  -1.,   0.,   1.,  16.,  81., 256.])\n\n\n\nx[np.abs(x) &gt; 2]\n\narray([-4., -3.,  3.,  4.])\n\n\n\n\nA.7.6 Set operations\n\na = np.unique([1, 2, 3, 3])\na\n\narray([1, 2, 3])\n\n\n\nb = np.unique([2, 3, 4, 4, 5, 6, 5])\nb\n\narray([2, 3, 4, 5, 6])\n\n\n\nnp.in1d(a, b)\n\n/var/folders/4x/8kn2nym12cn7x7qmg_6s4b8h0000gn/T/ipykernel_76845/924698060.py:1: DeprecationWarning: `in1d` is deprecated. Use `np.isin` instead.\n  np.in1d(a, b)\n\n\narray([False,  True,  True])\n\n\n\n1 in a, 1 in b\n\n(True, False)\n\n\n\nnp.all(np.in1d(a, b))  # to test if a is a subset of b\n\n/var/folders/4x/8kn2nym12cn7x7qmg_6s4b8h0000gn/T/ipykernel_76845/423008062.py:1: DeprecationWarning: `in1d` is deprecated. Use `np.isin` instead.\n  np.all(np.in1d(a, b))  # to test if a is a subset of b\n\n\nnp.False_\n\n\n\n\nnp.union1d(a, b)\n\narray([1, 2, 3, 4, 5, 6])\n\n\n\nnp.intersect1d(a, b)\n\narray([2, 3])\n\n\n\nnp.setdiff1d(a, b)\n\narray([1])\n\n\n\nnp.setdiff1d(b, a)\n\narray([4, 5, 6])\n\n\n\n\nA.7.7 Operations on arrays\n\ndata = np.arange(9).reshape(3, 3)\ndata\n\narray([[0, 1, 2],\n       [3, 4, 5],\n       [6, 7, 8]])\n\n\n\nnp.transpose(data)\n\narray([[0, 3, 6],\n       [1, 4, 7],\n       [2, 5, 8]])\n\n\n\n\ndata = np.random.randn(1, 2, 3, 4, 5)\n\n\ndata.shape\n\n(1, 2, 3, 4, 5)\n\n\n\ndata.T.shape\n\n(5, 4, 3, 2, 1)",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>Numpy: Vectors, Matrices, and Multidimensional Arrays</span>"
    ]
  },
  {
    "objectID": "x_numpy_vectors_matrices_and_multidimensional_arrays.html#matrix-and-vector-operations",
    "href": "x_numpy_vectors_matrices_and_multidimensional_arrays.html#matrix-and-vector-operations",
    "title": "Appendix A — Numpy: Vectors, Matrices, and Multidimensional Arrays",
    "section": "A.8 Matrix and vector operations",
    "text": "A.8 Matrix and vector operations\n\nA = np.arange(1, 7).reshape(2, 3)\nA\n\narray([[1, 2, 3],\n       [4, 5, 6]])\n\n\n\nB = np.arange(1, 7).reshape(3, 2)\nB\n\narray([[1, 2],\n       [3, 4],\n       [5, 6]])\n\n\n\nnp.dot(A, B)\n\narray([[22, 28],\n       [49, 64]])\n\n\n\nnp.dot(B, A)\n\narray([[ 9, 12, 15],\n       [19, 26, 33],\n       [29, 40, 51]])\n\n\n\nA @ B  # python 3.5 above\n\narray([[22, 28],\n       [49, 64]])\n\n\n\nB @ A\n\narray([[ 9, 12, 15],\n       [19, 26, 33],\n       [29, 40, 51]])\n\n\n\n\nA = np.arange(9).reshape(3, 3)\nA\n\narray([[0, 1, 2],\n       [3, 4, 5],\n       [6, 7, 8]])\n\n\n\nx = np.arange(3)\nx\n\narray([0, 1, 2])\n\n\n\nnp.dot(A, x)\n\narray([ 5, 14, 23])\n\n\n\nA.dot(x)\n\narray([ 5, 14, 23])\n\n\n\nA @ x\n\narray([ 5, 14, 23])\n\n\n\n\nA = np.random.rand(3, 3)\nB = np.random.rand(3, 3)\n\n\nAp = np.dot(B, np.dot(A, np.linalg.inv(B)))\n\n\nAp = B.dot(A.dot(np.linalg.inv(B)))\n\n\nB @ A @ np.linalg.inv(B)\n\narray([[-2.37625054,  0.83934428,  4.20566574],\n       [-0.53237348,  0.16362199,  4.63972309],\n       [-2.04694973,  0.71992986,  2.77055746]])\n\n\n\n\nnp.inner(x, x)\n\nnp.int64(5)\n\n\n\nnp.dot(x, x)\n\nnp.int64(5)\n\n\n\ny = x[:, np.newaxis]\ny\n\narray([[0],\n       [1],\n       [2]])\n\n\n\nnp.dot(y.T, y)\n\narray([[5]])\n\n\n\n\nGiven two vectors, \\(\\mathbf{a} = [a_0, a_1, ..., a_M]~\\) and \\(~\\mathbf{b} = [b_0, b_1, ..., b_N]\\), \\(~\\)the outer product is \\(\\mathbf{a}^T\\mathbf{b}\\)\n\n\\[\\begin{pmatrix}\na_0 b_0 & a_0b_1 & \\cdots & a_0 b_N \\\\\na_1 b_0 & \\cdots & \\cdots & a_1 b_N \\\\\n\\vdots  & \\ddots &        & \\vdots  \\\\\na_M b_0 &        & \\ddots & a_M b_N \\\\\n\\end{pmatrix}\n\\]\n\nx = np.array([1, 2, 3])\n\n\nnp.outer(x, x)\n\narray([[1, 2, 3],\n       [2, 4, 6],\n       [3, 6, 9]])\n\n\n\nnp.kron(x, x)\n\narray([1, 2, 3, 2, 4, 6, 3, 6, 9])\n\n\n\nnp.kron(x[:, np.newaxis], x[np.newaxis, :])\n\narray([[1, 2, 3],\n       [2, 4, 6],\n       [3, 6, 9]])\n\n\n\nnp.kron(np.ones((2, 2)), np.identity(2))\n\narray([[1., 0., 1., 0.],\n       [0., 1., 0., 1.],\n       [1., 0., 1., 0.],\n       [0., 1., 0., 1.]])\n\n\n\nnp.kron(np.identity(2), np.ones((2, 2)))\n\narray([[1., 1., 0., 0.],\n       [1., 1., 0., 0.],\n       [0., 0., 1., 1.],\n       [0., 0., 1., 1.]])\n\n\n\n\nx = np.array([1, 2, 3, 4])\ny = np.array([5, 6, 7, 8])\n\n\nnp.einsum(\"n,n\", x, y)\n\nnp.int64(70)\n\n\n\nnp.inner(x, y)\n\nnp.int64(70)\n\n\n\nA = np.arange(9).reshape(3, 3)\nB = A.T\n\n\nnp.einsum(\"mk,kn\", A, B)\n\narray([[  5,  14,  23],\n       [ 14,  50,  86],\n       [ 23,  86, 149]])\n\n\n\nnp.all(np.einsum(\"mk,kn\", A, B) == np.dot(A, B))\n\nnp.True_",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>Numpy: Vectors, Matrices, and Multidimensional Arrays</span>"
    ]
  },
  {
    "objectID": "x_optimization.html",
    "href": "x_optimization.html",
    "title": "Appendix E — Optimization",
    "section": "",
    "text": "E.1 Importing modules\n\\(~\\)\nIn this appendix,\nimport numpy as np\nimport sympy\nsympy.init_printing()\n\nfrom scipy import optimize\nimport cvxopt\n\nimport matplotlib as mpl\nimport matplotlib.pyplot as plt\n\nfrom IPython.display import display\n\nprint(\"cvxopt: \", cvxopt.__version__)\n\ncvxopt:  1.3.2",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>E</span>  <span class='chapter-title'>Optimization</span>"
    ]
  },
  {
    "objectID": "x_optimization.html#importing-modules",
    "href": "x_optimization.html#importing-modules",
    "title": "Appendix E — Optimization",
    "section": "F.1 Importing modules",
    "text": "F.1 Importing modules\n\nimport numpy as np\nimport sympy\nsympy.init_printing()\n\nfrom scipy import optimize\nimport cvxopt\n\nimport matplotlib as mpl\nimport matplotlib.pyplot as plt\n\nfrom IPython.display import display\n\nprint(\"cvxopt: \", cvxopt.__version__)\n\ncvxopt:  1.3.2",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>E</span>  <span class='chapter-title'>Optimization</span>"
    ]
  },
  {
    "objectID": "x_optimization.html#classification-of-optimization-problems",
    "href": "x_optimization.html#classification-of-optimization-problems",
    "title": "Appendix E — Optimization",
    "section": "E.2 Classification of optimization problems",
    "text": "E.2 Classification of optimization problems\nA general optimization problem considered here can be formulated as a minimization problem, \\(\\min_x f(x)\\), subject to sets of \\(m\\) equality constraints \\(g(x)=0\\) and \\(p\\) inequality constraints \\(h(x) \\leq 0\\)\nDepending on the properties of the objective function \\(f(x)\\) and the equality and inequality constraints \\(g(x)\\) and \\(h(x)\\), this formulation includes a rich variety of problems\n\nunivariate(one dimensional) or multivariate(multidimensional)\nlinear programming problem or nonlinear programming problem\nunconstrained, or constrained(equality and inequality constraints require different approaches)\nconvex or non-convex\ncontinuous(smooth) or discontinuous",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>E</span>  <span class='chapter-title'>Optimization</span>"
    ]
  },
  {
    "objectID": "x_optimization.html#univariate-optimization",
    "href": "x_optimization.html#univariate-optimization",
    "title": "Appendix E — Optimization",
    "section": "E.3 Univariate optimization",
    "text": "E.3 Univariate optimization\nMinimize the area of a cylinder with unit volume. Here, suitable variables are the radius and height of the cylinder, and the objective function is\n\\[ f(r,h) = 2\\pi r^2 + 2\\pi rh\\]\nsubject to the equality constraint\n\\[ g(r,h) = \\pi r^2h -1 = 0 \\]\n\nr, h = sympy.symbols(\"r, h\")\nArea = 2 *sympy.pi *r**2 + 2 *sympy.pi *r *h \nVolume = sympy.pi *r**2 *h\n\n\nh_r = sympy.solve(Volume -1)[0]\nh_r\n\n\\(\\displaystyle \\left\\{ h : \\frac{1}{\\pi r^{2}}\\right\\}\\)\n\n\n\nArea_r = Area.subs(h_r)\nArea_r\n\n\\(\\displaystyle 2 \\pi r^{2} + \\frac{2}{r}\\)\n\n\n\n# f'(r_sol) = 0\nrsol = sympy.solve(Area_r.diff(r))[0]\nrsol\n\n\\(\\displaystyle \\frac{2^{\\frac{2}{3}}}{2 \\sqrt[3]{\\pi}}\\)\n\n\n\n_.evalf()\n\n\\(\\displaystyle 0.541926070139289\\)\n\n\n\n# f''(r_sol) &gt; 0\nArea_r.diff(r, 2).subs(r, rsol)\n\n\\(\\displaystyle 12 \\pi\\)\n\n\n\nArea_r.subs(r, rsol)\n\n\\(\\displaystyle 3 \\sqrt[3]{2} \\sqrt[3]{\\pi}\\)\n\n\n\n# Minimum Area\n_.evalf()\n\n\\(\\displaystyle 5.53581044593209\\)\n\n\n\n\ndef f(r):\n  return 2 *np.pi *r**2 + 2 /r\n\n\nr_min = optimize.brent(f, brack=(0.1, 4))\nr_min\n\n\\(\\displaystyle 0.541926077255714\\)\n\n\n\nf(r_min)\n\n\\(\\displaystyle 5.53581044593209\\)\n\n\n\n\noptimize.minimize_scalar(f, bracket=(0.1, 4))\n\n message: \n          Optimization terminated successfully;\n          The returned value satisfies the termination criteria\n          (using xtol = 1.48e-08 )\n success: True\n     fun: 5.535810445932086\n       x: 0.5419260772557135\n     nit: 15\n    nfev: 18\n\n\n\n\nr = np.linspace(1.e-2, 2, 100)\n\nfig, ax = plt.subplots(figsize=(6, 4))\n\nax.plot(r, f(r), lw=2, color='b')\nax.plot(r_min, f(r_min), 'ro', markersize=12)\nax.set_title(r\"$f(r) = 2\\pi r^2 +2/r$\", fontsize=12)\n\nax.tick_params(axis='x', pad=7)\nax.set_xlabel(r\"$r$\", fontsize=14)\nax.set_ylabel(r\"$A$\", fontsize=14)\nax.set_xticks([0, 0.5, 1, 1.5, 2])\nax.set_xlim(0, 2)\nax.set_ylim(0, 30)\nax.tick_params(which='both', direction='in')",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>E</span>  <span class='chapter-title'>Optimization</span>"
    ]
  },
  {
    "objectID": "x_optimization.html#unconstrained-multivariate-optimization",
    "href": "x_optimization.html#unconstrained-multivariate-optimization",
    "title": "Appendix E — Optimization",
    "section": "E.4 Unconstrained multivariate optimization",
    "text": "E.4 Unconstrained multivariate optimization\nWe consider the following problem:\n\\[\\min_x f(x)\\]\nwhere the objective function is\n\\[ f(x) = (x_1 -1)^4 +5(x_2-1)^2 -2x_1 x_2 \\]\n\nx1, x2 = sympy.symbols(\"x_1, x_2\")\n\nf_sym = (x1 -1)**4 +5 *(x2 -1)**2 -2 *x1 *x2\nfprime_sym = [f_sym.diff(x_) for x_ in (x1, x2)]\nfhess_sym = [[f_sym.diff(x1_, x2_) \n              for x1_ in (x1, x2)] \n              for x2_ in (x1, x2)]\n\n\nsympy.Matrix(fprime_sym)\n\n\\(\\displaystyle \\left[\\begin{matrix}- 2 x_{2} + 4 \\left(x_{1} - 1\\right)^{3}\\\\- 2 x_{1} + 10 x_{2} - 10\\end{matrix}\\right]\\)\n\n\n\nsympy.Matrix(fhess_sym)\n\n\\(\\displaystyle \\left[\\begin{matrix}12 \\left(x_{1} - 1\\right)^{2} & -2\\\\-2 & 10\\end{matrix}\\right]\\)\n\n\n\nf_lmbda = sympy.lambdify((x1, x2), f_sym, 'numpy')\nfprime_lmbda = sympy.lambdify((x1, x2), fprime_sym, 'numpy')\nfhess_lmbda = sympy.lambdify((x1, x2), fhess_sym, 'numpy')\n\n\ndef func_XY_X_Y(f):\n    \"\"\"\n    Wrapper for f(X) -&gt; f(X[0], X[1])\n    \"\"\"\n    return lambda X: np.array(f(X[0], X[1]))\n\n\nf = func_XY_X_Y(f_lmbda)\nfprime = func_XY_X_Y(fprime_lmbda)\nfhess = func_XY_X_Y(fhess_lmbda)\n\n\nIn scipy, Newton conjugate gradient method is implemented in the function optimize.fmin_ncg. This function takes the following arguments: a python function for the objective function, a starting point, a python function for evaluating the gradient, and (optionally) a python function for evaluating the Hessian\n\n\nx_opt = optimize.fmin_ncg(f, (0.0, 0.0), fprime=fprime, fhess=fhess)\n\nOptimization terminated successfully.\n         Current function value: -3.867223\n         Iterations: 8\n         Function evaluations: 10\n         Gradient evaluations: 10\n         Hessian evaluations: 8\n\n\n\nx_opt\n\narray([1.88292613, 1.37658523])\n\n\n\nx_ = y_ = np.linspace(-1, 4, 100)\nX, Y = np.meshgrid(x_, y_)\n\nfig, ax = plt.subplots(figsize=(7, 4))\n\nc = ax.contour(X, Y, f_lmbda(X, Y), 50)\nax.plot(x_opt[0], x_opt[1], 'ro', markersize=10)\nax.set_xlabel(r\"$x_1$\", fontsize=12)\nax.set_ylabel(r\"$x_2$\", fontsize=12)\nax.tick_params(which='both', direction='in')\nplt.colorbar(c, ax=ax, ticks=[0, 25, 50, 75, 100])\n\n\n\n\n\n\n\n\n\nMethods that approximate the Hessian are known as quasi-Newton methods, and there are also alternative iterative methods that completely avoid using the Hessian\nTwo popular methods are the BFGS and the conjugate-gradient methods, which are implemented in scipy as the functions optimize.fmin_bfgs (the quasi-Newton method of Broyden, Fletcher, Goldfarb, and Shanno) and optimize.fmin_cg (the conjugate-gradient method of Polak and Ribiere)\n\n\nx_opt = optimize.fmin_bfgs(f, (0, 0), fprime=fprime)\nx_opt\n\nOptimization terminated successfully.\n         Current function value: -3.867223\n         Iterations: 9\n         Function evaluations: 13\n         Gradient evaluations: 13\n\n\narray([1.88292645, 1.37658596])\n\n\n\nx_opt = optimize.fmin_cg(f, (0, 0), fprime=fprime)\nx_opt\n\nOptimization terminated successfully.\n         Current function value: -3.867223\n         Iterations: 8\n         Function evaluations: 18\n         Gradient evaluations: 18\n\n\narray([1.88292612, 1.37658523])\n\n\n\n\nx_opt = optimize.fmin_bfgs(f, (0, 0))\nx_opt\n\nOptimization terminated successfully.\n         Current function value: -3.867223\n         Iterations: 9\n         Function evaluations: 39\n         Gradient evaluations: 13\n\n\narray([1.88292644, 1.37658595])\n\n\n\nx_opt = optimize.fmin_cg(f, (0, 0))\nx_opt\n\nOptimization terminated successfully.\n         Current function value: -3.867223\n         Iterations: 8\n         Function evaluations: 54\n         Gradient evaluations: 18\n\n\narray([1.88292612, 1.37658522])\n\n\n\nE.4.1 Brute force search for a global minimum\n\nThe methods for multivariate optimization that we have discussed so far all converge to a local minimum in general. For problems with many local minima, this can easily lead to a situation when the solver easily gets stuck in a local minimum, even if a global minimum exists\nAlthough there is no complete and general solution to this problem, a practical approach that can partially alleviate this problem is to use a brute force search over a coordinate grid to find a suitable starting point for an iterative solver. At least this gives a systematic approach to find a global minimum within given coordinate ranges\nIn scipy, the function optimize.brute can carry out such a systematic search\n\nTo illustrate this method, consider the problem of minimizing the function\n\\[4 \\sin\\pi x + 6\\sin \\pi y +(x -1)^2 +(y -1)^2\\]\nwhich has a large number of local minima\n\ndef f(X):\n  x, y = X\n  return 4*np.sin(np.pi *x) +6*np.sin(np.pi *y) +(x -1)**2 +(y -1)**2\n\n\nx_start = optimize.brute(f, (slice(-3, 5, 0.5), slice(-3, 5, 0.5)), finish=None); \nprint(f'{x_start = },', f'{f(x_start) = }')\n\nx_start = array([1.5, 1.5]), f(x_start) = np.float64(-9.5)\n\n\n\nx_opt = optimize.fmin_bfgs(f, x_start)\nprint(f'{x_opt = },', f'{f(x_opt) = }')\n\nOptimization terminated successfully.\n         Current function value: -9.520229\n         Iterations: 4\n         Function evaluations: 21\n         Gradient evaluations: 7\nx_opt = array([1.47586906, 1.48365787]), f(x_opt) = np.float64(-9.520229273055016)\n\n\n\n\nresult = optimize.minimize(f, x_start, method='BFGS')\nresult\n\n  message: Optimization terminated successfully.\n  success: True\n   status: 0\n      fun: -9.520229273055016\n        x: [ 1.476e+00  1.484e+00]\n      nit: 4\n      jac: [-7.153e-07 -8.345e-07]\n hess_inv: [[ 2.416e-02  4.605e-06]\n            [ 4.605e-06  1.635e-02]]\n     nfev: 21\n     njev: 7\n\n\n\nresult.x\n\narray([1.47586906, 1.48365787])\n\n\n\nresult.fun\n\n\\(\\displaystyle -9.52022927305502\\)\n\n\n\ndef func_X_Y_to_XY(f, X, Y):\n  s = np.shape(X)\n  return f(np.vstack([X.ravel(), Y.ravel()])).reshape(*s)\n\nx_ = y_ = np.linspace(-3, 5, 100)\nX, Y = np.meshgrid(x_, y_)\n\nfig, ax = plt.subplots(figsize=(6, 4))\n\nc = ax.contour(X, Y, func_X_Y_to_XY(f, X, Y), 25)\nax.plot(x_opt[0], x_opt[1], 'ro', markersize=8)\nax.set_xlabel(r\"$x$\", fontsize=12)\nax.set_ylabel(r\"$y$\", fontsize=12)\nplt.colorbar(c, ax=ax)\nax.tick_params(which='both', direction='in')",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>E</span>  <span class='chapter-title'>Optimization</span>"
    ]
  },
  {
    "objectID": "x_optimization.html#noninear-least-square-problems",
    "href": "x_optimization.html#noninear-least-square-problems",
    "title": "Appendix E — Optimization",
    "section": "E.5 Noninear least square problems",
    "text": "E.5 Noninear least square problems\n\nIn general, a least square problem can be viewed as an optimization problem with the objective function \\(g(\\boldsymbol{\\beta}) = \\sum_{i=0}^m r_i^2(\\boldsymbol{\\beta})\\), with the residuals \\(r_i(\\boldsymbol{\\beta}) = y_i -f(x_i, \\boldsymbol{\\beta})\\) for a set of \\(m\\) obervations \\((x_i, y_i)\\). Here \\(\\boldsymbol{\\beta}\\) is a vector with unknown parameters that specifies the function \\(f(x, \\boldsymbol{\\beta})\\). If this problem is nonlinear in the parameters \\(\\boldsymbol{\\beta}\\), it is known as a nonlinear least square problem\nIn scipy, the function optimize.leastsq provides a nonlinear least square solver that uses the Levenberg-Marquardt method. To illustrate how this function can be used, consider a nonlinear model on the form \\(f(x,\\boldsymbol{\\beta})=\\beta_1 +\\beta_2 \\exp \\left( -\\beta_3 x^2 \\right)\\) and a set of observations \\((x_i, y_i)\\)\nSimulate the observations:\n\n\nbeta = (0.25, 0.75, 0.5)\ndef f(x, b0, b1, b2):\n  return b0 +b1 *np.exp(-b2 *x**2)\n\n\nxdata = np.linspace(0, 5, 50)\ny = f(xdata, *beta)\nydata = y +0.05 *np.random.randn(len(xdata))\n\n\nThe first step is to define a function for the residuals given the data and the model function, which is specified in terms of the yet-to-be determined model parameters\n\n\ndef g(beta):\n  return ydata -f(xdata, *beta)\n\n\nNext we define an initial guess for the parameter vector and let the optimize.leastsq function solve for the best least square fit for the parameter vector:\n\n\nbeta_start = (1, 1, 1)\nbeta_opt, beta_cov = optimize.leastsq(g, beta_start)\n\n\nbeta_opt\n\narray([0.24368959, 0.7816449 , 0.5479457 ])\n\n\n\n\nfig, ax = plt.subplots(figsize=(7, 4))\n\nax.scatter(xdata, ydata)\nax.plot(xdata, y, 'r', lw=2)\nax.plot(xdata, f(xdata, *beta_opt), 'b', lw=2)\nax.set_xlim(0, 5)\nax.set_xlabel(r\"$x$\", fontsize=14)\nax.set_ylabel(r\"$f(x, \\boldsymbol{\\beta})$\", fontsize=14)\nax.tick_params(which='both', direction='in')\n\n\n\n\n\n\n\n\n\nbeta_opt, beta_cov = optimize.curve_fit(f, xdata, ydata)\nbeta_opt  # a convenience wrapper around optimize.leastsq\n\narray([0.24368959, 0.7816449 , 0.5479457 ])",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>E</span>  <span class='chapter-title'>Optimization</span>"
    ]
  },
  {
    "objectID": "x_optimization.html#constrained-optimization",
    "href": "x_optimization.html#constrained-optimization",
    "title": "Appendix E — Optimization",
    "section": "E.6 Constrained optimization",
    "text": "E.6 Constrained optimization\n\nA simple form of constrained optimization is the optimization where the coordinate variables are subject to some bounds. These constraints are simple because they only restrict the range of the coordinate without dependencies on the other variables\nThis type of problem can be solved using the L-BFGS-B method in scipy, which is a variant of the BFGS method. This solver is available through the function optimize.fmin_l_bgfs_b or via optimize.minimize with the method argument set to ‘L-BFGS-B’. To define the coordinate boundaries, the bounds keyword argument must be used, and its value should be a list of tuples that contain the minimum and maximum value of each constrained variable\nConsider minimizing the objective function\n\\[ f(x) = (x_1 -1)^2 +(x_2 -1)^2 \\]\nsubject to the constraints\n\\[ 2 \\leq x_1 \\leq 3 \\;\\text{ and } \\; 0 \\leq x_2 \\leq 2 \\]\n\n\ndef f(X):\n  x_1, x_2 = X\n  return (x_1 -1)**2 +(x_2 -1)**2\n\n\nx_opt = optimize.minimize(f, (1, 1), method='BFGS').x\nx_opt\n\narray([1., 1.])\n\n\n\nbnd_x1, bnd_x2 = (2, 3), (0, 2)\nx_cons_opt = optimize.minimize(f, (1, 1), \n                      method='L-BFGS-B', bounds=[bnd_x1, bnd_x2]).x\nx_cons_opt\n\narray([2., 1.])\n\n\n\nx_ = y_ = np.linspace(-1, 3, 100)\nX, Y = np.meshgrid(x_, y_)\n\nfig, ax = plt.subplots(figsize=(7, 4))\n\nc = ax.contour(X, Y, func_X_Y_to_XY(f, X, Y), 50)\nax.plot(x_opt[0], x_opt[1], 'b*', markersize=10)\nax.plot(x_cons_opt[0], x_cons_opt[1], 'ro', markersize=10)\nbound_rect = plt.Rectangle((bnd_x1[0], bnd_x2[0]), \n                           bnd_x1[1] - bnd_x1[0], bnd_x2[1] - bnd_x2[0],\n                           facecolor=\"grey\")\nax.add_patch(bound_rect)\nax.tick_params(axis='x', pad=7)\nax.set_xlabel(r\"$x_1$\", fontsize=12)\nax.set_ylabel(r\"$x_2$\", fontsize=12)\nplt.colorbar(c, ax=ax)\nax.tick_params(which='both', direction='in')\n\n\n\n\n\n\n\n\n\n\nConstraints that are defined by equalities or inequalities that include more than one variable are more complicated to deal with. However, using the Lagrange multipliers, it is possible to convert a constrained optimization problem to an unconstrained problem by introducing additional variables\nFor example, consider the optimization problem \\(\\min_x f(x)\\) subject to the equality constraint \\(g(x)=0\\). In an unconstrained optimization problem the gradient of \\(f(x)\\) vanish at the optimal points, \\(\\nabla f(x)=0\\). It can be shown that the corresponding condition for constrained problems is that the negative gradient lies in the space supported by the constraint normal, \\(-\\nabla f(x) = \\lambda J_g^T(x)\\). Here \\(J_g(x)\\) is the Jacobian matrix of the constraint function \\(g(x)\\) and \\(\\lambda\\) is the vector of Lagrange multipliers (new variables). This condition is the gradient of the function \\(L(x,\\lambda) = f(x) +\\lambda^T g(x)\\), which is known as the Lagrangian function. Therefore, if both \\(f(x)\\) and \\(g(x)\\) have continuous and smooth, a stationary point \\((x_0, \\lambda)\\) of the \\(L(x,\\lambda)\\) corresponds to an optimum of the original constrained optimization problem, \\(x_0\\)\nConsider the problem of maximizing the volume of a rectangle with sides of length \\(x_0\\), \\(x_1\\) and \\(x_2\\), subject to the constraint that the total surface area should be unity:\n\\[ g(x) = 2x_1 x_2 +2 x_0x_2 +2 x_1 x_0 -1 = 0 \\]\nTo solve this optimization problem using Lagrange multipliers, we form the Lagrangian \\(L(x) = f(x) +\\lambda g(x)\\), and seek the stationary points for \\(L(x) = 0\\)\n\n\nx = x0, x1, x2, l = sympy.symbols(\"x_0, x_1, x_2, lambda\")\nf = x0 *x1 *x2\ng = 2 *(x0 *x1 +x1 *x2 +x2 *x0) -1\nL = f +l *g\n\n\ngrad_L = [sympy.diff(L, x_) for x_ in x]\nsols = sympy.solve(grad_L)\ndisplay(sols[0])\ndisplay(sols[1])\n\n\\(\\displaystyle \\left\\{ \\lambda : - \\frac{\\sqrt{6}}{24}, \\  x_{0} : \\frac{\\sqrt{6}}{6}, \\  x_{1} : \\frac{\\sqrt{6}}{6}, \\  x_{2} : \\frac{\\sqrt{6}}{6}\\right\\}\\)\n\n\n\\(\\displaystyle \\left\\{ \\lambda : \\frac{\\sqrt{6}}{24}, \\  x_{0} : - \\frac{\\sqrt{6}}{6}, \\  x_{1} : - \\frac{\\sqrt{6}}{6}, \\  x_{2} : - \\frac{\\sqrt{6}}{6}\\right\\}\\)\n\n\n\ng.subs(sols[0])\n\n\\(\\displaystyle 0\\)\n\n\n\nf.subs(sols[0])\n\n\\(\\displaystyle \\frac{\\sqrt{6}}{36}\\)\n\n\n\n\nThere exists various numerical methods of applying this approach. One example is the method known as sequential least squares programming, abbreviated as SLSQP, which is available in the scipy as the optimize.fmin_slsqp function and via optimize.minimize with method='SLSQP'\nThe optimize.minimize function takes the keyword argument constraints, which should be a list of dictionaries that each specifies a constraint. The allowed keys (values) in this dictionary are type ('eq' or 'ineq'), fun (constraint function), jac (Jacobian of the constraint function)\n\n\ndef f(X):\n  return -X[0] *X[1] *X[2]\n\ndef g(X):\n  return 2 *(X[0] *X[1] +X[1] *X[2] +X[2] *X[0]) -1\n\n\nconstraints = [dict(type='eq', fun=g)]\nx_cons_opt = optimize.minimize(f, [0.5, 1, 1.5], \n                      method='SLSQP', constraints=constraints)\nx_cons_opt\n\n     message: Optimization terminated successfully\n     success: True\n      status: 0\n         fun: -0.0680413686237617\n           x: [ 4.082e-01  4.083e-01  4.083e-01]\n         nit: 18\n         jac: [-1.667e-01 -1.667e-01 -1.667e-01]\n        nfev: 77\n        njev: 18\n multipliers: [-1.021e-01]\n\n\n\n\nTo solve problems with inequality constraints, all we need to do is to set type='ineq' in the constraint dictionary and provide the corresponding inequality function. To demonstrate minimization of a nonlinear objective function with a nonlinear inequality constraint, we return to the quadratic problem considered previously, but in this case with inequality constraint\n\\[ g(x) = x_1 -1.75 -(x_0 -0.75)^4 \\geq 0 \\]\n\n\ndef f(X):\n  return (X[0] -1)**2 + (X[1] -1)**2\n\ndef g(X):\n  return X[1] -1.75 -(X[0] -0.75)**4\n\n\nx_opt = optimize.minimize(f, (0, 0), method='BFGS').x\n\nconstraints = [dict(type='ineq', fun=g)]\nx_cons_opt = optimize.minimize(f, (0, 0), \n                      method='SLSQP', constraints=constraints).x\nx_cons_opt\n\narray([0.96857656, 1.75228252])\n\n\n\nx_ = y_ = np.linspace(-1, 3, 100)\nX, Y = np.meshgrid(x_, y_)\n\nfig, ax = plt.subplots(figsize=(7, 4))\n\nc = ax.contour(X, Y, func_X_Y_to_XY(f, X, Y), 50)\nax.plot(x_opt[0], x_opt[1], 'bo', markersize=10)\n\nax.plot(x_, 1.75 +(x_ -0.75)**4, 'k-', lw=2)\nax.fill_between(x_, 1.75 +(x_ -0.75)**4, 3, color=\"grey\")\nax.plot(x_cons_opt[0], x_cons_opt[1], 'ro', markersize=10)\n\nax.set_ylim(-1, 3)\nax.set_xlabel(r\"$x_0$\", fontsize=12)\nax.set_ylabel(r\"$x_1$\", fontsize=12)\nplt.colorbar(c, ax=ax)\nax.tick_params(which='both', direction='in')\n\n\n\n\n\n\n\n\n\nFor optimization problems with only inequality constraints, scipy provides an alternative solver using the constrained optimization by linear approximation (COBYLA) method. This solver is accessible either through optimize.fmin_cobyla or optimize.minimize with method='COBYLA'. The previous example could just as well have been solved with this solver, by replacing method='SLSQP’ with method='COBYLA'",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>E</span>  <span class='chapter-title'>Optimization</span>"
    ]
  },
  {
    "objectID": "x_optimization.html#linear-programming",
    "href": "x_optimization.html#linear-programming",
    "title": "Appendix E — Optimization",
    "section": "E.7 Linear programming",
    "text": "E.7 Linear programming\n\nThe solution to linear optimization problem must necessarily lie on a constraint boundary, so it is sufficient to search the vertices of the intersections of the linear constraints functions. This can be done efficiently in practice. A popular algorithm for this type of problems is known as simplex, which systematically moves from one vertix to another until the optimal vertix has been reached\nThere are also more recent interior point methods that efficiently solve linear programming problems. With these methods, linear programming problems with thousands of variables and constraints are readily solvable\nLinear programming problems are typically written in the so-called standard form:\n\\[ \\min_x \\mathbf{c}^T \\mathbf{x} \\]\nwhere\n\\[ \\mathbf{Ax} \\leq \\mathbf{b} \\; \\text{ and } \\; \\mathbf{x} \\geq \\mathbf{0}\\]\nHere \\(\\mathbf{c}\\) and \\(\\mathbf{x}\\) are vectors of length \\(n\\), and \\(\\mathbf{A}\\) is a \\(m \\times n\\) matrix and \\(\\mathbf{b}\\) a \\(m\\)-vector\nConsider the problem of minimizing the function\n\\[ f(\\mathbf{x}) = -x_0 +2x_1 -3x_2 \\]\nsubject to the three inequality constraints\n\\[ x_0 +x_1 \\leq 1, -x_0 +3x_1 \\leq 2, \\; \\text{ and } \\; -x_1 +x_2 \\leq 3\\]\nOn the standard form\n\\[\n  \\mathbf{c} =\n  \\begin{pmatrix}\n  -1 \\\\ \\phantom{-}2 \\\\ -3\n  \\end{pmatrix}, \\;\\;\n  \\mathbf{A} =\n  \\begin{pmatrix}\n  \\phantom{-}1 & \\phantom{-}1 & \\phantom{-}0 \\;\\\\\n  -1 & \\phantom{-}3 & \\phantom{-}0 \\;\\\\\n  \\phantom{-}0 & -1 & \\phantom{-}1 \\;\n  \\end{pmatrix}, \\;\\;\n  \\mathbf{b} =\n  \\begin{pmatrix}\n  1 \\\\ 2 \\\\ 3\n  \\end{pmatrix}\n\\]\nTo solve this problem, here we use the cvxopt library, which provides the linear programming solver with the cvxopt.solvers.lp function\n\n\nc = cvxopt.matrix([-1.0, 2.0, -3.0])\nA = cvxopt.matrix([[ 1.0, 1.0, 0.0],\n                   [-1.0, 3.0, 0.0],\n                   [ 0.0, -1.0, 1.0]])\nb = cvxopt.matrix([1.0, 2.0, 3.0])\n\n\nsol = cvxopt.solvers.lp(c, A, b)\n\n     pcost       dcost       gap    pres   dres   k/t\n 0: -9.0000e+00 -1.6500e+01  8e+00  5e-01  9e-01  1e+00\n 1: -9.1144e+00 -1.0550e+01  1e+00  1e-01  2e-01  3e-01\n 2: -4.5831e+01 -8.7539e+01  7e+03  5e+00  1e+01  5e+01\n 3: -5.5905e+01 -1.2363e+01  7e+02  2e-01  4e-01  5e+01\n 4: -4.7236e+03 -1.2363e+01  7e+04  2e-01  4e-01  5e+03\n 5: -4.7149e+05 -1.2363e+01  7e+06  2e-01  4e-01  5e+05\n 6: -4.7148e+07 -1.2363e+01  7e+08  2e-01  4e-01  5e+07\nCertificate of dual infeasibility found.\n\n\n\nx = np.array(sol['x'])\nx\n\narray([[-8.10074318],\n       [-6.34803624],\n       [-1.1984431 ]])\n\n\n\nsol['primal objective']\n\n\\(\\displaystyle -1.0\\)",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>E</span>  <span class='chapter-title'>Optimization</span>"
    ]
  },
  {
    "objectID": "x_optimization.html#quadratic-programming",
    "href": "x_optimization.html#quadratic-programming",
    "title": "Appendix E — Optimization",
    "section": "E.8 Quadratic programming",
    "text": "E.8 Quadratic programming\n\nQuadratic programming problems are typically written in the so-called standard form:\n\\[ \\min_\\mathbf{x} \\frac{1}{2} \\mathbf{x}^T \\mathbf{Q} \\mathbf{x} +\\mathbf{p}^T \\mathbf{x} \\]\nsubject to\n\\[ \\mathbf{Gx} \\leq \\mathbf{h} \\; \\text{ and } \\; \\mathbf{Ax} = \\mathbf{b} \\]\nQuadratic programs can be solved via the cvxopt.solvers.qp() function\nAs an example, consider the following QP:\n\\[ \\min_{x_1, x_2} 2x_1^2 +x_2^2 + x_1 x_2 +x_1 +x_2\\]\nsubject to\n\\[ x_1 \\geq 0,\\, x_2 \\geq 0 \\; \\text{ and } \\; x_1 + x_2 =1\\]\n\n\nQ = 2*cvxopt.matrix([[2, 0.5], [0.5, 1]])\np = cvxopt.matrix([1.0, 1.0])\nG = cvxopt.matrix([[-1.0, 0.0], [0.0,-1.0]])\nh = cvxopt.matrix([0.0, 0.0])\nA = cvxopt.matrix([1.0, 1.0], (1, 2))\nb = cvxopt.matrix(1.0)\nsol = cvxopt.solvers.qp(Q, p, G, h, A, b)\n\n     pcost       dcost       gap    pres   dres\n 0:  1.8889e+00  7.7778e-01  1e+00  3e-16  2e+00\n 1:  1.8769e+00  1.8320e+00  4e-02  2e-16  6e-02\n 2:  1.8750e+00  1.8739e+00  1e-03  1e-16  5e-04\n 3:  1.8750e+00  1.8750e+00  1e-05  1e-16  5e-06\n 4:  1.8750e+00  1.8750e+00  1e-07  3e-16  5e-08\nOptimal solution found.\n\n\n\nx = np.array(sol['x'])\nx\n\narray([[0.2500001],\n       [0.7499999]])\n\n\n\nsol['primal objective']\n\n\\(\\displaystyle 1.87500000000002\\)",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>E</span>  <span class='chapter-title'>Optimization</span>"
    ]
  },
  {
    "objectID": "x_integration.html",
    "href": "x_integration.html",
    "title": "Appendix G — Integration",
    "section": "",
    "text": "G.1 Importing modules\n\\(~\\)\nimport numpy as np\n\nfrom scipy import integrate\nimport sympy\nsympy.init_printing()\n\nimport matplotlib.pyplot as plt\nimport matplotlib as mpl\nfrom scipy import __version__\nprint(\"numpy: \", np.__version__)\nprint(\"sympy: \", sympy.__version__)\nprint(\"scipy: \", __version__)\n\nnumpy:  2.3.1\nsympy:  1.14.0\nscipy:  1.16.0",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>G</span>  <span class='chapter-title'>Integration</span>"
    ]
  },
  {
    "objectID": "x_integration.html#importing-modules",
    "href": "x_integration.html#importing-modules",
    "title": "Appendix G — Integration",
    "section": "H.1 Importing modules",
    "text": "H.1 Importing modules\n\nimport numpy as np\n\nfrom scipy import integrate\nimport sympy\nsympy.init_printing()\n\nimport matplotlib.pyplot as plt\nimport matplotlib as mpl\n\n\nfrom scipy import __version__\nprint(\"numpy: \", np.__version__)\nprint(\"sympy: \", sympy.__version__)\nprint(\"scipy: \", __version__)\n\nnumpy:  2.3.1\nsympy:  1.14.0\nscipy:  1.16.0",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>G</span>  <span class='chapter-title'>Integration</span>"
    ]
  },
  {
    "objectID": "x_integration.html#numerical-integration-methods",
    "href": "x_integration.html#numerical-integration-methods",
    "title": "Appendix G — Integration",
    "section": "G.2 Numerical integration methods",
    "text": "G.2 Numerical integration methods\n\nHere we are concerned with evaluating definite integrals on the form\n\\[I(f) = \\int_a^b \\,f(x)\\, dx\\]\nwith given integration limits \\(a\\) and \\(b\\). The interval \\([a,b]\\) can be finite, semi-infinite (where either \\(a=-\\infty\\) or \\(b=\\infty\\)), or infinite (where \\(a=-\\infty\\) and \\(b=\\infty\\))\n\\[ I(f) \\approx \\sum_{i=0}^{n-1} \\omega_i f(x_i) +r_n\\]\nQuadrature rules can be derived from interpolations of \\(f(x)\\) on the interval \\([a,b]\\). If the points \\(x_i\\) are evenly spaced in the interval \\([a,b]\\), and a polynomial interpolation is used, then the resulting quadrature rule is known as a Newton-Cotes quadrature rule\nFor instance, approximating \\(f(x)\\) with a zeroth order polynomial (constant value) using the midpoint value \\(x_0 = (a +b) /2\\), \\(\\,\\)we obtain\n\\[ I(f) \\approx f \\left( \\frac{b -a}{2} \\right)  \\int_a^b dx = (b -a) f \\left( \\frac{b -a}{2} \\right) \\]\nThis is known as the midpoint rule, and it integrates polynomials up to order one (linear functions) exactly, and it is therefore said to be of polynomial degree one\nApproximating \\(f(x)\\) by a polynomial of degree one, evaluated at the endpoints of the interval, results in\n\\[ I(f) \\approx \\frac{b -a}{2} \\left[ f(a) +f(b) \\right] \\]\nThis is known as the trapezoidal rule, and it is also of polynomial degree one\nUsing an interpolation polynomial of second order results in Simpson’s rule,\n\\[ I(f) \\approx \\frac{b -a}{6} \\left[ f(a) +4 f \\left( \\frac{a +b}{2} \\right) +f(b) \\right] \\]\nwhich uses function evaluations at the endpoints and the midpoint. This method is of polynomial degree three, meaning that it integrates exactly polynomials up to order three\n\n\na, b, X = sympy.symbols(\"a, b, x\")\nf = sympy.Function(\"f\")\n\nx = a, (a +b)/2, b # for Simpson's rule\nw = [sympy.symbols(f\"w_{i}\") for i in range(len(x))]\n\nq_rule = sum([w[i] *f(x[i]) for i in range(len(x))])\nq_rule\n\n\\(\\displaystyle w_{0} f{\\left(a \\right)} + w_{1} f{\\left(\\frac{a}{2} + \\frac{b}{2} \\right)} + w_{2} f{\\left(b \\right)}\\)\n\n\n\nTo compute the appropriate values of the weight factors \\(w_i\\), we choose the polynomial basis functions \\(\\{ \\phi_n(x) = x^n \\}_{n=0}^2\\) for the interpolation of \\(f(x)\\)\n\n\nphi = [sympy.Lambda(X, X**n) for n in range(len(x))]\nphi\n\n\\(\\displaystyle \\left[ \\left( x \\mapsto 1 \\right), \\  \\left( x \\mapsto x \\right), \\  \\left( x \\mapsto x^{2} \\right)\\right]\\)\n\n\n\neqs = [q_rule.subs(f, phi[n]) \n        -sympy.integrate(phi[n](X), (X, a, b)) \n          for n in range(len(phi))]\neqs\n\n\\(\\displaystyle \\left[ a - b + w_{0} + w_{1} + w_{2}, \\  \\frac{a^{2}}{2} + a w_{0} - \\frac{b^{2}}{2} + b w_{2} + w_{1} \\left(\\frac{a}{2} + \\frac{b}{2}\\right), \\  \\frac{a^{3}}{3} + a^{2} w_{0} - \\frac{b^{3}}{3} + b^{2} w_{2} + w_{1} \\left(\\frac{a}{2} + \\frac{b}{2}\\right)^{2}\\right]\\)\n\n\n\nw_sol = sympy.solve(eqs, w)\nw_sol\n\n\\(\\displaystyle \\left\\{ w_{0} : - \\frac{a}{6} + \\frac{b}{6}, \\  w_{1} : - \\frac{2 a}{3} + \\frac{2 b}{3}, \\  w_{2} : - \\frac{a}{6} + \\frac{b}{6}\\right\\}\\)\n\n\n\nq_rule.subs(w_sol).simplify()\n\n\\(\\displaystyle \\frac{\\left(a - b\\right) \\left(- f{\\left(a \\right)} - f{\\left(b \\right)} - 4 f{\\left(\\frac{a}{2} + \\frac{b}{2} \\right)}\\right)}{6}\\)\n\n\n\nWe recognize this result as Simpson’s quadrature rule given above. Choosing different sample points (the \\(x\\) tuple in this code), results in different quadrature rules\nHigher-order quadrature rules can similarly be derived using higher-order polynomial interpolation (more sample points in the \\([a,b]\\)). However, high-order polynomial interpolation can have undesirable behavior between the sample points\nRather than using higher-order quadrature rules, it is therefore often better to divide the integration interval \\([a,b]\\) into subintervals \\([a=x_0, x_1], [x_1, x_2], \\cdots, [x_{n-1},x_n = b]\\) and use a low-order quadrature rule in each of these sub-intervals. Such methods are known as composite quadrature rules\nAn important parameter that characterize composite quadrature rules is the sub-interval length \\(h=(b-a)/N\\). Estimates for the errors in an approximate quadrature rule, and the scaling of the error with respect to \\(h\\), can be obtained from Taylor series expansions of the integrand and the analytical integration of the term in the resulting series\nWe have seen that the Newton-Cotes quadrature rules uses evenly spaced sample points of the integrand \\(f(x)\\). However, this is not necessarily the most efficient choice of quadrature nodes, and then it can be advantageous to use quadrature rules that do not use evenly spaced sample points\nAn example of such a method is a Gaussian quadrature, which also uses polynomial interpolation to determine the values of the weight factors in the quadrature rule, but where the quadrature nodes \\(x_i\\) are chosen to maximize the order of polynomials that can be integrated exactly (the polynomial degree) given a fixed number of quadrature points",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>G</span>  <span class='chapter-title'>Integration</span>"
    ]
  },
  {
    "objectID": "x_integration.html#numerical-integration-with-scipy",
    "href": "x_integration.html#numerical-integration-with-scipy",
    "title": "Appendix G — Integration",
    "section": "G.3 Numerical integration with Scipy",
    "text": "G.3 Numerical integration with Scipy\n\nThe numerical quadrature routines in the scipy integrate module can be categorized into two types: routines that take the integrand as a python function, and routines that take arrays with samples of the integrand at given points\nThe functions of the first type use Gaussian quadrature (quad, quadrature, fixed_quad), while functions of the second type use Newton-Cotes methods (trapz, simps, and romb)\nAs a concrete example, consider the numerical evaluation of the integral\n\\[ \\int_{-1}^1 \\, e^{-x^2}\\, dx\\]\n\n\ndef f(x):\n  return np.exp(-x**2)\n\nval, err = integrate.quad(f, -1, 1)\n\n\nval, err\n\n\\(\\displaystyle \\left( 1.49364826562485, \\  1.65828269518814 \\cdot 10^{-14}\\right)\\)\n\n\n\nG.3.1 Extra arguments\n\nWe wish to evaluate\n\\[ \\int_{-1}^1 \\, a e^{-(x -b)^2/c^2} \\,dx \\]\nfor the specific values of the parameters \\(a=1\\), \\(b=2\\), and \\(c=3\\)\n\n\ndef f(x, a, b, c):\n  return a *np.exp(-((x -b)/c)**2)\n\nval, err = integrate.quad(f, -1, 1, args=(1, 2, 3))\n\n\nval, err\n\n\\(\\displaystyle \\left( 1.27630683510222, \\  1.41698523481695 \\cdot 10^{-14}\\right)\\)\n\n\n\n\nG.3.2 Reshuffle arguments\n\nWe wish to compute the integral\n\\[\\int_{0}^5 J_0(x) \\,dx\\]\nwhere the integrand \\(J_0(x)\\) is the zero-th order Bessel function of the first kind,\n\n\nfrom scipy.special import jv\n\nf = lambda x: jv(0, x)\n\nval, err = integrate.quad(f, 0, 5)\n\n\nval, err\n\n\\(\\displaystyle \\left( 0.715311917784768, \\  2.47260738289741 \\cdot 10^{-14}\\right)\\)\n\n\n\n\nG.3.3 Infinite limits\n\nConsider the integral \\[ \\int_{-\\infty}^\\infty e^{-x^2} \\,dx \\]\n\n\nf = lambda x: np.exp(-x**2)\n\nval, err = integrate.quad(f, -np.inf, np.inf, epsabs=1.49e-14, epsrel=1.49e-14)\n\n\nval, err\n\n\\(\\displaystyle \\left( 1.77245385090552, \\  2.04282393124645 \\cdot 10^{-14}\\right)\\)\n\n\n\n\nG.3.4 Singularity\n\nConsider the integral \\[ \\int_{-1}^1 \\frac{1}{\\sqrt{|x|}} \\,dx \\]\nThe integrand diverges at \\(x=0\\), but the value of the integral does not diverge, and its value is \\(4\\). Naively trying to compute this integral using quad may fail because of the diverging integrand:\n\n\nimport warnings\nwarnings.filterwarnings(\"error\")\n\nf = lambda x: 1/np.sqrt(abs(x))\n\na, b = -1, 1\n\ntry:\n  integrate.quad(f, a, b)\nexcept Exception as e:\n  print(e)\n\ndivide by zero encountered in scalar divide\n\n\n\nx = np.linspace(a, b, 10000)\n\nfig, ax = plt.subplots(figsize=(6, 4))\n\nax.plot(x, f(x), lw=2)\nax.fill_between(x, f(x), color='green', alpha=0.5)\nax.set_xlabel(\"$x$\", fontsize=12)\nax.set_ylabel(\"$f(x)$\", fontsize=12)\nax.set_xlim(-1, 1)\nax.set_ylim(0, 25)\nax.tick_params(which='both', direction='in')\n\n\n\n\n\n\n\n\n\nIn this case, the evaluation of the integral fails because the integrand diverges exactly at one of the sample points in the Gaussian quadrature rule (the midpoint). We can guide the quad routine by specifying a list of points that should be avoided using the points keyword arguments, and using points=[0] in the current example allows quad to correctly evaluate the integral:\n\n\nval, err = integrate.quad(f, a, b, points=[0])\n\n\nval, err\n\n\\(\\displaystyle \\left( 4.0, \\  2.04281036531029 \\cdot 10^{-14}\\right)\\)\n\n\n\n\nG.3.5 Tabulated integrand\n\nLet’s evaluate the integral \\(\\displaystyle\\int_0^2 \\sqrt{x}\\, dx\\) by taking \\(25\\) samples of the integrand in the integration interval \\([0, 2]\\),\n\n\nf = lambda x: np.sqrt(x)\n\na, b = 0, 2\nx = np.linspace(a, b, 25)\ny = f(x)\n\n#----------------\n\nfig, ax = plt.subplots(figsize=(6, 4))\n\nax.plot(x, y, 'bo')\n\nxx = np.linspace(a, b, 500)\nax.plot(xx, f(xx), 'b-')\nax.fill_between(xx, f(xx), color='green', alpha=0.5)\n\nax.set_xlim(0, 2)\nax.set_ylim(0, 1.6)\nax.set_xlabel(r\"$x$\", fontsize=12)\nax.set_ylabel(r\"$f(x)$\", fontsize=12)\nax.tick_params(which='both', direction='in')\n\n\n\n\n\n\n\n\n\nval_trapz = integrate.trapezoid(y, x)\nval_trapz\n\n\\(\\displaystyle 1.88082171605085\\)\n\n\n\nval_simps = integrate.simpson(y, x)\nval_simps\n\n\\(\\displaystyle 1.88366510244871\\)\n\n\n\nval_exact = 2.0/3.0 *(b-a)**(3.0/2.0)\nval_exact\n\n\\(\\displaystyle 1.88561808316413\\)\n\n\n\nval_exact -val_trapz\n\n\\(\\displaystyle 0.00479636711327625\\)\n\n\n\nval_exact -val_simps\n\n\\(\\displaystyle 0.00195298071541172\\)\n\n\n\n\nx = np.linspace(a, b, 1 +2**6)\ny = f(x)\ndx = x[1] -x[0]\n\nval_exact -integrate.romb(y, dx=dx)\n\n\\(\\displaystyle 0.000378798422913107\\)\n\n\n\nval_exact -integrate.simpson(y, dx=dx)\n\n\\(\\displaystyle 0.000448485554158218\\)",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>G</span>  <span class='chapter-title'>Integration</span>"
    ]
  },
  {
    "objectID": "x_integration.html#multiple-integration",
    "href": "x_integration.html#multiple-integration",
    "title": "Appendix G — Integration",
    "section": "G.4 Multiple integration",
    "text": "G.4 Multiple integration\n\nThe double integral routine dblquad can evaluate integrals on the form\n\\[\\int_a^b \\int_{g(y)}^{h(y)} f(x,y)\\, dxdy \\]\nand it has the function signature dblquad(f, a, b, g, h), \\(~\\)where f is a python function for the integrand, a and b are constant integration limits along the \\(y\\) dimension, and g and h are python functions (taking \\(y\\) as argument) that specify the integration limits along the \\(x\\) dimension\nConsider the integral \\(\\displaystyle\\int_0^1 \\int_0^1 e^{-(x^2+y^2)}\\,dxdy\\),\n\n\ndef f(x, y):\n  return np.exp(-x**2 -y**2)\n\na, b = 0, 1\ng = lambda y: 0\nh = lambda y: 1\n\n\nintegrate.dblquad(f, a, b, g, h)\n\n\\(\\displaystyle \\left( 0.557746285351034, \\  8.29137438153541 \\cdot 10^{-15}\\right)\\)\n\n\n\nThe tplquad function can compute integrals on the form\n\\[ \\int_a^b \\int_{g(z)}^{h(z)} \\int_{q(y, z)}^{r(y, z)} f(x,y,z)\\,dxdydz \\]\nConsider the generalization of the previous integral to three variables:\n\\[\\int_0^1 \\int_0^1 \\int_0^1 e^{-(x^2+y^2+z^2)}\\,dxdydz\\]\n\n\ndef f(x, y, z):\n  return np.exp(-x**2 -y**2 -z**2)\n\na, b = 0, 1\ng, h = lambda z: 0, lambda z: 1\nq, r = lambda y, z: 0, lambda y, z: 1\n\nintegrate.tplquad(f, 0, 1, g, h, q, r)\n\n\\(\\displaystyle \\left( 0.416538385886638, \\  8.29133528731443 \\cdot 10^{-15}\\right)\\)\n\n\n\nFor arbitrary number of integrations, we can use the nquad function\n\n\nintegrate.nquad(f, [(0, 1), (0, 1), (0, 1)])\n\n\\(\\displaystyle \\left( 0.416538385886638, \\  8.29133528731443 \\cdot 10^{-15}\\right)\\)\n\n\n\n\ndef f(*args):\n  return np.exp(-np.sum(np.array(args)**2))\n\n\n%time integrate.nquad(f, [(0, 1)] *1)\n\nCPU times: user 115 μs, sys: 65 μs, total: 180 μs\nWall time: 183 μs\n\n\n\\(\\displaystyle \\left( 0.746824132812427, \\  8.29141347594073 \\cdot 10^{-15}\\right)\\)\n\n\n\n%time integrate.nquad(f, [(0, 1)] *2)\n\nCPU times: user 1.18 ms, sys: 67 μs, total: 1.25 ms\nWall time: 1.25 ms\n\n\n\\(\\displaystyle \\left( 0.557746285351034, \\  8.29137438153541 \\cdot 10^{-15}\\right)\\)\n\n\n\n%time integrate.nquad(f, [(0, 1)] *3)\n\nCPU times: user 24.8 ms, sys: 416 μs, total: 25.2 ms\nWall time: 25.3 ms\n\n\n\\(\\displaystyle \\left( 0.416538385886638, \\  8.29133528731443 \\cdot 10^{-15}\\right)\\)\n\n\n\n%time integrate.nquad(f, [(0, 1)] *4)\n\nCPU times: user 504 ms, sys: 2.55 ms, total: 507 ms\nWall time: 507 ms\n\n\n\\(\\displaystyle \\left( 0.311080918822877, \\  8.29129619327777 \\cdot 10^{-15}\\right)\\)",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>G</span>  <span class='chapter-title'>Integration</span>"
    ]
  },
  {
    "objectID": "x_integration.html#symbolic-and-arbitrary-precision-integration",
    "href": "x_integration.html#symbolic-and-arbitrary-precision-integration",
    "title": "Appendix G — Integration",
    "section": "G.5 Symbolic and arbitrary-precision integration",
    "text": "G.5 Symbolic and arbitrary-precision integration\n\nFor example, to compute the integral \\(\\displaystyle\\int_{-1}^{1} 2\\sqrt{1-x^2}\\,dx\\), \\(~\\)we first create a symbol for \\(x\\), and define expressions for the integrand and the integration\n\n\nx = sympy.symbols(\"x\")\nf = 2 *sympy.sqrt(1 -x**2)\n\na, b = -1, 1\nval_sym = sympy.integrate(f, (x, a, b))\nval_sym\n\n\\(\\displaystyle \\pi\\)\n\n\n\nAs pointed out earlier, this situation is the exception, and in general we will not be able to find an analytical closed-form expression. We then need to resort to numerical quadrature, for example, using scipy’s integrate.quad\nHowever, the mpmath library, which comes bundled with sympy, \\(~\\)provides an alternative implementation of numerical quadrature, using multiple-precision computations. With this library, we can evaluate an integral to arbitrary precision, without being restricted to the limitations of floating-point numbers\nFor example, if we require 75 accurate decimal places, we set:\n\n\nimport mpmath\n\n\nmpmath.mp.dps = 75\nf_mpmath = sympy.lambdify(x, f, 'mpmath')\n\n\nval = mpmath.quad(f_mpmath, (a, b))\nsympy.sympify(val)\n\n\\(\\displaystyle 3.14159265358979323846264338327950288419716939937510582097494459230781640629\\)\n\n\n\nsympy.N(val_sym, mpmath.mp.dps +1) -val\n\n\\(\\displaystyle 6.90893484407555570030908149024031965689280029154902510801896277613487344253 \\cdot 10^{-77}\\)\n\n\n\n\n%time mpmath.quad(f_mpmath, [a, b])\n\nCPU times: user 1.22 ms, sys: 237 μs, total: 1.46 ms\nWall time: 1.46 ms\n\n\nmpf('3.14159265358979323846264338327950288419716939937510582097494459230781640628613')\n\n\n\nf_numpy = sympy.lambdify(x, f, 'numpy')\n%time integrate.quad(f_numpy, a, b)\n\nCPU times: user 253 μs, sys: 16 μs, total: 269 μs\nWall time: 271 μs\n\n\n\\(\\displaystyle \\left( 3.1415926535898, \\  2.00047223231081 \\cdot 10^{-9}\\right)\\)\n\n\n\nG.5.1 Double and triple integrals\n\nThe mpmath library’s quad function can also be used to evaluate double and triple integrals.\nFor example, to compute the double integral:\n\\[ \\int_0^1 \\int_0^1 \\cos(x) \\cos(y)\\, e^{-(x^2+y^2)}\\, dxdy \\]\nand the triple integral:\n\\[ \\int_0^1 \\int_0^1 \\int_0^1 \\cos(x) \\cos(y) \\cos(z)\\, e^{-(x^2+y^2+z^2)}\\, dx dy dz \\]\nto 30 significant decimals (this example cannot be solved symbolically with sympy)\n\n\nx, y, z = sympy.symbols('x, y, z')\n\n\nf2 = sympy.cos(x) *sympy.cos(y) *sympy.exp(-x**2 -y**2)\nf3 = sympy.cos(x) *sympy.cos(y) *sympy.cos(z) *sympy.exp(-x**2 -y**2 -z**2)\n\nf2_mpmath = sympy.lambdify((x, y), f2, 'mpmath')\nf3_mpmath = sympy.lambdify((x, y, z), f3, 'mpmath')\n\n\nmpmath.mp.dps = 30\n\nres2 = mpmath.quad(f2_mpmath, (0, 1), (0, 1))\nres3 = mpmath.quad(f3_mpmath, (0, 1), (0, 1), (0, 1))\n\n\nsympy.sympify(res2), sympy.sympify(res3)\n\n\\(\\displaystyle \\left( 0.430564794306099099242308990196, \\  0.282525579518426896867622772405\\right)\\)",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>G</span>  <span class='chapter-title'>Integration</span>"
    ]
  },
  {
    "objectID": "x_integration.html#integral-transforms",
    "href": "x_integration.html#integral-transforms",
    "title": "Appendix G — Integration",
    "section": "G.6 Integral transforms",
    "text": "G.6 Integral transforms\n\nIn general, an integral transform of a function \\(f(t)\\) can be written as\n\\[ T_f(u) = \\int_{t_1}^{t_2} \\, K(t, u) f(t) \\,dt\\]\nwhere \\(T_f(u)\\) is the transformed function. The choice of the kernel \\(K(t, u)\\) and the integration limits determines the type of integral transform. The inverse of the integral transform is given by\n\\[ f(t)=\\int_{u_1}^{u_2} K^{-1}(u, t) \\, T_f(u) \\, du\\]\nwhere \\(K^{-1} (u,t)\\) is the kernel of the inverse transform\nSympy provides functions for several types of integral transform, but here we focus on the Laplace transform\n\\[ L_f(s) = \\int_0^{\\infty} e^{-st} f(t) \\,dt \\]\nwith the inverse transform\n\\[ f(t) = \\frac{1}{2\\pi i} \\int_{\\gamma -i\\infty}^{\\gamma +i \\infty} e^{st} L_f(s)\\,ds\\]\nand the Fourier transform\n\\[ F_f(\\omega) = \\frac{1}{\\sqrt{2\\pi}} \\int_{-\\infty}^{\\infty} e^{-i\\omega t} f(t)\\,dt\\]\nwith the inverse transform\n\\[ f(t) = \\frac{1}{\\sqrt{2\\pi}} \\int_{-\\infty}^{\\infty} e^{i\\omega t} F_f(\\omega)\\,d\\omega\\]\nWith sympy, we can perform these transforms with the\n\nsympy.laplace_transform and sympy.fourier_transform, respectively,\n\nand the corresponding inverse transforms can be computed with the\n\nsympy.inverse_laplace_transform and sympy.inverse_fourier_transform\n\n\n\ns = sympy.symbols('s')\na, t = sympy.symbols('a, t', positive=True)\n\nf = sympy.sin(a*t)\nsympy.laplace_transform(f, t, s)\n\n\\(\\displaystyle \\left( \\frac{a}{a^{2} + s^{2}}, \\  0, \\  \\text{True}\\right)\\)\n\n\n\nF = sympy.laplace_transform(f, t, s, noconds=True)\nF\n\n\\(\\displaystyle \\frac{a}{a^{2} + s^{2}}\\)\n\n\n\nsympy.inverse_laplace_transform(F, s, t, noconds=True)\n\n\\(\\displaystyle \\sin{\\left(a t \\right)}\\)\n\n\n\n\n[sympy.laplace_transform(f, t, s, noconds=True) for f in [t, t**2, t**3, t**4]]\n\n\\(\\displaystyle \\left[ \\frac{1}{s^{2}}, \\  \\frac{2}{s^{3}}, \\  \\frac{6}{s^{4}}, \\  \\frac{24}{s^{5}}\\right]\\)\n\n\n\na = sympy.symbols('a', positive=True)\nsympy.laplace_transform(t**a, t, s, noconds=True)\n\n\\(\\displaystyle s^{- a - 1} \\Gamma\\left(a + 1\\right)\\)\n\n\n\nsympy.laplace_transform((1 -a*t) *sympy.exp(-a*t), t, s, noconds=True)\n\n\\(\\displaystyle - \\frac{a}{\\left(a + s\\right)^{2}} + \\frac{1}{a + s}\\)\n\n\n\n\nx, w = sympy.symbols(\"x, omega\")\nf = sympy.exp(-x**2)\n\nF = sympy.fourier_transform(f, x, w)\nF\n\n\\(\\displaystyle \\sqrt{\\pi} e^{- \\pi^{2} \\omega^{2}}\\)\n\n\n\nsympy.inverse_fourier_transform(F, w, x)\n\n\\(\\displaystyle e^{- x^{2}}\\)",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>G</span>  <span class='chapter-title'>Integration</span>"
    ]
  },
  {
    "objectID": "x_PDE_FEniCS.html",
    "href": "x_PDE_FEniCS.html",
    "title": "Appendix K — The FEniCS computing platform",
    "section": "",
    "text": "K.1 Getting started\nFEniCS is a popular open-source computing platform for solving partial differential equations (PDEs) with the finite element method (FEM). FEniCS enables users to quickly translate scientific models into efficient finite element code. With the high-level Python and C++ interfaces to FEniCS, it is easy to get started, but FEniCS offers also powerful capabilities for more experienced programmers. FEniCS runs on a multitude of platforms ranging from laptops to high-performance computers\nThe latest stable release of FEniCSx is version 0.9, which was released in October 2024. The easiest way to start using FEniCSx on MacOS and other systems is to install it using conda:\nimport dolfinx\nprint(f'DOLFINx version: {dolfinx.__version__}')\n\nDOLFINx version: 0.9.0",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>K</span>  <span class='chapter-title'>The FEniCS computing platform</span>"
    ]
  },
  {
    "objectID": "x_PDE_FEniCS.html#sec-fenicsx-getting-started",
    "href": "x_PDE_FEniCS.html#sec-fenicsx-getting-started",
    "title": "Appendix K — The FEniCS computing platform",
    "section": "",
    "text": "$ conda create -n fenicsx\n$ conda activate fenicsx\n$ conda install -c conda-forge fenics-dolfinx mpich pyvista \n$ conda install -c conda-forge petsc petsc4py\n$ conda install ipykernel\n$ python -m ipykernel install \\\n&gt;       --user --name fenicsx --display-name \"FEniCSx\"",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>K</span>  <span class='chapter-title'>The FEniCS computing platform</span>"
    ]
  },
  {
    "objectID": "x_PDE_FEniCS.html#sec-fenicsx-overview",
    "href": "x_PDE_FEniCS.html#sec-fenicsx-overview",
    "title": "Appendix K — The FEniCS computing platform",
    "section": "K.2 An Overview of the FEniCS Project",
    "text": "K.2 An Overview of the FEniCS Project\n\nThe FEniCS Project is a research and software initiative focused on developing mathematical methods and software for solving partial differential equations (PDEs). Its goal is to provide intuitive, efficient, and flexible tools for scientific computing. Launched in 2003, the project is the result of collaboration among researchers from universities and research institutes worldwide. For the latest updates and more information, visit the FEniCS Project\nThe latest version of the FEniCS project, FEniCSx, consists of several building blocks, namely DOLFINx, UFL, FFCx, and Basix. We will now go through the main objectives of each of these building blocks\n\nDOLFINx is the high performance C++ backend of FEniCSx, where structures such as meshes, function spaces and functions are implemented. Additionally, DOLFINx also contains compute intensive functions such as finite element assembly and mesh refinement algorithms. It also provides an interface to linear algebra solvers and data-structures, such as PETSc\nUFL is a high-level form language for describing variational formulations with a high-level mathematical syntax\nFFCx is the form compiler of FEniCSx; given variational formulations written with UFL, it generates efficient C code\nBasix is the finite element backend of FEniCSx, responsible for generating finite element basis functions",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>K</span>  <span class='chapter-title'>The FEniCS computing platform</span>"
    ]
  },
  {
    "objectID": "x_PDE_FEniCS.html#what-you-will-learn",
    "href": "x_PDE_FEniCS.html#what-you-will-learn",
    "title": "Appendix K — The FEniCS computing platform",
    "section": "K.3 What you will learn",
    "text": "K.3 What you will learn\nThe goal of this tutorial is to demonstrate how to apply the finite element to solve PDEs using FEniCS. Through a series of examples, we will demonstrate how to:\n\nSolve linear PDEs (such as the Poisson equation)\nSolve time-dependent PDEs (such as the heat equation)\nSolve non-linear PDEs\nSolve systems of time-dependent non-linear PDEs\n\nImportant topics include: how to set boundary conditions of various types (Dirichlet, Neumann, Robin), how to create meshes, how to define variable coefficients, how to interact with linear and non-linear solvers, and how to post-process and visualize solutions",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>K</span>  <span class='chapter-title'>The FEniCS computing platform</span>"
    ]
  },
  {
    "objectID": "x_PDE_FEniCS.html#sec-fenicsx-Poisson",
    "href": "x_PDE_FEniCS.html#sec-fenicsx-Poisson",
    "title": "Appendix K — The FEniCS computing platform",
    "section": "K.4 Solving the Poisson equation",
    "text": "K.4 Solving the Poisson equation\nAuthors: Hans Petter Langtangen, Anders Logg, Jørgen S. Dokken\nThe goal of this section is to solve one of the most basic PDEs, the Poisson equation, with a few lines of code in FEniCSx. We start by introducing some fundamental FEniCSx objects, such as functionspace,Function, TrialFunction and TestFunction, and learn how to write a basic PDE solver. This will include:\n\nHow to formulate a mathematical variational problem\nHow to apply boundary conditions\nHow to solve the discrete linear system\nHow to visualize the solution\n\nThe Poisson equation is the following boundary-value problem:\n\\[\\begin{aligned}\n  -\\nabla^2 u(\\mathbf{x}) &= f(\\mathbf{x})&&\\mathbf{x} \\in \\Omega\\\\\n  u(\\mathbf{x}) &= u_D(\\mathbf{x})&& \\mathbf{x} \\in \\partial\\Omega\n\\end{aligned}\\]\nHere, \\(u=u(\\mathbf{x})\\) is the unknown function, \\(f=f(\\mathbf{x})\\) is a prescribed function, \\(\\nabla^2\\) (often written as \\(\\Delta\\)) is the Laplace operator, \\(\\Omega\\) is the spatial domain, and \\(\\partial\\Omega\\) is its boundary. The Poisson problem — consisting of the PDE \\(-\\nabla^2 u = f\\) together with the boundary condition \\(u=u_D\\) on \\(\\partial\\Omega\\) — is a boundary value problem that must be precisely defined before we can solve it numerically with FEniCSx\n\nIn the two-dimensional space with coordinates \\(x\\) and \\(y\\), we can expand the Poisson equation as\n\n\\[-\\frac{\\partial^2 u}{\\partial x^2} - \\frac{\\partial^2 u}{\\partial y^2} = f(x,y)\\]\nThe unknown \\(u\\) is now a function of two variables, \\(u=u(x,y)\\), defined over the two-dimensional domain \\(\\Omega\\)\n\nThe Poisson equation arises in numerous physical contexts, including heat conduction, electrostatics, diffusion of substances, twisting of elastic rods, inviscid fluid flow, and water waves. Moreover, the equation appears in numerical splitting strategies for more complicated systems of PDEs, in particular the Navier–Stokes equations\n\nSolving a boundary value problem in FEniCSx consists of the following steps:\n\nIdentify the computational domain \\(\\Omega\\), the PDE, and its corresponding boundary conditions and source terms \\(f\\)\nReformulate the PDE as a finite element variational problem\nWrite a Python program defining the computational domain, the boundary conditions, the variational problem, and the source terms, using FEniCSx\nRun the Python program to solve the boundary-value problem. Optionally, you can extend the program to derive quantities such as fluxes and averages, and visualize the results\n\nAs we have already covered step 1, we shall now cover steps 2-4\n\nK.4.1 Finite element variational formulation\nFEniCSx is based on the finite element method, which is a general and efficient mathematical technique for the numerical solution of PDEs. The starting point for finite element methods is a PDE expressed in variational form\nThe basic recipe for turning a PDE into a variational problem is:\n\nMultiply the PDE by a function \\(v\\)\nIntegrate the resulting equation over the domain \\(\\Omega\\)\nPerform integration by parts of those terms with second order derivatives\n\nThe function \\(v\\) that multiplies the PDE is called a test function, while the unknown function \\(u\\) to be approximated is referred to as a trial function. The terms trial function and test function are also used in FEniCSx. Both test and trial functions belong to certain specific function spaces that define their properties\n\nIn the present case, we multiply the Poisson equation by a test function \\(v\\) and integrate over \\(\\Omega\\):\n\\[\\int_\\Omega (-\\nabla^2 u) v~\\mathrm{d} x = \\int_\\Omega f v~\\mathrm{d} x\\]\nHere \\(\\mathrm{d} x\\) denotes the differential element for integration over the domain \\(\\Omega\\). We will later let \\(\\mathrm{d} s\\) denote the differential element for integration over \\(\\partial\\Omega\\), the boundary of \\(\\Omega\\)\nA rule of thumb when deriving variational formulations is that one tries to keep the order of derivatives of \\(u\\) and \\(v\\) as small as possible. Here, we have a second-order differential of \\(u\\), which can be transformed to a first derivative by employing the technique of integration by parts. The formula reads\n\\[-\\int_\\Omega (\\nabla^2 u)v~\\mathrm{d}x\n= \\int_\\Omega\\nabla u\\cdot\\nabla v~\\mathrm{d}x -\n\\underbrace{\\int_\\Omega \\nabla u \\cdot v ~\\mathrm{d}x}_{\\displaystyle \\scriptsize\\int_{\\partial\\Omega}\\frac{\\partial u}{\\partial n}v~\\mathrm{d}s}\\]\nwhere \\(\\frac{\\partial u}{\\partial n}=\\nabla u \\cdot \\mathbf{n}\\) is the derivative of \\(u\\) in the outward normal direction \\(\\mathbf{n}\\) on the boundary\nAnother feature of variational formulations is that the test function \\(v\\) must vanish on the parts of the boundary where the solution \\(u\\) is prescribed\nIn the present problem, this means that \\(v = 0\\) on the entire boundary \\(\\partial\\Omega\\). Consequently, the second term in the integration by parts formula vanishes, and we obtain\n\\[\\int_\\Omega \\nabla u \\cdot \\nabla v~\\mathrm{d} x = \\int_\\Omega f v~\\mathrm{d} x\\]\nIf we require that this equation holds for all test functions \\(v\\) in some suitable space \\(\\hat{V}\\), the so-called test space, we obtain a well-defined mathematical problem that uniquely determines the solution \\(u\\), which lies in some function space \\(V\\). Note that \\(V\\) does not necessarily coincide with \\(\\hat{V}\\). We call the space \\(V\\) the trial space. The equation above is referred to as the weak form(or variational form) of the original boundary-value problem. We can now state our variational problem more precisely: \\(~\\) Find \\(u\\in V\\) such that\n\\[\\int_\\Omega \\nabla u \\cdot \\nabla v~\\mathrm{d} x = \\int_\\Omega f v~\\mathrm{d} x\\qquad \\forall v \\in \\hat{V}\\]\nFor the present problem, the trial and test spaces, \\(V\\) and \\(\\hat{V}\\), are defined as follows\n\\[\\color{red}{\\begin{aligned}\n   V&=\\{v\\in H^1(\\Omega) \\,\\vert\\, v=u_D \\;\\text{on } \\partial \\Omega \\}\\\\\n   \\hat{V}&=\\{v\\in H^1(\\Omega) \\,\\vert\\, v=0 \\;\\text{on } \\partial \\Omega \\}\n\\end{aligned}}\\]\nIn short, \\(H^1(\\Omega)\\) is the Sobolev space consisting of functions \\(v\\) for which both \\(v^2\\) and \\(\\lvert \\nabla v \\rvert^2\\) have finite integrals over \\(\\Omega\\). The solution of the underlying PDE must belong to a function space in which derivatives are continuous. However, the Sobolev space \\(H^1(\\Omega)\\) also admits functions with discontinuous derivatives\nThis weaker continuity requirement in the weak formulation (arising from the integration by parts) is crucial for constructing finite element function spaces. In particular, it permits the use of piecewise polynomial function spaces. Such spaces are built by stitching together polynomial functions over simple domains, such as intervals, triangles, quadrilaterals, tetrahedra, and hexahedra\nThe variational problem is a continuous problem: it defines the solution \\(u\\) in the infinite-dimensional function space \\(V\\). The finite element method for the Poisson equation approximates this solution by replacing the infinite-dimensional function spaces \\(V\\) and \\(\\hat{V}\\), with discrete (finite-dimensional) spaces \\(V_h\\subset V\\) and \\(\\hat{V}_h \\subset \\hat{V}\\). The resulting discrete variational problem is then stated as: \\(~\\) Find \\(u_h\\in V_h\\) such that\n\\[\\color{red}{\n\\begin{aligned}\n   \\int_\\Omega \\nabla u_h \\cdot \\nabla v~\\mathrm{d} x &= \\int_\\Omega fv~\\mathrm{d} x && \\forall v \\in \\hat{V}_h\n\\end{aligned}}\n\\]\nThis variational problem, together with appropriate definitions of \\(V_h\\) and \\(\\hat{V}_h\\), uniquely determines our approximate numerical solution to the Poisson equation. Note that the boundary condition is incorporated into the trial and test spaces. While this may appear complicated at first, it ensures that the finite element variational problem has the same form as the continuous variational problem\n\n\n\nK.4.2 Abstract finite element variational formulation\nWe will introduce the following notation for variational problems: \\(\\,\\) Find \\(u\\in V\\) such that\n\\[\\begin{aligned}\n  a(u,v)&=L(v)&& \\forall v \\in \\hat{V}\n\\end{aligned}\\]\nFor the Poisson equation, we have:\n\\[\\begin{aligned}\na(u,v) &= \\int_{\\Omega} \\nabla u \\cdot \\nabla v~\\mathrm{d} x\\\\\nL(v) &= \\int_{\\Omega} fv~\\mathrm{d} x\n\\end{aligned}\\]\nIn the literature \\(a(u,v)\\) is known as the bilinear form and \\(L(v)\\) as a linear form. For every linear problem, we will identify all terms with the unknown \\(u\\) and collect them in \\(a(u,v)\\), and collect all terms with only known functions in \\(L(v)\\).\nTo solve a linear PDE in FEniCSx, such as the Poisson equation, a user thus needs to perform two steps:\n\nChoose the finite element spaces \\(V\\) and \\(\\hat{V}\\) by specifying the domain (the mesh) and the type of function space (polynomial degree and type)\nExpress the PDE as a (discrete) variational problem: \\(\\,\\) Find \\(u\\in V\\) such that \\(a(u,v)=L(v)\\) for all \\(v \\in \\hat{V}\\)\n\n\n\nK.4.3 Implementation\nIn this section, you will learn:\n\nHow to use the built-in meshes in DOLFINx\nHow to create a spatially varying Dirichlet boundary conditions on the whole domain boundary\nHow to define a weak formulation of your PDE\nHow to solve the resulting system of linear equations\nHow to visualize the solution using a variety of tools\nHow to compute the \\(L^2(\\Omega)\\) error and the error at mesh vertices\n\nUp to this point, we’ve looked at the Poisson problem in very general terms: the domain \\(\\Omega\\), the boundary condition \\(u_D\\), and the right-hand side \\(f\\) were all left unspecified. To actually solve something, we now need to pick concrete choices for \\(\\Omega\\), \\(u_D\\), and \\(f\\)\nA good strategy is to set up the problem in a way that we already know the exact solution. That way, we can easily check whether our numerical solution is correct. Polynomials of low degree are usually the best choice here, because continuous Galerkin finite element spaces of degree \\(r\\) can reproduce any polynomial of degree \\(r\\) exactly\n\nTo test our solver, we’ll construct a problem where we already know the exact solution. This approach is known as the method of manufactured solutions. The idea is simple:\n\nStart by picking a function \\(u_e(x,y)\\) that looks nice and simple\nPlug \\(u_e\\) into the PDE to figure out what the right-hand side \\(f(x,y)\\) should be\nUse \\(u_e\\) as the boundary condition \\(u_D\\)\nFinally, solve the problem numerically and compare the computed solution with \\(u_e\\)\n\n\nStep 1: Choosing the exact solution\nLet’s take a quadratic function in 2D:\n\\[ u_e(x,y) = 1 + x^2 + 2y^2 \\]\nStep 2: Computing the source term\nIf we insert \\(u_e\\) into the Poisson equation, we obtain\n\\[f(x,y) = -6,\n\\;\\;\nu_D(x,y) = u_e(x,y) = 1 + x^2 + 2y^2\\]\nNotice that this holds regardless of the domain shape, as long as we prescribe \\(u_e\\) on the boundary\nStep 3: Choosing the domain\nFor simplicity, let’s use the unit square:\n\\[\\Omega = [0,1] \\times [0,1]\\]\nStep 4: Summary\nThis small example illustrates a very powerful strategy:\n\nPick an exact solution\nPlug it into the PDE to generate the corresponding source term\nSolve the PDE with these inputs\nVerify that the numerical solution reproduces the exact solution\n\nThis workflow is at the heart of the method of manufactured solutions, and it provides a simple but rigorous way to validate our solver\nGenerating simple meshes\nThe next step is to define the discrete domain, called the mesh. We do this using one of FEniCSx’s built-in mesh generators\nHere, we create a unit square mesh spanning \\([0,1]\\times[0,1]\\). The cells of the mesh can be either triangles or quadrilaterals:\n\nimport numpy as np\n\nfrom mpi4py import MPI\nfrom dolfinx import mesh\n\nN = 8\ndomain = mesh.create_unit_square(\n  MPI.COMM_WORLD, \n  nx=N, \n  ny=N, \n  cell_type=mesh.CellType.quadrilateral\n)\n\nNotice that we need to provide an MPI communicator. This determines how the program behaves in parallel:\n\nIf we pass MPI.COMM_WORLD, a single mesh is created and distributed across the number of processors we specify. For example, to run the program on two processors, we can use:\n\n$ mpirun -n 2 python tutorial_poisson.py\n\nIf instead we use MPI.COMM_SELF, each processor will create its own independent copy of the mesh. This can be useful when running many small problems in parallel, for example when sweeping over different parameters\n\nDefining the finite element function space\nOnce the mesh is created, the next step is to define the finite element function space \\(V\\). DOLFINx supports a wide variety of finite element spaces of arbitrary order. For a full overview, see the list of Supported elements in DOLFINx\nWhen creating a function space, we need to specify:\n\nThe mesh on which the space is defined\nThe element family (e.g., Lagrange, Raviart–Thomas, etc.)\nThe polynomial degree of the element\n\nIn DOLFINx, this can be done by passing a tuple of the form (\"family\", degree), as shown below:\n\nfrom dolfinx import fem\n\nV = fem.functionspace(domain, (\"Lagrange\", 1))\n\nSee Degree 1 Lagrange on a quadrilateral\nThe next step is to create a function that stores the Dirichlet boundary condition. We then use interpolation to fill this function with the prescribed values\n\nuD = fem.Function(V)\nuD.interpolate(lambda x: 1 +x[0]**2 +2 *x[1]**2)\n\nWith the boundary data defined (which, in this case, coincides with the exact solution of our finite element problem), we now need to enforce it along the boundary of the mesh\nThe first step is to identify which parts of the mesh correspond to the outer boundary. In DOLFINx, the boundary is represented by facets (that is, the line segments making up the outer edges in 2D or surfaces in 3D).\nWe can extract the indices of all exterior facets using:\n\ntdim = domain.topology.dim\nfdim = tdim -1\n\ndomain.topology.create_connectivity(fdim, tdim)\nboundary_facets = mesh.exterior_facet_indices(domain.topology)\n\nThis gives us the set of facets lying on the boundary of our discrete domain. Once we know where the boundary is, we can proceed to apply the Dirichlet boundary conditions to all degrees of freedom (DoFs) located on these facets\nFor our current problem, we are using the “Lagrange” degree-1 function space. In this case, the degrees of freedom (DoFs) are located at the vertices of each cell, so every boundary facet contains exactly two DoFs\nTo identify the local indices of these boundary DoFs, we use dolfinx.fem.locate_dofs_topological. This function takes three arguments:\n\nthe function space\nthe dimension of the mesh entities we want to target, and\nthe list of entities (in our case, the boundary facets)\n\nOnce we have the boundary DoFs, we can create the Dirichlet boundary condition as follows:\n\nboundary_dofs = fem.locate_dofs_topological(V, fdim, boundary_facets)\nbc = fem.dirichletbc(uD, boundary_dofs)\n\nDefining the trial and test function\nIn mathematics, we usually distinguish between the trial space \\(V\\) and the test space \\(\\hat{V}\\). For the present problem, the only difference between the two would be the treatment of boundary conditions\nIn FEniCSx, however, boundary conditions are not built directly into the function space. This means we can simply use the same space for both the trial and test functions\nTo express the variational formulation, we make use of the Unified Form Language (UFL)\n\nimport ufl\n\nu = ufl.TrialFunction(V)\nv = ufl.TestFunction(V)\n\nDefining the source term\nSince the source term is constant throughout the domain, we can represent it using dolfinx.fem.Constant:\n\nfrom dolfinx import default_scalar_type\n\nf = fem.Constant(domain, default_scalar_type(-6))\n\nWhile we could simply define the source term as f = -6, this has a limitation: if we later want to change its value, we would need to redefine the entire variational problem. By using dolfinx.fem.Constant, we can easily update the value during the simulation, for example with f.value = 5\nAnother advantage is performance: declaring f as a constant allows the compiler to optimize the variational formulation, leading to faster assembly of the resulting linear system\nDefining the variational problem\nNow that we have defined all the components of our variational problem, we can write down the weak formulation:\n\na = ufl.dot(ufl.grad(u), ufl.grad(v)) *ufl.dx\nL = f *v *ufl.dx\n\nNotice how closely the Python syntax mirrors the mathematical expressions:\n\\[a(u,v) = \\int_{\\Omega} \\nabla u \\cdot \\nabla v \\,\\mathrm{d}x,\n\\;\\;\nL(v) = \\int_{\\Omega} f v \\,\\mathrm{d}x\\]\nHere, ufl.dx represents integration over the domain \\(\\Omega\\), i.e. over all cells of the mesh. This illustrates one of the major strengths of FEniCSx: \\(\\,\\) variational formulations can be written in Python in a way that almost directly matches their mathematical form, making it both natural and convenient to specify and solve complex PDE problems\nExpressing inner products\nThe inner product\n\\[\\int_\\Omega \\nabla u \\cdot \\nabla v \\,\\mathrm{d}x\\]\ncan be expressed in different ways in UFL. In our example, we wrote it as: ufl.dot(ufl.grad(u), ufl.grad(v)) *ufl.dx. In UFL, the dot operator performs a contraction: it sums over the last index of the first argument and the first index of the second argument. Since both \\(\\nabla u\\) and \\(\\nabla v\\) are rank-1 tensors (vectors), this reduces to a simple dot product.\nFor higher-rank tensors, such as matrices (rank-2 tensors), the appropriate operator is ufl.inner, which computes the full Frobenius inner product. For vectors, however, ufl.dot and ufl.inner are equivalent\nForming and solving the linear system\nHaving defined the finite element variational problem and boundary conditions, we can now create a dolfinx.fem.petsc.LinearProblem. This class provides a convenient interface for solving\nFind \\(u_h\\in V\\) such that \\(a(u_h, v)=L(v), \\;\\; \\forall v \\in \\hat{V}\\)\nIn this example, we will use PETSc as the linear algebra backend, together with a direct solver (LU factorization)\nFor more details on Krylov subspace(KSP) solvers and preconditioners, see the PETSc-documentation. Note that PETSc is not a required dependency of DOLFINx, so we explicitly import the DOLFINx wrapper to interface with PETSc. Finally, to ensure that the solver options passed to the LinearProblem apply only to this specific KSP solver, we assign a unique option prefix\n\nfrom dolfinx.fem.petsc import LinearProblem\n\nproblem = LinearProblem(\n    a, L, bcs=[bc],\n    petsc_options={\n        # Direct solver using LU factorization\n        \"ksp_type\": \"preonly\",\n        \"pc_type\": \"lu\"\n    }\n)\n\n# Solve the system\nuh = problem.solve()\n\n# Optionally, view solver information\n#ksp = problem.solver\n#ksp.view()\n\nThe ksp_type option in PETSc KSP solver specifies which algorithm to use for solving the linear system, while pc_type specifies the type of preconditioner. For most FEM problems, Symmetric Positive Definite(SPD) systems typically use cg with ilu or amg, and if a direct LU solver is desired, one can use ksp_type=\"preonly\" with pc_type=\"lu\"\nUsing problem.solve(), we solve the linear system of equations and return a dolfinx.fem.Function containing the solution\nComputing the error\nFinally, we want to compute the error to check the accuracy of the solution. We do this by comparing the finite element solution uh with the exact solution. We do this by interpolating the exact solution into the the \\(P_2\\)-function space\n\nV2 = fem.functionspace(domain, (\"Lagrange\", 2))\nuex = fem.Function(V2)\nuex.interpolate(lambda x: 1 +x[0]**2 +2 *x[1]**2)\n\nWe compute the error in two different ways. First, we compute the \\(L^2\\) norm of the error, defined by\n\\[E=\\sqrt{\\int_\\Omega (u_D-u_h)^2 \\,\\mathrm{d} x}\\]\nWe use UFL to express the \\(L^2\\) error, and use dolfinx.fem.assemble_scalar to compute the scalar value. In DOLFINx, assemble_scalar only assembles over the cells on the local process. This means that if we use 2 processes to solve our problem, we need to gather the solution to one. We can do this with the MPI.allreduce function\n\nL2_error = fem.form(ufl.inner(uh -uex, uh -uex) *ufl.dx)\nerror_local = fem.assemble_scalar(L2_error)\nerror_L2 = np.sqrt(domain.comm.allreduce(error_local, op=MPI.SUM))\n\nSecondly, we compute the maximum error at any degree of freedom (dof). The finite element solution uh can be expressed as a linear combination of the basis functions \\(\\phi_j\\) spanning the space \\(V\\):\n\\[u = \\sum_{j=1}^N U_j \\phi_j\\]\nWhen we call problem.solve(), we obtain all coefficients \\(U_1\\), \\(\\dots\\), \\(U_N\\). These coefficients are the degrees of freedom (dofs). We can access the dofs by retrieving the underlying vector from uh\nHowever, note that a second-order function space contains more dofs than a first-order space, so the corresponding arrays cannot be compared directly. Fortunately, since we have already interpolated the exact solution into the first-order space when defining the boundary condition, we can compare the maximum values at the dofs of the approximation space\n\nerror_max = np.max(np.abs(uD.x.array -uh.x.array))\n\n# Only print the error on one process\nif domain.comm.rank == 0:\n    print(f\"Error_L2 : {error_L2:.2e}\")\n    print(f\"Error_max : {error_max:.2e}\")\n\nError_L2 : 8.24e-03\nError_max : 5.33e-15\n\n\nPlotting the mesh using pyvista\nWe will visualize the mesh using pyvista, a Python interface to the VTK toolkit. To begin, We convert the mesh into a format compatible with pyvista using the function dolfinx.plot.vtk_mesh. The first step is to create an unstructured grid that pyvista can work with\nYou can check the current plotting backend with:\n\nimport pyvista\nfrom dolfinx import plot\n\ntopology, cell_types, geometry = plot.vtk_mesh(domain, tdim)\ngrid = pyvista.UnstructuredGrid(topology, cell_types, geometry)\n\nPyVista supports several backends, each with its own advantages and limitations. For more information and installation instructions, see the pyvista documentation\nWe can now use the pyvista.Plotter to visualize the mesh. We will show it both as a 2D and as a 3D warped representation.\nIn the jupyter notebook, we use the default setting pyvista.OFF_SCREEN=False, which will renders the plots directly within the notebook\n\nfrom pathlib import Path\n\nresults_folder = Path(\"fenicsx/fundamentals\")\nresults_folder.mkdir(exist_ok=True, parents=True)\n\nplotter = pyvista.Plotter(off_screen=True)\nplotter.add_mesh(grid, show_edges=True)\nplotter.add_axes()\nplotter.view_xy()\n\n# if not pyvista.OFF_SCREEN:\n#     plotter.show()\n\n# HTML 저장\nplotter.export_html(\"fenicsx/fundamentals/unit_square_mesh.html\")\n\n\n\nPlotting a function using pyvista\nWe want to plot the solution uh. Since the function space used to define uh is disconnected from the one used to define the mesh, we first create a mesh based on the DOF coordinates of the function space V. We then use dolfinx.plot.vtk_mesh, passing the function space as input, to generate a mesh whose geometry is based on these DOF coordinates\n\nu_topology, u_cell_types, u_geometry = plot.vtk_mesh(V)\n\nNext, we create the pyvista.UnstructuredGrid and add the DOF-values to the mesh\n\nu_grid = pyvista.UnstructuredGrid(u_topology, u_cell_types, u_geometry)\nu_grid.point_data[\"u\"] = uh.x.array.real\nu_grid.set_active_scalars(\"u\")\n\nu_plotter = pyvista.Plotter(off_screen=True)\nu_plotter.add_mesh(\n    u_grid, \n    show_edges=True,\n    scalar_bar_args={\n        \"title\": \"u\",\n        \"fmt\": \"%.1f\",\n        \"color\": \"black\",\n        \"label_font_size\": 12,\n        # \"vertical\": True,\n        \"n_labels\": 7,\n    },\n)\nu_plotter.add_axes()\nu_plotter.view_xy()\n\n# if not pyvista.OFF_SCREEN:\n#     u_plotter.show()\n\n# HTML 저장\nu_plotter.export_html(\"fenicsx/fundamentals/poisson_solution_2D.html\")\n\n\n\nExternal post-processing\nFor post-processing outside Python, it is recommended to save the solution to a file using either dolfinx.io.VTXWriter or dolfinx.io.XDMFFile, and then visualize it in ParaView. This approach is especially useful for 3D visualization\n\nfrom dolfinx import io\n\nfilename = results_folder/\"poisson\"\n\nwith io.VTXWriter(domain.comm, filename.with_suffix(\".bp\"), [uh]) as vtx:\n    vtx.write(0.0)\n    \nwith io.XDMFFile(domain.comm, filename.with_suffix(\".xdmf\"), \"w\") as xdmf:\n    xdmf.write_mesh(domain)\n    xdmf.write_function(uh)",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>K</span>  <span class='chapter-title'>The FEniCS computing platform</span>"
    ]
  },
  {
    "objectID": "x_PDE_FEniCS.html#weak-imposition-of-dirichlet-conditions-for-the-poisson-problem",
    "href": "x_PDE_FEniCS.html#weak-imposition-of-dirichlet-conditions-for-the-poisson-problem",
    "title": "Appendix K — The FEniCS computing platform",
    "section": "K.5 Weak imposition of Dirichlet conditions for the Poisson problem",
    "text": "K.5 Weak imposition of Dirichlet conditions for the Poisson problem\nAuthor: Jørgen S. Dokken\nThis section shows how to solve the previous Poisson problem using Nitsche’s method. Weak imposition works by adding terms to the variational formulation to enforce the boundary condition, instead of altering the matrix system via strong imposition (lifting).\nFirst, we import the necessary modules and set up the mesh and function space for the solution\n\nimport numpy as np\nfrom mpi4py import MPI\n\nfrom dolfinx import fem, mesh, plot, default_scalar_type\nfrom dolfinx.fem.petsc import LinearProblem\n\nfrom ufl import (Circumradius, FacetNormal, \n                 SpatialCoordinate, \n                 TrialFunction, TestFunction,\n                 dx, ds, div, grad, inner)\n\nN = 8\ndomain = mesh.create_unit_square(MPI.COMM_WORLD, N, N)\nV = fem.functionspace(domain, (\"Lagrange\", 1))\n\nNext, we create a function for the exact solution (also used in the Dirichlet boundary condition) and the corresponding source function for the right-hand side. The exact solution is defined using ufl.SpatialCoordinate, then interpolated into uD and used to generate the source function f\n\nx = SpatialCoordinate(domain)\nu_ex = 1 +x[0]**2 +2 *x[1]**2\n\nuD = fem.Function(V)\nuD.interpolate(fem.Expression(u_ex, V.element.interpolation_points()))\nf = -div(grad(u_ex))\n\nUnlike the first tutorial, we now need to revisit the variational form. We begin by integrating the problem by parts to obtain\n\\[\\begin{aligned}\n    \\int_{\\Omega} \\nabla u \\cdot \\nabla v~\\mathrm{d}x - \\int_{\\partial\\Omega}\\nabla u \\cdot n v~\\mathrm{d}s = \\int_{\\Omega} f v~\\mathrm{d}x\n\\end{aligned}\\]\nAs we are not enforcing the boundary condition strongly, the trace of the test function is not set to zero on the boundary. We instead add the following two terms to the variational formulation:\n\\[\\begin{aligned}\n    -\\int_{\\partial\\Omega} \\nabla  v \\cdot n (u-u_D)~\\mathrm{d}s + \\frac{\\alpha}{h} \\int_{\\partial\\Omega} (u-u_D)v~\\mathrm{d}s\n\\end{aligned}\\]\nThe first term enforces symmetry in the bilinear form, and the second term ensures coercivity. u_D denotes the known Dirichlet condition, and h is the diameter of the circumscribed sphere of the mesh element. The bilinear and linear forms, a and L, are then defined as\n\\[\\begin{aligned}\n    a(u, v) &= \\int_{\\Omega} \\nabla u \\cdot \\nabla v ~\\mathrm{d}x + \\int_{\\partial\\Omega} -(n \\cdot\\nabla u) v - (n \\cdot \\nabla v) u + \\frac{\\alpha}{h} uv ~\\mathrm{d}s \\\\\n    L(v) &= \\int_{\\Omega} fv ~\\mathrm{d}x + \\int_{\\partial\\Omega} -(n \\cdot \\nabla v) u_D + \\frac{\\alpha}{h} u_D v ~\\mathrm{d}s\n\\end{aligned}\\]\n\nu = TrialFunction(V)\nv = TestFunction(V)\n\nn = FacetNormal(domain)\nh = 2 *Circumradius(domain)\nalpha = fem.Constant(domain, default_scalar_type(10))\n\na = inner(grad(u), grad(v)) *dx -inner(n, grad(u)) *v *ds\na += -inner(n, grad(v)) *u *ds +alpha /h *inner(u, v) *ds\nL = inner(f, v) *dx \nL += -inner(n, grad(v)) *uD *ds +alpha /h *inner(uD, v) *ds\n\nWith the variational form in place, we can solve the linear problem\n\nproblem = LinearProblem(a, L)\nuh = problem.solve()\n\nWe compute the error by comparing the numerical solution with the analytical solution\n\nerror_form = fem.form(inner(uh -uD, uh -uD) *dx)\nerror_local = fem.assemble_scalar(error_form)\nerror_L2 = np.sqrt(domain.comm.allreduce(error_local, op=MPI.SUM))\nif domain.comm.rank == 0:\n    print(f\"Error_L2: {error_L2:.2e}\")\n\nError_L2: 1.59e-03\n\n\nThe \\(L^2\\)-error has the same order of magnitude as in the first tutorial, and we also compute the maximum error over all degrees of freedom\n\nerror_max = domain.comm.allreduce(\n  np.max(np.abs(uD.x.array -uh.x.array)), \n  op=MPI.MAX)\nif domain.comm.rank == 0:\n    print(f\"Error_max : {error_max:.2e}\")\n\nError_max : 5.41e-03\n\n\nWe observe that, due to the weak imposition of the boundary condition, the equation is not satisfied to machine precision at the mesh vertices. The solution is subsequently visualized using pyvista\n\nimport pyvista\n\nu_grid = pyvista.UnstructuredGrid(*plot.vtk_mesh(V))\nu_grid.point_data[\"u\"] = uh.x.array.real\nu_grid.set_active_scalars(\"u\")\n\nu_plotter = pyvista.Plotter(off_screen=True)\nu_plotter.add_mesh(\n  u_grid, \n  show_edges=True, \n  scalar_bar_args={\n        \"title\": \"u\",\n        \"fmt\": \"%.1f\",\n        \"color\": \"black\",\n        \"label_font_size\": 12,\n        # \"vertical\": True,\n        \"n_labels\": 7,\n  },  \n  show_scalar_bar=True\n)\n\nu_plotter.add_axes()\nu_plotter.view_xy()\n\n# if not pyvista.OFF_SCREEN:\n#     u_plotter.show()\n\n# HTML 저장\nu_plotter.export_html(\n  \"fenicsx/fundamentals/poisson_nitsche_solution_2D.html\"\n)",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>K</span>  <span class='chapter-title'>The FEniCS computing platform</span>"
    ]
  },
  {
    "objectID": "x_PDE_FEniCS.html#deflection-of-a-membrane",
    "href": "x_PDE_FEniCS.html#deflection-of-a-membrane",
    "title": "Appendix K — The FEniCS computing platform",
    "section": "K.6 Deflection of a membrane",
    "text": "K.6 Deflection of a membrane\nAuthors: Hans Petter Langtangen, Anders Logg, Jørgen S. Dokken\nIn the first FEniCSx example, we addressed a simple, easily verifiable problem. In this section, we consider a more physically relevant case that produces solutions with richer structure. In particular, we compute the deflection \\(D(x,y)\\) of a two-dimensional circular membrane of radius \\(R\\), under a load distribution \\(p(x,y)\\). The governing PDE is:\n\\[\n\\begin{aligned}\n     -T \\nabla^2D&=p \\quad\\text{in }\\; \\Omega=\\{(x,y)\\,\\vert\\, x^2+y^2\\leq R^2 \\}\n\\end{aligned}\n\\]\nHere, \\(T\\) denotes the constant membrane tension, and \\(p\\) represents the external pressure load. The boundary of the membrane is fixed, implying the boundary condition \\(D=0\\). We model a localized load using a Gaussian function:\n\\[\n\\begin{aligned}\n     p(x,y)&=\\frac{A}{2\\pi\\sigma}\\exp\\left(-\\frac{1}{2}\\left[\\left(\\frac{x-x_0}{\\sigma}\\right)^2 +\\left(\\frac{y-y_0}{\\sigma}\\right)^2\\right] \\right)\n\\end{aligned}\n\\]\nwhere \\(A\\) is the load amplitude, \\((x_0, y_0)\\) is the location of the load maximum, and \\(\\sigma\\) characterizes the width of \\(p\\). We place the load center at \\((x_0, y_0) = (0, R_0)\\), with \\(0 &lt; R_0 &lt; R\\). The resulting expression becomes\n\\[\n\\begin{aligned}\n     p(x,y)&=\\frac{A}{2\\pi\\sigma}\\exp\\left(-\\frac{1}{2}\\left[\\left(\\frac{x}{\\sigma}\\right)^2\n     +\\left(\\frac{y-R_0}{\\sigma}\\right)^2\\right]\\right)\n\\end{aligned}\n\\]\n\nK.6.1 Scaling the equation\nThis problem involves several physical parameters, and it is useful to simplify the formulation by introducing dimensionless variables. We define the scaled coordinates \\(\\bar{x} = \\tfrac{x}{R}\\), \\(\\bar{y} = \\tfrac{y}{R}\\), and the dimensionless deflection \\(w = \\tfrac{D}{D_e}\\), where \\(D_e\\) is a characteristic deflection. Introducing \\(\\bar{R}_0 = \\tfrac{R_0}{R}\\), we obtain\n\\[\n\\begin{aligned}\n    -\\frac{\\partial^2 w}{\\partial \\bar{x}^2} -\\frac{\\partial^2 w}{\\partial \\bar{y}^2}\n    &=\\frac{R^2A}{2\\pi\\sigma TD_e} \\exp\\left(-\\frac{R^2}{2\\sigma^2}\\left[\\bar{x}^2+(\\bar{y}-\\bar{R}_0)^2\\right]\\right)\\\\\n    &=\\alpha \\exp(-\\beta^2 \\left[\\bar{x}^2+(\\bar{y}-\\bar{R}_0)^2\\right])\n\\end{aligned}\n\\]\nvalid for \\(\\bar{x}^2+\\bar{y}^2&lt;1\\), where \\(\\alpha = \\frac{R^2A}{2\\pi\\sigma TD_e}\\) and \\(\\beta=\\frac{R}{\\sqrt{2}\\sigma}\\)\nWith an appropriate scaling, both \\(w\\) and its derivatives are of order unity. Consequently, the left-hand side of the scaled PDE is also of order unity, while the right-hand side is governed by the parameter \\(\\alpha\\). This motivates choosing \\(\\alpha\\) to be of order one; in this case, we set \\(\\alpha = 4\\). (Alternatively, one can derive the analytical solution in scaled coordinates and verify that the maximum deflection equals \\(D_e\\) when \\(\\alpha = 4\\), which provides the definition of \\(D_e\\))\nWith \\(D_e = \\tfrac{R^2 A}{8 \\pi \\sigma T}\\) and omitting the overbars, the scaled problem becomes\n\\[-\\nabla^2 w = 4 \\exp\\left(-\\beta^2 \\left[x^2 + (y-R_0)^2\\right]\\right)\\]\nto be solved over the unit disk, with \\(w=0\\) on the boundary\nIn the nondimensional formulation, the problem depends only on two parameters: the dimensionless width of the pressure distribution \\(\\beta\\) and the location of the pressure maximum \\(R_0 \\in [0,1]\\). In the limit \\(\\beta \\to 0\\), the solution converges to the special case \\(w = 1 - x^2 - y^2\\)\nGiven a computed scaled solution \\(w\\), the corresponding physical deflection is recovered as\n\\[\n\\begin{aligned}\n    D=\\frac{AR^2}{8\\pi\\sigma T}w\n\\end{aligned}\n\\]\n\n\nK.6.2 Implementation\nAuthor: Jørgen S. Dokken\nIn this section, we will solve the membrane deflection problem. By the end of this section, you should be able to:\n\nConstruct a simple mesh using the GMSH Python API and import it into DOLFINx\nSpecify constant boundary conditions via a geometrical identifier\nEmploy ufl.SpatialCoordinate to define a spatially varying function\nInterpolate a ufl.Expression into an suitable function space\nEvaluate a dolfinx.Function at arbitrary points\n\nCreating the mesh\nTo construct the computational geometry, we use the Python API of GMSH. We begin by importing the gmsh module and initializing it\n\n# $ conda install -c conda-forge python-gmsh\nimport gmsh\n\nif not gmsh.isInitialized():\n    gmsh.initialize()\n\nNext, we define the membrane geometry and begin the setup using the GMSH CAD kernel, which automatically generates the required data structures in the background. When calling addDisk, the first three arguments specify the \\(x,\\) \\(y,\\) and \\(z\\) coordinates of the circle’s center, while the final two define the radii in the \\(x\\)- and \\(y\\)-directions\n\n# gmsh.model.occ.addDisk(xc, yc, zc, rx, ry)\n#   xc, yc, zc : center coordinates\n#   rx, ry     : radii in x- and y-directions\nmembrane = gmsh.model.occ.addDisk(0.0, 0.0, 0.0, 1, 1)\n\n# Synchronize the CAD kernel with the gmsh model\ngmsh.model.occ.synchronize()\n\nNext, we define the membrane as a physical surface so that GMSH will recognize it during mesh generation. Because a surface is a two-dimensional entity, we pass 2 as the first argument, the membrane’s entity tag as the second, and the physical tag as the last. In a later example, we will explain in more detail when and why this physical tag becomes important\n\ngdim = 2\nphysical_tag = 1\n\n# Remove any existing physical groups with the same (dim, tag)\nfor dim, tag in gmsh.model.getPhysicalGroups():\n    if dim == gdim and tag == physical_tag:\n        gmsh.model.removePhysicalGroups([(dim, tag)])\n\n# Now safely add the new physical group\npg = gmsh.model.addPhysicalGroup(gdim, [membrane], physical_tag)\ngmsh.model.setPhysicalName(gdim, pg, \"Circular Membrane\")\n\nFinally, we generate the two-dimensional mesh, setting a uniform element size by adjusting the GMSH options\n\ngmsh.option.setNumber(\"Mesh.CharacteristicLengthMin\", 0.05)\ngmsh.option.setNumber(\"Mesh.CharacteristicLengthMax\", 0.05)\ngmsh.model.mesh.generate(gdim)\n\nInfo    : Meshing 1D...\nInfo    : Meshing curve 1 (Ellipse)\nInfo    : Done meshing 1D (Wall 0.00110621s, CPU 0.001081s)\nInfo    : Meshing 2D...\nInfo    : Meshing surface 1 (Plane, Frontal-Delaunay)\nInfo    : Done meshing 2D (Wall 0.0531777s, CPU 0.087872s)\nInfo    : 1552 nodes 3103 elements\n\n\nInterfacing with GMSH in DOLFINx\nWe import the GMSH-generated mesh directly into DOLFINx using the dolfinx.io.gmshio interface. In this example, we did not specify which process created the GMSH model, so a copy of the model is created on each MPI process. However, our goal is to work with a single mesh distributed across all processes. To accomplish this, we take the model generated on rank 0 of MPI.COMM_WORLD and distribute it to all available ranks\nThe import also provides two sets of mesh tags: one for cells defined by physical groups and one for facets defined by physical groups. Since we did not add any physical groups of dimension gdim -1, the facet_tags object will be empty\n\nfrom mpi4py import MPI\nfrom dolfinx.io import gmshio\nfrom dolfinx.fem.petsc import LinearProblem\n\ndomain, cell_tags, facet_tags = gmshio.model_to_mesh(\n  gmsh.model, \n  MPI.COMM_WORLD, \n  rank=0, \n  gdim=gdim\n)\n\ngmsh.finalize()\n\nWe define the function space as in the previous tutorial\n\nfrom dolfinx import fem, plot\n\nV = fem.functionspace(domain, (\"Lagrange\", 1))\n\n\nimport pyvista\n\n# Extract topology from mesh and create pyvista mesh\ntopology, cell_types, x = plot.vtk_mesh(V)\ngrid = pyvista.UnstructuredGrid(topology, cell_types, x)\n\nplotter = pyvista.Plotter(off_screen=True)\nplotter.add_mesh(grid, show_edges=True)\nplotter.add_axes()\nplotter.view_xy()\n\n# if not pyvista.OFF_SCREEN:\n#     plotter.show()\n\n# HTML 저장\nplotter.export_html(\"fenicsx/fundamentals/membrane_mesh.html\")\n\n        \n\nDefining a spatially varying load\nThe pressure function on the right-hand side is defined with ufl.SpatialCoordinate and two constants, \\(\\beta\\) and \\(R_0\\)\n\nfrom dolfinx import default_scalar_type\nimport ufl\n\nx = ufl.SpatialCoordinate(domain)\n\nbeta = fem.Constant(domain, default_scalar_type(12))\nR0 = fem.Constant(domain, default_scalar_type(0.3))\n\np = 4 *ufl.exp(-beta**2 *(x[0]**2 +(x[1] -R0)**2))\n\nInterpolation of a ufl expression\nSince the load p is defined as a spatially varying function, we interpolate it into an appropriate function space for visualization. To do this, we use dolfinx.Expression, which accepts any ufl expression together with a set of points on the reference element. In practice, we provide the interpolation points of the target space. Because p exhibits rapid spatial variation, we select a high-order function space to represent it\n\nQ = fem.functionspace(domain, (\"Lagrange\", 5))\nexpr = fem.Expression(p, Q.element.interpolation_points())\n\npressure = fem.Function(Q)\npressure.interpolate(expr)\n\nWe next plot the load on the domain\n\np_grid = pyvista.UnstructuredGrid(*plot.vtk_mesh(Q))\np_grid.point_data[\"p\"] = pressure.x.array.real\n\nwarped_p = p_grid.warp_by_scalar(\"p\", factor=0.5)\nwarped_p.set_active_scalars(\"p\")\n\nload_plotter = pyvista.Plotter(off_screen=True)\nload_plotter.add_mesh(\n  warped_p,\n  show_edges=True, \n  show_scalar_bar=True,\n  cmap=\"jet\"\n)\nload_plotter.add_axes() \n\n# if not pyvista.OFF_SCREEN:\n#     load_plotter.show()\n\n# HTML 저장\nload_plotter.export_html(\"fenicsx/fundamentals/membrane_load.html\")\n\n        \n\nCreate a Dirichlet boundary condition using geometrical conditions\nThe next step is to define the homogeneous boundary condition. Unlike in the first tutorial, we use dolfinx.fem.locate_dofs_geometrical to identify the degrees of freedom on the boundary. Since our domain is the unit circle, these degrees of freedom correspond to coordinates \\((x, y)\\) such that \\(\\sqrt{x^2 + y^2} = 1\\)\n\nimport numpy as np\n\ndef on_boundary(x):\n    return np.isclose(np.sqrt(x[0]**2 +x[1]**2), 1)\n\nboundary_dofs = fem.locate_dofs_geometrical(V, on_boundary)\n\nSince our Dirichlet condition is homogeneous (u=0 on the entire boundary), we can define it using dolfinx.fem.dirichletbc by specifying a constant value, the boundary degrees of freedom and the function space on which it should be applied\n\nbc = fem.dirichletbc(default_scalar_type(0), boundary_dofs, V)\n\nDefining and solving the variational problem\nThe variational problem is identical to our first Poisson problem, with p replacing f\n\nu = ufl.TrialFunction(V)\nv = ufl.TestFunction(V)\n\na = ufl.dot(ufl.grad(u), ufl.grad(v)) *ufl.dx\nL = p *v *ufl.dx\n\nproblem = LinearProblem(\n  a, \n  L, \n  bcs=[bc], \n  petsc_options={\"ksp_type\": \"preonly\", \"pc_type\": \"lu\"}\n)\nuh = problem.solve()\n\nWe plot the deflection uh over the domain \\(\\Omega\\)\n\n# Set deflection values and add it to plotter\ngrid.point_data[\"u\"] = uh.x.array\nwarped = grid.warp_by_scalar(\"u\", factor=25)\n\nu_plotter = pyvista.Plotter(off_screen=True)\nu_plotter.add_mesh(\n  warped, \n  show_edges=True, \n  show_scalar_bar=True, \n  scalars=\"u\",\n  cmap='jet'\n)\nu_plotter.add_axes() \n\n# if not pyvista.OFF_SCREEN:\n#     u_plotter.show()\n\n# HTML 저장\nu_plotter.export_html(\"fenicsx/fundamentals/membrane_u.html\")\n\n        \n\nPlotting along a line in the domain\nA convenient way to compare the deflection and load is by plotting them along \\(x=0\\), using a set of points along the \\(y\\)-axis to evaluate the finite element functions \\(u\\) and \\(p\\)\n\ntol = 0.001  # Avoid hitting the outside of the domain\ny = np.linspace(-1 +tol, 1 -tol, 101)\n\npoints = np.zeros((3, 101))\npoints[1] = y\nu_values = []\np_values = []\n\nA finite element function can be expressed as a linear combination of all its degrees of freedom:\n\\[u_h(x) = \\sum_{i=1}^N c_i \\, \\phi_i(x)\\]\nwhere \\(c_i\\) are the coefficients of \\(u_h\\) and \\(\\phi_i\\) are the basis functions. In principle, this allows us to evaluate the solution at any point in \\(\\Omega\\)\nHowever, since a mesh typically contains a large number of degrees of freedom (i.e., \\(N\\) is large), evaluating every basis function at each point would be inefficient. Instead, we first identify which mesh cell contains the point \\(x\\). This can be done efficiently using a bounding box tree, which enables a fast recursive search through the mesh cells\n\nfrom dolfinx import geometry\n\nbb_tree = geometry.bb_tree(domain, domain.topology.dim)\n\nWe can now determine which cells each point intersects by using dolfinx.geometry.compute_collisions_points. This function returns, for every input point, a list of cells whose bounding boxes overlap with that point. Since different points may correspond to a varying number of cells, the results are stored in a dolfinx.cpp.graph.AdjacencyList_int32. The cells associated with the \\(i\\)-th point can be accessed with links(i)\nBecause a cell’s bounding box generally extends beyond the cell itself in \\(\\mathbb{R}^n\\), we must verify whether the point actually inside the cell. This is done with dolfinx.geometry.compute_colliding_cells, which computes the exact distance between the point and the cell (approximating higher-order cells as convex hulls). Like the previous function, it also returns an adjacency list, since a point may lie on a facet, edge, or vertex shared by multiple cells\nFinally, to ensure that the code runs correctly in parallel when the mesh is distributed across multiple processors, we create a subset, points_on_proc, that includes only the points located on the current processor\n\ncells = []\npoints_on_proc = []\n\n# Find cells whose bounding-box collide with the the points\ncell_candidates = geometry.compute_collisions_points(\n  bb_tree, \n  points.T\n)\n\n# Choose one of the cells that contains the point\ncolliding_cells = geometry.compute_colliding_cells(\n  domain, \n  cell_candidates, \n  points.T\n)\n\nfor i, point in enumerate(points.T):\n    if len(colliding_cells.links(i)) &gt; 0:\n        points_on_proc.append(point)\n        cells.append(colliding_cells.links(i)[0])\n\nWe now have a list of points associated with the processor and the cell each point belongs to. This allows us to use uh.eval and pressure.eval to compute the function values at these points\n\npoints_on_proc = np.array(points_on_proc, dtype=np.float64)\n\nu_values = uh.eval(points_on_proc, cells)\np_values = pressure.eval(points_on_proc, cells)\n\nWith the coordinates and the corresponding function values, we can now plot the results using matplotlib\n\nimport matplotlib.pyplot as plt\n\nfig = plt.figure()\n\nplt.plot(points_on_proc[:, 1], 50 *u_values, \n  \"k\", lw=2, label=\"Deflection ($\\\\times 50$)\")\nplt.plot(points_on_proc[:, 1], p_values, \n  \"b--\", lw=2, label=\"Load\")\n\nplt.grid(True)\nplt.legend()\nplt.xlabel(\"y\")\nplt.show()\n\n\n\n\n\n\n\n\nSaving functions to file\nTo visualize the solution in ParaView, we can save it to a file as follows:\n\nfrom pathlib import Path\nimport dolfinx.io\n\nresults_folder = Path(\"fenicsx/fundamentals\")\nresults_folder.mkdir(exist_ok=True, parents=True)\n\nfilename = results_folder/\"membrane\"\n\npressure.name = \"Load\"\nuh.name = \"Deflection\"\n\nwith dolfinx.io.VTXWriter(\n  MPI.COMM_WORLD, results_folder/\"membrane_pressure.bp\", \n  [pressure], engine=\"BP4\") as vtx:\n    vtx.write(0.0)\n\nwith dolfinx.io.VTXWriter(\n  MPI.COMM_WORLD, results_folder/\"membrane_deflection.bp\", \n  [uh], engine=\"BP4\") as vtx:\n    vtx.write(0.0)",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>K</span>  <span class='chapter-title'>The FEniCS computing platform</span>"
    ]
  },
  {
    "objectID": "x_PDE_FEniCS.html#a-gallery-of-finite-element-solvers",
    "href": "x_PDE_FEniCS.html#a-gallery-of-finite-element-solvers",
    "title": "Appendix K — The FEniCS computing platform",
    "section": "K.7 A Gallery of finite element solvers",
    "text": "K.7 A Gallery of finite element solvers\nThe aim of this chapter is to demonstrate how a variety of important PDEs from science and engineering can be solved using just a few lines of DOLFINx code. We start with the heat equation, then proceed to the nonlinear Poisson equation, the equations of linear elasticity, and the Navier–Stokes equations. Finally, we consider systems of nonlinear advection–diffusion–reaction equations. These examples illustrate how to handle time-dependent problems, nonlinear problems, vector-valued problems, and systems of PDEs. For each case, we derive the variational formulation and implement the problem in Python in a way that closely mirrors the underlying mathematics\n\nK.7.1 The heat equation\nAuthors: Hans Petter Langtangen, Anders Logg, Jørgen S. Dokken\nAs a first extension of the Poisson problem introduced in the previous chapter, we now turn to the time-dependent heat equation (also known as the time-dependent diffusion equation). This equation can be viewed as the natural generalization of the Poisson equation, which describes the stationary distribution of heat in a body, to the case where the distribution evolves over time. By discretizing time into small intervals and applying standard time-stepping methods, we can solve the heat equation as a sequence of variational problems, in much the same way as we solved the Poisson equation\nThe PDE problem\nThe model problem for the time-dependent PDE is given by\n\\[\n\\begin{aligned}\n\\frac{\\partial u}{\\partial t}&=\\nabla^2 u + f && \\text{in } \\Omega \\times (0, T] \\\\\nu &= u_D && \\text{on } \\partial\\Omega \\times (0,T] \\\\\nu &= u_0 && \\text{at } t=0\n\\end{aligned}\\]\nHere, \\(u\\) depends on both space and time; for example, \\(u = u(x,y,t)\\) if the spatial domain \\(\\Omega\\) is two-dimensional. The source term \\(f\\) and the boundary condition \\(u_D\\) may also vary with space and time, while the initial condition \\(u_0\\) is a function of space alone\nThe variational formulation\nA simple approach to solving time-dependent PDEs with the finite element method is to first discretize the time derivative using a finite difference approximation. This reduces the problem to a sequence of stationary equations, each of which can then be written in variational form. We use the superscript \\(n\\) to denote a quantity at time \\(t_n\\), where \\(n\\) indexes the discrete time levels. For example, \\(u^n\\) represents the value of \\(u\\) at time level \\(n\\). The first step in a finite difference discretization of time is to evaluate the PDE at a chosen time level, such as \\(t_{n+1}\\)\n\\[\n\\begin{aligned}\n    \\left(\\frac{\\partial u }{\\partial t}\\right)^{n+1}= \\nabla^2 u^{n+1}+ f^{n+1}\n\\end{aligned}\\]\nThe time derivative can be approximated by a difference quotient. For reasons of both simplicity and stability, we adopt the backward difference scheme:\n\\[\\begin{aligned}\n\\left(\\frac{\\partial u }{\\partial t}\\right)^{n+1}\\approx \\frac{u^{n+1}-u^n}{\\Delta t}\n\\end{aligned}\\]\nwhere \\(\\Delta t\\) denotes the time-step size. Substituting this expression into the equation at time level \\(n +1\\) gives\n\\[\\begin{aligned}\n\\frac{u^{n+1}-u^n}{\\Delta t}= \\nabla^2 u^{n+1}+ f^{n+1}\n\\end{aligned}\\]\nThis is the time-discrete form of the heat equation, known as the backward Euler or implicit Euler scheme\nWe rearrange the equation so that the left-hand side contains only the unknown \\(u^{n+1}\\), while the right-hand side contains terms that are already known. This yields a sequence of stationary problems for \\(u^{n+1}\\), given that \\(u^{n}\\) is available from the previous time step:\n\\[\\begin{aligned}\nu^0 &= u_0\\\\\nu^{n+1} - \\Delta t \\nabla^2 u^{n+1} &= u^{n} + \\Delta t f^{n+1}, \\quad n = 0,1,2,\\dots\n\\end{aligned}\\]\nStarting from the initial condition \\(u_0\\), we can then compute \\(u^0\\), \\(u^1\\), \\(u^2\\), and so forth\nNext, we apply the finite element method. To do this, we first derive the weak formulation of the equation: we multiply by a test function \\(v \\in \\hat{V}\\) and integrate the second-order derivatives by parts. For simplicity, we now denote \\(u^{n+1}\\) by \\(u\\). The resulting weak formulation can be written as\n\\[\\begin{aligned}\na(u,v) &= L_{n+1}(v)\n\\end{aligned}\\]\nwhere\n\\[\\begin{aligned}\na(u,v) &= \\int_{\\Omega} \\big( u v + \\Delta t \\nabla u \\cdot \\nabla v \\big)\\,\\mathrm{d}x \\\\\nL_{n+1}(v) &= \\int_{\\Omega} \\big(u^n + \\Delta t f^{n+1}\\big)\\, v \\,\\mathrm{d}x\n\\end{aligned}\\]\nProjection or interpolation of the initial condition\nIn addition to the variational problem solved at each time step, we also need to approximate the initial condition. This can likewise be expressed as a variational problem:\n\\[\\begin{aligned}\na_0(u,v) &= L_0(v)\n\\end{aligned}\\]\nwith\n\\[\\begin{aligned}\na_0(u,v) &= \\int_{\\Omega} u v \\,\\mathrm{d}x\\\\\nL_0(v) &= \\int_{\\Omega} u_0 v \\,\\mathrm{d}x\n\\end{aligned}\\]\nSolving this variational problem gives \\(u^0\\), which is the \\(L^2\\) projection of the prescribed initial condition \\(u_0\\) onto the finite element space\nAn alternative way to construct \\(u^0\\) is by directly interpolating the initial value \\(u_0\\). We discussed the use of interpolation in DOLFINx in the membrane deflection\nIn DOLFINx, the initial condition can be obtained either by projection or by interpolation. The most common approach is projection, which provides an approximation of \\(u_0\\). However, in applications where we want to verify the implementation against exact solutions, interpolation must be used. In this chapter, we will consider such a case\n\nK.7.1.1 Diffusion of a Gaussian function\nAuthor: Jørgen S. Dokken\nLet us now consider a more interesting problem: the diffusion of a Gaussian hill. We take the initial condition to be\n\\[\\begin{aligned}\nu_0(x,y) &= e^{-a (x^2 +y^2)}\n\\end{aligned}\\]\nwith \\(a = 5\\) on the domain \\([-2,2]\\times[-2,2]\\). For this problem, we impose homogeneous Dirichlet boundary conditions (\\(u_D = 0\\))\nThe first difference from the previous problem is that the domain is no longer the unit square. We create the rectangular domain using dolfinx.mesh.create_rectangle\n\nimport numpy as np\nimport matplotlib as mpl\n\nfrom mpi4py import MPI\nfrom petsc4py import PETSc\nimport pyvista\n\nfrom dolfinx import fem, mesh, io, plot\nfrom dolfinx.fem.petsc import (\n  assemble_vector, assemble_matrix,\n  create_vector, apply_lifting, set_bc\n)\nimport ufl\n\n# Define temporal parameters\nt = 0.0            # Start time\nT = 1.0            # Final time\nnum_steps = 50\ndt = T /num_steps  # time step size\n\n# Define mesh\nnx, ny = 50, 50\ndomain = mesh.create_rectangle(\n  MPI.COMM_WORLD, \n  [np.array([-2, -2]), np.array([2, 2])],\n  [nx, ny], \n  mesh.CellType.triangle\n)\n\nV = fem.functionspace(domain, (\"Lagrange\", 1))\n\n\nresults_folder = Path(\"fenicsx/heat\")\nresults_folder.mkdir(exist_ok=True, parents=True)\n\ntdim = domain.topology.dim\ngrid = pyvista.UnstructuredGrid(*plot.vtk_mesh(domain, tdim))\n\nplotter = pyvista.Plotter(off_screen=True)\nplotter.add_mesh(grid, show_edges=True)\nplotter.add_axes()\nplotter.view_xy()\n\n# if not pyvista.OFF_SCREEN:\n#     plotter.show()\n\n# HTML 저장\nplotter.export_html(results_folder/\"heat_mesh.html\")\n\n        \n\nNote that we have used a much higher resolution than before to better capture the features of the solution. We can also easily update the initial and boundary conditions. Instead of defining the initial condition using a class, we simply use a function\n\n# Create initial condition\ndef initial_condition(x, a=5):\n    return np.exp(-a *(x[0]**2 +x[1]**2))\n\nu_n = fem.Function(V)\nu_n.name = \"u_n\"\nu_n.interpolate(initial_condition)\n\n# Create boundary condition\nfdim = tdim -1\n\n# Select all boundary facets\nboundary_facets = mesh.locate_entities_boundary(\n  domain, \n  fdim, \n  lambda x: np.full(x.shape[1], True, dtype=bool)\n)\n\n# Extract boundary DOFs\nboundary_dofs = fem.locate_dofs_topological(\n  V, \n  fdim, \n  boundary_facets\n)\n\n# Define boundary condition (u = 0 on the entire boundary)\n# For scalar constants, \n#   also pass V to specify the function space\nbc = fem.dirichletbc(\n  PETSc.ScalarType(0), \n  boundary_dofs, \n  V\n)\n\nVariational formulation\nAs in the previous example, we set up the necessary objects for the time-dependent problem so that we do not need to recreate the data structures\n\nu = ufl.TrialFunction(V)\nv = ufl.TestFunction(V)\n\nf = fem.Constant(domain, PETSc.ScalarType(0))\n\na = (u *v +dt *ufl.dot(ufl.grad(u), ufl.grad(v))) *ufl.dx\nL = (u_n +dt *f) *v *ufl.dx\n\nPreparing linear algebra structures for time dependent problems\nEven though u_n depends on time, we use the same function for both f and u_n at each time step. We then call dolfinx.fem.form to create the assembly kernels for the matrix and vector\n\nbilinear_form = fem.form(a)\nlinear_form = fem.form(L)\n\nThe left-hand side of the system, the matrix A, does not change between time steps, so it only needs to be assembled once. The right-hand side, however, depends on the previous solution u_n and must be assembled at every time step. For this reason, we create the vector b from L and reuse it at each step\n\nA = assemble_matrix(bilinear_form, bcs=[bc])\nA.assemble()\nb = create_vector(linear_form)\n\nUsing petsc4py to create a linear solver\nSince we have already assembled a into the matrix A, we cannot use the dolfinx.fem.petsc.LinearProblem class. Instead, we create a PETSc linear solver, assign the matrix A to it, and select a solution strategy\n\nsolver = PETSc.KSP().create(domain.comm)\nsolver.setOperators(A)\nsolver.setType(PETSc.KSP.Type.PREONLY)\nsolver.getPC().setType(PETSc.PC.Type.LU)\n\nSaving time-dependent solutions with XDMFFile\nTo visualize the solution in an external program such as Paraview, we create an XDMFFile, which can store multiple solutions. The main advantage of using an XDMFFile is that the mesh only needs to be stored once, and multiple solutions can be appended to the same grid, thereby reducing the storage requirements\nThe first argument to XDMFFile specifies the communicator used to store the data. Since we want a single output file regardless of the number of processors, we use the COMM_WORLD. The second argument is the name of the output file, and the third argument specifies the file mode, which can be read (\"r\"), write (\"w\") or append (\"a\")\n\nxdmf = io.XDMFFile(\n  domain.comm, \n  results_folder/\"heat.xdmf\", \n  \"w\"\n)\nxdmf.write_mesh(domain)\n\n# Define solution variable, \n#  and interpolate initial solution for visualization\nuh = fem.Function(V)\nuh.name = \"uh\"\nuh.interpolate(initial_condition)\n\nxdmf.write_function(uh, t)\n\nVisualizing time-dependent solutions with PyVista\nWe use the DOLFINx plotting tools, based on PyVista, to plot the solution at every time step. We also display a colorbar showing the initial maximum values of u. For this, we use the convenience function renderer:\n\nfrom functools import partial\n\nviridis = mpl.colormaps.get_cmap(\"viridis\").resampled(25)\nsargs = dict(\n  title_font_size=25, \n  label_font_size=20, \n  fmt=\"%.2f\", \n  color=\"black\",\n  position_x=0.1, \n  position_y=0.8, \n  width=0.8, \n  height=0.1\n)\n\nplotter = pyvista.Plotter()\n# conda install -c conda-forge imageio\nplotter.open_gif(results_folder/\"heat_animation.gif\", fps=10) \n\nrenderer = partial(\n  plotter.add_mesh,\n  show_edges=True, \n  lighting=False,\n  cmap=viridis, \n  scalar_bar_args=sargs,\n  clim=[0, max(uh.x.array)]\n)\n\n\ngrid.point_data[\"uh\"] = uh.x.array\nwarped = grid.warp_by_scalar(\"uh\", factor=3)\n\nrenderer(warped);\n\nUpdating the right hand side and solution per time step\nTo solve the variational problem at each time step, we must assemble the right-hand side and apply the boundary conditions before calling solver.solve(b, uh.x.petsc_vec). We begin by resetting the values in b, since the vector is reused at every step. Next, we assemble the vector with dolfinx.fem.petsc.assemble_vector(b, L), which inserts the linear form L(v) into b\nNote that boundary conditions are not supplied during this assembly, unlike for the left-hand side. Instead, we apply them later using lifting, which preserves the symmetry of the matrix in the bilinear form without Dirichlet conditions. After applying the boundary conditions, we solve the linear system and update any values that may be shared across processors. Finally, before advancing to the next step, we update the previous solution with the current one\n\nfor i in range(num_steps):\n    t += dt\n\n    # Update the right hand side reusing the initial vector\n    with b.localForm() as loc_b:\n        loc_b.set(0)\n    assemble_vector(b, linear_form)\n\n    # Apply Dirichlet boundary condition to the vector\n    apply_lifting(b, [bilinear_form], [[bc]])\n    b.ghostUpdate(\n      addv=PETSc.InsertMode.ADD_VALUES, \n      mode=PETSc.ScatterMode.REVERSE\n    )\n    set_bc(b, [bc])\n\n    # Solve linear problem\n    solver.solve(b, uh.x.petsc_vec)\n    uh.x.scatter_forward()\n\n    # Update solution at previous time step (u_n)\n    u_n.x.array[:] = uh.x.array\n\n    # Write solution to file\n    xdmf.write_function(uh, t)\n\n    # Update plot\n    new_warped = grid.warp_by_scalar(\"uh\", factor=3)\n    warped.points[:, :] = new_warped.points\n    warped.point_data[\"uh\"][:] = uh.x.array\n    \n    renderer(warped)\n    plotter.write_frame()\n\nplotter.close()\nxdmf.close()\n\n\n\n\nK.7.1.2 A known analytical solution\nAuthor: Jørgen S. Dokken\nJust as in the Poisson problem, we construct a test case that makes it straightforward to verify the correctness of the computations\nSince our first-order time-stepping scheme is exact for linear functions in time, we design a problem with linear temporal variation combined with quadratic spatial variation. Accordingly, we choose the analytical solution\n\\[\\begin{aligned}\nu = 1 + x^2+\\alpha y^2 + \\beta t\n\\end{aligned}\\]\nwhich ensures that the computed values at the degrees of freedom are exact, regardless of the mesh size or time step \\(\\Delta t\\), provided that the mesh is uniformly partitioned\nSubstituting this expression into the original PDE yields the right hand side \\(f = \\beta -2 -2\\alpha\\). The corresponding boundary condition is \\(u_D(x,y,t)= 1 +x^2 +\\alpha y^2 +\\beta t\\) and the initial condition is \\(u_0(x,y)=1+x^2+\\alpha y^2\\)\nWe start by defining the temporal discretization parameters, along with the parameters for \\(\\alpha\\) and \\(\\beta\\)\n\nimport numpy as np\nfrom mpi4py import MPI\nfrom petsc4py import PETSc\n\nfrom dolfinx import mesh, fem\nfrom dolfinx.fem.petsc import (\n  assemble_matrix, assemble_vector, \n  apply_lifting, create_vector, set_bc\n)\n\nimport ufl\n\nt = 0  # Start time\nT = 2  # End time\n\nnum_steps = 20          # Number of time steps\ndt = (T -t) /num_steps  # Time step size\n\nalpha = 3\nbeta = 1.2\n\nAs in the previous problem, we define the mesh and the appropriate function spaces\n\nnx, ny = 5, 5\ndomain = mesh.create_unit_square(\n  MPI.COMM_WORLD, \n  nx, ny, \n  mesh.CellType.triangle\n)\n\nV = fem.functionspace(domain, (\"Lagrange\", 1))\n\nDefining the exact solution\nWe implement a Python class to represent the exact solution\n\nclass exact_solution():\n    def __init__(self, alpha, beta, t):\n        self.alpha = alpha\n        self.beta = beta\n        self.t = t\n\n    def __call__(self, x):\n        return 1 +x[0]**2 +self.alpha *x[1]**2 +self.beta *self.t\n\nu_exact = exact_solution(alpha, beta, t)\n\nDefining the boundary condition\nAs in the previous sections, we define a Dirichlet boundary condition over the whole boundary\n\nu_D = fem.Function(V)\nu_D.interpolate(u_exact)\n\ntdim = domain.topology.dim\nfdim = tdim -1\n\ndomain.topology.create_connectivity(fdim, tdim)\nboundary_facets = mesh.exterior_facet_indices(domain.topology)\n\nbc = fem.dirichletbc(\n  u_D, \n  fem.locate_dofs_topological(V, fdim, boundary_facets)\n)\n\n\n\n\nFEniCSx mesh connectivity\n\n\nDefining the variational formualation\nSince we set \\(t=0\\) in u_exact, we can reuse this variable to obtain \\(u_n\\) for the first time step\n\nu_n = fem.Function(V)\nu_n.interpolate(u_exact)\n\nBecause \\(f\\) is time-independent, we can treat it as a constant\n\nf = fem.Constant(domain, beta -2 -2 *alpha)\n\nWe can now create our variational formulation, with the bilinear form a and linear form L\n\nu = ufl.TrialFunction(V)\nv = ufl.TestFunction(V)\n\nF = u *v *ufl.dx +dt *ufl.dot(ufl.grad(u), ufl.grad(v)) *ufl.dx -(u_n +dt *f) *v *ufl.dx\n\na = fem.form(ufl.lhs(F))\nL = fem.form(ufl.rhs(F))\n\nCreate the matrix and vector for the linear problem\nTo ensure that the variational problem is solved efficiently, we construct several structures that allow data reuse, such as matrix sparisty patterns. In particular, since the bilinear form a is independent of time, the matrix only needs to be assembled once\n\nA = assemble_matrix(a, bcs=[bc])\nA.assemble()\nb = create_vector(L)\n\nuh = fem.Function(V)\n\nDefine a linear variational solver\nThe resulting linear algebra problem is solved with PETSc, using the Python API petsc4py to define a linear solver\n\nsolver = PETSc.KSP().create(domain.comm)\nsolver.setOperators(A)\nsolver.setType(PETSc.KSP.Type.PREONLY)\nsolver.getPC().setType(PETSc.PC.Type.LU)\n\nSolving the time-dependent problem\nWith these structures in place, we construct our time-stepping loop. Within this loop, we first update the Dirichlet boundary condition by interpolating the updated expression u_exact into V. Next, we reassemble the vector b using the current solution u_n. The boundary condition is then applied to this vector via a lifting operation, which preserves the symmetry of the matrix. Finally, we solve the problem using PETSc and update u_n with the values from uh\n\nfor n in range(num_steps):\n\n    # Update Diriclet boundary condition\n    u_exact.t += dt\n    u_D.interpolate(u_exact)\n\n    # Update the right hand side reusing the initial vector\n    with b.localForm() as loc_b:\n        loc_b.set(0)\n    assemble_vector(b, L)\n\n    # Apply Dirichlet boundary condition to the vector\n    apply_lifting(b, [a], [[bc]])\n    b.ghostUpdate(\n      addv=PETSc.InsertMode.ADD_VALUES, \n      mode=PETSc.ScatterMode.REVERSE\n    )\n    set_bc(b, [bc])\n\n    # Solve linear problem\n    solver.solve(b, uh.x.petsc_vec)\n    uh.x.scatter_forward()\n\n    # Update solution at previous time step (u_n)\n    u_n.x.array[:] = uh.x.array\n\nVerifying the numerical solution\nwe compute the \\(L^2\\)-error and the maximum error at the mesh vertices for the final time step. This allows us to verify the correctness of our implementation\n\n# Compute L2 error and error at nodes\nV_ex = fem.functionspace(domain, (\"Lagrange\", 2))\nu_ex = fem.Function(V_ex)\nu_ex.interpolate(u_exact)\n\nerror_L2 = np.sqrt(\n  domain.comm.allreduce(\n    fem.assemble_scalar(fem.form((uh -u_ex)**2 *ufl.dx)), \n      op=MPI.SUM)\n)\nif domain.comm.rank == 0:\n    print(f\"Error_L2: {error_L2:.2e}\")\n\n# Compute values at mesh vertices\nerror_max = domain.comm.allreduce(\n  np.max(np.abs(uh.x.array -u_D.x.array)), op=MPI.MAX\n)\nif domain.comm.rank == 0:\n    print(f\"Error_max: {error_max:.2e}\")\n\nError_L2: 2.83e-02\nError_max: 1.78e-15\n\n\n\n\n\nK.7.2 Singular Poisson problem\nAuthor: Jørgen S. Dokken\nIn this example, we solve the singular Poisson problem by incorporating information about the nullspace of the discretized system into the matrix formulation\nThe problem is defined as\n\\[\\begin{aligned}\n   -\\Delta u &= f &&\\text{in } \\Omega\\\\\n   -\\nabla u \\cdot \\mathbf{n} &= g &&\\text{on } \\partial\\Omega\n\\end{aligned}\\]\nThis problem possesses a nullspace: if \\(\\tilde u\\) is a solution, then for any constant \\(c\\), \\(u_c = \\tilde u + c\\) is also a solution\nTo investigate this problem, we consider a manufactured solution on the unit square, given by\n\\[\\begin{aligned}\nu(x, y) &= \\sin(2\\pi x)\\\\\nf(x, y) &= -4\\pi^2\\sin(2\\pi x)\\\\\ng(x, y) &=\n\\begin{cases}\n   -2\\pi  & \\text{if } x=0,\\\\\n   \\phantom{-}2\\pi & \\text{if } x=1,\\\\\n   \\phantom{-}0 & \\text{otherwise}\n\\end{cases}\n\\end{aligned}\\]\nHere we define a simple wrapper function to set up the variational problem for a given manufactured solution\n\nimport typing\nimport numpy as np\n\nfrom mpi4py import MPI\n\nfrom dolfinx import fem, mesh\nimport dolfinx.fem.petsc\nimport ufl\n\ndef u_ex(mod, x):\n  return mod.sin(2 *mod.pi *x[0])\n\ndef setup_problem(N: int) \\\n  -&gt; typing.Tuple[fem.FunctionSpace, fem.Form, fem.Form]:\n  \"\"\"\n  Set up bilinear and linear form of \n    the singular Poisson problem\n  Args: \n    N, number of elements in each direction of the mesh   \n  Returns:\n    The function space, the bilinear form \n      and the linear form of the problem\n  \"\"\"\n\n  domain = dolfinx.mesh.create_unit_square(\n    MPI.COMM_WORLD, \n    N, N, \n    cell_type=mesh.CellType.quadrilateral\n  )\n  V = fem.functionspace(domain, (\"Lagrange\", 1))\n    \n  u = ufl.TrialFunction(V)\n  v = ufl.TestFunction(V)\n\n  x = ufl.SpatialCoordinate(domain)\n\n  u_exact = u_ex(ufl, x)\n    \n  f = -ufl.div(ufl.grad(u_exact))\n  n = ufl.FacetNormal(domain)\n  g = -ufl.dot(ufl.grad(u_exact), n)\n\n  F = ufl.dot(ufl.grad(u), ufl.grad(v)) *ufl.dx\n  F += ufl.inner(g, v) *ufl.ds\n  F -= f *v *ufl.dx\n\n  return V, *fem.form(ufl.system(F))\n\nUsing the convenience function defined above, we can now handle the nullspace. To do this, we employ PETSc, attaching additional information to the assembled matrices. Here, we make use of PETSc’s built-in function for creating constant nullspaces\n\nfrom petsc4py import PETSc\n\nnullspace = PETSc.NullSpace().create(\n  constant=True, \n  comm=MPI.COMM_WORLD\n)\n\nDirect solver\nWe begin by solving the singular problem using a direct solver (MUMPS). MUMPS provides additional options to handle singular matrices, which we utilize here\n\npetsc_options = {\n  \"ksp_error_if_not_converged\": True,\n  \"ksp_type\": \"preonly\",\n  \"pc_type\": \"lu\",\n  \"pc_factor_mat_solver_type\": \"mumps\",\n  \"ksp_monitor\": None,\n}\n\nNext, we configure the KSP solver\n\nksp = PETSc.KSP().create(MPI.COMM_WORLD)\nksp.setOptionsPrefix(\"singular_direct\")\nopts = PETSc.Options()\nopts.prefixPush(ksp.getOptionsPrefix())\n\nfor key, value in petsc_options.items():\n  opts[key] = value\nksp.setFromOptions()\n\nfor key, value in petsc_options.items():\n  del opts[key]\nopts.prefixPop()\n\nWe then assemble the bilinear and linear forms and construct the matrix A and the right-hand side vector b\n\nV, a, L = setup_problem(40)\n\nA = fem.petsc.assemble_matrix(a)\nA.assemble()\nb = fem.petsc.assemble_vector(L)\nb.ghostUpdate(\n  addv=PETSc.InsertMode.ADD_VALUES, \n  mode=PETSc.ScatterMode.REVERSE\n)\nksp.setOperators(A)\n\nWe begin by verifying that this is indeed the nullspace of A, after which we attach it to the matrix\n\nassert nullspace.test(A)\nA.setNullSpace(nullspace)\n\nWe can then solve the linear system of equations\n\nuh = fem.Function(V)\nksp.solve(b, uh.x.petsc_vec)\nuh.x.scatter_forward()\n\nksp.destroy()\n\n  Residual norms for singular_direct solve.\n  0 KSP Residual norm 1.553142231547e+00\n  1 KSP Residual norm 1.537746672407e-14\n\n\n&lt;petsc4py.PETSc.KSP at 0x305ba7bf0&gt;\n\n\nThe \\(L^2\\)-error can now be evaluated against the analytical solution\n\ndef compute_L2_error(uh: fem.Function) -&gt; float:\n  mesh = uh.function_space.mesh\n  u_exact = u_ex(ufl, ufl.SpatialCoordinate(mesh))\n    \n  error_L2 = fem.form(\n    ufl.inner(uh -u_exact, uh -u_exact) *ufl.dx\n  )\n  error_local = fem.assemble_scalar(error_L2)\n  return np.sqrt(\n    mesh.comm.allreduce(error_local, op=MPI.SUM)\n  )\n\nprint(\"Direct solver L2 error: \"\n     f\"{compute_L2_error(uh):.5e}\")    \n\nDirect solver L2 error: 1.59184e-03\n\n\nWe additionally confirm that the solution’s mean value coincides with that of the manufactured solution\n\nu_exact = u_ex(ufl, ufl.SpatialCoordinate(V.mesh))\nex_mean = V.mesh.comm.allreduce(\n  fem.assemble_scalar(fem.form(u_exact *ufl.dx)), \n  op=MPI.SUM\n)\napprox_mean = V.mesh.comm.allreduce(\n  fem.assemble_scalar(fem.form(uh *ufl.dx)), \n  op=MPI.SUM\n)\n\nprint(\"Mean value of manufactured solution: \"\n     f\"{ex_mean:.5e}\")\nprint(\"Mean value of computed solution (direct solver): \"\n     f\"{approx_mean:.5e}\")\n\nassert np.isclose(ex_mean, approx_mean), \"Mean values do not match!\"\n\nMean value of manufactured solution: -1.17019e-15\nMean value of computed solution (direct solver): -4.16960e-15\n\n\nIterative solver\nWe can also solve the problem using an iterative solver, such as GMRES with AMG preconditioning. To do this, we select a new set of PETSc options and create a new KSP solver\n\nksp_iterative = PETSc.KSP().create(MPI.COMM_WORLD)\nksp_iterative.setOptionsPrefix(\"singular_iterative\")\n\npetsc_options_iterative = {\n  \"ksp_error_if_not_converged\": True,\n  \"ksp_monitor\": None,\n  \"ksp_type\": \"gmres\",\n  \"pc_type\": \"hypre\",\n  \"pc_hypre_type\": \"boomeramg\",\n  \"pc_hypre_boomeramg_max_iter\": 1,\n  \"pc_hypre_boomeramg_cycle_type\": \"v\",\n  \"ksp_rtol\": 1.0e-13,\n}\n\nopts.prefixPush(ksp_iterative.getOptionsPrefix())\nfor key, value in petsc_options_iterative.items():\n  opts[key] = value\nksp_iterative.setFromOptions()\n\nfor key, value in petsc_options_iterative.items():\n  del opts[key]\nopts.prefixPop()\n\nRather than defining the nullspace explicitly, we provide it as a near-nullspace to the multigrid preconditioner\n\nA_iterative = fem.petsc.assemble_matrix(a)\nA_iterative.assemble()\nA_iterative.setNearNullSpace(nullspace)\n\nksp_iterative.setOperators(A_iterative)\n\n\nuh_iterative = fem.Function(V)\n\n\nksp_iterative.solve(b, uh_iterative.x.petsc_vec)\nuh_iterative.x.scatter_forward()\n\n  Residual norms for singular_iterative solve.\n  0 KSP Residual norm 2.661001756726e+01\n  1 KSP Residual norm 6.492588815947e-01\n  2 KSP Residual norm 1.847006602521e-02\n  3 KSP Residual norm 4.324476514095e-04\n  4 KSP Residual norm 1.022324540344e-05\n  5 KSP Residual norm 1.784972954866e-07\n  6 KSP Residual norm 5.360051803078e-09\n  7 KSP Residual norm 1.003450493292e-10\n  8 KSP Residual norm 2.214497050213e-12\n\n\nWhen using the iterative solver, we correct the solution by subtracting its mean value and adding the mean value of the manufactured solution before evaluating the error\n\napprox_mean = V.mesh.comm.allreduce(\n  fem.assemble_scalar(fem.form(uh_iterative *ufl.dx)), \n  op=MPI.SUM\n)\nprint(\n  \"Mean value of computed solution (iterative solver):\", \n  approx_mean\n)\n\nuh_iterative.x.array[:] += ex_mean -approx_mean\n\napprox_mean = V.mesh.comm.allreduce(\n  fem.assemble_scalar(fem.form(uh_iterative *ufl.dx)), \n  op=MPI.SUM\n)\nprint(\n  \"Mean value of computed solution (iterative solver) post normalization:\",\n  approx_mean\n)\nprint(\"Iterative solver L2 error: \"\n     f\"{compute_L2_error(uh_iterative):.5e}\")\n\nnp.testing.assert_allclose(\n  uh.x.array, \n  uh_iterative.x.array, \n  rtol=1e-10, atol=1e-12\n)\n\nMean value of computed solution (iterative solver): -0.13100822065668918\nMean value of computed solution (iterative solver) post normalization: -6.375015600590568e-16\nIterative solver L2 error: 1.59184e-03\n\n\n\n\nK.7.3 A nonlinear Poisson equation\nAuthors: Anders Logg and Hans Petter Langtangen\nWe next address the solution of nonlinear PDEs. In contrast to linear problems, nonlinear equations lead to subtle but important differences in the definition of the variational form\nPDE problem\nTo illustrate, we consider the nonlinear Poisson equation\n\\[\\begin{aligned}\n-\\nabla \\cdot (q(u) \\nabla u) &=f &&\\text{in } \\Omega \\\\\n  u&=u_D  &&\\text{on } \\partial \\Omega\n  \\end{aligned}\\]\nThe nonlinearity arises from the coefficient \\(q(u)\\), which depends on the solution \\(u\\) itself (the problem reduces to the linear case when \\(q(u)\\) is constant)\nVariational formulation\nAs usual, we multiply the PDE by a test function \\(v \\in \\hat{V}\\), integrate over the domain, and apply integration by parts to reduce the order of derivatives. The boundary terms vanish under the Dirichlet conditions. The variational formulation of our model problem then takes the form\nFind \\(u\\in V\\) such that\n\\[\\begin{aligned}\n    F(u; v)&=0 && \\forall v \\in \\hat{V}\n\\end{aligned}\\] where \\[\\begin{aligned}\n    F(u; v)&=\\int_{\\Omega}(q(u)\\nabla u \\cdot \\nabla v - fv)\\,\\mathrm{d}x\n\\end{aligned}\\] and \\[\\begin{aligned}\n    V&=\\left\\{v\\in H^1(\\Omega)\\,\\vert\\, v=u_D \\text{ on } \\partial \\Omega \\right\\}\\\\\n    \\hat{V}&=\\left\\{v\\in H^1(\\Omega)\\,\\vert\\, v=0 \\text{ on } \\partial \\Omega \\right\\}\n\\end{aligned}\\]\nAs usual, the discrete problem is obtained by restricting \\(V\\) and \\(\\hat{V}\\) to corresponding finite-dimensional spaces. The resulting discrete nonlinear problem can then be written as:\nFind \\(u_h \\in V_h\\) such that\n\\[F(u_h; v) = 0 \\quad \\forall v \\in \\hat{V}_h\\] with \\[u_h = \\sum_{j=1}^N U_j \\phi_j\\]\nSince \\(F\\) is nonlinear in \\(u\\), this variational formulation leads to a system of nonlinear algebraic equations for the unknown coefficients \\(U_1, \\dots, U_N\\)\nTest problem\nTo set up a test problem, it is necessary to prescribe the right-hand side \\(f\\), the coefficient \\(q(u)\\), and the boundary condition \\(u_D\\). In earlier cases, we employed manufactured solutions that can be exactly reproduced, thereby avoiding approximation errors. For nonlinear problems this construction is more difficult, as the algebra becomes significantly more tedious. To address this, we employ the differentiation capabilities of UFL to derive a manufactured solution\nSpecifically, we select \\(q(u) = 1 + u^2\\) and define a two-dimensional manufactured solution that is linear in both \\(x\\) and \\(y\\)\n\nimport numpy as np\n\nfrom mpi4py import MPI\nfrom petsc4py import PETSc\n\nfrom dolfinx import mesh, fem, io, nls, log\nfrom dolfinx.fem.petsc import NonlinearProblem\nfrom dolfinx.nls.petsc import NewtonSolver\nimport ufl\n\nlog.set_log_level(log.LogLevel.WARNING)\n\ndef q(u):\n  return 1 +u**2\n\ndomain = mesh.create_unit_square(MPI.COMM_WORLD, 10, 10)\n\nx = ufl.SpatialCoordinate(domain)\n\n# manufactured solution and the source term\nu_ufl = 1 +x[0] +2 *x[1]\nf = - ufl.div(q(u_ufl) *ufl.grad(u_ufl))\n\nNote that since x is a 2D vector, the first component (index 0) represents the \\(x\\)-coordinate, while the second component (index 1) represents the \\(y\\)-coordinate. The resulting function f can be directly used in variational formulations in DOLFINx\nHaving defined both the source term and an exact solution, we can now construct the corresponding function space and boundary conditions. Since the exact solution is already specified, we only need to convert it into a Python function that can be evaluated for interpolation. This is accomplished using Python’s eval function\n\nV = fem.functionspace(domain, (\"Lagrange\", 1))\ndef u_exact(x): return eval(str(u_ufl))\n\n\nu_D = fem.Function(V)\nu_D.interpolate(u_exact)\n\nfdim = domain.topology.dim -1\nboundary_facets = mesh.locate_entities_boundary(\n  domain, \n  fdim, \n  lambda x: np.full(x.shape[1], True, dtype=bool)\n)\nbc = fem.dirichletbc(\n  u_D, \n  fem.locate_dofs_topological(V, fdim, boundary_facets)\n)\n\nWe are now ready to define the variational formulation. Since the problem is nonlinear, we replace the TrialFunction with a Function, which acts as the unknown of the problem\n\nuh = fem.Function(V)\nv = ufl.TestFunction(V)\nF = q(uh) *ufl.dot(ufl.grad(uh), ufl.grad(v)) *ufl.dx -f *v *ufl.dx\n\nNewton’s method\nThe next step is to define the nonlinear problem. Since the problem is nonlinear, we will use Newton’s method. Newton’s method requires routines for evaluating the residual F (including the enforcement of boundary conditions), as well as for computing the Jacobian matrix. DOLFINx provides the NonlinearProblem class, which implements these routines. In addition to the boundary conditions, you can specify the variational form of the Jacobian (automatically computed if not provided), along with form and JIT parameters\n\nproblem = NonlinearProblem(F, uh, bcs=[bc])\n\nNext, we use the DOLFINx Newton solver. The convergence criteria can be adjusted by setting the absolute tolerance (atol), the relative tolerance (rtol), or the convergence criterion type (residual or incremental)\n\nsolver = NewtonSolver(MPI.COMM_WORLD, problem)\nsolver.convergence_criterion = \"incremental\"\nsolver.rtol = 1e-6\nsolver.report = True\n\nWe can adjust the linear solver used in each Newton iteration by accessing the underlying PETSc object\n\nksp = solver.krylov_solver\n\nopts = PETSc.Options()\noption_prefix = ksp.getOptionsPrefix()\n\nopts[f\"{option_prefix}ksp_type\"] = \"gmres\"\nopts[f\"{option_prefix}ksp_rtol\"] = 1.0e-8\nopts[f\"{option_prefix}pc_type\"] = \"hypre\"\nopts[f\"{option_prefix}pc_hypre_type\"] = \"boomeramg\"\nopts[f\"{option_prefix}pc_hypre_boomeramg_max_iter\"] = 1\nopts[f\"{option_prefix}pc_hypre_boomeramg_cycle_type\"] = \"v\"\n\nksp.setFromOptions()\n\nWe are now ready to solve the nonlinear problem. After solving, we verify that the solver has converged and print the number of iterations\n\nlog.set_log_level(log.LogLevel.INFO)\n\nn, converged = solver.solve(uh)\nassert (converged)\nprint(f\"Number of interations: {n:d}\")\n\nNumber of interations: 8\n[2025-09-27 14:55:00.154] [info] PETSc Krylov solver starting to solve system.\n[2025-09-27 14:55:00.155] [info] PETSc Krylov solver starting to solve system.\n[2025-09-27 14:55:00.155] [info] Newton iteration 2: r (abs) = 20.37916572634954 (tol = 1e-10), r (rel) = 0.9225323398510277 (tol = 1e-06)\n[2025-09-27 14:55:00.155] [info] PETSc Krylov solver starting to solve system.\n[2025-09-27 14:55:00.156] [info] Newton iteration 3: r (abs) = 6.952713011092111 (tol = 1e-10), r (rel) = 0.31473823259321565 (tol = 1e-06)\n[2025-09-27 14:55:00.156] [info] PETSc Krylov solver starting to solve system.\n[2025-09-27 14:55:00.156] [info] Newton iteration 4: r (abs) = 2.935703719836487 (tol = 1e-10), r (rel) = 0.13289462670537136 (tol = 1e-06)\n[2025-09-27 14:55:00.156] [info] PETSc Krylov solver starting to solve system.\n[2025-09-27 14:55:00.156] [info] Newton iteration 5: r (abs) = 0.7005897377755923 (tol = 1e-10), r (rel) = 0.031714580407483046 (tol = 1e-06)\n[2025-09-27 14:55:00.156] [info] PETSc Krylov solver starting to solve system.\n[2025-09-27 14:55:00.157] [info] Newton iteration 6: r (abs) = 0.04908059170012947 (tol = 1e-10), r (rel) = 0.0022218001320755713 (tol = 1e-06)\n[2025-09-27 14:55:00.157] [info] PETSc Krylov solver starting to solve system.\n[2025-09-27 14:55:00.157] [info] Newton iteration 7: r (abs) = 0.00029947987676681893 (tol = 1e-10), r (rel) = 1.3556976529945522e-05 (tol = 1e-06)\n[2025-09-27 14:55:00.157] [info] PETSc Krylov solver starting to solve system.\n[2025-09-27 14:55:00.157] [info] Newton iteration 8: r (abs) = 1.52769046025878e-08 (tol = 1e-10), r (rel) = 6.915611138332291e-10 (tol = 1e-06)\n[2025-09-27 14:55:00.157] [info] Newton solver finished in 8 iterations and 33 linear solver iterations.\n\n\nWe observe that the solver converges after \\(8\\) iterations. If we view the problem in terms of finite differences on a uniform mesh, \\(\\mathcal{P}_1\\) elements mimic standard second-order finite differences, which compute derivatives of linear or quadratic functions exactly. In this case, \\(\\nabla u\\) is a constant vector, multiplied by \\(1+u^2\\), resulting in a second-order polynomial in \\(x\\) and \\(y\\). Such terms would also be computed exactly by the finite difference operator. Therefore, even with \\(\\mathcal{P}_1\\) elements, we can expect the manufactured solution to be reproduced by the numerical method. However, if we had chosen a different nonlinearity, such as \\(1+u^4\\), this would no longer hold, and we would need to verify convergence rates\n\nlog.set_log_level(log.LogLevel.WARNING)\n\n# Compute L2 error and error at nodes\nV_ex = fem.functionspace(domain, (\"Lagrange\", 2))\nu_ex = fem.Function(V_ex)\nu_ex.interpolate(u_exact)\n\nerror_local = fem.assemble_scalar(\n  fem.form((uh -u_ex)**2 *ufl.dx)\n)\nerror_L2 = np.sqrt(\n  domain.comm.allreduce(error_local, op=MPI.SUM)\n)\nif domain.comm.rank == 0:\n    print(f\"L2-error: {error_L2:.2e}\")\n\n# Compute values at mesh vertices\nerror_max = domain.comm.allreduce(\n  np.max(np.abs(uh.x.array -u_D.x.array)), \n  op=MPI.MAX\n)\nif domain.comm.rank == 0:\n    print(f\"Error_max: {error_max:.2e}\")\n\nL2-error: 4.65e-16\nError_max: 8.88e-16\n\n\n\n\nK.7.4 The equations of linear elasticity\nAuthors: Anders Logg and Hans Petter Langtangen\nAnalysis of structures is one of the central tasks in modern engineering, which likely makes the PDE governing the deformation of elastic bodies the most widely used PDE in practice. With DOLFINx, the equations of 2D or 3D elasticity can be solved in just a single page of code, as demonstrated in this section\nThe PDE problem\nThe equations governing small elastic deformations of a body \\(\\Omega\\) can be written as\n\\[\\begin{aligned}\n    -\\nabla \\cdot \\sigma (u) &= f && \\text{in } \\Omega\\\\\n    \\sigma(u)&= \\lambda \\mathrm{tr}(\\epsilon(u))I + 2 \\mu \\epsilon(u)\\\\\n    \\epsilon(u) &= \\frac{1}{2}\\left(\\nabla u + (\\nabla u )^T\\right)\n\\end{aligned}\\]\nwhere \\(\\sigma\\) is the stress tensor, \\(f\\) is the body force per unit volume, \\(\\lambda\\) and \\(\\mu\\) are Lamé’s elasticity parameters for the material in \\(\\Omega\\), \\(I\\) is the identity tensor, \\(\\mathrm{tr}\\) denotes the trace operator on a tensor, \\(\\epsilon\\) is the symmetric strain tensor (the symmetric gradient of \\(u\\)), and \\(u\\) is the displacement vector field. Here, we assume isotropic elastic conditions\nSubstituting \\(\\epsilon(u)\\) into the expression for \\(\\sigma\\), we obtain\n\\[\\begin{aligned}\n\\sigma(u) &= \\lambda(\\nabla \\cdot u)I + \\mu\\big(\\nabla u + (\\nabla u)^T\\big)\n\\end{aligned}\\]\nNote that the PDE above could alternatively be written as a single vector equation for \\(u\\), known as Navier’s equation, which governs the displacement field. However, it is often more convenient to keep the present representation when deriving the variational formulation\nThe variational formulation\nThe variational formulation is obtained by taking the inner product of the elasticity PDE with a vector-valued test function \\(v\\in\\hat{V}\\), where \\(\\hat{V}\\) denotes the corresponding test function space, and integrating over the domain \\(\\Omega\\):\n\\[ -\\int_{\\Omega}(\\nabla \\cdot \\sigma)\\cdot v ~\\mathrm{d} x = \\int_{\\Omega} f\\cdot v \\,\\mathrm{d}x \\]\nSince \\(\\nabla \\cdot \\sigma\\) contains second-order derivatives of the unknown \\(u\\), we integrate this term by parts:\n\\[-\\int_{\\Omega} (\\nabla \\cdot \\sigma)\\cdot v \\,\\mathrm{d}x\n= \\int_{\\Omega} \\sigma : \\nabla v \\,\\mathrm{d}x\n    -   \\int_{\\partial \\Omega} (\\sigma \\cdot n)\\cdot v \\,\\mathrm{d}s\\]\nwhere the colon operator denotes the inner product between tensors (the sum of pairwise products of all components), and \\(n\\) is the outward unit normal to the boundary. The quantity \\(\\sigma \\cdot n\\) is known as the traction or stress vector at the boundary, and it is often prescribed as a boundary condition. Here, we assume that the traction is prescribed on a portion \\(\\partial \\Omega_T\\) of the boundary as \\(\\sigma \\cdot n = T\\). On the remaining part of the boundary, we impose Dirichlet conditions on the displacement, which eliminates the corresponding boundary integral. Thus we obtain\n\\[\\int_{\\Omega} \\sigma : \\nabla v \\,\\mathrm{d}x\n= \\int_{\\Omega} f \\cdot v \\,\\mathrm{d}x\n    +   \\int_{\\partial \\Omega_T} T \\cdot v \\,\\mathrm{d}s\\]\nIf we now substitute the expression for \\(\\sigma\\) in terms of the unknown \\(u\\), the variational formulation can be stated as:\nFind \\(u \\in V\\) such that\n\\[a(u, v) = L(v) \\qquad \\forall v \\in \\hat{V}\\]\nwhere\n\\[\\begin{aligned}\na(u, v) &= \\int_{\\Omega} \\sigma(u) : \\nabla v \\,\\mathrm{d}x \\\\\n\\sigma(u) &= \\lambda (\\nabla \\cdot u) I + \\mu \\big(\\nabla u + (\\nabla u)^T\\big)\\\\\nL(v) &= \\int_{\\Omega} f \\cdot v \\,\\mathrm{d}x\n    +   \\int_{\\partial \\Omega_T} T \\cdot v \\,\\mathrm{d}s\n\\end{aligned}\\]\nIt can be shown that the inner product of a symmetric tensor \\(A\\) and an antisymmetric tensor \\(B\\) vanishes. Decomposing \\(\\nabla v\\) into its symmetric and antisymmetric parts therefore implies that only the symmetric part contributes to the product \\(\\sigma : \\nabla v\\), since \\(\\sigma\\) is symmetric. Replacing \\(\\nabla v\\) by the symmetric gradient \\(\\epsilon(v)\\) leads to a slightly different, but equivalent, variational formulation:\n\\[a(u, v) = \\int_{\\Omega} \\sigma(u) : \\epsilon(v) \\,\\mathrm{d}x\\]\nwhere the symmetric gradient of \\(v\\) is defined as\n\\[\\epsilon(v) = \\tfrac{1}{2}\\big(\\nabla v + (\\nabla v)^T\\big)\\]\nThis formulation naturally arises from the minimization of elastic potential energy and is more commonly used than the earlier form based on \\(\\sigma : \\nabla v\\)\nScaling\nIt is often advantageous to scale a problem, as this reduces the need to specify physical parameters and produces dimensionless numbers that capture the relative influence of different parameters and physical effects. We first develop the code for the original dimensional model and then run the scaled problem by appropriately adjusting the parameters. For the present application, scaling reduces the number of active parameters from six to two\nIn Navier’s equation for \\(u\\), obtained by substituting \\(\\sigma(u)\\) into elasticity PDE, we have:\n\\[ -(\\lambda + \\mu)\\nabla (\\nabla \\cdot u) - \\mu \\nabla^2 u = f \\]\nWe introduce dimensionless coordinates by scaling with \\(L\\), and define \\(\\bar{u} = \\frac{u}{U}\\). This leads to the dimensionless governing equations:\n\\[-\\beta\\,\\bar{\\nabla}(\\bar{\\nabla}\\cdot \\bar{u}) -\\bar{\\nabla}^2 \\bar{u} = \\bar{f}, \\quad \\bar{f} = (0,0,\\gamma)\\]\nwhere \\(\\beta = 1 +\\frac{\\lambda}{\\mu}\\) is a dimensionless elasticity parameter and\n\\[ \\gamma = \\frac{\\rho g L^2}{\\mu U} \\]\nis a dimensionless number representing the ratio of the applied load \\(\\rho g\\) to the shear stress term \\(\\mu \\nabla^2 u \\sim \\mu \\frac{U}{L^2}\\) in the PDE\nOne option for scaling is to choose \\(U\\) such that \\(\\gamma\\) is of order one, i.e., \\(U = \\frac{\\rho g L^2}{\\mu}\\). However, in elasticity, this choice leads to displacements comparable to the size of the geometry. A more practical approach is to set \\(U\\) equal to the maximum deflection of a clamped beam, for which a formula exists:\n\\[U = \\frac{3}{2} \\, \\rho g L^2 \\frac{\\delta^2}{E}\\]\nwhere \\(\\delta = \\frac{L}{W}\\) reflects the beam’s slenderness, and \\(E\\) is the modulus of elasticity. The dimensionless parameter \\(\\delta\\) is therefore very important in this problem (as expected, \\(\\delta \\gg 1\\) is what justifies beam theory). Taking \\(E\\) to be of the same order as \\(\\mu\\), which is typical for many materials, we find that \\(\\gamma \\sim \\delta^{-2}\\) is an appropriate choice. By experimenting with the code to obtain displacements that “look right” in plots of the deformed geometry, we select \\(\\gamma = 0.4 \\, \\delta^{-2}\\) as our final value\nThe simulation code handles the dimensional problem with parameters \\(\\lambda,\\) \\(\\mu,\\) \\(\\rho,\\) \\(g,\\) \\(L,\\) \\(W\\). For the scaled problem, we simply set \\(\\mu = \\rho = L = 1,\\) \\(W = \\delta^{-1},\\) \\(g = \\gamma,\\) and \\(\\lambda = \\beta\\)\n\nK.7.4.1 Implementation\nAuthor: Jørgen S. Dokken\nIn this section, you will learn how to:\n\nWork with vector function spaces\nDefine constant boundary conditions on vector spaces\nVisualize cell wise constant functions\nCompute Von Mises stresses\n\nTest problem\nAs a test case, we consider a clamped beam in 3D that deforms under its own weight. This is modeled by prescribing the body force per unit volume as \\(f = (0, 0, -\\rho g)\\), where \\(\\rho\\) is the density of the beam and \\(g\\) is the gravitational acceleration\nThe beam has a box shape with length \\(L\\) and a square cross-section of width \\(W\\). At the clamped end (\\(x = 0\\)), we impose the displacement condition \\(u = u_D = (0,0,0)\\). The remaining boundaries are traction-free, i.e., we set \\(T = 0\\)\nWe begin by defining the physical variables that will be used in the program\n\nimport numpy as np\nfrom mpi4py import MPI\nimport pyvista\n\nfrom dolfinx import mesh, fem, plot, io, default_scalar_type\nfrom dolfinx.fem.petsc import LinearProblem\n\nimport ufl\n\n# Scaled variable\nL = 1\nW = 0.2\n\nrho = 1\n\ndelta = W /L\ngamma = 0.4 *delta**2\ng = gamma\n\nmu = 1\n\nbeta = 1.25\nlambda_ = beta\n\nWe next create the mesh, which will be composed of hexahedral elements, together with the assoicated function space. Since we require a vector element with three components, we add (3, ) or (or more generally (domain.geometry.dim, )) to the element tuple to obtain a triplet\n\ndomain = mesh.create_box(\n  MPI.COMM_WORLD, \n  [np.array([0, 0, 0]), np.array([L, W, W])],\n  [20, 6, 6], \n  cell_type=mesh.CellType.hexahedron\n)\nV = fem.functionspace(domain, (\"Lagrange\", 1, (domain.geometry.dim, )))\n\nAlternatively, we can take advantage of basix.ufl to construct the vector element directly:\n\nimport basix\n\nelement = basix.ufl.element(\n  \"Lagrange\", \n  domain.topology.cell_name(), \n  1,\n  shape=(domain.geometry.dim,)\n)\nV = fem.functionspace(domain, element)\n\nBoundary conditions\nSince we want to clamp the boundary at \\(x = 0\\), we achieve this by using a marker function, which identifies the facets where \\(x\\) is (within machine precision) close to zero\n\ndef clamped_boundary(x):\n  return np.isclose(x[0], 0)\n\nfdim = domain.topology.dim -1\nboundary_facets = mesh.locate_entities_boundary(\n  domain, \n  fdim, \n  clamped_boundary\n)\n\nu_D = np.array([0, 0, 0], dtype=default_scalar_type)\nbc = fem.dirichletbc(\n  u_D, \n  fem.locate_dofs_topological(V, fdim, boundary_facets), \n  V\n)\n\nSince the traction \\(T\\) should vanish on the remaining boundary, we introduce a dolfinx.Constant\n\nT = fem.Constant(domain, default_scalar_type((0, 0, 0)))\n\nWe specify the integration measure \\(\\mathrm{d}s\\), corresponding to integration over the domain boundary, using ufl’s built-in measures\n\nds = ufl.Measure(\"ds\", domain=domain)\n\nVariational formulation\nWe are now ready to formulate the variational problem in a syntax that closely resembles the mathematical notation, as in the previous examples\n\ndef epsilon(u):\n  # Equivalent to 0.5 *(ufl.nabla_grad(u) +ufl.nabla_grad(u).T)  \n  return ufl.sym(ufl.grad(u))  \n\ndef sigma(u):\n  return (lambda_ *ufl.nabla_div(u) *ufl.Identity(len(u)) \n    +2 *mu *epsilon(u))\n\nu = ufl.TrialFunction(V)\nv = ufl.TestFunction(V)\n\nf = fem.Constant(domain, default_scalar_type((0, 0, -rho * g)))\na = ufl.inner(sigma(u), epsilon(v)) *ufl.dx\nL = ufl.dot(f, v) *ufl.dx +ufl.dot(T, v) *ds\n\n\n\n\n\n\n\nNote\n\n\n\nIn DOLFINx, grad(u) uses the \\(\\partial u_i / \\partial x_j\\) convention, but since continuum mechanics often prefers the opposite, ufl offers nabla_grad to support that usage\n\n\nSolve the linear variational problem\nAs in the previous examples, we assemble the system matrix and right-hand side vector, and solve the variational problem using PETSc\n\nproblem = LinearProblem(\n  a, \n  L, \n  bcs=[bc], \n  petsc_options={\"ksp_type\": \"preonly\", \"pc_type\": \"lu\"}\n)\nuh = problem.solve()\n\nVisualization\nAs in earlier demos, visualization can be done with PyVista or ParaView. We first use PyVista, adding vectors to the grid rather than scalars\n\nresults_folder = Path(\"fenicsx/linear_elasticity\")\nresults_folder.mkdir(exist_ok=True, parents=True)\n\n# Create plotter and pyvista grid\np = pyvista.Plotter(off_screen=True)\ntopology, cell_types, geometry = plot.vtk_mesh(V)\ngrid = pyvista.UnstructuredGrid(topology, cell_types, geometry)\n\n# Attach vector values to grid and warp grid by vector\ngrid[\"u\"] = uh.x.array.reshape((geometry.shape[0], 3))\nactor_0 = p.add_mesh(grid, style=\"wireframe\", color=\"k\")\n\nwarped = grid.warp_by_vector(\"u\", factor=1.5)\nactor_1 = p.add_mesh(warped, show_edges=True)\n\np.show_axes()\n\n# if not pyvista.OFF_SCREEN:\n#   p.show()\n# else:\n#   p.screenshot(results_folder/\"deflection.png\")\n\n# HTML 저장\np.export_html(results_folder/\"deflection.html\")\n\n        \n\nWe can also use ParaView for visualization. As explained in previous sections, the solution is saved using XDMFFile\n\nwith io.XDMFFile(\n  domain.comm, \n  results_folder/\"deformation.xdmf\", \n  \"w\"\n) as xdmf:\n  xdmf.write_mesh(domain)\n  uh.name = \"Deformation\"\n  xdmf.write_function(uh)\n\nStress computation\nOnce the displacement has been computed, we can evaluate various stress measures. In particular, we compute the von Mises stress, defined as\n\\[\\sigma_m = \\sqrt{\\tfrac{3}{2}\\, s : s}\\]\nwhere \\(s\\) is the deviatoric stress tensor, given by\n\\[s(u) = \\sigma(u) - \\tfrac{1}{3}\\,\\mathrm{tr}(\\sigma(u))\\, I\\]\n\ns = sigma(uh) -1. /3 *ufl.tr(sigma(uh)) *ufl.Identity(len(uh))\nvon_Mises = ufl.sqrt(3. /2 *ufl.inner(s, s))\n\nvon_Mises is an expression that needs to be projected onto a suitable function space for visualization. Because uh consists of first-order piecewise continuous functions, the resulting von Mises stress is cell-wise constant\n\n# This is suitable for representing \n#  cell-wise constant quantities like the von Mises stress\n# DG = Discontinuous Galerkin, 0 = cell-wise constant\nV_von_mises = fem.functionspace(domain, (\"DG\", 0))\nstress_expr = fem.Expression(\n  von_Mises, \n  V_von_mises.element.interpolation_points()\n)\n\nstresses = fem.Function(V_von_mises)\nstresses.interpolate(stress_expr)\n\nPreviously, we visualized only first-order Lagrange functions. Since the von Mises stress is piecewise constant per cell, we slightly adjust the plotting routine, setting values for each cell, which corresponds directly to the degrees of freedom in the function space\n\nwarped.cell_data[\"VonMises\"] = stresses.x.petsc_vec.array\nwarped.set_active_scalars(\"VonMises\")\n\np = pyvista.Plotter(off_screen=True)\np.add_mesh(warped)\np.show_axes()\n\n# if not pyvista.OFF_SCREEN:\n#   p.show()\n# else:\n#   p.screenshot(results_folder/\"stresses.png\")\n\n# HTML 저장\np.export_html(results_folder/\"stresses.html\")\n\n        \n\n\n\n\nK.7.5 The Navier-Stokes equations\nIn this section, we address the incompressible Navier–Stokes equations, which combine time dependence, nonlinearity, and vector-valued variables—challenges seen in earlier problems\nThe PDE problem\nThe incompressible Navier–Stokes equations form a system of equations governing the velocity \\(u\\) and pressure \\(p\\) of an incompressible fluid:\n\\[\\begin{aligned}\n\\rho \\left( \\frac{\\partial u }{\\partial t} + u \\cdot \\nabla u \\right) &= \\nabla \\cdot \\sigma (u, p) + f\\\\\n\\nabla \\cdot u &= 0\n\\end{aligned}\\tag{NS}\\label{eq:NS}\\]\nThe right-hand side \\(f\\) represents a given force per unit volume. As in the equations of linear elasticity, \\(\\sigma(u,p)\\) denotes the stress tensor, which for a Newtonian fluid is given by:\n\\[\\sigma(u, p) = 2 \\mu \\epsilon(u) - p I\\]\nwhere \\(\\epsilon(u)\\) is the strain-rate tensor:\n\\[\\epsilon(u) = \\frac{1}{2}\\left(\\nabla u + (\\nabla u)^T\\right)\\]\nThe parameter \\(\\mu\\) is the dynamic viscosity. Note that the momentum equation is very similar to the elasticity equation . The difference lies in the two additional terms, \\(\\rho\\left(\\frac{\\partial u}{\\partial t} + u \\cdot \\nabla u\\right)\\), and in the different form of the stress tensor. These extra terms represent the acceleration of the fluid, balanced by the force \\(F = \\nabla \\cdot \\sigma + f\\) per unit volume according to Newton’s second law of motion\nVariational formulation\nThe Navier–Stokes equations are more challenging than the time-dependent heat equation because they form a system of equations, rather than a single equation, and the system has a special structure\nIf we use the same approach as for the heat equation—replacing the time derivative with a simple difference quotient—we end up with a nonlinear system. Nonlinearity itself is not a problem, as we have already seen in the nonlinear Poisson equation\nThe main difficulty comes from the saddle point structure of the system. Intuitively, a saddle point system has competing constraints: in this case, the velocity and pressure fields are linked through the incompressibility condition \\(\\nabla \\cdot u = 0\\). This coupling makes standard solvers inefficient or unstable. To handle this efficiently, we need specialized iterative methods and preconditioners that respect the saddle point structure\nInstead, we will apply a simpler and often highly efficient approach known as a splitting method. The idea is to treat the two equations in \\(\\eqref{eq:NS}\\) separately. There are many splitting strategies for the incompressible Navier-Stokes equations. One of the earliest is Chorin’s method. Here, we will use a modified version of Chorin’s method, the so-called incremental pressure correction scheme (IPCS), which provides improved accuracy compared to the original scheme at little additional cost\nThe IPCS scheme involves three steps. First, we compute a tentative velocity \\(u^*\\) by advancing the momentum equation using a midpoint finite difference scheme in time, but with the pressure \\(p^n\\) from the previous time step. The nonlinear convective term is linearized by using the known velocity \\(u^n\\) from the previous step: \\(u^n \\cdot \\nabla u^n\\). Note that several alternative methods exist to linearize this term, such as the Adams-Bashforth method. The variational problem for the first step reads: for the \\((n+1)\\)-th step, find \\(u^*\\) such that\n\\[\\begin{aligned}\n\\left\\langle \\rho \\frac{u^*-u^n}{\\Delta t}, v\\right\\rangle\n    &+ \\left\\langle \\rho u^n\\cdot \\nabla u^n, v \\right\\rangle\n    +\\left\\langle \\sigma(u^{n+\\frac{1}{2}}, p^n), \\epsilon(v)\\right\\rangle\\\\\n    &+ \\left\\langle p^n n, v \\right\\rangle_{\\partial\\Omega}\n    -\\left\\langle \\mu \\nabla u^{n+\\frac{1}{2}}\\cdot n, v \\right \\rangle_{\\partial\\Omega}=\n    \\left\\langle f^{n+1}, v \\right\\rangle\n\\end{aligned}\\tag{V1}\\label{eq:NS-v1}\\]\nThis notation, which is particularly useful for problems involving many terms in the variational formulation, requires some explanation. First, we introduce the shorthand notation\n\\[\\begin{aligned}\n\\langle v, w \\rangle = \\int_{\\Omega} v w \\, \\mathrm{d}x, \\quad\n\\langle v, w \\rangle_{\\partial\\Omega} = \\int_{\\partial\\Omega} v w \\, \\mathrm{d}s\n\\end{aligned}\\]\nThis allows us to express the variational problem in a more compact form. Second, we introduce the notation \\(u^{n+\\frac{1}{2}}\\), which refers to the value of \\(u\\) at the midpoint of the time interval. This is usually approximated by the arithmetic mean:\n\\[u^{n+\\frac{1}{2}} \\approx \\frac{u^{n} + u^{n+1}}{2}\\]\nThird, we note that the variational problem \\(\\eqref{eq:NS-v1}\\) arises from the integration by parts of the term \\(\\langle -\\nabla \\cdot \\sigma, v\\rangle\\). As in the linear elasticity problem, this yields\n\\[\\langle -\\nabla \\cdot \\sigma, v\\rangle =\n    \\langle \\sigma, \\epsilon(v) \\rangle\n    - \\langle T, v\\rangle_{\\partial \\Omega}\\]\nwhere \\(T = \\sigma \\cdot n\\) denotes the boundary traction. If we solve a problem with a free boundary, we may set \\(T=0\\) on the boundary. However, in the case of flow through a channel or pipe, where we wish to model the continuation of the flow into an “imaginary channel” at the outflow, this term requires more careful treatment. In such situations, a common assumption is that the derivative of the velocity in the streamwise direction vanishes at the outflow. This corresponds to the flow being fully developed, meaning it does not change significantly downstream. Under this assumption, the remaining boundary term at the outflow becomes\n\\[pn - \\mu \\nabla u \\cdot n\\]\nwhich is precisely the term appearing in the variational problem \\(\\eqref{eq:NS-v1}\\)\nIt is important to note that both this argument and its implementation depend on the exact definition of \\(\\nabla u\\): whether it is taken as the matrix with components \\(\\frac{\\partial u_i}{\\partial x_j}\\) or with components \\(\\frac{\\partial u_j}{\\partial x_i}\\). In this work, we adopt the latter convention, \\(\\frac{\\partial u_j}{\\partial x_i}\\), which corresponds to using the UFL operator nabla_grad. If, instead, we use the operator grad (with the definition \\(\\frac{\\partial u_i}{\\partial x_j}\\)), then the boundary term must be written as\n\\[pn - \\mu (\\nabla u)^T \\cdot n\\]\n\n\n\n\n\n\nNote\n\n\n\nAs noted in Linear elasticity implementation, the use of nabla_grad and grad must be interpreted with care. For the Navier–Stokes equations, it is particularly important to consider the convective term \\(u \\cdot \\nabla u\\), which should be understood as the vector \\(w\\) with components\n\\[w_i = \\sum_{j} \\left(u_j \\frac{\\partial}{\\partial x_j}\\right) u_i = \\sum_j u_j \\frac{\\partial u_i}{\\partial x_j}\\]\nIn FEniCSx, this term can be implemented either as grad(u) *u, which corresponds to \\(\\sum_j \\frac{\\partial u_i}{\\partial x_j} u_j\\), or as dot(u, nabla_grad(u)), which corresponds to \\(\\sum_i u_i \\frac{\\partial u_j}{\\partial x_i}\\)\nIn what follows, we adopt the notation dot(u, nabla_grad(u)), since it most closely matches the standard mathematical form \\(u \\cdot \\nabla u\\)\n\n\nWe now proceed to the second step of our splitting scheme for the incompressible Navier–Stokes equations. In the first step, we computed the tentative velocity \\(u^*\\) using the pressure from the previous time step. We now use this tentative velocity to compute the updated pressure \\(p^{n+1}\\):\n\\[\\langle \\nabla p^{n+1}, \\nabla q \\rangle\n    = \\langle \\nabla p^n, \\nabla q \\rangle\n    - \\frac{\\rho}{\\Delta t} \\langle \\nabla \\cdot u^*, q \\rangle\n    \\tag{V2}\\label{eq:NS-v2}\\]\nHere, \\(q\\) denotes a scalar-valued test function from the pressure space, whereas the test function \\(v\\) in \\(\\eqref{eq:NS-v1}\\) is vector-valued and belongs to the velocity space\nOne way to interpret this step is as follows: subtract the Navier–Stokes momentum equation \\(\\eqref{eq:NS}\\) expressed in terms of the tentative velocity \\(u^*\\) and the pressure \\(p^n\\) from the same equation expressed in terms of the updated velocity \\(u^{n+1}\\) and pressure \\(p^{n+1}\\). This yields\n\\[\\frac{\\rho (u^{n+1}-u^*)}{\\Delta t} + \\nabla p^{n+1} - \\nabla p^n = 0\\]\nTaking the divergence and imposing the incompressibility condition \\(\\nabla \\cdot u^{n+1} = 0\\) from the Navier–Stokes continuity equation, we obtain\n\\[\n- \\frac{\\rho \\nabla \\cdot u^*}{\\Delta t} + \\nabla^2 p^{n+1} - \\nabla^2 p^n = 0\n\\tag{VT}\\label{eq:NS-vT}\\]\nwhich is a Poisson problem for the pressure \\(p^{n+1}\\) and leads directly to the variational formulation \\(\\eqref{eq:NS-v2}\\)\nFinally, the corrected velocity \\(u^{n+1}\\) is computed from \\(\\eqref{eq:NS-vT}\\). Multiplying this equation by a test function \\(v\\), we obtain\n\\[\\rho \\langle (u^{n+1} - u^*), v \\rangle\n    = - \\Delta t \\langle \\nabla(p^{n+1} - p^n), v \\rangle\n     \\tag{V3}\\label{eq:NS-v3}\\]\nIn summary, the incompressible Navier–Stokes equations can be solved efficiently by computing, at each time step, a sequence of three linear variational problems\n\nK.7.5.1 Channel flow (Poiseuille flow)\nIn this section, you will learn how to:\n\nSolve the Navier–Stokes problem using a splitting scheme\nVisualize functions from higher-order Lagrangian spaces\n\nWe will consider the flow between two infinite parallel plates, known as channel flow or Poiseuille flow. As we shall see, this problem admits an analytical solution. Let \\(H\\) denote the distance between the plates and \\(L\\) the length of the channel. We assume that no body forces are present\nWe first scale the problem to eliminate seemingly independent physical parameters. Since the physics of this flow is governed solely by viscous effects in the direction perpendicular to the flow, the natural time scale is based on diffusion across the channel: \\(t_v = H^2 / \\nu\\). We choose \\(U\\) (a characteristic inflow velocity) as the velocity scale and \\(H\\) as the spatial scale. For the pressure scale, we take the characteristic shear stress, \\(\\mu U / H\\), since this problem is a canonical example of shear-driven flow\nIntroducing the nondimensional variables\n\\[\\begin{aligned}\n  \\bar{t} &= \\frac{t}{t_v} = \\frac{\\nu t}{H^2} \\\\\n  \\bar{x} &= \\frac{x}{H}, \\;\n  \\bar{y} = \\frac{y}{H}, \\;\n  \\bar{z} = \\frac{z}{H} \\\\\n  \\bar{u} &= \\frac{u}{U}, \\;\n  \\bar{p} = \\frac{H p}{\\mu U}\n\\end{aligned}\\]\nand substituting into the governing equations, we obtain the nondimensional Navier–Stokes equations (where we drop the bars for simplicity):\n\\[\\begin{aligned}\n\\frac{\\partial u}{\\partial t} + \\mathrm{Re}\\, u \\cdot \\nabla u\n&= -\\nabla p + \\nabla^2 u\\\\\n\\nabla \\cdot u &= 0\n\\end{aligned}\\]\nHere, \\(\\mathrm{Re} = \\rho U H / \\mu\\) is the Reynolds number. Because the time and pressure scales differ from those in convection-dominated flows, the Reynolds number appears in association with the convective term rather than the viscous term\nThe exact solution can be derived by assuming \\(u = (u_x(x,y,z), 0, 0)\\), with the \\(x\\)-axis aligned with the channel. Since \\(\\nabla \\cdot u = 0\\), the velocity cannot depend on \\(x\\) (\\(u=(u_x(y, z), 0, 0)\\))\nThe physics of channel flow is two-dimensional, so we may omit the \\(z\\)-coordinate (more precisely, we set \\(\\partial/\\partial z = 0\\)). Substituting \\(u = (u_x, 0, 0)\\) into the scaled governing equations gives\n\\[\\frac{\\partial^2 u_x}{\\partial y^2 } = \\frac{\\partial p}{\\partial x}\\]\nDifferentiating this equation with respect to \\(x\\) yields\n\\[\\frac{\\partial^2 p}{\\partial x^2} = 0\\]\nso \\(\\partial p / \\partial x\\) is constant. We denote this constant by \\(-\\beta\\), which represents the driving force of the flow and can be specified as a parameter in the problem\nIntegrating \\(\\partial^2 u_x(y) / \\partial y^2 = -\\beta\\) across the channel width \\([0,1]\\) and imposing the no-slip condition \\(u = (0,0,0)\\) at the channel walls gives\n\\[u_x(y) = \\tfrac{1}{2} \\beta y(1-y)\\]\nThe characteristic velocity \\(U\\) can be defined as the maximum inflow at \\(y = 0.5\\), which implies \\(\\beta = 8\\). The channel length, \\(L/H\\) in the scaled model, does not affect the solution, so for simplicity we restrict the computation to the unit square\nFrom a mathematical perspective, the pressure must be prescribed at a point for uniqueness. However, since \\(p\\) is independent of \\(y\\), we can set it to a known value (e.g., zero) along the outlet boundary \\(x = 1\\). The resulting solution is\n\\[p(x) = 8(1-x), \\quad u_x(y) = 4y(1-y)\\]\nThe boundary conditions can be prescribed as \\(p = 8\\) at \\(x = 0\\), \\(p = 0\\) at \\(x = 1\\), and \\(u = (0,0,0)\\) on the walls \\(y = 0,1\\). This setup defines the pressure drop and yields a unit maximum velocity at both the inlet and outlet, along with a parabolic velocity profile, without requiring any further specifications\nIt is worth noting that the Navier–Stokes equations are only meaningful to solve in two- or three-dimensional geometries, even though in this case the underlying mathematical problem reduces to two one-dimensional problems: one for \\(u_x(y)\\) and one for \\(p(x)\\)\nThe scaled model is not straightforward to simulate using a standard dimensional Navier–Stokes solver. However, since the convection term vanishes, the Reynolds number coefficient in front of this term in the scaled PDEs is irrelevant and can be set to unity. In that case, setting \\(\\rho=\\mu=1\\) in the original Navier–Stokes equations reproduces the scaled model\nFor a specific engineering problem, one typically wants to simulate a particular fluid with its corresponding physical parameters. A general-purpose solver is therefore most naturally implemented in dimensional form, using the original physical parameters. However, nondimensionalization can greatly simplify numerical simulations. First, it reveals that all fluids behave in the same way: it does not matter whether oil, gas, or water flows between two plates, nor does the absolute flow speed matter—at least up to a critical Reynolds number beyond which the flow becomes unstable and transitions to turbulence of an entirely different nature. This means that a single simulation can effectively represent all types of channel flow. In other applications, nondimensionalization shows that it may be sufficient to prescribe only certain parameter ratios (dimensionless numbers) rather than the parameters themselves. This approach simplifies exploration of the input parameter space, which is often the main purpose of simulation. In practice, the nondimensional problem is frequently solved by fixing some dimensional input parameters to convenient values (often unity)\nImplementation\nAs in the previous example, we load the DOLFINx module together with the mpi4py module, create the unit square mesh, and specify the simulation time and temporal discretization\n\nimport numpy as np\nimport pyvista\n\nfrom mpi4py import MPI\nfrom petsc4py import PETSc\n\nfrom dolfinx.fem import (Constant, Function, functionspace,\n  assemble_scalar, dirichletbc, form, locate_dofs_geometrical)\nfrom dolfinx.fem.petsc import (assemble_matrix, assemble_vector, \n  apply_lifting, create_vector, set_bc)\nfrom dolfinx.io import VTXWriter\nfrom dolfinx.mesh import create_unit_square\nfrom dolfinx.plot import vtk_mesh\nfrom basix.ufl import element\nfrom ufl import (FacetNormal, Identity, TestFunction, TrialFunction,\n  div, dot, ds, dx, inner, lhs, nabla_grad, rhs, sym)\n\nmesh = create_unit_square(MPI.COMM_WORLD, 10, 10)\n\nt = 0\nT = 10\nnum_steps = 500\ndt = T /num_steps\n\nUnlike in the previous demos, we will define our two function spaces using ufl element definitions as input\n\nv_cg2 = element(\n  \"Lagrange\", \n  mesh.topology.cell_name(), \n  2, \n  shape=(mesh.geometry.dim, )\n)\ns_cg1 = element(\n  \"Lagrange\", \n  mesh.topology.cell_name(), \n  1\n)\n\nV = functionspace(mesh, v_cg2)\nQ = functionspace(mesh, s_cg1)\n\nThe first space, V, is a vector-valued function space for the velocity, while Q is a scalar-valued function space for the pressure. We use piecewise quadratic elements for the velocity and piecewise linear elements for the pressure. When creating the vector finite element, the dimension of the vector is set to the geometric dimension of the mesh\n\n\n\n\n\n\nNote\n\n\n\nIt is well known that certain finite element spaces are unstable for the Navier–Stokes equations, and even for the simpler Stokes equation. A prime example of an unstable pair is the use of continuous piecewise linear polynomials (first-order) for both velocity and pressure. Such an unstable choice typically produces solutions with spurious, non-physical oscillations in the pressure field. A simple remedy is to use continuous piecewise quadratic elements for the velocity and continuous piecewise linear elements for the pressure. This combination is known as the Taylor–Hood element. Note that spurious oscillations may also arise when using splitting methods with an unstable element pair\n\n\nSince we are working with two different function spaces, we need to define two corresponding sets of trial and test functions:\n\nu = TrialFunction(V)\nv = TestFunction(V)\np = TrialFunction(Q)\nq = TestFunction(Q)\n\nAs we saw in the Linear Elasticity Problem, we can use Python functions to define the different Dirichlet boundary conditions. For this problem, we impose three Dirichlet conditions. First, we set \\(u=0\\) at the channel walls, i.e., at \\(y=0\\) and \\(y=1\\). In this case, we use dolfinx.fem.locate_dofs_geometrical\n\ndef walls(x):\n  return np.logical_or(\n    np.isclose(x[1], 0), \n    np.isclose(x[1], 1)\n  )\n\n\nwall_dofs = locate_dofs_geometrical(V, walls)\nu_noslip = np.array((0,) *mesh.geometry.dim, dtype=PETSc.ScalarType)\nbc_noslip = dirichletbc(u_noslip, wall_dofs, V)\n\nSecond, we will impose \\(p=8\\) at the inflow boundary (\\(x=0\\))\n\ndef inflow(x):\n  return np.isclose(x[0], 0)\n\n\ninflow_dofs = locate_dofs_geometrical(Q, inflow)\nbc_inflow = dirichletbc(PETSc.ScalarType(8), inflow_dofs, Q)\n\nFinally, we set \\(p=0\\) at the outflow (\\(x=1\\)). This creates a pressure gradient that accelerates the flow from the initial state of zero velocity. We then collect the velocity and pressure boundary conditions in Python lists, so they can be easily accessed in the subsequent computations\n\ndef outflow(x):\n  return np.isclose(x[0], 1)\n\n\noutflow_dofs = locate_dofs_geometrical(Q, outflow)\nbc_outflow = dirichletbc(PETSc.ScalarType(0), outflow_dofs, Q)\nbcu = [bc_noslip]\nbcp = [bc_inflow, bc_outflow]\n\nWe now turn to the definition of the three variational forms, one for each step in the IPCS scheme. Let us first consider the formulation of the initial variational problem and the associated parameters\n\nu_n = Function(V)\nu_n.name = \"u_n\"\nU = 0.5 *(u_n +u)\n\nk = Constant(mesh, PETSc.ScalarType(dt))\n\nrho = Constant(mesh, PETSc.ScalarType(1))\nmu = Constant(mesh, PETSc.ScalarType(1))\n\nn = FacetNormal(mesh)\nf = Constant(mesh, PETSc.ScalarType((0, 0)))\n\n\n\n\n\n\n\nNote\n\n\n\nNote that we have wrapped several parameters as constants. This reduces the compilation time of the variational formulations. By wrapping them as constants, we can later modify their values without requiring recompilation\n\n\nThe next step is to set up the variational form for the first stage. Since the variational problem contains a mix of known and unknown quantities, we adopt the following naming convention:\n\nu (mathematically \\(u^{n+1}\\)) denotes the trial function in the variational form\nu_ represents the most recently computed approximation (\\(u^{n+1}\\) available as a Function object)\nu_n corresponds to \\(u^n\\)\nThe same convention applies to p, p_ (\\(p^{n+1}\\)), and p_n (\\(p^n\\))\n\n\n# Define strain-rate tensor\ndef epsilon(u):\n  return sym(nabla_grad(u))\n\n# Define stress tensor\ndef sigma(u, p):\n  return 2 *mu *epsilon(u) -p *Identity(len(u))\n\n# Define the variational problem for the first step\np_n = Function(Q)\np_n.name = \"p_n\"\n\nF1 = rho *dot((u -u_n) /k, v) *dx\nF1 += rho *dot(dot(u_n, nabla_grad(u_n)), v) *dx\nF1 += inner(sigma(U, p_n), epsilon(v)) *dx\nF1 += dot(p_n *n, v) *ds -dot(mu *nabla_grad(U) *n, v) *ds\nF1 -= dot(f, v) *dx\n\na1 = form(lhs(F1))\nL1 = form(rhs(F1))\n\nNote that we have used the ufl functions lhs and rhs to extract the bilinear form \\(a(u,v)\\) and the linear form \\(L(v)\\). This is particularly convenient for longer and more complex variational formulations. With our chosen discretization, \\(a(u,v)\\) (a1) is not time-dependent and therefore needs to be assembled only once, whereas the right-hand side depends on the solution from the previous time step (u_n). Consequently, as in the heat equation example, we create the matrix outside the time loop\n\nA1 = assemble_matrix(a1, bcs=bcu)\nA1.assemble()\nb1 = create_vector(L1)\n\nNext, we set up the variational formulations and data structures for the second and third steps of the IPCS scheme, following the same approach as in the first step\n\n# Define variational problem for step 2\nu_ = Function(V)\na2 = form(dot(nabla_grad(p), nabla_grad(q)) *dx)\nL2 = form(dot(nabla_grad(p_n), nabla_grad(q)) *dx -(rho /k) *div(u_) *q *dx)\n\nA2 = assemble_matrix(a2, bcs=bcp)\nA2.assemble()\nb2 = create_vector(L2)\n\n# Define variational problem for step 3\np_ = Function(Q)\na3 = form(rho *dot(u, v) *dx)\nL3 = form(rho *dot(u_, v) *dx -k *dot(nabla_grad(p_ -p_n), v) *dx)\n\nA3 = assemble_matrix(a3)\nA3.assemble()\nb3 = create_vector(L3)\n\nNow that the linear systems are ready, we can attach solvers to them using PETSc. Each step can have its own solution strategy. For the tentative velocity and pressure correction steps, we’ll go with the BiCGStab method and algebraic multigrid as a preconditioner. For the velocity update step, we’ll switch to the CGmethod with SOR (Gauss–Seidel) preconditioning\n\n# Solver for step 1\nsolver1 = PETSc.KSP().create(mesh.comm)\nsolver1.setOperators(A1)\nsolver1.setType(PETSc.KSP.Type.BCGS)\npc1 = solver1.getPC()\npc1.setType(PETSc.PC.Type.HYPRE)\npc1.setHYPREType(\"boomeramg\")\n\n# Solver for step 2\nsolver2 = PETSc.KSP().create(mesh.comm)\nsolver2.setOperators(A2)\nsolver2.setType(PETSc.KSP.Type.BCGS)\npc2 = solver2.getPC()\npc2.setType(PETSc.PC.Type.HYPRE)\npc2.setHYPREType(\"boomeramg\")\n\n# Solver for step 3\nsolver3 = PETSc.KSP().create(mesh.comm)\nsolver3.setOperators(A3)\nsolver3.setType(PETSc.KSP.Type.CG)\npc3 = solver3.getPC()\npc3.setType(PETSc.PC.Type.SOR)\n\nWe prepare output files for the velocity and pressure and write the mesh along with the initial conditions to file\n\nfrom pathlib import Path\n\nfolder = Path(\"fenicsx/ns\")\nfolder.mkdir(exist_ok=True, parents=True)\n\nvtx_u = VTXWriter(mesh.comm, folder/\"poiseuille_u.bp\", u_n, engine=\"BP4\")\nvtx_p = VTXWriter(mesh.comm, folder/\"poiseuille_p.bp\", p_n, engine=\"BP4\")\nvtx_u.write(t)\nvtx_p.write(t)\n\nWe also interpolate the analytical solution into our function space and set up a variational formulation to compute the \\(L^2\\) error\n\ndef u_exact(x):\n  values = np.zeros((2, x.shape[1]), dtype=PETSc.ScalarType)\n  values[0] = 4 *x[1] *(1.0 -x[1])\n  return values\n\nu_ex = Function(V)\nu_ex.interpolate(u_exact)\n\nL2_error = form(dot(u_ -u_ex, u_ -u_ex) *dx)\n\nThe next step is to set up the time-stepping loop. For all three steps, we only need to assemble the right-hand side and apply the boundary conditions using lifting, so that the solvers can use them at each time step\n\nfor i in range(num_steps):\n  # Update current time step\n  t += dt\n\n  # Step 1: Tentative veolcity step\n  with b1.localForm() as loc_1:\n    loc_1.set(0)\n  assemble_vector(b1, L1)\n  apply_lifting(b1, [a1], [bcu])\n  b1.ghostUpdate(\n    addv=PETSc.InsertMode.ADD_VALUES, \n    mode=PETSc.ScatterMode.REVERSE\n  )\n  set_bc(b1, bcu)\n    \n  solver1.solve(b1, u_.x.petsc_vec)\n  u_.x.scatter_forward()\n\n  # Step 2: Pressure corrrection step\n  with b2.localForm() as loc_2:\n    loc_2.set(0)\n  assemble_vector(b2, L2)\n  apply_lifting(b2, [a2], [bcp])\n  b2.ghostUpdate(\n    addv=PETSc.InsertMode.ADD_VALUES, \n    mode=PETSc.ScatterMode.REVERSE\n  )\n  set_bc(b2, bcp)\n\n  solver2.solve(b2, p_.x.petsc_vec)\n  p_.x.scatter_forward()\n\n  # Step 3: Velocity correction step\n  with b3.localForm() as loc_3:\n    loc_3.set(0)\n  assemble_vector(b3, L3)\n  b3.ghostUpdate(\n    addv=PETSc.InsertMode.ADD_VALUES, \n    mode=PETSc.ScatterMode.REVERSE\n  )\n\n  solver3.solve(b3, u_.x.petsc_vec)\n  u_.x.scatter_forward()\n    \n  # Update variable with solution form this time step\n  u_n.x.array[:] = u_.x.array[:]\n  p_n.x.array[:] = p_.x.array[:]\n\n  # Write solutions to file\n  vtx_u.write(t)\n  vtx_p.write(t)\n\n  # Compute error at current time-step\n  error_L2 = np.sqrt(\n    mesh.comm.allreduce(assemble_scalar(L2_error), \n    op=MPI.SUM)\n  )\n  error_max = mesh.comm.allreduce(\n    np.max(u_.x.petsc_vec.array -u_ex.x.petsc_vec.array), \n    op=MPI.MAX\n  )\n  \n  # Print error only every 20th step and at the last step\n  if (i % 20 == 0) or (i == num_steps -1):\n    print(f\"Time {t:.2f}, L2-error {error_L2:.2e}, Max error {error_max:.2e}\")\n\n# Close xmdf file\nvtx_u.close()\nvtx_p.close()\n\nb1.destroy()\nb2.destroy()\nb3.destroy()\n\nsolver1.destroy()\nsolver2.destroy()\nsolver3.destroy()\n\nTime 0.02, L2-error 5.88e-01, Max error 1.60e-01\nTime 0.42, L2-error 1.09e-02, Max error 1.28e-04\nTime 0.82, L2-error 2.11e-04, Max error 2.65e-04\nTime 1.22, L2-error 1.98e-05, Max error 1.56e-04\nTime 1.62, L2-error 9.53e-06, Max error 8.46e-05\nTime 2.02, L2-error 5.89e-06, Max error 5.24e-05\nTime 2.42, L2-error 4.55e-06, Max error 3.62e-05\nTime 2.82, L2-error 3.99e-06, Max error 2.70e-05\nTime 3.22, L2-error 3.71e-06, Max error 2.13e-05\nTime 3.62, L2-error 3.55e-06, Max error 1.75e-05\nTime 4.02, L2-error 3.46e-06, Max error 1.49e-05\nTime 4.42, L2-error 3.41e-06, Max error 1.30e-05\nTime 4.82, L2-error 3.37e-06, Max error 1.16e-05\nTime 5.22, L2-error 3.35e-06, Max error 1.06e-05\nTime 5.62, L2-error 3.34e-06, Max error 9.75e-06\nTime 6.02, L2-error 3.33e-06, Max error 9.11e-06\nTime 6.42, L2-error 3.32e-06, Max error 8.92e-06\nTime 6.82, L2-error 3.32e-06, Max error 8.92e-06\nTime 7.22, L2-error 3.32e-06, Max error 9.05e-06\nTime 7.62, L2-error 3.31e-06, Max error 9.26e-06\nTime 8.02, L2-error 3.31e-06, Max error 9.44e-06\nTime 8.42, L2-error 3.31e-06, Max error 9.58e-06\nTime 8.82, L2-error 3.31e-06, Max error 9.69e-06\nTime 9.22, L2-error 3.31e-06, Max error 9.79e-06\nTime 9.62, L2-error 3.31e-06, Max error 9.86e-06\nTime 10.00, L2-error 3.31e-06, Max error 1.05e-05\n\n\n&lt;petsc4py.PETSc.KSP at 0x3225b1530&gt;\n\n\nVerification\nAs in the previous problems, we compute the error at each degree of freedom as well as the \\(L^2(\\Omega)\\) error. We start from the initial condition \\(u=(0,0)\\). Although we have not explicitly specified the initial condition, FEniCSx initializes all Functions—including u_n and u_—to zero. Since the exact solution is quadratic, we expect the error to reach machine precision in a finite time. In our implementation, we observe that the error quickly approaches zero and is around \\(10^{-6}\\) at \\(T=10\\), which means the solution is basically exact by this time\nVisualization of vectors\nWe have already seen how to plot higher-order functions and vector functions. In this section, we’ll focus on visualizing vector functions using glyphs, rather than by warping the mesh\n\ntopology, cell_types, geometry = vtk_mesh(V)\nvalues = np.zeros((geometry.shape[0], 3), dtype=np.float64)\nvalues[:, :len(u_n)] = u_n.x.array.real.reshape((geometry.shape[0], len(u_n)))\n\n# Create a point cloud of glyphs\nfunction_grid = pyvista.UnstructuredGrid(topology, cell_types, geometry)\nfunction_grid[\"u\"] = values\nglyphs = function_grid.glyph(orient=\"u\", factor=0.2)\n\n# Create a pyvista-grid for the mesh\nmesh.topology.create_connectivity(mesh.topology.dim, mesh.topology.dim)\ngrid = pyvista.UnstructuredGrid(*vtk_mesh(mesh, mesh.topology.dim))\n\n# Create plotter\nplotter = pyvista.Plotter(off_screen=True)\nplotter.add_mesh(grid, style=\"wireframe\", color=\"k\")\nplotter.add_mesh(glyphs, scalar_bar_args={\"title\": \"u_x\"})\nplotter.view_xy()\n\n# if not pyvista.OFF_SCREEN:\n#   plotter.show()\n# else:\n#   plotter.screenshot(folder/\"glyphs.png\")\n\n# HTML 저장\nplotter.export_html(folder/\"glyphs.html\")\n\n        \n\n\n\n\nK.7.5.2 Flow past a cylinder\nAuthor: Jørgen S. Dokken — Modifications by Kee-Youn Yoo\nIn this section, we consider a slightly more challenging problem: flow past a cylinder. The geometry and parameters are taken from the DFG 2D-3 benchmark in FeatFlow\nTo solve this problem efficiently and ensure numerical stability, we replace the first-order backward difference scheme with a Crank–Nicolson time discretization, combined with a semi-implicit Adams–Bashforth approximation of the nonlinear term\n\n\n\n\n\n\nNote\n\n\n\nThis demo is computationally demanding, with a run time of up to 15 minutes, as it uses parameters from the DFG 2D-3 benchmark, which consists of 12,800 time steps. It is advised to download this demo rather than running it in a browser. The run time can be reduced by using 2 or 3 MPI processes\n\n\nThe computational geometry chosen for this example is \nThe kinematic viscosity is given by \\(\\nu = 0.001 = \\frac{\\mu}{\\rho}\\), and the inflow velocity profile is specified as\n\\[\nu(x,y,t) = \\left( \\frac{4U(t)y(0.41-y)}{0.41^2}, 0 \\right)\n\\]\nwhere \\[\nU(t) = 1.5 \\sin\\left(\\tfrac{\\pi t}{8}\\right)\n\\]\nThis profile attains a maximum value of \\(1.5\\) at \\(y = 0.41/2\\). No scaling is applied in this problem, since all parameters are given explicitly\nMesh generation\nAs in the Deflection of a Membrane example, we use GMSH to generate the mesh. We first create the rectangle and the obstacle\n\nimport sys\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom tqdm.notebook import tqdm\n\nfrom mpi4py import MPI\nfrom petsc4py import PETSc\nimport gmsh\n\nfrom dolfinx.cpp.mesh import to_type, cell_entity_type\nfrom dolfinx.fem import (Constant, Function, functionspace,\n  assemble_scalar, dirichletbc, form, locate_dofs_topological, set_bc)\nfrom dolfinx.fem.petsc import (apply_lifting, assemble_matrix, \n  assemble_vector, create_vector, create_matrix, set_bc)\nfrom dolfinx.mesh import create_mesh, meshtags_from_entities\nfrom dolfinx.graph import adjacencylist\nfrom dolfinx.geometry import (bb_tree, compute_collisions_points, \n  compute_colliding_cells)\nfrom dolfinx.io import VTXWriter, distribute_entity_data, gmshio\n\nfrom basix.ufl import element\nfrom ufl import (FacetNormal, Identity, Measure, \n  TestFunction, TrialFunction, as_vector, \n  div, dot, ds, dx, inner, lhs, grad, nabla_grad, rhs, sym, system)\n\nmesh_comm = MPI.COMM_WORLD\nmodel_rank = 0\n\nif not gmsh.isInitialized():\n    gmsh.initialize()\n\ngdim = 2\nL = 2.2\nH = 0.41\nc_x = c_y = 0.2\nr = 0.05\n\n# Ensure that mesh generation is performed only on\n#  the specified process (model_rank) in parallel runs,\n#  since Gmsh is not parallel-safe\nif mesh_comm.rank == model_rank:\n\n  for dim, tag in gmsh.model.occ.getEntities(gdim):\n    gmsh.model.occ.remove([(dim, tag)])\n\n  rectangle = gmsh.model.occ.addRectangle(0, 0, 0, L, H, tag=1)\n  obstacle = gmsh.model.occ.addDisk(c_x, c_y, 0, r, r)\n\nThe next step is to subtract the obstacle from the channel, so that the interior of the circle is not meshed\n\nif mesh_comm.rank == model_rank:\n  fluid = gmsh.model.occ.cut([(gdim, rectangle)], [(gdim, obstacle)])\n  gmsh.model.occ.synchronize()\n\nInfo    : [  0%] Difference                                                                                  Info    : [ 10%] Difference                                                                                  Info    : [ 20%] Difference                                                                                  Info    : [ 30%] Difference - Performing Face-Face intersection                                                                                Info    : [ 70%] Difference - Performing intersection of shapes                                                                                Info    : [ 80%] Difference - Making faces                                                                                Info    : [ 90%] Difference - Adding holes                                                                                                                                                                \n\n\nTo make GMSH mesh the fluid domain, we add a physical volume marker\n\n# ID for the fluid domain\nfluid_marker = 1\n\n# Remove any existing physical groups with the same (dim, tag)\nfor dim, tag in gmsh.model.getPhysicalGroups():\n    if dim == gdim and tag == fluid_marker:\n        gmsh.model.removePhysicalGroups([(dim, tag)])\n\nif mesh_comm.rank == model_rank:\n  # get all volume entities (2D: surfaces, 3D: volumes)\n  volumes = gmsh.model.getEntities(dim=gdim)\n\n  # ensure there is exactly one fluid region\n  assert (len(volumes) == 1)\n\n  # Register the volume as a physical group with marker = fluid_marker\n  gmsh.model.addPhysicalGroup(\n    volumes[0][0], \n    [volumes[0][1]], \n    fluid_marker\n  )\n\n  # Assign a human-readable name (\"Fluid\") to this physical group\n  gmsh.model.setPhysicalName(\n    volumes[0][0], \n    fluid_marker, \n    \"Fluid\"\n  )\n\nTo label the different surfaces of the mesh, we proceed as follows:\n\nAssign marker 2 to the inflow (left-hand side)\nAssign marker 3 to the outflow (right-hand side)\nAssign marker 4 to the fluid walls\nAssign marker 5 to the obstacle\n\nWe determine the correct marker for each surface by computing the center of mass of each geometric entity. This way, we can automatically identify and label each boundary in the mesh\n\n# Define boundary markers (unique IDs)\ninlet_marker, outlet_marker, wall_marker, obstacle_marker = 2, 3, 4, 5\ninflow, outflow, walls, obstacle = [], [], [], []\n\nif mesh_comm.rank == model_rank:\n  # Extract all 1D boundary entities of the fluid volume\n  boundaries = gmsh.model.getBoundary(volumes, oriented=False)\n  \n  # Identify each boundary by its center of mass\n  for boundary in boundaries:\n    center_of_mass = gmsh.model.occ.getCenterOfMass(\n      boundary[0], \n      boundary[1]\n    )\n    \n    # Left boundary → Inlet\n    if np.allclose(center_of_mass, [0, H /2, 0]):\n      inflow.append(boundary[1])\n    # Right boundary → Outlet\n    elif np.allclose(center_of_mass, [L, H /2, 0]):\n      outflow.append(boundary[1])\n    # Top/bottom → Walls\n    elif np.allclose(center_of_mass, [L /2, H, 0]) or \\\n         np.allclose(center_of_mass, [L /2, 0, 0]):\n      walls.append(boundary[1])\n    # Remaining boundary → Obstacle\n    else:\n      obstacle.append(boundary[1])\n  \n  # Register boundaries as physical groups with human-readable names\n  gmsh.model.addPhysicalGroup(1, walls, wall_marker)\n  gmsh.model.setPhysicalName(1, wall_marker, \"Walls\")\n  \n  gmsh.model.addPhysicalGroup(1, inflow, inlet_marker)\n  gmsh.model.setPhysicalName(1, inlet_marker, \"Inlet\")\n  \n  gmsh.model.addPhysicalGroup(1, outflow, outlet_marker)\n  gmsh.model.setPhysicalName(1, outlet_marker, \"Outlet\")\n    \n  gmsh.model.addPhysicalGroup(1, obstacle, obstacle_marker)\n  gmsh.model.setPhysicalName(1, obstacle_marker, \"Obstacle\")\n\nIn previous meshes, uniform element sizes were employed. In this example, variable mesh sizes are used to better capture the flow in the region of interest, particularly around the circular obstacle. This is accomplished using GMSH fields\n\n# Define minimum element size near obstacle (1/3 of radius)\nres_min = r /3\n\nif mesh_comm.rank == model_rank:\n  # Distance field: measure distance from obstacle edges\n  distance_field = gmsh.model.mesh.field.add(\"Distance\")\n  gmsh.model.mesh.field.setNumbers(distance_field, \"EdgesList\", obstacle)\n  \n  # Threshold field: refine mesh based on distance\n  threshold_field = gmsh.model.mesh.field.add(\"Threshold\")\n  gmsh.model.mesh.field.setNumber(threshold_field, \"IField\", distance_field)\n  # fine near obstacle\n  gmsh.model.mesh.field.setNumber(threshold_field, \"LcMin\", res_min)\n  # coarse far away\n  gmsh.model.mesh.field.setNumber(threshold_field, \"LcMax\", 0.25 *H)\n  # within radius → LcMin\n  gmsh.model.mesh.field.setNumber(threshold_field, \"DistMin\", r)\n  # beyond 2H → LcMax\n  gmsh.model.mesh.field.setNumber(threshold_field, \"DistMax\", 2 *H)\n  \n  # Min field: apply the smallest size rule if multiple fields exist\n  min_field = gmsh.model.mesh.field.add(\"Min\")\n  gmsh.model.mesh.field.setNumbers(min_field, \"FieldsList\", [threshold_field])\n\n  # Set background mesh: controls final mesh refinement\n  gmsh.model.mesh.field.setAsBackgroundMesh(min_field)\n\nGenerating mesh\nWe are now ready to generate the mesh. At this stage, we need to decide whether the mesh should consist of triangles or quadrilaterals. In this demo, to match the DFG 2D-3 benchmark, we use second-order quadrilateral elements\n\nif mesh_comm.rank == model_rank:\n  # Set mesh algorithm for quadrilaterals (Delquad/Delaunay)\n  gmsh.option.setNumber(\"Mesh.Algorithm\", 8)\n\n  # Choose recombination algorithm (Blossom)\n  gmsh.option.setNumber(\"Mesh.RecombinationAlgorithm\", 2)\n\n  # Apply recombination to all elements (triangles → quads)\n  gmsh.option.setNumber(\"Mesh.RecombineAll\", 1)\n  \n  # Set subdivision algorithm for recombined quads\n  gmsh.option.setNumber(\"Mesh.SubdivisionAlgorithm\", 1)\n\n  # Generate the mesh for the given dimension\n  gmsh.model.mesh.generate(gdim)\n\n  # Upgrade mesh to second-order (quadratic) elements\n  gmsh.model.mesh.setOrder(2)\n\n  # Optimize mesh quality using Netgen optimizer\n  gmsh.model.mesh.optimize(\"Netgen\")\n\nInfo    : Meshing 1D...\nInfo    : [  0%] Meshing curve 5 (Ellipse)\nInfo    : [ 30%] Meshing curve 6 (Line)\nInfo    : [ 50%] Meshing curve 7 (Line)\nInfo    : [ 70%] Meshing curve 8 (Line)\nInfo    : [ 90%] Meshing curve 9 (Line)\nInfo    : Done meshing 1D (Wall 0.0059325s, CPU 0.01051s)\nInfo    : Meshing 2D...\nInfo    : Meshing surface 1 (Plane, Frontal-Delaunay for Quads)\nInfo    : Simple recombination completed (Wall 0.00190204s, CPU 0.002754s): 101 quads, 24 triangles, 0 invalid quads, 0 quads with Q &lt; 0.1, avg Q = 0.813901, min Q = 0.470126\nInfo    : Simple recombination completed (Wall 0.00247208s, CPU 0.003976s): 476 quads, 0 triangles, 0 invalid quads, 0 quads with Q &lt; 0.1, avg Q = 0.848887, min Q = 0.511467\nInfo    : Done meshing 2D (Wall 0.00921125s, CPU 0.013628s)\nInfo    : Refining mesh...\nInfo    : Meshing order 2 (curvilinear on)...\nInfo    : [  0%] Meshing curve 5 order 2\nInfo    : [ 20%] Meshing curve 6 order 2\nInfo    : [ 40%] Meshing curve 7 order 2\nInfo    : [ 60%] Meshing curve 8 order 2\nInfo    : [ 70%] Meshing curve 9 order 2\nInfo    : [ 90%] Meshing surface 1 order 2\nInfo    : Done meshing order 2 (Wall 0.00349354s, CPU 0.004843s)\nInfo    : Done refining mesh (Wall 0.00412217s, CPU 0.005562s)\nInfo    : 2016 nodes 2133 elements\nInfo    : Meshing order 2 (curvilinear on)...\nInfo    : [  0%] Meshing curve 5 order 2\nInfo    : [ 20%] Meshing curve 6 order 2\nInfo    : [ 40%] Meshing curve 7 order 2\nInfo    : [ 60%] Meshing curve 8 order 2\nInfo    : [ 70%] Meshing curve 9 order 2\nInfo    : [ 90%] Meshing surface 1 order 2\nInfo    : Done meshing order 2 (Wall 0.0082355s, CPU 0.014021s)\nInfo    : Optimizing mesh (Netgen)...\nInfo    : Done optimizing mesh (Wall 1.29198e-06s, CPU 1e-06s)\n\n\nLoading mesh and boundary markers\nHaving generated the mesh, we now need to load it together with the corresponding facet markers into DOLFINx. We follow the same structure as in Deflection of a membrane, with the difference that facet markers are also loaded. For more details about the function used below, see A GMSH tutorial for DOLFINx\n\n# Convert GMSH model to DOLFINx mesh and facet markers\nmesh, _, ft = gmshio.model_to_mesh(\n  gmsh.model, \n  mesh_comm, \n  model_rank, \n  gdim=gdim\n)\n\n# Assign a human-readable name to the facet markers\nft.name = \"Facet markers\"\n\ngmsh.finalize()\n\n\n# For notebook only\nif \"ipykernel\" in sys.modules:\n\n  import pyvista\n  from dolfinx import plot\n\n  V = functionspace(mesh, (\"Lagrange\", 2))\n\n  # Extract topology from mesh and create pyvista mesh\n  topology, cell_types, x = plot.vtk_mesh(V)\n  grid = pyvista.UnstructuredGrid(topology, cell_types, x)\n\n  plotter = pyvista.Plotter(off_screen=True)\n  plotter.add_mesh(grid, show_edges=True)\n  plotter.add_axes()\n  plotter.view_xy()\n\n  # if not pyvista.OFF_SCREEN:\n  #   plotter.show()\n\n  # HTML 저장\n  plotter.export_html(\"fenicsx/ns/flow_past_a_cylinder_mesh.html\")\n\n\n\nPhysical and discretization parameters\nIn accordance with the DGF-2 benchmark, the problem-specific parameters are defined\n\nt = 0\nT = 8                        # Final time\ndt = 1 /1600                 # Time step size\nnum_steps = int(T /dt)\n\nk = Constant(mesh, PETSc.ScalarType(dt))\nmu = Constant(mesh, PETSc.ScalarType(0.001))  # Dynamic viscosity\nrho = Constant(mesh, PETSc.ScalarType(1))     # Density\n\n\n\n\n\n\n\nNote\n\n\n\nConverting the notebook to a Python script and running it with mpirun considerably decreases the runtime\n\n\nBoundary conditions\nAfter creating the mesh and the corresponding mesh tags, we can define the function spaces V and Q together with the boundary conditions. Since ft contains the facet markers, we use it to identify the facets corresponding to the inlet and the walls\n\nv_cg2 = element(\n  \"Lagrange\", \n  mesh.topology.cell_name(), \n  2, \n  shape=(mesh.geometry.dim, )\n)\ns_cg1 = element(\n  \"Lagrange\", \n  mesh.topology.cell_name(), \n  1\n)\nV = functionspace(mesh, v_cg2)\nQ = functionspace(mesh, s_cg1)\n\nfdim = mesh.topology.dim -1\n\n# Define boundary conditions\nclass InletVelocity():\n  def __init__(self, t):\n    self.t = t\n\n  def __call__(self, x):\n    values = np.zeros((gdim, x.shape[1]), dtype=PETSc.ScalarType)\n    values[0] = 4 *1.5 *np.sin(self.t *np.pi /8) *x[1] *(0.41 -x[1]) /(0.41**2)\n    return values\n\n# Inlet\nu_inlet = Function(V)\ninlet_velocity = InletVelocity(t)\nu_inlet.interpolate(inlet_velocity)\nbcu_inflow = dirichletbc(\n  u_inlet, \n  locate_dofs_topological(V, fdim, ft.find(inlet_marker))\n)\n\n# Walls\nu_nonslip = np.array((0,) *mesh.geometry.dim, dtype=PETSc.ScalarType)\nbcu_walls = dirichletbc(\n  u_nonslip, \n  locate_dofs_topological(V, fdim, ft.find(wall_marker)), \n  V\n)\n\n# Obstacle\nbcu_obstacle = dirichletbc(\n  u_nonslip, \n  locate_dofs_topological(V, fdim, ft.find(obstacle_marker)), \n  V\n)\n\nbcu = [bcu_inflow, bcu_obstacle, bcu_walls]\n\n# Outlet\nbcp_outlet = dirichletbc(\n  PETSc.ScalarType(0), \n  locate_dofs_topological(Q, fdim, ft.find(outlet_marker)), \n  Q\n)\nbcp = [bcp_outlet]\n\nVariational form\nIn contrast to Poiseuille flow, we employ a Crank–Nicolson discretization together with a semi-implicit Adams–Bashforth approximation\nThe first step can be written as\n\\[\n\\rho\\left(\\frac{u^* -u^n}{\\delta t} +\\left(\\frac{3}{2}u^{n} -\\frac{1}{2} u^{n-1}\\right)\\cdot \\frac{1}{2}\\nabla (u^* +u^n) \\right) - \\frac{1}{2}\\mu \\Delta( u^* + u^n )+ \\nabla p^{n-\\tfrac{1}{2}} = f^{n+\\tfrac{1}{2}} \\;\\, \\text{ in } \\Omega\n\\]\n\\[\nu^{*}=g(\\cdot, t^{n+1}) \\;\\, \\text{ on } \\partial \\Omega_{D}\n\\]\n\\[\n\\tfrac{1}{2}\\nu \\nabla (u^*+u^n) \\cdot n = p^{n-\\tfrac{1}{2}} \\;\\, \\text{ on } \\partial \\Omega_{N}\n\\]\nwhere the temporal derivative of the velocity uses the two previous time steps, and the pressure is computed in a staggered manner, at the midpoint between the previous and the current solution\nThe second step is\n\\[\n\\nabla^2 \\phi = \\frac{\\rho}{\\delta t} \\nabla \\cdot u^* \\;\\,\\text{in } \\Omega\n\\]\n\\[\n\\nabla \\phi \\cdot n = 0 \\;\\, \\text{on } \\partial \\Omega_D\n\\]\n\\[\n\\phi = 0 \\;\\,\\text{on } \\partial\\Omega_N\n\\]\nwhere \\(p^{n+\\tfrac{1}{2}}=p^{n-\\tfrac{1}{2}} + \\phi\\)\nFinally, the third step is\n\\[\n\\rho (u^{n+1}-u^{*}) = -\\delta t \\nabla\\phi\n\\]\nWe begin by defining all the variables required in the variational formulations\n\nu = TrialFunction(V)\nv = TestFunction(V)\n\nu_ = Function(V)\nu_.name = \"u\"\n\nu_s = Function(V)\nu_n = Function(V)\nu_n1 = Function(V)\n\np = TrialFunction(Q)\nq = TestFunction(Q)\n\np_ = Function(Q)\np_.name = \"p\"\n\nphi = Function(Q)\n\nNext, we define the variational formulation for the first step, where the diffusion and pressure terms are integrated by parts\n\nf = Constant(mesh, PETSc.ScalarType((0, 0)))\n\nF1 = rho /k *dot(u -u_n, v) *dx\nF1 += inner(dot(1.5 *u_n -0.5 *u_n1, 0.5 *nabla_grad(u +u_n)), v) *dx\nF1 += 0.5 *mu *inner(grad(u +u_n), grad(v)) *dx -dot(p_, div(v)) *dx\nF1 += dot(f, v) *dx\n\na1 = form(lhs(F1))\nL1 = form(rhs(F1))\n\nA1 = create_matrix(a1)\nb1 = create_vector(L1)\n\nNext, we set up the second step\n\na2 = form(dot(grad(p), grad(q)) *dx)\nL2 = form(-rho /k *dot(div(u_s), q) *dx)\n\nA2 = assemble_matrix(a2, bcs=bcp)\nA2.assemble()\nb2 = create_vector(L2)\n\nWe finally complete the last step\n\na3 = form(rho *dot(u, v) *dx)\nL3 = form(rho *dot(u_s, v) *dx -k *dot(nabla_grad(phi), v) *dx)\n\nA3 = assemble_matrix(a3)\nA3.assemble()\nb3 = create_vector(L3)\n\nAs in the previous tutorials, we use PETSc as the linear algebra backend\n\n# Solver for step 1\nsolver1 = PETSc.KSP().create(mesh.comm)\nsolver1.setOperators(A1)\nsolver1.setType(PETSc.KSP.Type.BCGS)\npc1 = solver1.getPC()\npc1.setType(PETSc.PC.Type.JACOBI)\n\n# Solver for step 2\nsolver2 = PETSc.KSP().create(mesh.comm)\nsolver2.setOperators(A2)\nsolver2.setType(PETSc.KSP.Type.MINRES)\npc2 = solver2.getPC()\npc2.setType(PETSc.PC.Type.HYPRE)\npc2.setHYPREType(\"boomeramg\")\n\n# Solver for step 3\nsolver3 = PETSc.KSP().create(mesh.comm)\nsolver3.setOperators(A3)\nsolver3.setType(PETSc.KSP.Type.CG)\npc3 = solver3.getPC()\npc3.setType(PETSc.PC.Type.SOR)\n\nVerification of the implementation by computing known physical quantities\nAs a further verification of our implementation, we compute the drag and lift coefficients over the obstacle, defined as\n\\[\nC_{\\text{D}}(u,p,t,\\partial\\Omega_S) = \\frac{2}{\\rho L U_{\\text{mean}}^2}\\int_{\\partial\\Omega_S}\\rho \\nu n \\cdot \\nabla u_{t_S}(t)n_y - p(t)n_x\\,\\mathrm{d} s\n\\]\n\\[\nC_{\\text{L}}(u,p,t,\\partial\\Omega_S) = -\\frac{2}{\\rho L U_{\\text{mean}}^2}\\int_{\\partial\\Omega_S}\\rho \\nu n \\cdot \\nabla u_{t_S}(t)n_x + p(t)n_y\\,\\mathrm{d} s\n\\]\nwhere \\(u_{t_S}\\) is the tangential velocity component at the obstacle interface \\(\\partial\\Omega_S\\), defined as \\(u_{t_S}=u\\cdot (n_y,-n_x)\\), \\(U_{\\text{mean}}=1\\) is the average inflow velocity, and \\(L\\) is the channel length. We use UFL to define the relevant integrals and assemble them at each time step\n\nn = -FacetNormal(mesh)  # Normal pointing out of obstacle\ndObs = Measure(\n  \"ds\", \n  domain=mesh, \n  subdomain_data=ft, \n  subdomain_id=obstacle_marker\n)\n\nu_t = inner(as_vector((n[1], -n[0])), u_)\n\ndrag = form(2 /0.1 *(mu /rho *inner(grad(u_t), n) *n[1] -p_ *n[0]) *dObs)\nlift = form(-2 /0.1 *(mu /rho *inner(grad(u_t), n) *n[0] +p_ *n[1]) *dObs)\n\nif mesh.comm.rank == 0:\n  C_D = np.zeros(num_steps, dtype=PETSc.ScalarType)\n  C_L = np.zeros(num_steps, dtype=PETSc.ScalarType)\n  t_u = np.zeros(num_steps, dtype=np.float64)\n  t_p = np.zeros(num_steps, dtype=np.float64)\n\nWe will also evaluate the pressure at two points: one in front of the obstacle, \\((0.15, 0.2)\\), and one behind it, \\((0.25, 0.2)\\). To do this, we first need to determine which cell contains each point, so that we can construct a linear combination of the local basis functions and their coefficients\n\ntree = bb_tree(mesh, mesh.geometry.dim)\npoints = np.array([[0.15, 0.2, 0], [0.25, 0.2, 0]])\n\ncell_candidates = compute_collisions_points(tree, points)\ncolliding_cells = compute_colliding_cells(mesh, cell_candidates, points)\n\nfront_cells = colliding_cells.links(0)\nback_cells = colliding_cells.links(1)\n\nif mesh.comm.rank == 0:\n  p_diff = np.zeros(num_steps, dtype=PETSc.ScalarType)\n\nSolving the time-dependent problem\nAs in the previous example, we create output files for the velocity and pressure and solve the time-dependent problem. Since the problem involves many time steps, we use the tqdm package to visualize the progress\n\nfrom pathlib import Path\n\nfolder = Path(\"fenicsx/ns\")\nfolder.mkdir(exist_ok=True, parents=True)\n\nvtx_u = VTXWriter(mesh.comm, folder/\"dfg2D-3-u.bp\", [u_], engine=\"BP4\")\nvtx_p = VTXWriter(mesh.comm, folder/\"dfg2D-3-p.bp\", [p_], engine=\"BP4\")\nvtx_u.write(t)\nvtx_p.write(t)\n\nfor i in tqdm(range(num_steps), desc=\"Time Stepping\", unit='step'):\n\n  # Update current time step\n  t += dt\n  \n  # Update inlet velocity\n  inlet_velocity.t = t\n  u_inlet.interpolate(inlet_velocity)\n\n  # Step 1: Tentative velocity step\n  A1.zeroEntries()\n  assemble_matrix(A1, a1, bcs=bcu)\n  A1.assemble()\n  \n  with b1.localForm() as loc:\n    loc.set(0)\n  assemble_vector(b1, L1)\n  apply_lifting(b1, [a1], [bcu])\n  \n  b1.ghostUpdate(\n    addv=PETSc.InsertMode.ADD_VALUES, \n    mode=PETSc.ScatterMode.REVERSE\n  )\n  \n  set_bc(b1, bcu)\n  \n  solver1.solve(b1, u_s.x.petsc_vec)\n  u_s.x.scatter_forward()\n\n  # Step 2: Pressure corrrection step\n  with b2.localForm() as loc:\n    loc.set(0)\n  assemble_vector(b2, L2)\n  apply_lifting(b2, [a2], [bcp])\n\n  b2.ghostUpdate(\n    addv=PETSc.InsertMode.ADD_VALUES, \n    mode=PETSc.ScatterMode.REVERSE\n  )\n\n  set_bc(b2, bcp)\n\n  solver2.solve(b2, phi.x.petsc_vec)\n  phi.x.scatter_forward()\n\n  p_.x.petsc_vec.axpy(1, phi.x.petsc_vec)\n  p_.x.scatter_forward()\n\n  # Step 3: Velocity correction step\n  with b3.localForm() as loc:\n    loc.set(0)\n  assemble_vector(b3, L3)\n\n  b3.ghostUpdate(\n    addv=PETSc.InsertMode.ADD_VALUES, \n    mode=PETSc.ScatterMode.REVERSE\n  )\n  \n  solver3.solve(b3, u_.x.petsc_vec)\n  u_.x.scatter_forward()\n\n  # Write solutions to file\n  vtx_u.write(t)\n  vtx_p.write(t)\n\n  # Update variable with solution form this time step\n  with (\n    u_.x.petsc_vec.localForm() as loc_, \n    u_n.x.petsc_vec.localForm() as loc_n, \n    u_n1.x.petsc_vec.localForm() as loc_n1\n  ):\n    loc_n.copy(loc_n1)\n    loc_.copy(loc_n)\n\n  # Compute physical quantities\n  # For this to work in paralell, we gather contributions from all processors\n  # to processor zero and sum the contributions\n  drag_coeff = mesh.comm.gather(assemble_scalar(drag), root=0)\n  lift_coeff = mesh.comm.gather(assemble_scalar(lift), root=0)\n  \n  p_front = None\n  if len(front_cells) &gt; 0:\n    p_front = p_.eval(points[0], front_cells[:1])\n  p_front = mesh.comm.gather(p_front, root=0)\n  \n  p_back = None\n  if len(back_cells) &gt; 0:\n    p_back = p_.eval(points[1], back_cells[:1])\n  p_back = mesh.comm.gather(p_back, root=0)\n  \n  if mesh.comm.rank == 0:\n    t_u[i] = t\n    t_p[i] = t -dt /2\n    C_D[i] = sum(drag_coeff)\n    C_L[i] = sum(lift_coeff)\n    \n    # Choose first pressure that is found from the different processors\n    for pressure in p_front:\n      if pressure is not None:\n        p_diff[i] = pressure[0]\n        break\n    for pressure in p_back:\n      if pressure is not None:\n        p_diff[i] -= pressure[0]\n        break\n\nvtx_u.close()\nvtx_p.close()\n\n\n\n\n\nVideo\n\n\nVideo\n\nVerification using data from FEATFLOW\nWe use matplotlib to compare our numerical results with the reference data provided by FEATFLOW for different discretization levels\n\nif mesh.comm.rank == 0:\n  # Compute the total number of velocity degrees of freedom (DOFs)\n  # - V.dofmap.index_map.size_global : global number of scalar DOFs\n  # - V.dofmap.index_map_bs          : block size (e.g., 2 in 2D, 3 in 3D for vector spaces)\n  #   → Total number of velocity DOFs = block size × scalar DOFs  \n  num_velocity_dofs = V.dofmap.index_map_bs *V.dofmap.index_map.size_global\n\n  # Compute the total number of pressure degrees of freedom (DOFs)\n  # - Typically: Q.dofmap.index_map.size_global × Q.dofmap.index_map_bs\n  #   (for scalar pressure spaces, block size = 1)\n  num_pressure_dofs = Q.dofmap.index_map_bs *Q.dofmap.index_map.size_global\n\n  turek = np.loadtxt(folder/\"bdforces_lv4\")\n  turek_p = np.loadtxt(folder/\"pointvalues_lv4\")\n  \n  fig = plt.figure(figsize=(25, 8))\n\n  l1 = plt.plot(\n    t_u, C_D, \n    label=f\"FEniCSx ({num_velocity_dofs +num_pressure_dofs:d} dofs)\", linewidth=2\n  )\n  l2 = plt.plot(\n    turek[1:, 1], turek[1:, 3], \n    marker=\"x\", markevery=50, linestyle=\"\", markersize=4, \n    label=\"FEATFLOW (42016 dofs)\")\n\n  plt.title(\"Drag Coefficient\")\n  plt.grid()\n  plt.legend()\n  plt.savefig(folder/\"drag_comparison.png\")\n  plt.show()\n\n  fig = plt.figure(figsize=(25, 8))\n\n  l1 = plt.plot(\n    t_u, C_L, \n    label=f\"FEniCSx ({num_velocity_dofs +num_pressure_dofs:d} dofs)\", linewidth=2\n  )\n  l2 = plt.plot(\n    turek[1:, 1], turek[1:, 4], \n    marker=\"x\", markevery=50, linestyle=\"\", markersize=4, \n    label=\"FEATFLOW (42016 dofs)\"\n  )\n  \n  plt.title(\"Lift Coefficient\")\n  plt.grid()\n  plt.legend()\n  plt.savefig(folder/\"lift_comparison.png\")\n  plt.show()  \n\n  fig = plt.figure(figsize=(25, 8))\n\n  l1 = plt.plot(\n    t_p, p_diff, \n    label=f\"FEniCSx ({num_velocity_dofs + num_pressure_dofs:d} dofs)\", linewidth=2\n  )\n  l2 = plt.plot(\n    turek[1:, 1], turek_p[1:, 6] -turek_p[1:, -1], \n    marker=\"x\", markevery=50, linestyle=\"\", markersize=4, \n    label=\"FEATFLOW (42016 dofs)\"\n  )\n  \n  plt.title(\"Pressure difference\")\n  plt.grid()\n  plt.legend()\n  plt.savefig(folder/\"pressure_comparison.png\")\n  plt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nK.7.6 Hyperelasticity\nAuthor: Jørgen S. Dokken and Garth N. Wells\nThis section demonstrates how to solve the hyperelasticity problem for the deformation of a beam. We also illustrate how to define a constant boundary condition for a vector function space\nWe begin by importing DOLFINx along with additional dependencies. Next, we construct a slender cantilever composed of hexahedral elements and define the function space V for the unknown field\n\nimport numpy as np\n\nfrom mpi4py import MPI\nimport pyvista\n\nfrom dolfinx import fem, mesh, plot\nfrom dolfinx import log, default_scalar_type\nfrom dolfinx.fem.petsc import NonlinearProblem\nfrom dolfinx.nls.petsc import NewtonSolver\n\nimport ufl\n\nL = 20.0\ndomain = mesh.create_box(\n  MPI.COMM_WORLD, \n  [[0.0, 0.0, 0.0], [L, 1, 1]], \n  [20, 5, 5], \n  mesh.CellType.hexahedron\n)\nV = fem.functionspace(domain, (\"Lagrange\", 2, (domain.geometry.dim, )))\n\nWe define two Python functions to identify the facets where boundary conditions should be applied\n\ndef left(x):\n  return np.isclose(x[0], 0)\n\ndef right(x):\n   return np.isclose(x[0], L)\n\nfdim = domain.topology.dim -1\nleft_facets = mesh.locate_entities_boundary(domain, fdim, left)\nright_facets = mesh.locate_entities_boundary(domain, fdim, right)\n\nNext, we create a marker using these two functions\n\n# Concatenate and sort the arrays based on facet indices\n# Left facets are marked with ID 1, right facets with ID 2\n\n# Combine the facet indices from the left and right boundary\nmarked_facets = np.hstack([left_facets, right_facets])\n\n# Assign marker values: 1 for left facets, 2 for right facets\nmarked_values = np.hstack([\n    np.full_like(left_facets, 1), \n    np.full_like(right_facets, 2)\n])\n\n# Sort facets (and corresponding marker values) by facet index\nsorted_facets = np.argsort(marked_facets)\n\n# Create a MeshTags object that stores the facet markers\n# This will allow us to apply different boundary conditions\n# depending on whether the facet belongs to the left or right boundary\nfacet_tag = mesh.meshtags(\n    domain, fdim, \n    marked_facets[sorted_facets], \n    marked_values[sorted_facets]\n)\n\nWe then create a function to prescribe the fixed boundary condition on the left side\n\nu_bc = np.array((0,) *domain.geometry.dim, dtype=default_scalar_type)\n\nTo apply the boundary condition, we identify the degrees of freedom (dofs) associated with the facets marked by the MeshTag\n\nleft_dofs = fem.locate_dofs_topological(\n  V, \n  facet_tag.dim, \n  facet_tag.find(1)\n)\nbcs = [fem.dirichletbc(u_bc, left_dofs, V)]\n\nNext, we define the body force B on the reference configuration and the nominal (first Piola-Kirchhoff) traction T\n\nB = fem.Constant(domain, default_scalar_type((0, 0, 0)))\nT = fem.Constant(domain, default_scalar_type((0, 0, 0)))\n\nWe define the test and solution functions in the space \\(V\\)\n\nv = ufl.TestFunction(V)\nu = fem.Function(V)\n\nWe define the kinematic quantities relevant to the problem\n\n# Spatial dimension\nd = len(u)\n\n# Identity tensor\nI = ufl.variable(ufl.Identity(d))\n\n# Deformation gradient\nF = ufl.variable(I +ufl.grad(u))\n\n# Right Cauchy-Green tensor\nC = ufl.variable(F.T *F)\n\n# Invariants of deformation tensors\nIc = ufl.variable(ufl.tr(C))\nJ = ufl.variable(ufl.det(F))\n\nWe define the elasticity model using the stored strain energy density function \\(\\psi\\), and derive the corresponding first Piola-Kirchhoff stress expression:\n\n# Elasticity parameters\nE = default_scalar_type(1.0e4)\nnu = default_scalar_type(0.3)\nmu = fem.Constant(domain, E /(2 *(1 +nu)))\n\nlmbda = fem.Constant(domain, E *nu /((1 +nu) *(1 -2 *nu)))\n\n# Stored strain energy density (compressible neo-Hookean model)\npsi = (mu /2) *(Ic -3) -mu *ufl.ln(J) +(lmbda /2) *(ufl.ln(J))**2\n\n# Stress - Hyper-elasticity\nP = ufl.diff(psi, F)\n\n\n\n\n\n\n\nNote\n\n\n\nTo illustrate the difference between linear elasticity and hyperelasticity, the following lines can be uncommented to solve the linear elasticity problem\n\n\n\n# P = 2.0 *mu *ufl.sym(ufl.grad(u)) +lmbda *ufl.tr(ufl.sym(ufl.grad(u))) *I\n\nDefine the variational form, including the traction integral over all facets marked with value 2. The quadrature degree for the integrals is set to 4\n\nmetadata = {\"quadrature_degree\": 4}\nds = ufl.Measure(\n  \"ds\", \n  domain=domain, \n  subdomain_data=facet_tag, \n  metadata=metadata\n)\n\ndx = ufl.Measure(\n  \"dx\", \n  domain=domain, \n  metadata=metadata\n)\n\n# Define form F (we want to find u such that F(u) = 0)\nF = ufl.inner(ufl.grad(v), P) *dx -ufl.inner(v, B) *dx -ufl.inner(v, T) *ds(2)\n\nAs the variational form is nonlinear and written in residual form, we use the nonlinear problem class from DOLFINx to set up the structures required for a Newton solver\n\nproblem = NonlinearProblem(F, u, bcs)\n\nand then create and configure the Newton solver\n\nsolver = NewtonSolver(domain.comm, problem)\n\n# Set Newton solver options\nsolver.atol = 1e-8\nsolver.rtol = 1e-8\nsolver.convergence_criterion = \"incremental\"\n\nWe define a function to plot the solution at each time step\n\nresults_folder = Path(\"fenicsx/hyperelasticity\")\nresults_folder.mkdir(exist_ok=True, parents=True)\n\nplotter = pyvista.Plotter(off_screen=True)\nplotter.open_gif(results_folder/\"deformation.gif\", fps=3)\n\ntopology, cells, geometry = plot.vtk_mesh(u.function_space)\nfunction_grid = pyvista.UnstructuredGrid(topology, cells, geometry)\n\nvalues = np.zeros((geometry.shape[0], 3))\nvalues[:, :len(u)] = u.x.array.reshape(geometry.shape[0], len(u))\nfunction_grid[\"u\"] = values\nfunction_grid.set_active_vectors(\"u\")\n\n# Warp mesh by deformation\nwarped = function_grid.warp_by_vector(\"u\", factor=1)\nwarped.set_active_vectors(\"u\")\n\n# Add mesh to plotter and visualize\nactor = plotter.add_mesh(\n  warped, \n  show_edges=True, \n  lighting=False, \n  clim=[0, 10],\n  scalar_bar_args={\"vertical\": True},\n)\n\n# Compute magnitude of displacement to visualize in GIF\nVs = fem.functionspace(domain, (\"Lagrange\", 2))\nmagnitude = fem.Function(Vs)\n\nus = fem.Expression(\n  ufl.sqrt(sum([u[i]**2 for i in range(len(u))])), \n  Vs.element.interpolation_points()\n)\nmagnitude.interpolate(us)\nwarped[\"mag\"] = magnitude.x.array\n\nFinally, we solve the problem over multiple time steps, updating the traction in the z-direction\n\nlog.set_log_level(log.LogLevel.INFO)\n\ntval0 = -1.5\nfor n in range(1, 10):\n    T.value[2] = n *tval0\n    \n    num_its, converged = solver.solve(u)\n    assert (converged)\n    u.x.scatter_forward()\n    print(f\"Time step {n}, Number of iterations {num_its}, Load {T.value}\")\n    function_grid[\"u\"][:, :len(u)] = u.x.array.reshape(geometry.shape[0], len(u))\n    \n    magnitude.interpolate(us)\n    \n    warped.set_active_scalars(\"mag\")\n    warped_n = function_grid.warp_by_vector(factor=1)\n    warped.points[:, :] = warped_n.points\n    warped.point_data[\"mag\"][:] = magnitude.x.array\n    \n    plotter.update_scalar_bar_range([0, 10])\n    plotter.write_frame()\n\nplotter.close()\n\nlog.set_log_level(log.LogLevel.WARNING)\n\n[2025-09-27 15:03:01.166] [info] PETSc Krylov solver starting to solve system.\n[2025-09-27 15:03:03.481] [info] PETSc Krylov solver starting to solve system.\n[2025-09-27 15:03:05.389] [info] Newton iteration 2: r (abs) = 22.245488847569664 (tol = 1e-08), r (rel) = 0.1342779422433199 (tol = 1e-08)\n[2025-09-27 15:03:05.597] [info] PETSc Krylov solver starting to solve system.\n[2025-09-27 15:03:07.556] [info] Newton iteration 3: r (abs) = 2.432613380788242 (tol = 1e-08), r (rel) = 0.01468371053942908 (tol = 1e-08)\n[2025-09-27 15:03:07.753] [info] PETSc Krylov solver starting to solve system.\n[2025-09-27 15:03:09.640] [info] Newton iteration 4: r (abs) = 4.431578720802735 (tol = 1e-08), r (rel) = 0.026749840185403984 (tol = 1e-08)\n[2025-09-27 15:03:09.839] [info] PETSc Krylov solver starting to solve system.\n[2025-09-27 15:03:11.813] [info] Newton iteration 5: r (abs) = 0.14418915583061978 (tol = 1e-08), r (rel) = 0.0008703527834971478 (tol = 1e-08)\n[2025-09-27 15:03:12.010] [info] PETSc Krylov solver starting to solve system.\n[2025-09-27 15:03:13.877] [info] Newton iteration 6: r (abs) = 0.02142394652831032 (tol = 1e-08), r (rel) = 0.00012931895874550384 (tol = 1e-08)\n[2025-09-27 15:03:14.071] [info] PETSc Krylov solver starting to solve system.\n[2025-09-27 15:03:16.004] [info] Newton iteration 7: r (abs) = 4.80066955935609e-06 (tol = 1e-08), r (rel) = 2.8977741700241697e-08 (tol = 1e-08)\n[2025-09-27 15:03:16.212] [info] PETSc Krylov solver starting to solve system.\nTime step 1, Number of iterations 8, Load [ 0.   0.  -1.5]\n[2025-09-27 15:03:18.123] [info] Newton iteration 8: r (abs) = 2.6593315879693925e-11 (tol = 1e-08), r (rel) = 1.6052224153042273e-13 (tol = 1e-08)\n[2025-09-27 15:03:18.123] [info] Newton solver finished in 8 iterations and 8 linear solver iterations.\n[2025-09-27 15:03:18.431] [info] PETSc Krylov solver starting to solve system.\n[2025-09-27 15:03:20.525] [info] PETSc Krylov solver starting to solve system.\n[2025-09-27 15:03:22.513] [info] Newton iteration 2: r (abs) = 17.325417842770662 (tol = 1e-08), r (rel) = 0.11784222264969324 (tol = 1e-08)\n[2025-09-27 15:03:22.712] [info] PETSc Krylov solver starting to solve system.\n[2025-09-27 15:03:24.581] [info] Newton iteration 3: r (abs) = 5.148824037029388 (tol = 1e-08), r (rel) = 0.03502073508771889 (tol = 1e-08)\n[2025-09-27 15:03:24.774] [info] PETSc Krylov solver starting to solve system.\n[2025-09-27 15:03:26.782] [info] Newton iteration 4: r (abs) = 7.240032569659952 (tol = 1e-08), r (rel) = 0.0492444994866836 (tol = 1e-08)\n[2025-09-27 15:03:26.981] [info] PETSc Krylov solver starting to solve system.\n[2025-09-27 15:03:28.894] [info] Newton iteration 5: r (abs) = 0.7778888011215269 (tol = 1e-08), r (rel) = 0.0052909630307540366 (tol = 1e-08)\n[2025-09-27 15:03:29.085] [info] PETSc Krylov solver starting to solve system.\n[2025-09-27 15:03:31.018] [info] Newton iteration 6: r (abs) = 1.2552533144730293 (tol = 1e-08), r (rel) = 0.008537851260402308 (tol = 1e-08)\n[2025-09-27 15:03:31.229] [info] PETSc Krylov solver starting to solve system.\n[2025-09-27 15:03:33.150] [info] Newton iteration 7: r (abs) = 0.008495122949791085 (tol = 1e-08), r (rel) = 5.778124251724872e-05 (tol = 1e-08)\n[2025-09-27 15:03:33.346] [info] PETSc Krylov solver starting to solve system.\n[2025-09-27 15:03:35.240] [info] Newton iteration 8: r (abs) = 0.00019210705108344808 (tol = 1e-08), r (rel) = 1.3066537322098655e-06 (tol = 1e-08)\n[2025-09-27 15:03:35.435] [info] PETSc Krylov solver starting to solve system.\nTime step 2, Number of iterations 9, Load [ 0.  0. -3.]\n[2025-09-27 15:03:37.430] [info] Newton iteration 9: r (abs) = 1.7065882144463082e-10 (tol = 1e-08), r (rel) = 1.1607693976745285e-12 (tol = 1e-08)\n[2025-09-27 15:03:37.430] [info] Newton solver finished in 9 iterations and 9 linear solver iterations.\n[2025-09-27 15:03:37.655] [info] PETSc Krylov solver starting to solve system.\n[2025-09-27 15:03:39.729] [info] PETSc Krylov solver starting to solve system.\n[2025-09-27 15:03:41.712] [info] Newton iteration 2: r (abs) = 10.001117239450483 (tol = 1e-08), r (rel) = 0.08874707904158333 (tol = 1e-08)\n[2025-09-27 15:03:41.909] [info] PETSc Krylov solver starting to solve system.\n[2025-09-27 15:03:43.790] [info] Newton iteration 3: r (abs) = 5.330258434852234 (tol = 1e-08), r (rel) = 0.04729920220952126 (tol = 1e-08)\n[2025-09-27 15:03:43.988] [info] PETSc Krylov solver starting to solve system.\n[2025-09-27 15:03:45.898] [info] Newton iteration 4: r (abs) = 11.990116721075117 (tol = 1e-08), r (rel) = 0.10639689655528227 (tol = 1e-08)\n[2025-09-27 15:03:46.105] [info] PETSc Krylov solver starting to solve system.\n[2025-09-27 15:03:48.059] [info] Newton iteration 5: r (abs) = 2.2970192412283055 (tol = 1e-08), r (rel) = 0.020383097536063585 (tol = 1e-08)\n[2025-09-27 15:03:48.256] [info] PETSc Krylov solver starting to solve system.\n[2025-09-27 15:03:50.122] [info] Newton iteration 6: r (abs) = 3.9023388069967275 (tol = 1e-08), r (rel) = 0.03462824825065298 (tol = 1e-08)\n[2025-09-27 15:03:50.321] [info] PETSc Krylov solver starting to solve system.\n[2025-09-27 15:03:52.310] [info] Newton iteration 7: r (abs) = 0.23653543302398086 (tol = 1e-08), r (rel) = 0.0020989483742786096 (tol = 1e-08)\n[2025-09-27 15:03:52.508] [info] PETSc Krylov solver starting to solve system.\n[2025-09-27 15:03:54.362] [info] Newton iteration 8: r (abs) = 0.04271420922498156 (tol = 1e-08), r (rel) = 0.0003790337830792644 (tol = 1e-08)\n[2025-09-27 15:03:54.557] [info] PETSc Krylov solver starting to solve system.\n[2025-09-27 15:03:56.533] [info] Newton iteration 9: r (abs) = 2.8779784817306148e-05 (tol = 1e-08), r (rel) = 2.553836513290955e-07 (tol = 1e-08)\n[2025-09-27 15:03:56.746] [info] PETSc Krylov solver starting to solve system.\nTime step 3, Number of iterations 10, Load [ 0.   0.  -4.5]\n[2025-09-27 15:03:58.674] [info] Newton iteration 10: r (abs) = 6.077785953131127e-10 (tol = 1e-08), r (rel) = 5.393254948084079e-12 (tol = 1e-08)\n[2025-09-27 15:03:58.674] [info] Newton solver finished in 10 iterations and 10 linear solver iterations.\n[2025-09-27 15:03:58.896] [info] PETSc Krylov solver starting to solve system.\n[2025-09-27 15:04:00.977] [info] PETSc Krylov solver starting to solve system.\n[2025-09-27 15:04:02.960] [info] Newton iteration 2: r (abs) = 5.506929934734036 (tol = 1e-08), r (rel) = 0.06539183682426397 (tol = 1e-08)\n[2025-09-27 15:04:03.159] [info] PETSc Krylov solver starting to solve system.\n[2025-09-27 15:04:05.053] [info] Newton iteration 3: r (abs) = 26.248919589460577 (tol = 1e-08), r (rel) = 0.3116918295584109 (tol = 1e-08)\n[2025-09-27 15:04:05.249] [info] PETSc Krylov solver starting to solve system.\n[2025-09-27 15:04:07.231] [info] Newton iteration 4: r (abs) = 2.309270521316821 (tol = 1e-08), r (rel) = 0.027421347811341937 (tol = 1e-08)\n[2025-09-27 15:04:07.422] [info] PETSc Krylov solver starting to solve system.\n[2025-09-27 15:04:09.301] [info] Newton iteration 5: r (abs) = 14.056244490622522 (tol = 1e-08), r (rel) = 0.16691035785570413 (tol = 1e-08)\n[2025-09-27 15:04:09.496] [info] PETSc Krylov solver starting to solve system.\n[2025-09-27 15:04:11.444] [info] Newton iteration 6: r (abs) = 0.22277413322257641 (tol = 1e-08), r (rel) = 0.0026453232456209053 (tol = 1e-08)\n[2025-09-27 15:04:11.653] [info] PETSc Krylov solver starting to solve system.\n[2025-09-27 15:04:13.574] [info] Newton iteration 7: r (abs) = 0.28667052510539687 (tol = 1e-08), r (rel) = 0.0034040585992898666 (tol = 1e-08)\n[2025-09-27 15:04:13.768] [info] PETSc Krylov solver starting to solve system.\n[2025-09-27 15:04:15.652] [info] Newton iteration 8: r (abs) = 0.00032186938773632337 (tol = 1e-08), r (rel) = 3.822026198086344e-06 (tol = 1e-08)\nTime step 4, Number of iterations 9, Load [ 0.  0. -6.]\n[2025-09-27 15:04:15.854] [info] PETSc Krylov solver starting to solve system.\n[2025-09-27 15:04:17.842] [info] Newton iteration 9: r (abs) = 2.637964780929628e-07 (tol = 1e-08), r (rel) = 3.132441569933224e-09 (tol = 1e-08)\n[2025-09-27 15:04:17.842] [info] Newton solver finished in 9 iterations and 9 linear solver iterations.\n[2025-09-27 15:04:18.070] [info] PETSc Krylov solver starting to solve system.\n[2025-09-27 15:04:20.143] [info] PETSc Krylov solver starting to solve system.\n[2025-09-27 15:04:22.134] [info] Newton iteration 2: r (abs) = 3.194619708234956 (tol = 1e-08), r (rel) = 0.04964789514888014 (tol = 1e-08)\n[2025-09-27 15:04:22.331] [info] PETSc Krylov solver starting to solve system.\n[2025-09-27 15:04:24.204] [info] Newton iteration 3: r (abs) = 7.714285832925293 (tol = 1e-08), r (rel) = 0.1198884653451208 (tol = 1e-08)\n[2025-09-27 15:04:24.395] [info] PETSc Krylov solver starting to solve system.\n[2025-09-27 15:04:26.352] [info] Newton iteration 4: r (abs) = 0.8508731194677975 (tol = 1e-08), r (rel) = 0.013223501786908416 (tol = 1e-08)\n[2025-09-27 15:04:26.561] [info] PETSc Krylov solver starting to solve system.\n[2025-09-27 15:04:28.500] [info] Newton iteration 5: r (abs) = 0.3714343524134822 (tol = 1e-08), r (rel) = 0.005772497344763917 (tol = 1e-08)\n[2025-09-27 15:04:28.697] [info] PETSc Krylov solver starting to solve system.\n[2025-09-27 15:04:30.590] [info] Newton iteration 6: r (abs) = 0.002150656410572117 (tol = 1e-08), r (rel) = 3.342356014961964e-05 (tol = 1e-08)\n[2025-09-27 15:04:30.787] [info] PETSc Krylov solver starting to solve system.\n[2025-09-27 15:04:32.827] [info] Newton iteration 7: r (abs) = 2.546067480793498e-06 (tol = 1e-08), r (rel) = 3.956868199446797e-08 (tol = 1e-08)\n[2025-09-27 15:04:33.016] [info] PETSc Krylov solver starting to solve system.\nTime step 5, Number of iterations 8, Load [ 0.   0.  -7.5]\n[2025-09-27 15:04:34.887] [info] Newton iteration 8: r (abs) = 1.6857449626793708e-13 (tol = 1e-08), r (rel) = 2.619832618546621e-15 (tol = 1e-08)\n[2025-09-27 15:04:34.887] [info] Newton solver finished in 8 iterations and 8 linear solver iterations.\n[2025-09-27 15:04:35.103] [info] PETSc Krylov solver starting to solve system.\n[2025-09-27 15:04:37.274] [info] PETSc Krylov solver starting to solve system.\n[2025-09-27 15:04:39.167] [info] Newton iteration 2: r (abs) = 2.006488301405612 (tol = 1e-08), r (rel) = 0.0395622031385681 (tol = 1e-08)\n[2025-09-27 15:04:39.368] [info] PETSc Krylov solver starting to solve system.\n[2025-09-27 15:04:41.233] [info] Newton iteration 3: r (abs) = 4.609768229355378 (tol = 1e-08), r (rel) = 0.09089142806549969 (tol = 1e-08)\n[2025-09-27 15:04:41.443] [info] PETSc Krylov solver starting to solve system.\n[2025-09-27 15:04:43.432] [info] Newton iteration 4: r (abs) = 0.18537214300427624 (tol = 1e-08), r (rel) = 0.0036550077927837146 (tol = 1e-08)\n[2025-09-27 15:04:43.621] [info] PETSc Krylov solver starting to solve system.\n[2025-09-27 15:04:45.498] [info] Newton iteration 5: r (abs) = 0.02468800140561847 (tol = 1e-08), r (rel) = 0.00048677668641781437 (tol = 1e-08)\n[2025-09-27 15:04:45.695] [info] PETSc Krylov solver starting to solve system.\n[2025-09-27 15:04:47.680] [info] Newton iteration 6: r (abs) = 5.6925390491283854e-06 (tol = 1e-08), r (rel) = 1.1224056780100924e-07 (tol = 1e-08)\n[2025-09-27 15:04:47.878] [info] PETSc Krylov solver starting to solve system.\nTime step 6, Number of iterations 7, Load [ 0.  0. -9.]\n[2025-09-27 15:04:49.753] [info] Newton iteration 7: r (abs) = 2.6204561860805414e-11 (tol = 1e-08), r (rel) = 5.166789154803981e-13 (tol = 1e-08)\n[2025-09-27 15:04:49.753] [info] Newton solver finished in 7 iterations and 7 linear solver iterations.\n[2025-09-27 15:04:49.986] [info] PETSc Krylov solver starting to solve system.\n[2025-09-27 15:04:52.098] [info] PETSc Krylov solver starting to solve system.\n[2025-09-27 15:04:54.029] [info] Newton iteration 2: r (abs) = 1.3850621350551318 (tol = 1e-08), r (rel) = 0.033662175723783674 (tol = 1e-08)\n[2025-09-27 15:04:54.221] [info] PETSc Krylov solver starting to solve system.\n[2025-09-27 15:04:56.121] [info] Newton iteration 3: r (abs) = 3.0373936052903123 (tol = 1e-08), r (rel) = 0.07381999312219266 (tol = 1e-08)\n[2025-09-27 15:04:56.321] [info] PETSc Krylov solver starting to solve system.\n[2025-09-27 15:04:58.333] [info] Newton iteration 4: r (abs) = 0.04123861973234785 (tol = 1e-08), r (rel) = 0.00100225226645253 (tol = 1e-08)\n[2025-09-27 15:04:58.535] [info] PETSc Krylov solver starting to solve system.\n[2025-09-27 15:05:00.417] [info] Newton iteration 5: r (abs) = 0.00205056641006533 (tol = 1e-08), r (rel) = 4.983641172614961e-05 (tol = 1e-08)\n[2025-09-27 15:05:00.615] [info] PETSc Krylov solver starting to solve system.\nTime step 7, Number of iterations 6, Load [  0.    0.  -10.5]\n[2025-09-27 15:05:02.578] [info] Newton iteration 6: r (abs) = 1.7885886176830885e-08 (tol = 1e-08), r (rel) = 4.346937427728534e-10 (tol = 1e-08)\n[2025-09-27 15:05:02.578] [info] Newton solver finished in 6 iterations and 6 linear solver iterations.\n[2025-09-27 15:05:02.808] [info] PETSc Krylov solver starting to solve system.\n[2025-09-27 15:05:04.885] [info] PETSc Krylov solver starting to solve system.\n[2025-09-27 15:05:06.768] [info] Newton iteration 2: r (abs) = 1.0633648528364101 (tol = 1e-08), r (rel) = 0.031085002034186824 (tol = 1e-08)\n[2025-09-27 15:05:06.983] [info] PETSc Krylov solver starting to solve system.\n[2025-09-27 15:05:08.973] [info] Newton iteration 3: r (abs) = 2.0477031306171987 (tol = 1e-08), r (rel) = 0.05985984566901875 (tol = 1e-08)\n[2025-09-27 15:05:09.173] [info] PETSc Krylov solver starting to solve system.\n[2025-09-27 15:05:11.037] [info] Newton iteration 4: r (abs) = 0.008977192522240718 (tol = 1e-08), r (rel) = 0.0002624273757692741 (tol = 1e-08)\n[2025-09-27 15:05:11.229] [info] PETSc Krylov solver starting to solve system.\n[2025-09-27 15:05:13.234] [info] Newton iteration 5: r (abs) = 0.00016742178144130685 (tol = 1e-08), r (rel) = 4.894186978992472e-06 (tol = 1e-08)\n[2025-09-27 15:05:13.435] [info] PETSc Krylov solver starting to solve system.\nTime step 8, Number of iterations 6, Load [  0.   0. -12.]\n[2025-09-27 15:05:15.318] [info] Newton iteration 6: r (abs) = 3.241819305822145e-11 (tol = 1e-08), r (rel) = 9.476705897053983e-13 (tol = 1e-08)\n[2025-09-27 15:05:15.318] [info] Newton solver finished in 6 iterations and 6 linear solver iterations.\n[2025-09-27 15:05:15.540] [info] PETSc Krylov solver starting to solve system.\n[2025-09-27 15:05:17.689] [info] PETSc Krylov solver starting to solve system.\n[2025-09-27 15:05:19.578] [info] Newton iteration 2: r (abs) = 0.898788614589174 (tol = 1e-08), r (rel) = 0.030966555717676078 (tol = 1e-08)\n[2025-09-27 15:05:19.776] [info] PETSc Krylov solver starting to solve system.\n[2025-09-27 15:05:21.651] [info] Newton iteration 3: r (abs) = 1.3835366517792282 (tol = 1e-08), r (rel) = 0.04766789890229271 (tol = 1e-08)\n[2025-09-27 15:05:21.859] [info] PETSc Krylov solver starting to solve system.\n[2025-09-27 15:05:23.858] [info] Newton iteration 4: r (abs) = 0.001850960693256602 (tol = 1e-08), r (rel) = 6.377236706003253e-05 (tol = 1e-08)\n[2025-09-27 15:05:24.056] [info] PETSc Krylov solver starting to solve system.\nTime step 9, Number of iterations 6, Load [  0.    0.  -13.5]\n[2025-09-27 15:05:25.990] [info] Newton iteration 5: r (abs) = 7.871830666129107e-06 (tol = 1e-08), r (rel) = 2.712133631490422e-07 (tol = 1e-08)\n[2025-09-27 15:05:26.181] [info] PETSc Krylov solver starting to solve system.\n[2025-09-27 15:05:28.167] [info] Newton iteration 6: r (abs) = 4.498496771177386e-13 (tol = 1e-08), r (rel) = 1.549896701507776e-14 (tol = 1e-08)\n[2025-09-27 15:05:28.167] [info] Newton solver finished in 6 iterations and 6 linear solver iterations.\n\n\n\n\n\nK.7.7 The Helmholtz equation\nAuthor: Antonio Baiano Svizzero\nThe study of computational acoustics is essential in fields such as noise, vibration, and harshness (NVH), noise control, and acoustic design. In this section, we focus on the theoretical foundations of the Helmholtz equation—which is valid for noise problems with harmonic time dependence—and its implementation in FEniCSx to compute the sound pressure in arbitrary acoustic systems\nThe PDE problem\nThe acoustic Helmholtz equation in its general form reads\n\\[\n\\begin{aligned}\n\\nabla^2 p + k^2 p = -j \\omega \\rho_0 q \\quad\\text{in } \\Omega\n\\end{aligned}\n\\]\nwhere \\(p\\) is the complex acoustic pressure, \\(k\\) the wavenumber, \\(\\omega\\) the angular frequency, \\(j\\) the imaginary unit, and \\(q\\) the volume velocity (\\(\\mathrm{m^3/s}\\)) of a generic source field. In the case of a monopole source, \\(q\\) can be written as\n\\[q = Q \\, \\delta(x_s, y_s, z_s)\\]\nwhere \\(\\delta(x_s, y_s, z_s)\\) is the three-dimensional Dirac delta function centered at the monopole location. This formulation allows the computation of the sound pressure field generated by any generic source distribution, while the monopole source provides a simple yet fundamental example for analytical and numerical studies in computational acoustics\nIn order to solve this problem with real-valued solvers in FEniCSx, we split the complex pressure into its real and imaginary parts:\n\\[p = p_r + j \\, p_i\\]\nwith \\(p_r\\) and \\(p_i\\) real-valued functions representing the real and imaginary components, respectively. Substituting this into the original equation and separating real and imaginary parts leads to two coupled real-valued PDEs:\n\\[\\begin{aligned}\n\\nabla^2 p_r + k^2 p_r &= 0, \\\\\n\\nabla^2 p_i + k^2 p_i &= - \\omega \\, \\rho_0 \\, q,\n\\end{aligned}\n\\quad \\text{in } \\Omega\\]\nAfter splitting the complex pressure \\(p = p_r + j\\, p_i\\) into its real and imaginary parts, the boundary conditions are applied separately to \\(p_r\\) and \\(p_i\\):\n\nDirichlet BC:\n\\[p_r = \\Re(\\bar{p}), \\quad p_i = \\Im(\\bar{p}) \\quad \\text{on } \\partial\\Omega_p\\]\nNeumann BC:\n\\[\\frac{\\partial p_r}{\\partial n} = 0, \\quad\n  \\frac{\\partial p_i}{\\partial n} = - \\omega \\, \\rho_0 \\, \\bar{v}_n \\quad \\text{on } \\partial\\Omega_v\\]\nRobin BC:\n\\[\\frac{\\partial p_r}{\\partial n} =\n    \\frac{\\omega \\, \\rho_0}{\\bar{Z}} \\, p_i, \\quad\n  \\frac{\\partial p_i}{\\partial n} = -\\frac{\\omega \\, \\rho_0}{\\bar{Z}} \\, p_r \\quad \\text{on } \\partial\\Omega_Z\\]\n\nwhere \\(\\bar{p}\\) is the complex prescribed acoustic pressure at \\(\\partial\\Omega_p\\), \\(\\bar{v}_n\\) is the prescribed normal sound particle velocity at \\(\\partial\\Omega_v\\), and \\(\\bar{Z}\\) is the prescribed acoustic impedance at \\(\\partial\\Omega_Z\\), with \\(n\\) denoting the outward unit normal. In this context, the overbar notation is used to denote prescribed boundary quantities (known data) in order to distinguish them from the unknown fields. It should be emphasized that the overbar does not indicate a complex conjugate\nThis formulation allows the use of real-valued solvers while fully accounting for the effect of complex-valued boundary conditions in the Helmholtz problem\nWeak formulation in terms of real and imaginary parts\nLet the complex source be\n\\[\nq = q_r + j\\,q_i,\n\\]\nand let the (possibly complex) surface impedance and the prescribed normal velocity be\n\\[\n\\bar Z = Z_r + j\\,Z_i,\\qquad \\bar v_n = v_{n,r} + j\\,v_{n,i},\n\\]\nwith \\(Z_r,Z_i,v_{n,r},v_{n,i}\\in\\mathbb{R}\\). Define \\(\\Delta_Z = Z_r^2+Z_i^2\\). Then\n\\[\n\\frac{j\\,\\omega\\rho_0}{\\bar Z}\n= \\frac{\\omega\\rho_0}{\\Delta_Z}\\bigl(Z_i + j\\,Z_r\\bigr)\n\\]\nMultiplying the complex Helmholtz equation by a real test function \\(v \\in \\hat V\\) and separating real and imaginary parts yields the following two real variational problems.\n\nReal part (for \\(p_r\\)):\n\n\\[\n\\begin{aligned}\n\\int_{\\Omega} \\nabla p_r \\cdot \\nabla v \\,\\mathrm{d}x\n&- k^2 \\int_{\\Omega} p_r\\,v\\,\\mathrm{d}x\n+ \\frac{\\omega\\rho_0}{\\Delta_Z}\\int_{\\partial\\Omega_Z} \\bigl(Z_i p_r - Z_r p_i\\bigr)\\,v\\,\\mathrm{d}s \\\\\n=& -\\,\\omega\\rho_0\\int_{\\Omega} q_i\\,v\\,\\mathrm{d}x\n\\;+\\; \\omega\\rho_0\\int_{\\partial\\Omega_v} v_{n,i}\\,v\\,\\mathrm{d}s\n\\end{aligned}\\]\n\nImaginary part (for \\(p_i\\)):\n\n\\[\n\\begin{aligned}\n\\int_{\\Omega} \\nabla p_i \\cdot \\nabla v \\,\\mathrm{d}x\n&- k^2 \\int_{\\Omega} p_i\\,v\\,\\mathrm{d}x\n+ \\frac{\\omega\\rho_0}{\\Delta_Z}\\int_{\\partial\\Omega_Z} \\bigl(Z_r p_r + Z_i p_i\\bigr)\\,v\\,\\mathrm{d}s \\\\\n=& \\;\\omega\\rho_0\\int_{\\Omega} q_r\\,v\\,\\mathrm{d}x\n\\;-\\; \\omega\\rho_0\\int_{\\partial\\Omega_v} v_{n,r}\\,v\\,\\mathrm{d}s\n\\end{aligned}\\]\nAfter solving these two coupled real systems for \\(p_r\\) and \\(p_i\\), the complex pressure is reconstructed as \\(p = p_r + \\mathrm{i}\\,p_i\\), from which amplitude \\(|p|\\), phase and derived quantities such as the Sound Pressure Level (SPL) can be computed\nIn this section, you will learn how to:\n\nDefine acoustic velocity and impedance boundary conditions\nCompute the acoustic sound pressure for multiple frequencies\nEvaluate the SPL at a given microphone position\n\nTest problem\nAs an example, we model a plane wave propagating through a tube. Although this is a basic test case, the code can be extended to more complex problems requiring velocity and impedance boundary conditions\nFinally, we create the mesh with GMSH, defining the physical groups for the velocity and impedance boundary conditions along with their respective tags\n\nimport gmsh\n\n# ---------------------------------\n# Create gmsh box and tag facets\n# ---------------------------------\n\nif not gmsh.isInitialized():\n  gmsh.initialize()\n\n# meshsize settings\nmeshsize = 0.02\ngmsh.option.setNumber(\"Mesh.MeshSizeMin\", meshsize)\ngmsh.option.setNumber(\"Mesh.MeshSizeMax\", meshsize)\n\n# create geometry\nL = 1.0\nW = 0.1\nH = 0.1\n\nbox = gmsh.model.occ.addBox(0, 0, 0, L, W, H)\ngmsh.model.occ.synchronize()\n\n# Remove any existing physical groups with the same (dim, tag)\nfor dim, tag in gmsh.model.getPhysicalGroups():\n  if dim == gdim and tag == physical_tag:\n    gmsh.model.removePhysicalGroups([(dim, tag)])\n\n# get surfaces\nsurfaces = gmsh.model.getEntities(dim=2)\n\n# tolerance for coordinate matching\ntol = 1e-6\n\nvelocity_surfaces = []\nimpedance_surfaces = []\n\nfor (dim, tag) in surfaces:\n  com = gmsh.model.occ.getCenterOfMass(dim, tag)\n  x, y, z = com\n  if abs(x -0.0) &lt; tol:      # x = 0 face\n    velocity_surfaces.append(tag)\n  elif abs(x -L) &lt; tol:      # x = L face\n    impedance_surfaces.append(tag)\n\n# setup physical groups\nvol_tag = gmsh.model.addPhysicalGroup(3, [box], 1)\ngmsh.model.setPhysicalName(3, vol_tag, \"air_volume\")\n\nv_bc_tag = 2\nZ_bc_tag = 3\n\nif velocity_surfaces:\n  gmsh.model.addPhysicalGroup(2, velocity_surfaces, v_bc_tag)\n  gmsh.model.setPhysicalName(2, v_bc_tag, \"velocity_BC\") \nif impedance_surfaces:\n  gmsh.model.addPhysicalGroup(2, impedance_surfaces, Z_bc_tag) \n  gmsh.model.setPhysicalName(2, Z_bc_tag, \"impedance_BC\")\n\n# mesh generation\ngmsh.model.mesh.generate(dim=3)\n\nInfo    : Meshing 1D...\nInfo    : [  0%] Meshing curve 1 (Line)\nInfo    : [ 10%] Meshing curve 2 (Line)\nInfo    : [ 20%] Meshing curve 3 (Line)\nInfo    : [ 30%] Meshing curve 4 (Line)\nInfo    : [ 40%] Meshing curve 5 (Line)\nInfo    : [ 50%] Meshing curve 6 (Line)\nInfo    : [ 60%] Meshing curve 7 (Line)\nInfo    : [ 60%] Meshing curve 8 (Line)\nInfo    : [ 70%] Meshing curve 9 (Line)\nInfo    : [ 80%] Meshing curve 10 (Line)\nInfo    : [ 90%] Meshing curve 11 (Line)\nInfo    : [100%] Meshing curve 12 (Line)\nInfo    : Done meshing 1D (Wall 0.000876541s, CPU 0.001626s)\nInfo    : Meshing 2D...\nInfo    : [  0%] Meshing surface 1 (Plane, Frontal-Delaunay)\nInfo    : [ 20%] Meshing surface 2 (Plane, Frontal-Delaunay)\nInfo    : [ 40%] Meshing surface 3 (Plane, Frontal-Delaunay)\nInfo    : [ 60%] Meshing surface 4 (Plane, Frontal-Delaunay)\nInfo    : [ 70%] Meshing surface 5 (Plane, Frontal-Delaunay)\nInfo    : [ 90%] Meshing surface 6 (Plane, Frontal-Delaunay)\nInfo    : Done meshing 2D (Wall 0.032451s, CPU 0.054946s)\nInfo    : Meshing 3D...\nInfo    : 3D Meshing 1 volume with 1 connected component\nInfo    : Tetrahedrizing 1282 nodes...\nInfo    : Done tetrahedrizing 1290 nodes (Wall 0.0103627s, CPU 0.017034s)\nInfo    : Reconstructing mesh...\nInfo    :  - Creating surface mesh\nInfo    :  - Identifying boundary edges\nInfo    :  - Recovering boundary\nInfo    : Done reconstructing mesh (Wall 0.0221695s, CPU 0.035885s)\nInfo    : Found volume 1\nInfo    : It. 0 - 0 nodes created - worst tet radius 2.57041 (nodes removed 0 0)\nInfo    : 3D refinement terminated (1735 nodes total):\nInfo    :  - 0 Delaunay cavities modified for star shapeness\nInfo    :  - 0 nodes could not be inserted\nInfo    :  - 6565 tetrahedra created in 0.0216964 sec. (302584 tets/s)\nInfo    : 0 node relocations\nInfo    : Done meshing 3D (Wall 0.077675s, CPU 0.128042s)\nInfo    : Optimizing mesh...\nInfo    : Optimizing volume 1\nInfo    : Optimization starts (volume = 0.01) with worst = 0.0110445 / average = 0.765866:\nInfo    : 0.00 &lt; quality &lt; 0.10 :        24 elements\nInfo    : 0.10 &lt; quality &lt; 0.20 :        51 elements\nInfo    : 0.20 &lt; quality &lt; 0.30 :        74 elements\nInfo    : 0.30 &lt; quality &lt; 0.40 :        61 elements\nInfo    : 0.40 &lt; quality &lt; 0.50 :       192 elements\nInfo    : 0.50 &lt; quality &lt; 0.60 :       307 elements\nInfo    : 0.60 &lt; quality &lt; 0.70 :       893 elements\nInfo    : 0.70 &lt; quality &lt; 0.80 :      1910 elements\nInfo    : 0.80 &lt; quality &lt; 0.90 :      1996 elements\nInfo    : 0.90 &lt; quality &lt; 1.00 :      1054 elements\nInfo    : 149 edge swaps, 2 node relocations (volume = 0.01): worst = 0.300158 / average = 0.781673 (Wall 0.00196021s, CPU 0.003281s)\nInfo    : No ill-shaped tets in the mesh :-)\nInfo    : 0.00 &lt; quality &lt; 0.10 :         0 elements\nInfo    : 0.10 &lt; quality &lt; 0.20 :         0 elements\nInfo    : 0.20 &lt; quality &lt; 0.30 :         0 elements\nInfo    : 0.30 &lt; quality &lt; 0.40 :        60 elements\nInfo    : 0.40 &lt; quality &lt; 0.50 :       159 elements\nInfo    : 0.50 &lt; quality &lt; 0.60 :       293 elements\nInfo    : 0.60 &lt; quality &lt; 0.70 :       900 elements\nInfo    : 0.70 &lt; quality &lt; 0.80 :      1940 elements\nInfo    : 0.80 &lt; quality &lt; 0.90 :      2017 elements\nInfo    : 0.90 &lt; quality &lt; 1.00 :      1050 elements\nInfo    : Done optimizing mesh (Wall 0.00624904s, CPU 0.010206s)\nInfo    : 1735 nodes 9230 elements\n\n\nWe then import the mesh generated by GMSH with the dolfinx.io.gmshio function\n\nimport csv\nfrom pathlib import Path\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport pyvista\n\nfrom mpi4py import MPI\nfrom dolfinx import (\n  fem,\n  io,\n  default_scalar_type,\n  geometry,\n  plot,\n)\nfrom dolfinx.fem.petsc import LinearProblem\nimport ufl\n\ncomm = MPI.COMM_WORLD\nrank = comm.rank\n\nmesh_data = io.gmshio.model_to_mesh(\n  gmsh.model, \n  comm, \n  0, \n  gdim=3\n)\n\ndomain, _, facet_tags = mesh_data\n\ngmsh.finalize()\n\nWe define the function space for the unknowns \\(p_r\\) and \\(p_i\\). We also specify the boundary integration measure \\(ds\\) using ufl\n\nV = fem.functionspace(domain, (\"Lagrange\", 1))\nds = ufl.Measure(\"ds\", domain=domain, subdomain_data=facet_tags)\n\n\nresults_folder = Path(\"fenicsx/helmholtz\")\nresults_folder.mkdir(exist_ok=True, parents=True)\n\n# Extract topology from mesh and create pyvista mesh\ntopology, cell_types, x = plot.vtk_mesh(V)\ngrid = pyvista.UnstructuredGrid(topology, cell_types, x)\n\nplotter = pyvista.Plotter(off_screen=True)\nplotter.add_mesh(grid, show_edges=True)\nplotter.add_axes()\n\n# if not pyvista.OFF_SCREEN:\n#     plotter.show()\n\n# HTML 저장\nplotter.export_html(results_folder/\"helmholtz_mesh.html\")\n\n\n\nBoundary conditions\nWe impose a velocity boundary condition of \\(v_n = 1.0 \\times 10^{-5}\\) at one end of the tube. For simplicity, we omit the point source in this example (although it could be included using scifem). At the opposite end of the tube, we prescribe an impedance \\(Z\\) computed with the Delaney–Bazley model, assuming a layer of thickness \\(d = 0.01\\) and a flow resistivity \\(\\sigma = 1.5 \\times 10^4\\). This choice of impedance—corresponding to a plane wave propagating in free field—produces a solution without reflections\nThe Delaney–Bazley model is used to compute the characteristic impedance and the wavenumber of the porous layer, which is treated as an equivalent fluid with complex-valued properties:\n\\[\n\\begin{aligned}\nZ_c(\\omega) &= \\rho_0 c_0 \\left[1 + 0.0571 X^{-0.754} - j\\, 0.087 X^{-0.732}\\right] \\\\\nk_c(\\omega) &= \\frac{\\omega}{c_0} \\left[1 + 0.0978 X^{-0.700} - j\\, 0.189 X^{-0.595}\\right]\n\\end{aligned}\n\\]\nwhere \\(X = \\tfrac{\\rho_0 f}{\\sigma}\\)\nUsing these values, we can compute the surface impedance. In the case of a rigid passive absorber mounted on a rigid wall, it is given by:\n\\[\nZ_s = -j\\, Z_c \\cot(k_c d)\n\\]\nLet’s create a function to calculate it\n\n# Air parameters\nrho0 = 1.225  # kg/m^3\nc = 340       # m/s\np_ref = 2e-5\n\n# Wall parameters\nsigma = 1.5e4  # ***\nd = 0.01       # ***\n\n# Impedance calculation\ndef delany_bazley_layer(f, rho0, c, sigma, d):\n  X = rho0 *f /sigma\n  Zc = rho0 *c *(1 +0.0571 *(X**-0.754) -1j *0.087 *(X**-0.732))\n  kc = 2 *np.pi *f /c *(1 +0.0978 *(X**-0.700) - 1j *0.189 *(X**-0.595))\n  Z_s = -1j *Zc *(1 /np.tan(kc *d))\n  return Z_s\n\nWe define the value of the normal velocity at the first end of the tube. Since we are going to compute a sound pressure spectrum, all frequency-dependent variables (i.e., \\(\\omega\\), \\(k\\), \\(Z_r\\) and \\(Z_i\\)) need to be updated within the frequency loop. To enable this, we initialize them as DOLFINx constants\n\n# sources (zero in this case)\nq_r = fem.Constant(domain, 0.0)\nq_i = fem.Constant(domain, 0.0)\n\n# velocity boundary condition (real part only here)\nv_nr = 1e-5  #***\nv_ni = 0.0\n\n# frequency-dependent variables\nomega = fem.Constant(domain, default_scalar_type(0))\nk = fem.Constant(domain, default_scalar_type(0))\n\nZ_r = fem.Constant(domain, default_scalar_type(0))\nZ_i = fem.Constant(domain, default_scalar_type(0))\n\n# probe point at center\nprobe_points = np.array(\n  [[L/2.0, W/2.0, H/2.0]], \n  dtype=np.float64\n)\ncoords = V.tabulate_dof_coordinates().reshape((-1, domain.geometry.dim))\ndof_indices = [\n  int(np.argmin(np.linalg.norm(coords -p, axis=1))) for p in probe_points\n]\n\nVariational formulation\nWe can now write the variational formulation\n\np_r = ufl.TrialFunction(V)\np_i = ufl.TrialFunction(V)\nv = ufl.TestFunction(V)\n\nDelta_Z = Z_r**2 +Z_i**2\ncoef = omega *rho0 /Delta_Z\n\n# bilinear forms\na_r = ( \n  (ufl.inner(ufl.grad(p_r), ufl.grad(v)) -k**2 *p_r *v) *ufl.dx \n  +coef *(Z_i *p_r -Z_r *p_i) *v *ds(3)\n)\n\na_i = (\n  (ufl.inner(ufl.grad(p_i), ufl.grad(v)) -k**2 *p_i *v) *ufl.dx\n  +coef *(Z_r *p_r +Z_i *p_i) *v *ds(3)\n)\n\n# RHS (velocity BC only)\nL_r = -omega *rho0 *q_i *v *ufl.dx +omega *rho0 *v_ni *v *ds(2)\nL_i =  omega *rho0 *q_r *v *ufl.dx -omega *rho0 *v_nr *v *ds(2)\n\nThe LinearProblem class is used to set up the PETSc backend and assemble the system matrix and vector. The solution is stored in a dolfinx.fem.Function, p_r_fun and p_i_fun\n\np_r_fun = fem.Function(V)\np_r_fun.name = \"p_real\"\n\np_i_fun = fem.Function(V)\np_i_fun.name = \"p_imag\"\n\nproblem_r = LinearProblem(\n  a_r, L_r, u=p_r_fun,\n  petsc_options={\"ksp_type\": \"preonly\", \"pc_type\": \"lu\"}\n)\nproblem_i = LinearProblem(\n  a_i, L_i, u=p_i_fun,\n  petsc_options={\"ksp_type\": \"preonly\", \"pc_type\": \"lu\"}\n)\n\nFrequency loop\nFinally, we implement the frequency loop. In this step, the frequency-dependent variables are updated and the system is solved for each frequency\n\n# Output files\nxdmf_real = io.XDMFFile(comm, results_folder/\"p_real.xdmf\", \"w\")\nxdmf_imag = io.XDMFFile(comm, results_folder/\"p_imag.xdmf\", \"w\")\nxdmf_real.write_mesh(domain)\nxdmf_imag.write_mesh(domain)\n\ncsv_file = results_folder/\"SPL_probes.csv\"\nif rank == 0:\n  with open(csv_file, \"w\", newline=\"\") as f_:\n    writer = csv.writer(f_)\n    writer.writerow([\"freq_Hz\", \"SPL_probe_center_dB\"])\n\n# Discrete frequency range\nfreq = np.arange(10, 1000, 5)  # Hz\nSPL_all = []\n\nfor f in freq:\n  omega.value = 2 *np.pi *f    \n  k.value = omega.value /c\n\n  Zs = delany_bazley_layer(f, rho0, c, sigma, d)\n  Z_r.value = Zs.real\n  Z_i.value = Zs.imag\n  \n  Delta_Z = Z_r**2 + Z_i**2\n  coef = omega *rho0 /Delta_Z\n\n  problem_r.solve()\n  problem_i.solve()\n\n  xdmf_real.write_function(p_r_fun, float(f))\n  xdmf_imag.write_function(p_i_fun, float(f))  \n\n  p_r_fun.x.scatter_forward()\n  p_i_fun.x.scatter_forward()\n\n  pr_vals = p_r_fun.x.array\n  pi_vals = p_i_fun.x.array\n  probe_SPLs = []\n  for idx in dof_indices:\n    pr = pr_vals[idx]\n    pi = pi_vals[idx]\n    p_complex = pr +1j *pi\n\n    spl_db = 20 *np.log10(np.abs(p_complex) /(np.sqrt(2) *p_ref))\n    probe_SPLs.append(spl_db)\n\n  all_probe_SPLs = comm.gather(probe_SPLs, root=0)\n  if rank == 0:\n    row = [float(f)] +all_probe_SPLs[0]\n    with open(csv_file, \"a\", newline=\"\") as f_:\n      csv.writer(f_).writerow(row)\n\n  SPL_all.append(probe_SPLs[0])\n  if rank == 0:\n    print(\n      f\"f: {f:.0f} Hz,\",\n      f\"|p|(probe): {10**(SPL_all[-1] /20) *np.sqrt(2) *p_ref:.3e} Pa,\",\n      f\"SPL: {SPL_all[-1]:.2f} dB\"\n    )\n\nxdmf_real.close()\nxdmf_imag.close()  \n\nf: 10 Hz, |p|(probe): 2.232e-02 Pa, SPL: 57.94 dB\nf: 15 Hz, |p|(probe): 1.490e-02 Pa, SPL: 54.43 dB\nf: 20 Hz, |p|(probe): 1.120e-02 Pa, SPL: 51.95 dB\nf: 25 Hz, |p|(probe): 8.984e-03 Pa, SPL: 50.04 dB\nf: 30 Hz, |p|(probe): 7.513e-03 Pa, SPL: 48.49 dB\nf: 35 Hz, |p|(probe): 6.467e-03 Pa, SPL: 47.18 dB\nf: 40 Hz, |p|(probe): 5.686e-03 Pa, SPL: 46.07 dB\nf: 45 Hz, |p|(probe): 5.082e-03 Pa, SPL: 45.09 dB\nf: 50 Hz, |p|(probe): 4.602e-03 Pa, SPL: 44.23 dB\nf: 55 Hz, |p|(probe): 4.212e-03 Pa, SPL: 43.46 dB\nf: 60 Hz, |p|(probe): 3.890e-03 Pa, SPL: 42.77 dB\nf: 65 Hz, |p|(probe): 3.620e-03 Pa, SPL: 42.14 dB\nf: 70 Hz, |p|(probe): 3.391e-03 Pa, SPL: 41.58 dB\nf: 75 Hz, |p|(probe): 3.195e-03 Pa, SPL: 41.06 dB\nf: 80 Hz, |p|(probe): 3.025e-03 Pa, SPL: 40.58 dB\nf: 85 Hz, |p|(probe): 2.878e-03 Pa, SPL: 40.15 dB\nf: 90 Hz, |p|(probe): 2.748e-03 Pa, SPL: 39.75 dB\nf: 95 Hz, |p|(probe): 2.634e-03 Pa, SPL: 39.38 dB\nf: 100 Hz, |p|(probe): 2.533e-03 Pa, SPL: 39.04 dB\nf: 105 Hz, |p|(probe): 2.442e-03 Pa, SPL: 38.72 dB\nf: 110 Hz, |p|(probe): 2.361e-03 Pa, SPL: 38.43 dB\nf: 115 Hz, |p|(probe): 2.288e-03 Pa, SPL: 38.16 dB\nf: 120 Hz, |p|(probe): 2.220e-03 Pa, SPL: 37.90 dB\nf: 125 Hz, |p|(probe): 2.158e-03 Pa, SPL: 37.65 dB\nf: 130 Hz, |p|(probe): 2.099e-03 Pa, SPL: 37.41 dB\nf: 135 Hz, |p|(probe): 2.040e-03 Pa, SPL: 37.16 dB\nf: 140 Hz, |p|(probe): 1.979e-03 Pa, SPL: 36.90 dB\nf: 145 Hz, |p|(probe): 1.909e-03 Pa, SPL: 36.59 dB\nf: 150 Hz, |p|(probe): 1.820e-03 Pa, SPL: 36.17 dB\nf: 155 Hz, |p|(probe): 1.681e-03 Pa, SPL: 35.48 dB\nf: 160 Hz, |p|(probe): 1.390e-03 Pa, SPL: 33.83 dB\nf: 165 Hz, |p|(probe): 1.664e-04 Pa, SPL: 15.39 dB\nf: 170 Hz, |p|(probe): 5.048e-03 Pa, SPL: 45.03 dB\nf: 175 Hz, |p|(probe): 2.958e-03 Pa, SPL: 40.39 dB\nf: 180 Hz, |p|(probe): 2.619e-03 Pa, SPL: 39.33 dB\nf: 185 Hz, |p|(probe): 2.489e-03 Pa, SPL: 38.89 dB\nf: 190 Hz, |p|(probe): 2.429e-03 Pa, SPL: 38.68 dB\nf: 195 Hz, |p|(probe): 2.404e-03 Pa, SPL: 38.59 dB\nf: 200 Hz, |p|(probe): 2.399e-03 Pa, SPL: 38.57 dB\nf: 205 Hz, |p|(probe): 2.409e-03 Pa, SPL: 38.61 dB\nf: 210 Hz, |p|(probe): 2.431e-03 Pa, SPL: 38.69 dB\nf: 215 Hz, |p|(probe): 2.464e-03 Pa, SPL: 38.80 dB\nf: 220 Hz, |p|(probe): 2.507e-03 Pa, SPL: 38.95 dB\nf: 225 Hz, |p|(probe): 2.560e-03 Pa, SPL: 39.13 dB\nf: 230 Hz, |p|(probe): 2.623e-03 Pa, SPL: 39.35 dB\nf: 235 Hz, |p|(probe): 2.698e-03 Pa, SPL: 39.59 dB\nf: 240 Hz, |p|(probe): 2.785e-03 Pa, SPL: 39.87 dB\nf: 245 Hz, |p|(probe): 2.887e-03 Pa, SPL: 40.18 dB\nf: 250 Hz, |p|(probe): 3.004e-03 Pa, SPL: 40.52 dB\nf: 255 Hz, |p|(probe): 3.140e-03 Pa, SPL: 40.91 dB\nf: 260 Hz, |p|(probe): 3.297e-03 Pa, SPL: 41.33 dB\nf: 265 Hz, |p|(probe): 3.481e-03 Pa, SPL: 41.80 dB\nf: 270 Hz, |p|(probe): 3.697e-03 Pa, SPL: 42.32 dB\nf: 275 Hz, |p|(probe): 3.951e-03 Pa, SPL: 42.90 dB\nf: 280 Hz, |p|(probe): 4.255e-03 Pa, SPL: 43.55 dB\nf: 285 Hz, |p|(probe): 4.622e-03 Pa, SPL: 44.27 dB\nf: 290 Hz, |p|(probe): 5.074e-03 Pa, SPL: 45.08 dB\nf: 295 Hz, |p|(probe): 5.639e-03 Pa, SPL: 45.99 dB\nf: 300 Hz, |p|(probe): 6.365e-03 Pa, SPL: 47.04 dB\nf: 305 Hz, |p|(probe): 7.328e-03 Pa, SPL: 48.27 dB\nf: 310 Hz, |p|(probe): 8.663e-03 Pa, SPL: 49.72 dB\nf: 315 Hz, |p|(probe): 1.063e-02 Pa, SPL: 51.50 dB\nf: 320 Hz, |p|(probe): 1.381e-02 Pa, SPL: 53.78 dB\nf: 325 Hz, |p|(probe): 1.981e-02 Pa, SPL: 56.91 dB\nf: 330 Hz, |p|(probe): 3.527e-02 Pa, SPL: 61.92 dB\nf: 335 Hz, |p|(probe): 1.646e-01 Pa, SPL: 75.30 dB\nf: 340 Hz, |p|(probe): 6.128e-02 Pa, SPL: 66.72 dB\nf: 345 Hz, |p|(probe): 2.578e-02 Pa, SPL: 59.20 dB\nf: 350 Hz, |p|(probe): 1.632e-02 Pa, SPL: 55.22 dB\nf: 355 Hz, |p|(probe): 1.194e-02 Pa, SPL: 52.51 dB\nf: 360 Hz, |p|(probe): 9.415e-03 Pa, SPL: 50.45 dB\nf: 365 Hz, |p|(probe): 7.778e-03 Pa, SPL: 48.79 dB\nf: 370 Hz, |p|(probe): 6.632e-03 Pa, SPL: 47.40 dB\nf: 375 Hz, |p|(probe): 5.786e-03 Pa, SPL: 46.22 dB\nf: 380 Hz, |p|(probe): 5.136e-03 Pa, SPL: 45.18 dB\nf: 385 Hz, |p|(probe): 4.623e-03 Pa, SPL: 44.27 dB\nf: 390 Hz, |p|(probe): 4.207e-03 Pa, SPL: 43.45 dB\nf: 395 Hz, |p|(probe): 3.864e-03 Pa, SPL: 42.71 dB\nf: 400 Hz, |p|(probe): 3.576e-03 Pa, SPL: 42.04 dB\nf: 405 Hz, |p|(probe): 3.332e-03 Pa, SPL: 41.42 dB\nf: 410 Hz, |p|(probe): 3.122e-03 Pa, SPL: 40.86 dB\nf: 415 Hz, |p|(probe): 2.940e-03 Pa, SPL: 40.34 dB\nf: 420 Hz, |p|(probe): 2.780e-03 Pa, SPL: 39.85 dB\nf: 425 Hz, |p|(probe): 2.638e-03 Pa, SPL: 39.40 dB\nf: 430 Hz, |p|(probe): 2.511e-03 Pa, SPL: 38.97 dB\nf: 435 Hz, |p|(probe): 2.397e-03 Pa, SPL: 38.56 dB\nf: 440 Hz, |p|(probe): 2.292e-03 Pa, SPL: 38.18 dB\nf: 445 Hz, |p|(probe): 2.196e-03 Pa, SPL: 37.80 dB\nf: 450 Hz, |p|(probe): 2.104e-03 Pa, SPL: 37.43 dB\nf: 455 Hz, |p|(probe): 2.017e-03 Pa, SPL: 37.06 dB\nf: 460 Hz, |p|(probe): 1.930e-03 Pa, SPL: 36.68 dB\nf: 465 Hz, |p|(probe): 1.841e-03 Pa, SPL: 36.27 dB\nf: 470 Hz, |p|(probe): 1.745e-03 Pa, SPL: 35.81 dB\nf: 475 Hz, |p|(probe): 1.636e-03 Pa, SPL: 35.24 dB\nf: 480 Hz, |p|(probe): 1.500e-03 Pa, SPL: 34.49 dB\nf: 485 Hz, |p|(probe): 1.316e-03 Pa, SPL: 33.35 dB\nf: 490 Hz, |p|(probe): 1.029e-03 Pa, SPL: 31.22 dB\nf: 495 Hz, |p|(probe): 4.823e-04 Pa, SPL: 24.63 dB\nf: 500 Hz, |p|(probe): 1.079e-03 Pa, SPL: 31.63 dB\nf: 505 Hz, |p|(probe): 6.165e-02 Pa, SPL: 66.77 dB\nf: 510 Hz, |p|(probe): 5.665e-03 Pa, SPL: 46.03 dB\nf: 515 Hz, |p|(probe): 3.850e-03 Pa, SPL: 42.68 dB\nf: 520 Hz, |p|(probe): 3.277e-03 Pa, SPL: 41.28 dB\nf: 525 Hz, |p|(probe): 3.005e-03 Pa, SPL: 40.53 dB\nf: 530 Hz, |p|(probe): 2.855e-03 Pa, SPL: 40.08 dB\nf: 535 Hz, |p|(probe): 2.768e-03 Pa, SPL: 39.81 dB\nf: 540 Hz, |p|(probe): 2.719e-03 Pa, SPL: 39.66 dB\nf: 545 Hz, |p|(probe): 2.697e-03 Pa, SPL: 39.59 dB\nf: 550 Hz, |p|(probe): 2.694e-03 Pa, SPL: 39.58 dB\nf: 555 Hz, |p|(probe): 2.708e-03 Pa, SPL: 39.62 dB\nf: 560 Hz, |p|(probe): 2.736e-03 Pa, SPL: 39.71 dB\nf: 565 Hz, |p|(probe): 2.778e-03 Pa, SPL: 39.84 dB\nf: 570 Hz, |p|(probe): 2.833e-03 Pa, SPL: 40.01 dB\nf: 575 Hz, |p|(probe): 2.901e-03 Pa, SPL: 40.22 dB\nf: 580 Hz, |p|(probe): 2.985e-03 Pa, SPL: 40.47 dB\nf: 585 Hz, |p|(probe): 3.084e-03 Pa, SPL: 40.75 dB\nf: 590 Hz, |p|(probe): 3.201e-03 Pa, SPL: 41.07 dB\nf: 595 Hz, |p|(probe): 3.338e-03 Pa, SPL: 41.44 dB\nf: 600 Hz, |p|(probe): 3.500e-03 Pa, SPL: 41.85 dB\nf: 605 Hz, |p|(probe): 3.690e-03 Pa, SPL: 42.31 dB\nf: 610 Hz, |p|(probe): 3.915e-03 Pa, SPL: 42.82 dB\nf: 615 Hz, |p|(probe): 4.182e-03 Pa, SPL: 43.40 dB\nf: 620 Hz, |p|(probe): 4.504e-03 Pa, SPL: 44.04 dB\nf: 625 Hz, |p|(probe): 4.895e-03 Pa, SPL: 44.76 dB\nf: 630 Hz, |p|(probe): 5.380e-03 Pa, SPL: 45.58 dB\nf: 635 Hz, |p|(probe): 5.992e-03 Pa, SPL: 46.52 dB\nf: 640 Hz, |p|(probe): 6.786e-03 Pa, SPL: 47.60 dB\nf: 645 Hz, |p|(probe): 7.853e-03 Pa, SPL: 48.87 dB\nf: 650 Hz, |p|(probe): 9.358e-03 Pa, SPL: 50.39 dB\nf: 655 Hz, |p|(probe): 1.163e-02 Pa, SPL: 52.28 dB\nf: 660 Hz, |p|(probe): 1.545e-02 Pa, SPL: 54.75 dB\nf: 665 Hz, |p|(probe): 2.319e-02 Pa, SPL: 58.27 dB\nf: 670 Hz, |p|(probe): 4.702e-02 Pa, SPL: 64.41 dB\nf: 675 Hz, |p|(probe): 1.206e+00 Pa, SPL: 92.60 dB\nf: 680 Hz, |p|(probe): 4.325e-02 Pa, SPL: 63.69 dB\nf: 685 Hz, |p|(probe): 2.194e-02 Pa, SPL: 57.80 dB\nf: 690 Hz, |p|(probe): 1.468e-02 Pa, SPL: 54.30 dB\nf: 695 Hz, |p|(probe): 1.102e-02 Pa, SPL: 51.81 dB\nf: 700 Hz, |p|(probe): 8.813e-03 Pa, SPL: 49.87 dB\nf: 705 Hz, |p|(probe): 7.343e-03 Pa, SPL: 48.29 dB\nf: 710 Hz, |p|(probe): 6.295e-03 Pa, SPL: 46.95 dB\nf: 715 Hz, |p|(probe): 5.510e-03 Pa, SPL: 45.79 dB\nf: 720 Hz, |p|(probe): 4.900e-03 Pa, SPL: 44.77 dB\nf: 725 Hz, |p|(probe): 4.414e-03 Pa, SPL: 43.87 dB\nf: 730 Hz, |p|(probe): 4.017e-03 Pa, SPL: 43.05 dB\nf: 735 Hz, |p|(probe): 3.687e-03 Pa, SPL: 42.30 dB\nf: 740 Hz, |p|(probe): 3.408e-03 Pa, SPL: 41.62 dB\nf: 745 Hz, |p|(probe): 3.169e-03 Pa, SPL: 40.99 dB\nf: 750 Hz, |p|(probe): 2.962e-03 Pa, SPL: 40.40 dB\nf: 755 Hz, |p|(probe): 2.781e-03 Pa, SPL: 39.85 dB\nf: 760 Hz, |p|(probe): 2.620e-03 Pa, SPL: 39.33 dB\nf: 765 Hz, |p|(probe): 2.475e-03 Pa, SPL: 38.84 dB\nf: 770 Hz, |p|(probe): 2.344e-03 Pa, SPL: 38.37 dB\nf: 775 Hz, |p|(probe): 2.223e-03 Pa, SPL: 37.91 dB\nf: 780 Hz, |p|(probe): 2.110e-03 Pa, SPL: 37.45 dB\nf: 785 Hz, |p|(probe): 2.002e-03 Pa, SPL: 37.00 dB\nf: 790 Hz, |p|(probe): 1.897e-03 Pa, SPL: 36.53 dB\nf: 795 Hz, |p|(probe): 1.793e-03 Pa, SPL: 36.04 dB\nf: 800 Hz, |p|(probe): 1.684e-03 Pa, SPL: 35.50 dB\nf: 805 Hz, |p|(probe): 1.567e-03 Pa, SPL: 34.87 dB\nf: 810 Hz, |p|(probe): 1.435e-03 Pa, SPL: 34.11 dB\nf: 815 Hz, |p|(probe): 1.276e-03 Pa, SPL: 33.09 dB\nf: 820 Hz, |p|(probe): 1.071e-03 Pa, SPL: 31.57 dB\nf: 825 Hz, |p|(probe): 7.809e-04 Pa, SPL: 28.82 dB\nf: 830 Hz, |p|(probe): 3.158e-04 Pa, SPL: 20.96 dB\nf: 835 Hz, |p|(probe): 5.912e-04 Pa, SPL: 26.40 dB\nf: 840 Hz, |p|(probe): 3.254e-03 Pa, SPL: 41.22 dB\nf: 845 Hz, |p|(probe): 2.715e-01 Pa, SPL: 79.65 dB\nf: 850 Hz, |p|(probe): 7.706e-03 Pa, SPL: 48.71 dB\nf: 855 Hz, |p|(probe): 4.893e-03 Pa, SPL: 44.76 dB\nf: 860 Hz, |p|(probe): 3.978e-03 Pa, SPL: 42.96 dB\nf: 865 Hz, |p|(probe): 3.534e-03 Pa, SPL: 41.93 dB\nf: 870 Hz, |p|(probe): 3.280e-03 Pa, SPL: 41.29 dB\nf: 875 Hz, |p|(probe): 3.123e-03 Pa, SPL: 40.86 dB\nf: 880 Hz, |p|(probe): 3.025e-03 Pa, SPL: 40.58 dB\nf: 885 Hz, |p|(probe): 2.965e-03 Pa, SPL: 40.41 dB\nf: 890 Hz, |p|(probe): 2.934e-03 Pa, SPL: 40.32 dB\nf: 895 Hz, |p|(probe): 2.924e-03 Pa, SPL: 40.29 dB\nf: 900 Hz, |p|(probe): 2.933e-03 Pa, SPL: 40.31 dB\nf: 905 Hz, |p|(probe): 2.958e-03 Pa, SPL: 40.39 dB\nf: 910 Hz, |p|(probe): 2.999e-03 Pa, SPL: 40.51 dB\nf: 915 Hz, |p|(probe): 3.054e-03 Pa, SPL: 40.67 dB\nf: 920 Hz, |p|(probe): 3.126e-03 Pa, SPL: 40.87 dB\nf: 925 Hz, |p|(probe): 3.215e-03 Pa, SPL: 41.11 dB\nf: 930 Hz, |p|(probe): 3.321e-03 Pa, SPL: 41.39 dB\nf: 935 Hz, |p|(probe): 3.448e-03 Pa, SPL: 41.72 dB\nf: 940 Hz, |p|(probe): 3.599e-03 Pa, SPL: 42.09 dB\nf: 945 Hz, |p|(probe): 3.778e-03 Pa, SPL: 42.51 dB\nf: 950 Hz, |p|(probe): 3.989e-03 Pa, SPL: 42.99 dB\nf: 955 Hz, |p|(probe): 4.242e-03 Pa, SPL: 43.52 dB\nf: 960 Hz, |p|(probe): 4.545e-03 Pa, SPL: 44.12 dB\nf: 965 Hz, |p|(probe): 4.914e-03 Pa, SPL: 44.80 dB\nf: 970 Hz, |p|(probe): 5.368e-03 Pa, SPL: 45.57 dB\nf: 975 Hz, |p|(probe): 5.938e-03 Pa, SPL: 46.44 dB\nf: 980 Hz, |p|(probe): 6.671e-03 Pa, SPL: 47.45 dB\nf: 985 Hz, |p|(probe): 7.644e-03 Pa, SPL: 48.64 dB\nf: 990 Hz, |p|(probe): 8.993e-03 Pa, SPL: 50.05 dB\nf: 995 Hz, |p|(probe): 1.098e-02 Pa, SPL: 51.78 dB\n\n\n\nif rank == 0:\n  plt.figure(figsize=(6, 4))\n\n  plt.plot(freq, SPL_all, marker=\"o\")\n  \n  plt.xlabel(\"Frequency [Hz]\")\n  plt.ylabel(\"SPL [dB re 20 µPa]\")\n  plt.title(\"SPL at domain center (velocity-inlet, DBZ-impedance outlet)\")\n  plt.grid(True)\n  plt.tight_layout()\n  plt.savefig(results_folder/\"SPL_spectrum.png\", dpi=150)\n  \n  plt.show()\n\n\n\n\n\n\n\n\n\n\nK.7.8 Adaptive mesh refinement with GMSH/FEniCSx\nIn this section, we consider an adaptive mesh refinement method applied to the Laplace eigenvalue problem\n\nK.7.8.1 Solving the eigenvalue problem\nWe consider the Laplace eigenvalue problem with adaptive mesh refinement. The finite element discretization is performed using dolfinx, while PETSc/SLEPc are used for the linear algebra and eigenvalue solver. The geometry is generated with GMSH\nProblem definition\nWe study the Laplace eigenvalue problem:\n\\[\n-\\Delta u = \\lambda u \\quad \\text{in } \\Omega,\n\\quad u = 0 \\quad \\text{on } \\partial\\Omega\n\\]\nWe use a variational formulation: find \\((\\lambda, u) \\in \\mathbb{R} \\times V_h\\) such that\n\\[\na(u, v) = \\lambda m(u, v), \\quad \\forall v \\in V_h\n\\]\nwith\n\\[\na(u, v) = \\int_\\Omega \\nabla u \\cdot \\nabla v \\, dx,\n\\quad\nm(u, v) = \\int_\\Omega u v \\, dx\n\\]\n\n\nK.7.8.2 Adaptive mesh refinement\nAdaptive mesh refinement improves solution accuracy by selectively refining elements where errors are largest. Two common approaches for eigenvalue problems are gradient-based and residual-based error estimation\nGradient-based refinement\nUses the magnitude of the solution gradient as an error indicator:\n\\[ \\eta_K = \\int_K |\\nabla u_h| \\, dx \\]\n\nAdvantages\n\nSimple and computationally cheap\nWorks well for smooth solutions\nIntuitive: large gradients suggest refinement needs\n\nLimitations\n\nMay over-refine in regions with naturally large gradients\nDoesn’t account for actual PDE residual\nLess theoretically rigorous\n\n\nResidual-based refinement\nUses the PDE residual and flux jumps across element boundaries:\n\\[\\eta_K^2 = h_K^2 \\|R_K\\|_{L^2(K)}^2 + \\frac{1}{2} \\sum_{e \\in \\partial K} h_e \\|J_e\\|_{L^2(e)}^2\\]​\nwhere \\(R_K = \\lambda_h u_h\\) (interior residual) and \\(J_e = [\\nabla u_h \\cdot n]\\) (flux jump)\n\nAdvantages\n\nTheoretically rigorous error bound\nAccounts for both interior and boundary contributions\nBetter handles singularities and discontinuities\nMore efficient refinement strategy\n\nLimitations\n\nMore complex implementation\nHigher computational cost\nRequires careful treatment of jumps\n\n\nImplementation tips\n\nThreshold selection: Use percentile-based marking (e.g., top 25%)\nScaling: Residual estimator naturally scales with mesh size\nStability: Ensure non-negative error indicators\nConvergence: Monitor eigenvalue changes across refinement levels\n\nBoth methods provide effective adaptive strategies, with residual-based estimation offering superior theoretical foundations at the cost of implementation complexity\n\n\nK.7.8.3 Implementation\nWe begin by importing the necessary packages:\n\nimport numpy as np\nfrom pathlib import Path\nfrom mpi4py import MPI\nfrom petsc4py import PETSc\nfrom slepc4py import SLEPc\nimport pyvista\nimport gmsh\n\nfrom dolfinx import fem, mesh, plot, default_scalar_type\nfrom dolfinx.fem.petsc import assemble_matrix, assemble_vector  # fenicsx 0.9.0\nfrom dolfinx.mesh import create_unit_square, locate_entities_boundary, refine\nfrom dolfinx.io import gmshio, XDMFFile\nimport ufl\n\nIn this section, we’ll go through a complete workflow for generating, converting, and visualizing 2D meshes—specifically a square, an L-shaped domain, and a Pac-Man shape—using GMSH, FEniCSx, and PyVista.\nOnce the meshes are ready, we’ll set up a helper function that solves the eigenvalue problem with SLEPc on a discretized domain.\nNext, we’ll run the adaptive mesh refinement (AMR) algorithm. To make visualization easier, we’ll also add a helper function that attaches the relevant data to the plotter at each refinement step.\nFinally, we’ll set up some parameters to check the convergence of the algorithm\n\nclass AdaptiveLaplaceEigenSolver:\n    \"\"\"\n    Adaptive Laplace eigenvalue solver with visualization\n    \"\"\"\n    \n    def __init__(self, domain_type=\"unit_square\", initial_mesh_size=0.1):\n        self.domain_type = domain_type\n        self.initial_mesh_size = initial_mesh_size\n        self.mesh = None\n        self.V = None\n\n        self.eigenvalues = []\n        self.eigenfunctions = []\n\n        self.results_folder = Path(\"fenicsx/adaptive_mesh_refinement\")\n        self.results_folder.mkdir(exist_ok=True, parents=True)\n        \n        print(f\"Initialized solver for {domain_type} domain\")\n        \n    def create_domain(self):\n        \"\"\"Create computational domain\"\"\"\n        if self.domain_type == \"unit_square\":\n            self.mesh = create_unit_square(MPI.COMM_WORLD, 20, 20, mesh.CellType.triangle)\n            print(f\"Created unit square: {self._count_n_cells()} cells\")\n            \n        elif self.domain_type == \"l_shape\":\n            self._create_l_shape_domain()\n            \n        elif self.domain_type == \"pacman\":\n            self._create_pacman_domain()\n            \n        else:\n            raise ValueError(f\"Unknown domain type: {self.domain_type}\")\n\n    def _count_n_cells(self):\n        return self.mesh.topology.index_map(2).size_global\n    \n    def _create_l_shape_domain(self):\n        \"\"\"Create L-shaped domain using GMSH\"\"\"\n        gmsh.initialize()\n        gmsh.model.add(\"l_shape\")\n        \n        try:\n            # Create L-shape\n            rect1 = gmsh.model.occ.addRectangle(0, 0, 0, 1, 1)\n            rect2 = gmsh.model.occ.addRectangle(0, 1, 0, 1, 1)\n            rect3 = gmsh.model.occ.addRectangle(1, 0, 0, 1, 1)\n            \n            # Fuse rectangles\n            fused1, _ = gmsh.model.occ.fuse([(2, rect1)], [(2, rect2)])\n            fused2, _ = gmsh.model.occ.fuse(fused1, [(2, rect3)])\n            \n            gmsh.model.occ.synchronize()\n            \n            # Add physical groups\n            surfaces = gmsh.model.getEntities(2)\n            if surfaces:\n                gmsh.model.addPhysicalGroup(2, [s[1] for s in surfaces], 1)\n                gmsh.model.setPhysicalName(2, 1, \"LDomain\")\n            \n            # Set mesh size\n            gmsh.option.setNumber(\"Mesh.CharacteristicLengthMin\", self.initial_mesh_size * 0.5)\n            gmsh.option.setNumber(\"Mesh.CharacteristicLengthMax\", self.initial_mesh_size * 2.0)\n            gmsh.model.mesh.generate(2)\n            \n            # Convert to dolfinx\n            self.mesh, ct, ft = gmshio.model_to_mesh(gmsh.model, MPI.COMM_WORLD, 0, gdim=2)\n            \n        finally:\n            gmsh.finalize()\n            \n        print(f\"Created L-shape: {self._count_n_cells()} cells\")\n    \n    def _create_pacman_domain(self):\n        \"\"\"Create Pac-Man domain using GMSH\"\"\"\n        gmsh.initialize()\n        gmsh.model.add(\"pacman\")\n        \n        try:\n            # Create Pac-Man shape\n            disk = gmsh.model.occ.addDisk(0, 0, 0, 1.0, 1.0)\n            \n            # Create mouth triangle\n            angle_rad = np.radians(30)  # 30-degree mouth opening\n            mouth_length = 1.2\n            \n            p1 = gmsh.model.occ.addPoint(0, 0, 0)\n            p2 = gmsh.model.occ.addPoint(mouth_length * np.cos(angle_rad), \n                                       mouth_length * np.sin(angle_rad), 0)\n            p3 = gmsh.model.occ.addPoint(mouth_length * np.cos(-angle_rad), \n                                       mouth_length * np.sin(-angle_rad), 0)\n            \n            l1 = gmsh.model.occ.addLine(p1, p2)\n            l2 = gmsh.model.occ.addLine(p2, p3)  \n            l3 = gmsh.model.occ.addLine(p3, p1)\n            \n            mouth_loop = gmsh.model.occ.addCurveLoop([l1, l2, l3])\n            mouth_surface = gmsh.model.occ.addPlaneSurface([mouth_loop])\n            \n            # Cut mouth from disk\n            pacman, _ = gmsh.model.occ.cut([(2, disk)], [(2, mouth_surface)])\n            \n            gmsh.model.occ.synchronize()\n            \n            # Add physical groups\n            if pacman:\n                dim, tag = pacman[0]\n                gmsh.model.addPhysicalGroup(dim, [tag], 1)\n                gmsh.model.setPhysicalName(dim, 1, \"PacmanDomain\")\n            \n            # Set mesh options\n            gmsh.option.setNumber(\"Mesh.CharacteristicLengthMin\", self.initial_mesh_size * 0.5)\n            gmsh.option.setNumber(\"Mesh.CharacteristicLengthMax\", self.initial_mesh_size * 2.0)\n            gmsh.option.setNumber(\"Mesh.Algorithm\", 6)  # Frontal-Delaunay\n            gmsh.model.mesh.generate(2)\n            \n            # Convert to dolfinx\n            self.mesh, ct, ft = gmshio.model_to_mesh(gmsh.model, MPI.COMM_WORLD, 0, gdim=2)\n            \n        finally:\n            gmsh.finalize()\n            \n        print(f\"Created Pac-Man: {self._count_n_cells()} cells\")\n    \n    def solve_eigenvalue_problem(self, n_eigenvalues=6):\n        \"\"\"Solve eigenvalue problem on current mesh\"\"\"\n        # Create function space\n        self.V = fem.functionspace(self.mesh, (\"Lagrange\", 1))\n        \n        # Define variational forms\n        u = ufl.TrialFunction(self.V)\n        v = ufl.TestFunction(self.V)\n        \n        a = ufl.inner(ufl.grad(u), ufl.grad(v)) * ufl.dx  # Stiffness\n        m = ufl.inner(u, v) * ufl.dx                      # Mass\n        \n        # Apply homogeneous Dirichlet boundary conditions\n        boundary_facets = locate_entities_boundary(\n            self.mesh, self.mesh.topology.dim-1, \n            lambda x: np.ones(x.shape[1], dtype=bool)  # All boundary\n        )\n        boundary_dofs = fem.locate_dofs_topological(self.V, self.mesh.topology.dim-1, boundary_facets)\n        bc = fem.dirichletbc(default_scalar_type(0.0), boundary_dofs, self.V)\n        \n        # Assemble matrices\n        A = assemble_matrix(fem.form(a), bcs=[bc])\n        A.assemble()\n        M = assemble_matrix(fem.form(m), bcs=[bc])\n        M.assemble()\n        \n        # Solve using shift-invert\n        eps = SLEPc.EPS().create(MPI.COMM_WORLD)\n        eps.setOperators(A, M)\n        eps.setProblemType(SLEPc.EPS.ProblemType.GHEP)\n        \n        # Set shift based on domain type\n        if self.domain_type == \"unit_square\":\n            shift = 15.0\n        elif self.domain_type == \"l_shape\":\n            shift = 2.0\n        elif self.domain_type == \"pacman\":\n            shift = 3.0\n        \n        st = eps.getST()\n        st.setType(SLEPc.ST.Type.SINVERT)\n        st.setShift(shift)\n        \n        eps.setWhichEigenpairs(SLEPc.EPS.Which.TARGET_MAGNITUDE)\n        eps.setTarget(shift)\n        eps.setDimensions(n_eigenvalues + 3)\n        eps.setTolerances(1e-10, 3000)\n        eps.setType(SLEPc.EPS.Type.KRYLOVSCHUR)\n        \n        eps.solve()\n        \n        # Extract results\n        nconv = eps.getConverged()\n        results = []\n        \n        for i in range(nconv):\n            eigenval = eps.getEigenvalue(i).real\n            if eigenval &gt; shift * 0.8:  # Filter based on shift\n                eigenvec = fem.Function(self.V)\n                eps.getEigenvector(i, eigenvec.x.petsc_vec)\n                eigenvec.x.scatter_forward()\n                \n                # Normalize\n                norm = np.sqrt(eigenvec.x.petsc_vec.dot(eigenvec.x.petsc_vec))\n                if norm &gt; 1e-12:\n                    eigenvec.x.petsc_vec.scale(1.0 / norm)\n                \n                results.append((eigenval, eigenvec))\n        \n        # Sort and store results\n        results.sort(key=lambda x: x[0])\n        self.eigenvalues = [r[0] for r in results[:n_eigenvalues]]\n        self.eigenfunctions = [r[1] for r in results[:n_eigenvalues]]\n        \n        # Cleanup\n        eps.destroy()\n        A.destroy()\n        M.destroy()\n        \n        return self.eigenvalues, self.eigenfunctions\n    \n    def gradient_estimate_error(self, eigenfunction, eigenvalue):\n        \"\"\"\n        Estimate error using gradient-based indicator\n        \"\"\"\n        # Create DG0 space for error indicators\n        W = fem.functionspace(self.mesh, (\"DG\", 0))\n        error_indicator = fem.Function(W)\n        \n        # Compute gradient magnitude as error indicator\n        u_dg = ufl.TrialFunction(W)        \n        v_dg = ufl.TestFunction(W)\n  \n        grad_u = ufl.grad(eigenfunction)\n        grad_norm_expr = ufl.sqrt(ufl.inner(grad_u, grad_u))\n        \n        # Project gradient norm to DG0\n        a_proj = ufl.inner(u_dg, v_dg) * ufl.dx\n        L_proj = ufl.inner(grad_norm_expr, v_dg) * ufl.dx\n        \n        A_proj = assemble_matrix(fem.form(a_proj))\n        A_proj.assemble()\n        b_proj = assemble_vector(fem.form(L_proj))\n        b_proj.assemble()\n        \n        # Solve projection\n        solver = PETSc.KSP().create()\n        solver.setOperators(A_proj)\n        solver.setType(PETSc.KSP.Type.CG)\n        solver.solve(b_proj, error_indicator.x.petsc_vec)\n        \n        A_proj.destroy()\n        b_proj.destroy()\n        solver.destroy()\n        \n        return error_indicator.x.array\n\n    def residual_estimate_error(self, eigenfunction, eigenvalue):\n        \"\"\"\n        Residual-based error estimator for eigenvalue problems\n        \n        η_K² = h_K² ||R_K||²_L²(K) + (1/2) Σ h_e ||J_e||²_L²(e)\n        \n        where:\n        - R_K = λ_h u_h (interior residual, since -Δu_h = λ_h u_h)\n        - J_e = [∇u_h · n] (jump in normal flux across interior facets)\n        - h_K = cell diameter, h_e = facet size\n        \"\"\"\n        # Create DG0 space for error indicators\n        W = fem.functionspace(self.mesh, (\"DG\", 0))\n        error_indicator = fem.Function(W)\n        \n        # Define test function and cell/facet measures\n        w = ufl.TestFunction(W)\n        h_K = ufl.CellDiameter(self.mesh)\n        h_e = ufl.FacetArea(self.mesh)  # In 2D, this gives edge length\n        n = ufl.FacetNormal(self.mesh)\n        \n        # Interior residual: R_K = λ_h * u_h \n        # (since the strong form is -Δu = λu, the residual is λu when Δu ≈ 0)\n        R_K = eigenvalue * eigenfunction\n        \n        # Jump in normal flux: J_e = [∇u_h · n]\n        grad_u = ufl.grad(eigenfunction)\n        flux_jump = ufl.jump(grad_u, n)  # Jump across interior facets\n        \n        # Residual-based error estimator\n        # Interior contribution: h_K² ||R_K||²\n        interior_contrib = h_K**2 * R_K**2 * w * ufl.dx\n        \n        # Boundary contribution: (1/2) h_e ||J_e||²  \n        # Note: ufl.dS integrates over interior facets only\n        boundary_contrib = 0.5 * h_e * flux_jump**2 * ufl.avg(w) * ufl.dS\n        \n        # Total error estimator\n        error_form = interior_contrib + boundary_contrib\n        \n        # Assemble the error estimator\n        error_vector = assemble_vector(fem.form(error_form))\n        error_vector.assemble()\n        \n        # Extract local error indicators and take square root\n        # (since we computed η_K², we need sqrt to get η_K)\n        error_indicators_squared = error_vector.array\n        error_indicators = np.sqrt(np.maximum(error_indicators_squared, 0.0))  # Ensure non-negative\n        \n        return error_indicators        \n    \n    def adaptive_refinement(self, err_estimator='residual', n_iterations=4, refinement_fraction=0.25, n_eigenvalues=6):\n        \"\"\"\n        Perform adaptive mesh refinement\n        \"\"\"\n        print(f\"\\n{'='*60}\")\n        print(f\"ADAPTIVE REFINEMENT for {self.domain_type.upper()}\")\n        print(f\"{'='*60}\")\n        \n        refinement_history = []\n        \n        for iteration in range(n_iterations):\n            print(f\"\\n--- Iteration {iteration + 1}/{n_iterations} ---\")\n            \n            # Solve eigenvalue problem\n            eigenvalues, eigenfunctions = self.solve_eigenvalue_problem(n_eigenvalues)\n            \n            print(f\"Mesh: {self._count_n_cells()} cells, \"\n                  f\"DOFs: {self.V.dofmap.index_map.size_global}\")\n            \n            # Display first few eigenvalues\n            for i, eigenval in enumerate(eigenvalues[:3]):\n                print(f\"λ_{i+1} = {eigenval:.6f}\")\n            \n            # Save current results\n            self.save_iteration_results(iteration)\n            \n            # Estimate error for first eigenfunction\n            if eigenfunctions:\n\n                if err_estimator == 'residual':\n                    error_indicators = self.residual_estimate_error(eigenfunctions[0], eigenvalues[0])\n                elif err_estimator == 'gradient':\n                    error_indicators = self.gradient_estimate_error(eigenfunctions[0], eigenvalues[0])\n                \n                # Mark cells for refinement\n                threshold = np.percentile(error_indicators, (1 - refinement_fraction) * 100)\n                cells_to_refine = np.where(error_indicators &gt; threshold)[0]\n                \n                print(f\"Refining {len(cells_to_refine)} cells (threshold: {threshold:.3f})\")\n                \n                if len(cells_to_refine) == 0:\n                    print(\"No cells to refine, stopping.\")\n                    break\n                \n                # Store history\n                refinement_history.append({\n                    'iteration': iteration + 1,\n                    'n_cells': self._count_n_cells(),\n                    'n_dofs': self.V.dofmap.index_map.size_global,\n                    'eigenvalues': eigenvalues.copy(),\n                    'error_max': error_indicators.max(),\n                    'error_mean': error_indicators.mean()\n                })\n                \n                # Refine mesh\n                if iteration &lt; n_iterations - 1:  # Don't refine on last iteration\n                    refined_result = refine(self.mesh, cells_to_refine)\n                    # In dolfinx 0.9.0, refine may return a tuple (mesh, cell_map, vertex_map, facet_map)\n                    if isinstance(refined_result, tuple):\n                        self.mesh = refined_result[0]  # Extract just the mesh\n                    else:\n                        self.mesh = refined_result\n        \n        # Final solve and save\n        final_eigenvalues, final_eigenfunctions = self.solve_eigenvalue_problem(n_eigenvalues)\n        self.save_final_results()\n        self.print_convergence_summary(refinement_history)\n        \n        return refinement_history\n    \n    def save_iteration_results(self, iteration):\n        \"\"\"Save results for current iteration\"\"\"\n        if not self.eigenvalues or not self.eigenfunctions:\n            return\n            \n        # Save eigenvalues\n        eigenval_file = self.results_folder / f\"{self.domain_type}_eigenvalues_iter_{iteration}.txt\"\n        np.savetxt(eigenval_file, self.eigenvalues)\n        \n        # Save eigenfunctions to XDMF\n        try:\n            xdmf_file = self.results_folder / f\"{self.domain_type}_eigenfunctions_iter_{iteration}.xdmf\"\n            with XDMFFile(self.mesh.comm, xdmf_file, \"w\") as file:\n                file.write_mesh(self.mesh)\n                for i, u in enumerate(self.eigenfunctions[:3]):\n                    u.name = f\"eigenmode_{i+1}\"\n                    file.write_function(u)\n        except Exception as e:\n            print(f\"Could not save XDMF for iteration {iteration}: {e}\")\n    \n    def save_final_results(self):\n        \"\"\"Save final results\"\"\"\n        if not self.eigenvalues or not self.eigenfunctions:\n            return\n            \n        # Save final eigenvalues\n        final_eigenval_file = self.results_folder / f\"{self.domain_type}_final_eigenvalues.txt\"\n        np.savetxt(final_eigenval_file, self.eigenvalues)\n        \n        # Save final eigenfunctions\n        try:\n            final_xdmf_file = self.results_folder / f\"{self.domain_type}_final_eigenfunctions.xdmf\"\n            with XDMFFile(self.mesh.comm, final_xdmf_file, \"w\") as file:\n                file.write_mesh(self.mesh)\n                for i, u in enumerate(self.eigenfunctions):\n                    u.name = f\"final_eigenmode_{i+1}\"\n                    file.write_function(u)\n        except Exception as e:\n            print(f\"Could not save final XDMF: {e}\")\n    \n    def visualize_eigenmodes(self, max_modes=4, save_html=True, save_png=True):\n        \"\"\"\n        Create beautiful visualizations of eigenmodes using PyVista\n        \"\"\"\n        if not self.eigenfunctions:\n            print(\"No eigenfunctions to visualize\")\n            return\n        \n        print(f\"\\nCreating visualizations for {len(self.eigenfunctions)} eigenmodes...\")\n        \n        for i, (eigenval, eigenfunction) in enumerate(zip(self.eigenvalues[:max_modes], \n                                                         self.eigenfunctions[:max_modes])):\n            try:\n                # Create PyVista grid\n                topology, cell_types, geometry = plot.vtk_mesh(self.V)\n                grid = pyvista.UnstructuredGrid(topology, cell_types, geometry)\n                grid.point_data[\"Eigenfunction\"] = eigenfunction.x.array.real\n                \n                # Create beautiful plot\n                plotter = pyvista.Plotter(off_screen=True, window_size=(1200, 800))\n                \n                # Add mesh with scalar field\n                mesh_actor = plotter.add_mesh(\n                    grid, \n                    scalars=\"Eigenfunction\",\n                    cmap=\"RdBu_r\",\n                    show_edges=True,\n                    scalar_bar_args={\n                        'title': f'Mode {i+1}\\n',\n                        'title_font_size': 14,\n                        'label_font_size': 13,\n                        'n_labels': 5,\n                        'width': 0.6,\n                        'height': 0.05,\n                        'position_x': 0.2,\n                        'position_y': 0.02\n                    }\n                )\n                \n                # Set camera and lighting\n                plotter.view_xy()\n                plotter.camera.zoom(1.1)\n                \n                # Add title\n                plotter.add_text(\n                    f'{self.domain_type.replace(\"_\", \" \").title()}: lambda_{i+1} = {eigenval:.6f}',\n                    position='upper_left',\n                    font_size=14,\n                    color='black'\n                )\n                \n                # Add mesh info\n                plotter.add_text(\n                    f'\\n\\nCells: {self._count_n_cells()}, '\n                    f'DOFs: {self.V.dofmap.index_map.size_global}',\n                    position='upper_left',#'lower_right',\n                    font_size=9,\n                    color='gray'\n                )\n                \n                # Save visualizations\n                if save_html:\n                    html_file = self.results_folder / f\"{self.domain_type}_mode_{i+1}.html\"\n                    plotter.export_html(html_file)\n                \n                if save_png:\n                    png_file = self.results_folder / f\"{self.domain_type}_mode_{i+1}.png\"\n                    plotter.screenshot(png_file, transparent_background=True)\n                \n                print(f\"✓ Saved visualizations for mode {i+1}\")\n                \n            except Exception as e:\n                print(f\"✗ Visualization error for mode {i+1}: {e}\")\n    \n    def create_comparison_plot(self):\n        \"\"\"Create comparison plot of all eigenmodes\"\"\"\n        if len(self.eigenfunctions) &lt; 4:\n            return\n            \n        try:\n            # Create 2x2 subplot\n            plotter = pyvista.Plotter(shape=(2, 2), off_screen=True, window_size=(1600, 1200))\n            \n            for i in range(min(4, len(self.eigenfunctions))):\n                eigenval = self.eigenvalues[i]\n                eigenfunction = self.eigenfunctions[i]\n                \n                # Create grid\n                topology, cell_types, geometry = plot.vtk_mesh(self.V)\n                grid = pyvista.UnstructuredGrid(topology, cell_types, geometry)\n                grid.point_data[\"Eigenfunction\"] = eigenfunction.x.array.real\n                \n                # Set subplot\n                row, col = divmod(i, 2)\n                plotter.subplot(row, col)\n                \n                plotter.add_mesh(\n                    grid,\n                    scalars=\"Eigenfunction\", \n                    cmap=\"RdBu_r\",\n                    show_edges=False,\n                    scalar_bar_args={\n                        'title': f'Mode {i+1}\\n',\n                        'title_font_size': 14,\n                        'label_font_size': 13,\n                    }                    \n                )\n                \n                plotter.add_text(\n                    f'lambda_{i+1} = {eigenval:.4f}', \n                    position='upper_left',\n                    font_size=14)\n                plotter.view_xy()\n                plotter.camera.zoom(0.95)\n            \n            # Save comparison\n            comparison_file = self.results_folder / f\"{self.domain_type}_comparison.png\"\n            plotter.screenshot(comparison_file)\n            \n            print(f\"✓ Saved comparison plot: {comparison_file}\")\n            \n        except Exception as e:\n            print(f\"✗ Comparison plot error: {e}\")\n    \n    def print_convergence_summary(self, history):\n        \"\"\"Print convergence summary\"\"\"\n        print(f\"\\n{'='*60}\")\n        print(f\"CONVERGENCE SUMMARY - {self.domain_type.upper()}\")\n        print(f\"{'='*60}\")\n        print(\"Iter | Cells | DOFs  | λ_1      | λ_2      | λ_3      | Error\")\n        print(\"-\" * 60)\n        \n        for h in history:\n            eigenvals = h['eigenvalues']\n            print(f\"{h['iteration']:4d} | {h['n_cells']:5d} | {h['n_dofs']:5d} | \"\n                  f\"{eigenvals[0]:8.4f} | {eigenvals[1]:8.4f} | {eigenvals[2]:8.4f} | \"\n                  f\"{h['error_max']:8.3e}\")\n        \n        if history:\n            final = history[-1]['eigenvalues']\n            print(f\"\\nFinal eigenvalues:\")\n            for i, eigenval in enumerate(final[:6]):\n                print(f\"  λ_{i+1} = {eigenval:.8f}\")\n\n\ndef run_complete_tests():\n    \"\"\"Run complete test suite with all domains\"\"\"\n    domains = [\"unit_square\", \"l_shape\", \"pacman\"]\n    \n    for domain in domains:\n        print(f\"\\n{'='*80}\")\n        print(f\"TESTING {domain.upper()} DOMAIN\")\n        print(f\"{'='*80}\")\n        \n        try:\n            # Create solver\n            solver = AdaptiveLaplaceEigenSolver(domain_type=domain, initial_mesh_size=0.1)\n            \n            # Create domain\n            solver.create_domain()\n            \n            # Run adaptive refinement\n            history = solver.adaptive_refinement(\n                err_estimator='residual', # or 'gradient'\n                n_iterations=4, \n                refinement_fraction=0.25, \n                n_eigenvalues=6\n            )\n            \n            # Create visualizations\n            print(f\"\\nCreating visualizations for {domain}...\")\n            solver.visualize_eigenmodes(max_modes=4)\n            solver.create_comparison_plot()\n            \n            print(f\"✓ {domain} test completed successfully!\")\n            \n        except Exception as e:\n            print(f\"✗ {domain} test failed: {e}\")\n            import traceback\n            traceback.print_exc()\n\nif __name__ == \"__main__\":\n    run_complete_tests()\n\n\n================================================================================\nTESTING UNIT_SQUARE DOMAIN\n================================================================================\nInitialized solver for unit_square domain\nCreated unit square: 800 cells\n\n============================================================\nADAPTIVE REFINEMENT for UNIT_SQUARE\n============================================================\n\n--- Iteration 1/4 ---\nMesh: 800 cells, DOFs: 441\nλ_1 = 19.861105\nλ_2 = 49.871661\nλ_3 = 50.168029\nRefining 200 cells (threshold: 0.003)\n\n--- Iteration 2/4 ---\nMesh: 1377 cells, DOFs: 733\nλ_1 = 19.830708\nλ_2 = 49.802740\nλ_3 = 49.881808\nRefining 344 cells (threshold: 0.002)\n\n--- Iteration 3/4 ---\nMesh: 2452 cells, DOFs: 1281\nλ_1 = 19.810581\nλ_2 = 49.696385\nλ_3 = 49.793126\nRefining 613 cells (threshold: 0.001)\n\n--- Iteration 4/4 ---\nMesh: 4385 cells, DOFs: 2268\nλ_1 = 19.770349\nλ_2 = 49.537673\nλ_3 = 49.561696\nRefining 1096 cells (threshold: 0.000)\n\n============================================================\nCONVERGENCE SUMMARY - UNIT_SQUARE\n============================================================\nIter | Cells | DOFs  | λ_1      | λ_2      | λ_3      | Error\n------------------------------------------------------------\n   1 |   800 |   441 |  19.8611 |  49.8717 |  50.1680 | 5.218e-03\n   2 |  1377 |   733 |  19.8307 |  49.8027 |  49.8818 | 3.736e-03\n   3 |  2452 |  1281 |  19.8106 |  49.6964 |  49.7931 | 2.560e-03\n   4 |  4385 |  2268 |  19.7703 |  49.5377 |  49.5617 | 1.934e-03\n\nFinal eigenvalues:\n  λ_1 = 19.77034867\n  λ_2 = 49.53767254\n  λ_3 = 49.56169589\n  λ_4 = 79.47168019\n  λ_5 = 99.49882125\n  λ_6 = 99.52280091\n\nCreating visualizations for unit_square...\n\nCreating visualizations for 6 eigenmodes...\n✓ Saved visualizations for mode 1\n✓ Saved visualizations for mode 2\n✓ Saved visualizations for mode 3\n✓ Saved visualizations for mode 4\n✓ Saved comparison plot: fenicsx/adaptive_mesh_refinement/unit_square_comparison.png\n✓ unit_square test completed successfully!\n\n================================================================================\nTESTING L_SHAPE DOMAIN\n================================================================================\nInitialized solver for l_shape domain\nInfo    : [  0%] Union                                                                                  Info    : [ 10%] Union                                                                                  Info    : [ 20%] Union                                                                                  Info    : [ 30%] Union                                                                                  Info    : [ 40%] Union                                                                                  Info    : [ 50%] Union                                                                                  Info    : [ 60%] Union                                                                                  Info    : [ 70%] Union                                                                                  Info    : [ 80%] Union - Splitting faces                                                                                                                                                                Info    : Cannot bind existing OpenCASCADE surface 1 to second tag 2\nInfo    : Could not preserve tag of 2D object 2 (-&gt;1)\nInfo    : [  0%] Union                                                                                  Info    : [ 10%] Union                                                                                  Info    : [ 20%] Union - Performing Vertex-Face intersection                                                                                Info    : [ 30%] Union                                                                                  Info    : [ 40%] Union                                                                                  Info    : [ 50%] Union                                                                                  Info    : [ 60%] Union                                                                                  Info    : [ 70%] Union - Filling splits of vertices                                                                                Info    : [ 80%] Union - Splitting faces                                                                                                                                                                Info    : Cannot bind existing OpenCASCADE surface 1 to second tag 3\nInfo    : Could not preserve tag of 2D object 3 (-&gt;1)\nInfo    : Meshing 1D...\nInfo    : [  0%] Meshing curve 10 (Line)\nInfo    : [ 20%] Meshing curve 11 (Line)\nInfo    : [ 40%] Meshing curve 12 (TrimmedCurve)\nInfo    : [ 60%] Meshing curve 13 (TrimmedCurve)\nInfo    : [ 70%] Meshing curve 14 (Line)\nInfo    : [ 90%] Meshing curve 15 (TrimmedCurve)\nInfo    : Done meshing 1D (Wall 0.000471166s, CPU 0.000811s)\nInfo    : Meshing 2D...\nInfo    : Meshing surface 1 (Plane, Frontal-Delaunay)\nInfo    : Done meshing 2D (Wall 0.00267821s, CPU 0.004491s)\nInfo    : 117 nodes 238 elements\nCreated L-shape: 192 cells\n\n============================================================\nADAPTIVE REFINEMENT for L_SHAPE\n============================================================\n\n--- Iteration 1/4 ---\nMesh: 192 cells, DOFs: 117\nλ_1 = 10.060838\nλ_2 = 15.710424\nλ_3 = 20.658319\nRefining 48 cells (threshold: 0.038)\n\n--- Iteration 2/4 ---\nMesh: 327 cells, DOFs: 187\nλ_1 = 9.981471\nλ_2 = 15.611212\nλ_3 = 20.499806\nRefining 82 cells (threshold: 0.016)\n\n--- Iteration 3/4 ---\nMesh: 580 cells, DOFs: 317\nλ_1 = 9.957306\nλ_2 = 15.576428\nλ_3 = 20.428195\nRefining 145 cells (threshold: 0.007)\n\n--- Iteration 4/4 ---\nMesh: 1069 cells, DOFs: 575\nλ_1 = 9.897279\nλ_2 = 15.461876\nλ_3 = 20.171329\nRefining 267 cells (threshold: 0.003)\n\n============================================================\nCONVERGENCE SUMMARY - L_SHAPE\n============================================================\nIter | Cells | DOFs  | λ_1      | λ_2      | λ_3      | Error\n------------------------------------------------------------\n   1 |   192 |   117 |  10.0608 |  15.7104 |  20.6583 | 8.393e-02\n   2 |   327 |   187 |   9.9815 |  15.6112 |  20.4998 | 6.263e-02\n   3 |   580 |   317 |   9.9573 |  15.5764 |  20.4282 | 4.397e-02\n   4 |  1069 |   575 |   9.8973 |  15.4619 |  20.1713 | 3.239e-02\n\nFinal eigenvalues:\n  λ_1 = 9.89727924\n  λ_2 = 15.46187553\n  λ_3 = 20.17132861\n  λ_4 = 30.51937004\n  λ_5 = 33.34623296\n\nCreating visualizations for l_shape...\n\nCreating visualizations for 5 eigenmodes...\n✓ Saved visualizations for mode 1\n✓ Saved visualizations for mode 2\n✓ Saved visualizations for mode 3\n✓ Saved visualizations for mode 4\n✓ Saved comparison plot: fenicsx/adaptive_mesh_refinement/l_shape_comparison.png\n✓ l_shape test completed successfully!\n\n================================================================================\nTESTING PACMAN DOMAIN\n================================================================================\nInitialized solver for pacman domain\nInfo    : [  0%] Difference                                                                                  Info    : [ 10%] Difference                                                                                  Info    : [ 20%] Difference                                                                                  Info    : [ 30%] Difference                                                                                  Info    : [ 40%] Difference                                                                                  Info    : [ 50%] Difference                                                                                  Info    : [ 70%] Difference - Filling splits of edges                                                                                Info    : [ 80%] Difference                                                                                  Info    : [ 90%] Difference                                                                                  Info    : Meshing 1D...\nInfo    : [  0%] Meshing curve 1 (Line)\nInfo    : [ 40%] Meshing curve 2 (Line)\nInfo    : [ 70%] Meshing curve 3 (Ellipse)\nInfo    : Done meshing 1D (Wall 0.000251417s, CPU 0.000431s)\nInfo    : Meshing 2D...\nInfo    : Meshing surface 1 (Plane, Frontal-Delaunay)\nInfo    : Done meshing 2D (Wall 0.00227921s, CPU 0.003802s)\nInfo    : 106 nodes 213 elements\nCreated Pac-Man: 173 cells\n\n============================================================\nADAPTIVE REFINEMENT for PACMAN\n============================================================\n\n--- Iteration 1/4 ---\nMesh: 173 cells, DOFs: 106\nλ_1 = 11.443015\nλ_2 = 17.443094\nλ_3 = 25.203978\nRefining 43 cells (threshold: 0.045)\n\n--- Iteration 2/4 ---\nMesh: 309 cells, DOFs: 178\nλ_1 = 11.210718\nλ_2 = 17.344920\nλ_3 = 24.855278\nRefining 77 cells (threshold: 0.018)\n\n--- Iteration 3/4 ---\nMesh: 548 cells, DOFs: 301\nλ_1 = 11.145025\nλ_2 = 17.288168\nλ_3 = 24.746407\nRefining 137 cells (threshold: 0.006)\n\n--- Iteration 4/4 ---\nMesh: 955 cells, DOFs: 507\nλ_1 = 11.075163\nλ_2 = 17.260539\nλ_3 = 24.691399\nRefining 239 cells (threshold: 0.002)\n\n============================================================\nCONVERGENCE SUMMARY - PACMAN\n============================================================\nIter | Cells | DOFs  | λ_1      | λ_2      | λ_3      | Error\n------------------------------------------------------------\n   1 |   173 |   106 |  11.4430 |  17.4431 |  25.2040 | 1.105e-01\n   2 |   309 |   178 |  11.2107 |  17.3449 |  24.8553 | 4.971e-02\n   3 |   548 |   301 |  11.1450 |  17.2882 |  24.7464 | 3.649e-02\n   4 |   955 |   507 |  11.0752 |  17.2605 |  24.6914 | 2.538e-02\n\nFinal eigenvalues:\n  λ_1 = 11.07516308\n  λ_2 = 17.26053949\n  λ_3 = 24.69139857\n  λ_4 = 33.42244310\n  λ_5 = 43.31722488\n  λ_6 = 44.00755407\n\nCreating visualizations for pacman...\n\nCreating visualizations for 6 eigenmodes...\n✓ Saved visualizations for mode 1\n✓ Saved visualizations for mode 2\n✓ Saved visualizations for mode 3\n✓ Saved visualizations for mode 4\n✓ Saved comparison plot: fenicsx/adaptive_mesh_refinement/pacman_comparison.png\n✓ pacman test completed successfully!",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>K</span>  <span class='chapter-title'>The FEniCS computing platform</span>"
    ]
  },
  {
    "objectID": "x_PDE_FEniCS.html#subdomains-and-boundary-conditions",
    "href": "x_PDE_FEniCS.html#subdomains-and-boundary-conditions",
    "title": "Appendix K — The FEniCS computing platform",
    "section": "K.8 Subdomains and boundary conditions",
    "text": "K.8 Subdomains and boundary conditions\n\nK.8.1 Combining Dirichlet and Neumann conditions\nAuthor: Jørgen S. Dokken\nLet us return to the Poisson problem and explore how to extend both the mathematical formulation and the implementation to handle a Dirichlet condition in combination with a Neumann condition. The domain is still the unit square, but this time we impose the Dirichlet condition u = u_D on the left and right boundaries, while the Neumann condition\n\\[-\\frac{\\partial u}{\\partial n} = g\\]\nis applied to the remaining boundaries, \\(y = 0\\) and \\(y = 1\\)\nThe PDE problem\nLet \\(\\Lambda_D\\) and \\(\\Lambda_N\\) denote the portions of the boundary \\(\\partial \\Omega\\) where the Dirichlet and Neumann conditions are prescribed, respectively The full boundary-value problem is then given by\n\\[\n\\begin{aligned}\n-\\nabla^2 u &= f \\quad \\text{in } \\Omega \\\\\nu &= u_D \\quad \\text{on } \\Lambda_D \\\\\n-\\frac{\\partial u}{\\partial n} &= g \\quad \\text{on } \\Lambda_N\n\\end{aligned}\n\\]\nAs before, we choose \\(u = 1 + x^2 + 2y^2\\) as the exact solution and then set \\(f\\), \\(g\\), and \\(u_D\\) to match this choice\n\\[\n\\begin{aligned}\n    f(x,y) &=-6 \\\\\n    g(x,y) &=\n    \\begin{cases}\n        \\phantom{-}0, & y=0\\\\\n        -4, & y=1\n    \\end{cases}\\\\\n    u_D(x,y) &=1+x^2+2y^2\n\\end{aligned}\n\\]\nTo simplify the implementation, we define \\(g\\) as a function over the whole domain , making sure it has the correct values at \\(y=0\\) and \\(y=1\\). One possible choice is\n\\[\ng(x,y)=-4y\n\\]\nThe variational formulation\nThe first step is to derive the variational formulation. In this case, the boundary term resulting from integration by parts cannot be omitted, since \\(v\\) vanishes only on \\(\\Lambda_D\\). We obtain\n\\[-\\int_\\Omega (\\nabla^2 u)\\, v \\,\\mathrm{d}x \\;=\\; \\int_\\Omega \\nabla u \\cdot \\nabla v \\,\\mathrm{d}x \\;-\\; \\int_{\\partial \\Omega}\\frac{\\partial u}{\\partial n} v \\,\\mathrm{d}s\\]\nand because \\(v=0\\) on \\(\\Lambda_D\\), it follows that\n\\[-\\int_{\\partial \\Omega}\\frac{\\partial u}{\\partial n} v \\,\\mathrm{d}s \\;=\\; - \\int_{\\Lambda_N}\\frac{\\partial u}{\\partial n} v \\,\\mathrm{d}s \\;=\\; \\int_{\\Lambda_N} g v \\,\\mathrm{d}s\\]\nby applying the boundary condition on \\(\\Lambda_N\\). The resulting weak form is therefore\n\\[\\int_\\Omega \\nabla u \\cdot \\nabla v \\,\\mathrm{d}x \\;=\\; \\int_\\Omega f v \\,\\mathrm{d}x \\;-\\; \\int_{\\Lambda_N} g v \\,\\mathrm{d}s\\]\nExpressing this equation in the standard notation \\(a(u,v) = L(v)\\), we have\n\\[\\begin{aligned}\na(u,v) &= \\int_\\Omega \\nabla u \\cdot \\nabla v \\,\\mathrm{d}x \\\\\nL(v) &= \\int_\\Omega f v \\,\\mathrm{d}x \\;-\\; \\int_{\\Lambda_N} g v \\,\\mathrm{d}s\n\\end{aligned}\\]\nImplementation\nAs in the previous example, we start by defining the mesh, the function space, and the bilinear form \\(a(u,v)\\)\n\nimport numpy as np\n\nfrom mpi4py import MPI\nimport pyvista\n\nfrom dolfinx import default_scalar_type\nfrom dolfinx.fem import (Constant, Function, functionspace,\n  assemble_scalar, dirichletbc, form, locate_dofs_geometrical)\nfrom dolfinx.fem.petsc import LinearProblem\nfrom dolfinx.mesh import create_unit_square\nfrom dolfinx.plot import vtk_mesh\n\nfrom ufl import SpatialCoordinate, TestFunction, TrialFunction, dot, ds, dx, grad\n\nmesh = create_unit_square(MPI.COMM_WORLD, 10, 10)\n\nV = functionspace(mesh, (\"Lagrange\", 1))\nu = TrialFunction(V)\nv = TestFunction(V)\n\na = dot(grad(u), grad(v)) *dx\n\nNow we turn to the Neumann and Dirichlet boundary conditions. As before, we use a Python function to mark the part of the boundary where the Dirichlet condition should apply. With this function, we can then identify the corresponding degrees of freedom that satisfy the condition\n\ndef u_exact(x):\n  return 1 +x[0]**2 +2 *x[1]**2\n\ndef boundary_D(x):\n  return np.logical_or(np.isclose(x[0], 0), np.isclose(x[0], 1))\n\ndofs_D = locate_dofs_geometrical(V, boundary_D)\n\nu_bc = Function(V)\nu_bc.interpolate(u_exact)\nbc = dirichletbc(u_bc, dofs_D)\n\nThe next step is to define the Neumann condition. We begin by defining \\(g\\) using UFL’s SpatialCoordinate function, and then create a boundary integration measure ds. Since the test function \\(v\\) vanishes on the Dirichlet boundary, the corresponding integrals drop out. This allows us to simply integrate g *v *ds over the entire boundary\n\nf = Constant(mesh, default_scalar_type(-6))\n\nx = SpatialCoordinate(mesh)\ng = -4 *x[1]\n\nL = f *v *dx -g *v *ds\n\nAt this stage, we are ready to assemble the linear system and solve it\n\nproblem = LinearProblem(\n    a, \n    L, \n    bcs=[bc], \n    petsc_options={\n        \"ksp_type\": \"preonly\", \n        \"pc_type\": \"lu\"}\n)\n\nuh = problem.solve()\n\nV2 = functionspace(mesh, (\"Lagrange\", 2))\nuex = Function(V2)\nuex.interpolate(u_exact)\n\nerror_L2 = assemble_scalar(form((uh -uex)**2 *dx))\nerror_L2 = np.sqrt(MPI.COMM_WORLD.allreduce(error_L2, op=MPI.SUM))\n\nu_vertex_values = uh.x.array\n\nuex_1 = Function(V)\nuex_1.interpolate(uex)\nu_ex_vertex_values = uex_1.x.array\n\nerror_max = np.max(np.abs(u_vertex_values -u_ex_vertex_values))\nerror_max = MPI.COMM_WORLD.allreduce(error_max, op=MPI.MAX)\n\nprint(f\"Error_L2 : {error_L2:.2e}\")\nprint(f\"Error_max : {error_max:.2e}\")\n\nError_L2 : 5.27e-03\nError_max : 1.78e-15\n\n\nVisualization\nTo visualize the solution, run the code either as a Python script with off_screen=True, or inside a Jupyter notebook with off_screen=False\n\nfrom pathlib import Path\n\nresults_folder = Path(\"fenicsx/bcs_subdomains\")\nresults_folder.mkdir(exist_ok=True, parents=True)\n\npyvista_cells, cell_types, geometry = vtk_mesh(V)\ngrid = pyvista.UnstructuredGrid(pyvista_cells, cell_types, geometry)\ngrid.point_data[\"u\"] = uh.x.array\ngrid.set_active_scalars(\"u\")\n\nplotter = pyvista.Plotter(off_screen=False)\nplotter.add_text(\"uh\", position=\"upper_edge\", font_size=14, color=\"black\")\nplotter.add_mesh(grid, show_edges=True)\nplotter.view_xy()\n\nif not pyvista.OFF_SCREEN:\n    #plotter.show()\n    plotter.export_html(results_folder/\"neumann_dirichlet.html\")\nelse:\n    figure = plotter.screenshot(results_folder/\"neumann_dirichlet.png\")\n\n        \n\n\n\nK.8.2 Setting multiple Dirichlet condition\nIn the previous section, we applied the same Dirichlet condition to both the left and right boundaries using a single function. While this works, it is often more flexible to define separate boundary conditions for each side.\nLet us consider a similar setup to the earlier example, but this time with distinct Dirichlet conditions on the left and right boundaries:\n\\[\n\\begin{aligned}\n-\\nabla^2 u &= f \\quad &&\\text{in } \\Omega \\\\\nu &= u_L \\quad &&\\text{on } \\Lambda_D^L \\\\\nu &= u_R \\quad &&\\text{on } \\Lambda_D^R \\\\\n-\\frac{\\partial u}{\\partial n} &= g \\quad &&\\text{on } \\Lambda_N\n\\end{aligned}\n\\]\nHere, \\(\\Lambda_D^L\\) represents the left boundary (\\(x=0\\)), and \\(\\Lambda_D^R\\) represents the right boundary (\\(x=1\\))\nFor this example, we choose\n\n\\(u_L(y) = 1 + 2y^2\\)\n\\(u_R(y) = 2 + 2y^2\\)\n\\(g(y) = -4y\\)\n\nin line with the analytical solution introduced earlier\nAs before, we begin by defining the mesh, the function space, and the variational formulation\n\nimport numpy as np\nfrom mpi4py import MPI\nimport pyvista\n\nfrom dolfinx import default_scalar_type\nfrom dolfinx.fem import (\n  Constant, Function, functionspace,\n  assemble_scalar, dirichletbc, \n  form, locate_dofs_geometrical\n)\nfrom dolfinx.fem.petsc import LinearProblem\nfrom dolfinx.mesh import create_unit_square\nfrom dolfinx.plot import vtk_mesh\n\nfrom ufl import (\n  SpatialCoordinate, \n  TrialFunction, TestFunction, \n  dot, dx, ds, grad\n)\n\ndef u_exact(x):\n  return 1 +x[0]**2 +2 *x[1]**2\n\nmesh = create_unit_square(MPI.COMM_WORLD, 10, 10)\n\nV = functionspace(mesh, (\"Lagrange\", 1))\nu = TrialFunction(V)\nv = TestFunction(V)\n\na = dot(grad(u), grad(v)) *dx\n\nx = SpatialCoordinate(mesh)\nf = Constant(mesh, default_scalar_type(-6))\ng = - 4 *x[1]\nL = f *v *dx -g *v *ds\n\nOur next step is to mark the two boundaries individually, beginning with the left boundary\n\ndofs_L = locate_dofs_geometrical(V, lambda x: np.isclose(x[0], 0))\nu_L = Function(V)\nu_L.interpolate(lambda x: 1 +2 *x[1]**2)\nbc_L = dirichletbc(u_L, dofs_L)\n\nNote that we have used lambda-functions to compactly define the functions returning the subdomain evaluation and function evaluation. We can use a similar procedure for the right boundary condition, and gather both boundary conditions in a vector bcs\n\ndofs_R = locate_dofs_geometrical(V, lambda x: np.isclose(x[0], 1))\nu_R = Function(V)\nu_R.interpolate(lambda x: 2 +2 *x[1]**2)\nbc_R = dirichletbc(u_R, dofs_R)\nbcs = [bc_R, bc_L]\n\nWe are now ready to solve the problem once more and evaluate both the \\(L^2\\) error and the maximum error at the mesh vertices\n\nproblem = LinearProblem(\n  a, \n  L, \n  bcs=bcs, \n  petsc_options={\"ksp_type\": \"preonly\", \"pc_type\": \"lu\"}\n)\nuh = problem.solve()\n\nV2 = functionspace(mesh, (\"Lagrange\", 2))\nuex = Function(V2)\nuex.interpolate(u_exact)\n\nerror_L2 = assemble_scalar(form((uh -uex)**2 *dx))\nerror_L2 = np.sqrt(MPI.COMM_WORLD.allreduce(error_L2, op=MPI.SUM))\n\nu_vertex_values = uh.x.array\nuex_1 = Function(V)\nuex_1.interpolate(uex)\nu_ex_vertex_values = uex_1.x.array\n\nerror_max = np.max(np.abs(u_vertex_values -u_ex_vertex_values))\nerror_max = MPI.COMM_WORLD.allreduce(error_max, op=MPI.MAX)\n\nprint(f\"Error_L2 : {error_L2:.2e}\")\nprint(f\"Error_max : {error_max:.2e}\")\n\nError_L2 : 5.27e-03\nError_max : 2.22e-15\n\n\nVisualization\nTo visualize the solution, run the code either as a Python script with off_screen=True, or inside a Jupyter notebook with off_screen=False\n\nfrom pathlib import Path\n\nresults_folder = Path(\"fenicsx/bcs_subdomains\")\nresults_folder.mkdir(exist_ok=True, parents=True)\n\npyvista_cells, cell_types, geometry = vtk_mesh(V)\ngrid = pyvista.UnstructuredGrid(pyvista_cells, cell_types, geometry)\ngrid.point_data[\"u\"] = uh.x.array\ngrid.set_active_scalars(\"u\")\n\nplotter = pyvista.Plotter(off_screen=False)\nplotter.add_text(\"uh\", position=\"upper_edge\", font_size=14, color=\"black\")\nplotter.add_mesh(grid, show_edges=True)\nplotter.view_xy()\n\nif not pyvista.OFF_SCREEN:\n    #plotter.show()\n    plotter.export_html(results_folder/\"multiple_dirichlet.html\")\nelse:\n    figure = plotter.screenshot(results_folder/\"multiple_dirichlet.png\")\n\n        \n\n\n\nK.8.3 Defining subdomains for different materials\nAuthor: Jørgen S. Dokken\nMany PDE problems involve domains consisting of different materials. In FEniCSx, these cases can be treated by introducing a discontinuous, cell-wise constant function. We can create such a function on any mesh in the following way\nSubdomains on built-in meshes\n\nimport numpy as np\nfrom mpi4py import MPI\nimport pyvista\n\nimport gmsh\n\nfrom dolfinx import default_scalar_type\nfrom dolfinx.mesh import create_unit_square, locate_entities\nfrom dolfinx.fem import (\n  Constant, dirichletbc, Function, functionspace, \n  assemble_scalar, form, \n  locate_dofs_geometrical, locate_dofs_topological\n)\nfrom dolfinx.fem.petsc import LinearProblem\nfrom dolfinx.io import XDMFFile, gmshio\nfrom dolfinx.plot import vtk_mesh\n\nfrom ufl import (\n  SpatialCoordinate, TestFunction, TrialFunction,\n  dx, grad, inner\n)\n\nmesh = create_unit_square(MPI.COMM_WORLD, 10, 10)\nQ = functionspace(mesh, (\"DG\", 0))\n\nTo illustrate the concept, let’s consider a simple two-dimensional example with two materials. The domain \\(\\Omega=[0,1]\\times[0,1]\\) is split into two subdomains, \\(\\Omega_1=[0,1]\\times [0,1/2]\\) and \\(\\Omega_2=[0,1]\\times[1/2, 1]\\). We start by defining two Python functions, where each function returns True whenever the given coordinate falls inside its corresponding subdomain\n\ndef Omega_1(x):\n  return x[1] &lt;= 0.5\n\ndef Omega_2(x):\n  return x[1] &gt;= 0.5\n\nNotice that both functions make use of \\(\\leq\\) and \\(\\geq\\). This is because FEniCSx evaluates each cell at all of its vertices, and we need every vertex on the interface to return True in order for the interface to be marked correctly.\nWith this in place, we now move on to a variable-coefficient version of the Poisson equation:\n\\[\\begin{aligned}\n-\\nabla \\cdot &[\\kappa(x,y)\\,\\nabla u(x, y)] = 1 &&\\text{in } \\Omega \\\\[5pt]\nu &= u_D = 1  &&\\text{on } \\partial\\Omega_D = {(0,y),|, y \\in [0,1]} \\\\[5pt]\n-\\frac{\\partial u}{\\partial n} &= 0 &&\\text{on } \\partial\\Omega \\setminus \\partial\\Omega_D\n\\end{aligned}\n\\]\nThe next step is to define the coefficient \\(\\kappa\\)\n\nkappa = Function(Q)\ncells_1 = locate_entities(mesh, mesh.topology.dim, Omega_1)\ncells_2 = locate_entities(mesh, mesh.topology.dim, Omega_2)\n\nIn the previous code block, we determined which cells (triangular elements) belong to \\(\\Omega_1\\) and \\(\\Omega_2\\). Since a DG-0 function has a single degree of freedom per cell, there is a one-to-one correspondence between the degrees of freedom and the cells. We then define the coefficient \\(\\kappa\\) by\n\\[\n\\kappa =\n\\begin{cases}\n\\phantom{.}1 & \\text{if } x \\in \\Omega_1 \\\\\n0.1 & \\text{if } x \\in \\Omega_2\n\\end{cases}\\]\n\nkappa.x.array[cells_1] = np.full_like(cells_1, 1, dtype=default_scalar_type)\nkappa.x.array[cells_2] = np.full_like(cells_2, 0.1, dtype=default_scalar_type)\n\n\n# Filter out ghosted cells\ntdim = mesh.topology.dim\nnum_cells_local = mesh.topology.index_map(tdim).size_local\nmarker = np.zeros(num_cells_local, dtype=np.int32)\n\ncells_1 = cells_1[cells_1 &lt; num_cells_local]\ncells_2 = cells_2[cells_2 &lt; num_cells_local]\n\nmarker[cells_1] = 1\nmarker[cells_2] = 2\n\nmesh.topology.create_connectivity(tdim, tdim)\ntopology, cell_types, x = vtk_mesh(mesh, tdim, np.arange(num_cells_local, dtype=np.int32))\n\n\nfrom pathlib import Path\n\nresults_folder = Path(\"fenicsx/bcs_subdomains\")\nresults_folder.mkdir(exist_ok=True, parents=True)\n\nplotter = pyvista.Plotter(off_screen=False, window_size=[800, 800])\n\ngrid = pyvista.UnstructuredGrid(topology, cell_types, x)\ngrid.cell_data[\"Marker\"] = marker\ngrid.set_active_scalars(\"Marker\")\nplotter.add_mesh(grid, show_edges=True)\n\nif not pyvista.OFF_SCREEN:\n    #plotter.show()\n    plotter.export_html(results_folder/\"subdomains_structured.html\")\nelse:\n    figure = plotter.screenshot(results_folder/\"subdomains_structured.png\")\n\n        \n\nAfter performing integration by parts, we are now ready to define the variational formulation and the Dirichlet boundary condition\n\nV = functionspace(mesh, (\"Lagrange\", 1))\nu = TrialFunction(V)\nv = TestFunction(V)\n\na = inner(kappa *grad(u), grad(v)) *dx\n\nx = SpatialCoordinate(mesh)\nL = Constant(mesh, default_scalar_type(1)) *v *dx\n\ndofs = locate_dofs_geometrical(V, lambda x: np.isclose(x[0], 0))\nbcs = [dirichletbc(default_scalar_type(1), dofs, V)]\n\nWe can now solve the problem and visualize its solution\n\nproblem = LinearProblem(\n  a, \n  L, \n  bcs=bcs, \n  petsc_options={\"ksp_type\": \"preonly\", \"pc_type\": \"lu\"}\n)\nuh = problem.solve()\n\n\nplotter = pyvista.Plotter(off_screen=False, window_size=[800, 800])\ngrid_uh = pyvista.UnstructuredGrid(*vtk_mesh(V))\ngrid_uh.point_data[\"u\"] = uh.x.array.real\ngrid_uh.set_active_scalars(\"u\")\n\nplotter.add_mesh(grid_uh, show_edges=True)\nif not pyvista.OFF_SCREEN:\n  #plotter.show()\n  plotter.export_html(results_folder/\"subdomains_structured2.html\")\nelse:\n  figure = plotter.screenshot(results_folder/\"subdomains_structured2.png\")\n\n        \n\nDistinct behaviors are clearly observed in the two regions, despite both having the same Dirichlet boundary condition at x=0 on the left boundary\nInterpolation with Python-function\nAs we saw in the first approach, in many cases the geometrical coordinates can be used to determine which coefficient to apply. Using the unstructured mesh from the previous example, we illustrate an alternative approach based on interpolation\n\ndef eval_kappa(x):\n  values = np.zeros(x.shape[1], dtype=default_scalar_type)\n  \n  # Create a boolean array indicating which dofs \n  #   that are in each domain\n  top_coords = x[1] &gt; 0.5\n  bottom_coords = x[1] &lt; 0.5\n\n  values[top_coords] = np.full(sum(top_coords), 0.1)\n  values[bottom_coords] = np.full(sum(bottom_coords), 1)\n\n  return values\n\n\nkappa2 = Function(Q)\nkappa2.interpolate(eval_kappa)\n\nWe verify this by computing the error between the new function and the previous one\n\n# Difference in kappa's\nerror = mesh.comm.allreduce(assemble_scalar(form((kappa -kappa2)**2 *dx)))\nprint(f'{error = }')\n\nerror = 0.0\n\n\nSubdomains defined from external mesh data\nLet us now consider the same problem, but using GMSH to generate both the mesh and the subdomains. We will then show how to use this data to create discontinuous functions in dolfinx\n\ngmsh.initialize()\n\nproc = MPI.COMM_WORLD.rank\n\ntop_marker = 2\nbottom_marker = 1\nleft_marker = 1\n\nif proc == 0:\n  # We create one rectangle for each subdomain\n  gmsh.model.occ.addRectangle(0, 0, 0, 1, 0.5, tag=1)\n  gmsh.model.occ.addRectangle(0, 0.5, 0, 1, 0.5, tag=2)\n  # We fuse the two rectangles and keep the interface between them\n  gmsh.model.occ.fragment([(2, 1)], [(2, 2)])\n  gmsh.model.occ.synchronize()\n\n  # Mark the top (2) and bottom (1) rectangle\n  top, bottom = None, None\n  for surface in gmsh.model.getEntities(dim=2):\n    com = gmsh.model.occ.getCenterOfMass(surface[0], surface[1])\n    if np.allclose(com, [0.5, 0.25, 0]):\n      bottom = surface[1]\n    else:\n      top = surface[1]\n  gmsh.model.addPhysicalGroup(2, [bottom], bottom_marker)\n  gmsh.model.addPhysicalGroup(2, [top], top_marker)\n  \n  # Tag the left boundary\n  left = []\n  for line in gmsh.model.getEntities(dim=1):\n    com = gmsh.model.occ.getCenterOfMass(line[0], line[1])\n    if np.isclose(com[0], 0):\n      left.append(line[1])\n  gmsh.model.addPhysicalGroup(1, left, left_marker)\n  \n  gmsh.model.mesh.generate(2)\n  gmsh.write(str(results_folder/\"gmsh_mesh.msh\"))\n\ngmsh.finalize()\n\nInfo    : [  0%] Fragments                                                                                  Info    : [ 10%] Fragments                                                                                  Info    : [ 20%] Fragments                                                                                  Info    : [ 30%] Fragments                                                                                  Info    : [ 40%] Fragments                                                                                  Info    : [ 50%] Fragments                                                                                  Info    : [ 60%] Fragments                                                                                  Info    : [ 70%] Fragments                                                                                  Info    : [ 80%] Fragments - Splitting faces                                                                                                                                                                Info    : Meshing 1D...\nInfo    : [  0%] Meshing curve 1 (Line)\nInfo    : [ 20%] Meshing curve 2 (Line)\nInfo    : [ 30%] Meshing curve 3 (Line)\nInfo    : [ 50%] Meshing curve 4 (Line)\nInfo    : [ 60%] Meshing curve 5 (Line)\nInfo    : [ 80%] Meshing curve 6 (Line)\nInfo    : [ 90%] Meshing curve 7 (Line)\nInfo    : Done meshing 1D (Wall 0.00056525s, CPU 0.001001s)\nInfo    : Meshing 2D...\nInfo    : [  0%] Meshing surface 1 (Plane, Frontal-Delaunay)\nInfo    : [ 60%] Meshing surface 2 (Plane, Frontal-Delaunay)\nInfo    : Done meshing 2D (Wall 0.00340479s, CPU 0.005623s)\nInfo    : 101 nodes 214 elements\nInfo    : Writing 'fenicsx/bcs_subdomains/gmsh_mesh.msh'...\nInfo    : Done writing 'fenicsx/bcs_subdomains/gmsh_mesh.msh'\n\n\nRead in MSH files with dolfinx\nYou can read MSH files with dolfinx, which initially loads them on a single process and then distributes them across the available ranks in the MPI communicator\n\nmesh, cell_markers, facet_markers = gmshio.read_from_msh(\n  results_folder/\"gmsh_mesh.msh\", \n  MPI.COMM_WORLD, \n  gdim=2\n)\n\nInfo    : Reading 'fenicsx/bcs_subdomains/gmsh_mesh.msh'...\nInfo    : 15 entities\nInfo    : 101 nodes\nInfo    : 176 elements\nInfo    : Done reading 'fenicsx/bcs_subdomains/gmsh_mesh.msh'\n\n\nConvert msh-files to XDMF using meshio\nWe will use meshio to read in the msh file, and convert it to a more suitable IO format. e start by creating a convenience function for extracting data for a single cell type, and creating a new meshio.Mesh\n\nimport meshio\n\ndef create_mesh(mesh, cell_type, prune_z=False):\n  \n  points = mesh.points[:, :2] if prune_z else mesh.points\n  cells = mesh.get_cells_type(cell_type)\n  cell_data = mesh.get_cell_data(\"gmsh:physical\", cell_type)\n\n  out_mesh = meshio.Mesh(\n    points=points, \n    cells={cell_type: cells}, \n    cell_data={\"name_to_read\": [cell_data.astype(np.int32)]}\n  )\n  \n  return out_mesh\n\nThis function returns a meshio mesh, including physical markers for the specified type. The prune_z argument is used when working with two-dimensional meshes. The last coordinate in the mesh (since it is generated in 3D space) must be removed for dolfinx to recognize it as a two-dimensional geometry\n\nif proc == 0:\n  # Read in mesh\n  msh = meshio.read(results_folder/\"gmsh_mesh.msh\")\n\n  # Create and save one file for the mesh, and one file for the facets\n  triangle_mesh = create_mesh(msh, \"triangle\", prune_z=True)\n  line_mesh = create_mesh(msh, \"line\", prune_z=True)\n  meshio.write(results_folder/\"mesh.xdmf\", triangle_mesh)\n  meshio.write(results_folder/\"mt.xdmf\", line_mesh)\n\nMPI.COMM_WORLD.barrier()\n\n\n\n\nWe have written the mesh and cell markers to one file, and the facet markers to a separate file. This data can be read in dolfinx using XDMFFile.read_mesh and XDMFFile.read_meshtags. A dolfinx.MeshTags object stores the entity indices and their corresponding marker values in two one-dimensional arrays.\nNote that the mesh was generated and written using a single processor. However, since the XDMF format supports parallel I/O, the mesh can be read in parallel\n\nwith XDMFFile(\n  MPI.COMM_WORLD, \n  results_folder/\"mesh.xdmf\", \"r\"\n) as xdmf:\n  mesh = xdmf.read_mesh(name=\"Grid\")\n  ct = xdmf.read_meshtags(mesh, name=\"Grid\")\n\nmesh.topology.create_connectivity(mesh.topology.dim, mesh.topology.dim -1)\n\nwith XDMFFile(\n  MPI.COMM_WORLD, \n  results_folder/\"mt.xdmf\", \"r\"\n) as xdmf:\n  ft = xdmf.read_meshtags(mesh, name=\"Grid\")\n\nHaving read the mesh together with the associated cell and facet data, we can now construct the discontinuous function kappa as follows:\n\nQ = functionspace(mesh, (\"DG\", 0))\nkappa = Function(Q)\n\nbottom_cells = ct.find(bottom_marker)\nkappa.x.array[bottom_cells] = np.full_like(\n  bottom_cells, \n  1, \n  dtype=default_scalar_type\n)\n\ntop_cells = ct.find(top_marker)\nkappa.x.array[top_cells] = np.full_like(\n  top_cells, \n  0.1, \n  dtype=default_scalar_type\n)\n\nThe facet data ft can also be used efficiently to construct the Dirichlet boundary condition.\n\nV = functionspace(mesh, (\"Lagrange\", 1))\nu_bc = Function(V)\nleft_facets = ft.find(left_marker)\n\nmesh.topology.create_connectivity(mesh.topology.dim -1, mesh.topology.dim)\nleft_dofs = locate_dofs_topological(V, mesh.topology.dim -1, left_facets)\n\nbcs = [dirichletbc(default_scalar_type(1), left_dofs, V)]\n\nWe can now solve the problem in a fashion similar to that used above\n\nu = TrialFunction(V)\nv = TestFunction(V)\n\na = inner(kappa *grad(u), grad(v)) *dx\n\nx = SpatialCoordinate(mesh)\nL = Constant(mesh, default_scalar_type(1)) *v *dx\n\nproblem = LinearProblem(\n  a, \n  L, \n  bcs=bcs, \n  petsc_options={\"ksp_type\": \"preonly\", \"pc_type\": \"lu\"}\n)\nuh = problem.solve()\n\n\n# Since dolfinx.MeshTag contains a value for every cell in the geometry, \n# we can attach it directly to the grid\n\ntdim = mesh.topology.dim\nmesh.topology.create_connectivity(tdim, tdim)\n\ntopology, cell_types, x = vtk_mesh(mesh, tdim)\ngrid = pyvista.UnstructuredGrid(topology, cell_types, x)\n\nnum_local_cells = mesh.topology.index_map(tdim).size_local\ngrid.cell_data[\"Marker\"] = ct.values[ct.indices &lt; num_local_cells]\ngrid.set_active_scalars(\"Marker\")\n\nplotter = pyvista.Plotter(off_screen=False, window_size=[800, 800])\nplotter.add_mesh(grid, show_edges=True)\n\nif not pyvista.OFF_SCREEN:\n  #plotter.show()\n  plotter.export_html(results_folder/\"subdomains_unstructured.html\")\nelse:\n  figure = plotter.screenshot(results_folder/\"subdomains_unstructured.png\")\n\n        \n\n\ngrid_uh = pyvista.UnstructuredGrid(*vtk_mesh(V))\ngrid_uh.point_data[\"u\"] = uh.x.array.real\ngrid_uh.set_active_scalars(\"u\")\n\nplotter = pyvista.Plotter(off_screen=False, window_size=[800, 800])\nplotter.add_mesh(grid_uh, show_edges=True)\n\nif not pyvista.OFF_SCREEN:\n  #plotter.show()\n  plotter.export_html(results_folder/\"subdomains_unstructured_u.html\")\nelse:\n  figure = plotter.screenshot(results_folder/\"subdomains_unstructured_u.png\")\n\n        \n\n\n\nK.8.4 Setting multiple Dirichlet, Neumann, and Robin conditions\nAuthor: Hans Petter Langtangen and Anders Logg\nWe now look at the variable-coefficient example from the previous section. In this part, we will show how to apply a combination of Dirichlet, Neumann, and Robin boundary conditions to this problem\nWe split the boundary into three distinct parts:\n\n\\(\\Gamma_D\\) for Dirichlet conditions:\n\\(\\phantom{-\\kappa\\partial}u=u_D^i \\quad\\text{on } \\Gamma_D^i \\quad\\) where \\(\\;\\Gamma_D=\\Gamma_D^0\\cup \\Gamma_D^1 \\cup \\dots\\)\n\\(\\Gamma_N\\) for Neumann conditions:\n\\(\\displaystyle -\\kappa \\frac{\\partial u}{\\partial n}=g_j \\quad\\text{on } \\Gamma_N^j \\quad\\) where \\(\\;\\Gamma_N=\\Gamma_N^0\\cup \\Gamma_N^1 \\cup \\dots\\)\n\\(\\Gamma_R\\) for Robin conditions:\n\\(\\displaystyle -\\kappa \\frac{\\partial u}{\\partial n}=r_k (u -s_k) \\quad\\text{on } \\Gamma_R^k \\quad\\) where \\(\\;\\Gamma_R=\\Gamma_R^0\\cup \\Gamma_R^1 \\cup \\dots\\)\n\nwhere \\(r_k\\) and \\(s_k\\) are prescribed functions. The Robin condition is commonly used to model heat transfer to the surroundings and arises naturally from Newton’s law of cooling. In this case, \\(r_k\\) represents the heat transfer coefficient, and \\(s_k\\) is the ambient temperature. Both may depend on space and time\nThe PDE problem and variational formulation\nWe can summarize the PDE problem as follows:\n\\[\\begin{aligned}\n-\\nabla &\\cdot (\\kappa \\nabla u) = f &&\\text{in } \\Omega\\\\[6pt]\nu &= u_D^i \\quad &&\\text{on } \\Gamma_D^i \\\\[5pt]\n-\\kappa \\frac{\\partial u}{\\partial n} &= g_j &&\\text{on } \\Gamma_N^j\\\\\n-\\kappa \\frac{\\partial u}{\\partial n} &= r_k (u - s_k) &&\\text{on } \\Gamma_R^k\n\\end{aligned}\\]\nAs usual, we multiply the equation by a test function \\(v\\) and integrate by parts:\n\\[-\\int_{\\Omega} \\nabla \\cdot (\\kappa \\nabla u)\\, v \\,\\mathrm{d}x\n= \\int_{\\Omega} \\kappa \\nabla u \\cdot \\nabla v \\,\\mathrm{d}x\n    -   \\int_{\\partial \\Omega} \\kappa \\frac{\\partial u}{\\partial n} v \\,\\mathrm{d}s\\]\nOn the Dirichlet part (\\(\\Gamma_D^i\\)), the boundary integral vanishes since \\(v = 0\\). On the remaining parts of the boundary, we split the contributions into the Neumann boundaries (\\(\\Gamma_N^i\\)) and the Robin boundaries (\\(\\Gamma_R^i\\)). Inserting the boundary conditions, we obtain\n\\[-\\int_{\\partial\\Omega} \\kappa \\frac{\\partial u}{\\partial n} v \\,\\mathrm{d}s\n= \\sum_i \\int_{\\Gamma_N^i} g_i v \\,\\mathrm{d}s\n    +   \\sum_i \\int_{\\Gamma_R^i} r_i (u - s_i) v \\,\\mathrm{d}s\\]\nThus, the variational problem can be written as\n\\[\\begin{aligned}\nF(u, v)\n&= \\int_\\Omega \\kappa \\nabla u \\cdot \\nabla v \\,\\mathrm{d}x\n    +   \\sum_i \\int_{\\Gamma_N^i} g_i v \\,\\mathrm{d}s \\\\\n&\\quad + \\sum_i \\int_{\\Gamma_R^i} r_i (u - s_i) v \\,\\mathrm{d}s\n    -   \\int_\\Omega f v \\,\\mathrm{d}x = 0\n\\end{aligned}\\]\nWe are accustomed to writing the variational formulation in the form \\(a(u, v) = L(v)\\). This requires identifying the integrals that depend on the trial function \\(u\\) and collecting them in \\(a(u, v)\\), while the remaining terms form \\(L(v)\\). Note that the Robin condition contributes to both \\(a(u, v)\\) and \\(L(v)\\)\nThus, we have\n\\[a(u, v) = \\int_{\\Omega} \\kappa \\nabla u \\cdot \\nabla v \\,\\mathrm{d}x\n    +   \\sum_i \\int_{\\Gamma_R^i} r_i \\, u \\, v \\,\\mathrm{d}s\\]\n\\[L(v) = \\int_{\\Omega} f \\, v \\,\\mathrm{d}x\n    -\\sum_i \\int_{\\Gamma_N^i} g_i \\, v \\,\\mathrm{d}s\n    +\\sum_i \\int_{\\Gamma_R^i} r_i \\, s_i \\, v \\,\\mathrm{d}s\\]\nImplementation\nFirst, we define the domain to be the unit square \\([0,1] \\times [0,1]\\)\n\nimport numpy as np\nfrom mpi4py import MPI\nimport pyvista\n\nfrom dolfinx import default_scalar_type\nfrom dolfinx.mesh import create_unit_square, locate_entities, meshtags\nfrom dolfinx.fem import (\n  Constant, Function, functionspace, assemble_scalar, \n  dirichletbc, form, locate_dofs_topological\n)\nfrom dolfinx.fem.petsc import LinearProblem\nfrom dolfinx.io import XDMFFile\nfrom dolfinx.plot import vtk_mesh\n\nfrom ufl import (\n  FacetNormal, Measure, SpatialCoordinate, \n  TestFunction, TrialFunction, \n  div, dot, dx, grad, inner, lhs, rhs\n)\n\nmesh = create_unit_square(MPI.COMM_WORLD, 10, 10)\n\nIn this section, we solve the Poisson problem for the manufactured solution\n\\[u_{ex} = 1 + x^2 + 2y^2\\]\nwhich gives \\(\\kappa = 1\\) and \\(f = -6\\). The next step is to define the boundary condition parameters and specify where to apply them. In this example, we apply the following:\n\\[\n\\begin{aligned}\n\\phantom{-\\kappa} u &= u_D &&\\quad \\text{for } x=0,1\\\\[5pt]\n-\\kappa \\frac{\\partial u}{\\partial n} &= r(u - s) &&\\quad \\text{for } y=0\\\\\n-\\kappa \\frac{\\partial u}{\\partial n} &= g_0 &&\\quad \\text{for } y=1\n\\end{aligned}\\]\nTo reproduce the analytical solution, we set\n\\[\n\\begin{aligned}\nu_D &= u_{ex} = 1 + x^2 + 2y^2 \\\\\ng_0 &= -\\left.\\frac{\\partial u_{ex}}{\\partial y}\\right\\vert_{y=1} = -4y \\big\\vert_{y=1} = -4\n\\end{aligned}\\]\nThe Robin condition can be specified in many ways. Since\n\\[-\\left.\\frac{\\partial u_{ex}}{\\partial n}\\right\\vert_{y=0} = \\left.\\frac{\\partial u_{ex}}{\\partial y}\\right\\vert_{y=0} = 4y \\big\\vert_{y=0} = 0\\]\nwe can choose \\(r \\neq 0\\) arbitrarily and set \\(s = u_{ex}\\). Here, we choose \\(r = 1000\\)\nWe can now define all the necessary variables and assemble the traditional part of the variational form\n\nx = SpatialCoordinate(mesh)\nu_ex = lambda x: 1 +x[0]**2 +2*x[1]**2\n\n# Define physical parameters and boundary condtions\nkappa = Constant(mesh, default_scalar_type(1))\n\nf = -div(grad(u_ex(x)))\nn = FacetNormal(mesh)\ng = -dot(n, grad(u_ex(x)))\ns = u_ex(x)\nr = Constant(mesh, default_scalar_type(1000))\n\n# Define function space and standard part of variational form\nV = functionspace(mesh, (\"Lagrange\", 1))\nu = TrialFunction(V)\nv = TestFunction(V)\n\nF = kappa *inner(grad(u), grad(v)) *dx -inner(f, v) *dx\n\nWe begin by identifying the facets on each boundary and creating a custom integration measure, ds\n\nboundaries = [\n  (1, lambda x: np.isclose(x[0], 0)),\n  (2, lambda x: np.isclose(x[0], 1)),\n  (3, lambda x: np.isclose(x[1], 0)),\n  (4, lambda x: np.isclose(x[1], 1))\n]\n\nNext, we go through each boundary condition and generate MeshTags that mark the corresponding facets\n\nfacet_indices, facet_markers = [], []\nfdim = mesh.topology.dim -1\n\nfor (marker, locator) in boundaries:\n  facets = locate_entities(mesh, fdim, locator)\n  facet_indices.append(facets)\n  facet_markers.append(np.full_like(facets, marker))\n\nfacet_indices = np.hstack(facet_indices).astype(np.int32)\nfacet_markers = np.hstack(facet_markers).astype(np.int32)\nsorted_facets = np.argsort(facet_indices)\n\nfacet_tag = meshtags(\n  mesh, \n  fdim, \n  facet_indices[sorted_facets], \n  facet_markers[sorted_facets]\n)\n\nDebugging boundary condition\nA simple way to debug boundary conditions is to visualize the boundaries in ParaView. We do this by writing the MeshTags to a file, after which individual boundaries can be examined using the Threshold filter\n\nfrom pathlib import Path\n\nresults_folder = Path(\"fenicsx/fundamentals\")\nresults_folder.mkdir(exist_ok=True, parents=True)\n\nmesh.topology.create_connectivity(mesh.topology.dim -1, mesh.topology.dim)\nwith XDMFFile(mesh.comm, results_folder/\"facet_tags.xdmf\", \"w\") as xdmf:\n    xdmf.write_mesh(mesh)\n    xdmf.write_meshtags(facet_tag, mesh.geometry)\n\nNext, we define a custom integration measure, ds, which can be used to restrict integration to selected facets. Using ds(1) ensures that we integrate only over facets that have been assigned the value 1 in the corresponding facet_tag\n\nds = Measure(\"ds\", domain=mesh, subdomain_data=facet_tag)\n\nNext, we define a general boundary condition class that can handle different types of boundary conditions\n\nclass BoundaryCondition():\n  def __init__(self, type, marker, values):\n    self._type = type\n    if type == \"Dirichlet\":\n      u_D = Function(V)\n      u_D.interpolate(values)\n      facets = facet_tag.find(marker)\n      dofs = locate_dofs_topological(V, fdim, facets)\n      self._bc = dirichletbc(u_D, dofs)\n    elif type == \"Neumann\":\n      self._bc = inner(values, v) *ds(marker)\n    elif type == \"Robin\":\n      self._bc = values[0] *inner(u -values[1], v) *ds(marker)\n    else:\n      raise TypeError(f\"Unknown boundary condition: {type:s}\")\n  \n  @property\n  def bc(self):\n    return self._bc\n\n  @property\n  def type(self):\n    return self._type\n\n# Define the Dirichlet condition\nboundary_conditions = [\n  BoundaryCondition(\"Dirichlet\", 1, u_ex),\n  BoundaryCondition(\"Dirichlet\", 2, u_ex),\n  BoundaryCondition(\"Robin\", 3, (r, s)),\n  BoundaryCondition(\"Neumann\", 4, g)\n]\n\nNext, we go through each boundary condition and add it to \\(L(v)\\) or include it in the list of Dirichlet boundary conditions, depending on its type\n\nbcs = []\nfor condition in boundary_conditions:\n  if condition.type == \"Dirichlet\":\n    bcs.append(condition.bc)\n  else:\n    F += condition.bc\n\nWe can now create the bilinear form a and the linear form L using the ufl functions lhs and rhs\n\n# Solve linear variational problem\na = lhs(F)\nL = rhs(F)\n\nproblem = LinearProblem(\n  a, \n  L, \n  bcs=bcs, \n  petsc_options={\"ksp_type\": \"preonly\", \"pc_type\": \"lu\"}\n)\nuh = problem.solve()\n\n\nfrom pathlib import Path\n\nresults_folder = Path(\"fenicsx/bcs_subdomains\")\nresults_folder.mkdir(exist_ok=True, parents=True)\n\n# Visualize solution\npyvista_cells, cell_types, geometry = vtk_mesh(V)\ngrid = pyvista.UnstructuredGrid(pyvista_cells, cell_types, geometry)\ngrid.point_data[\"u\"] = uh.x.array\ngrid.set_active_scalars(\"u\")\n\nplotter = pyvista.Plotter(off_screen=True)\nplotter.add_text(\"uh\", position=\"upper_edge\", font_size=14, color=\"black\")\nplotter.add_mesh(grid, show_edges=True)\nplotter.view_xy()\n\nif not pyvista.OFF_SCREEN:\n  #plotter.show()\n  plotter.export_html(results_folder/\"robin_neumann_dirichlet.html\")\nelse:\n  figure = plotter.screenshot(results_folder/\"robin_neumann_dirichlet.png\")\n\n        \n\nVerification\nFollowing the approach used in the previous problems, we calculate the error of the computed solution and compare it to the analytical solution\n\n# Compute L2 error and error at nodes\nV_ex = functionspace(mesh, (\"Lagrange\", 2))\nu_exact = Function(V_ex)\nu_exact.interpolate(u_ex)\n\nerror_L2 = np.sqrt(\n  mesh.comm.allreduce(\n    assemble_scalar(form((uh -u_exact)**2 *dx)), \n    op=MPI.SUM\n  )\n)\n\nu_vertex_values = uh.x.array\nuex_1 = Function(V)\nuex_1.interpolate(u_ex)\nu_ex_vertex_values = uex_1.x.array\n\nerror_max = np.max(np.abs(u_vertex_values -u_ex_vertex_values))\nerror_max = mesh.comm.allreduce(error_max, op=MPI.MAX)\n\nprint(f\"Error_L2 : {error_L2:.2e}\")\nprint(f\"Error_max : {error_max:.2e}\")\n\nError_L2 : 4.86e-03\nError_max : 2.07e-03\n\n\n\n\nK.8.5 Component-wise Dirichlet BC\nAuthor: Jørgen S. Dokken\nWe consider the linear elasticity problem on the rectangular domain \\(\\Omega=[0,L]\\times[0,H]\\) with displacement \\(u=(u_x,u_y)\\). The strong form with the boundary conditions is\n\\[\n\\begin{aligned}\n-\\,\\nabla\\cdot\\sigma(u) &= f &&\\quad\\text{in }\\Omega\\\\[2mm]\nu &= 0 &&\\quad\\text{on }\\Gamma_D \\;(\\text{bottom } y=0)\\\\[2mm]\nu_x &= 0 &&\\quad\\text{on }\\Gamma_{D_x} \\;(\\text{right } x=L) \\\\[2mm]\n\\sigma(u)\\cdot n &= 0 &&\\quad\\text{on }\\Gamma_N \\;(\\text{left } x=0 \\text{ and top } y=H)\n\\end{aligned}\\]\nwhere the stress and strain are given by\n\\[\\begin{aligned}\n\\sigma(u) &= \\lambda\\,\\mathrm{tr}(\\epsilon(u))\\,I + 2\\mu\\,\\epsilon(u)\\\\[2mm]\n\\epsilon(u) &= \\tfrac{1}{2}\\big(\\nabla u + \\nabla u^\\top\\big)\n\\end{aligned}\\]\nPhysical interpretation\n\n\\(\\Gamma_D\\) (bottom): The boundary is fully clamped, meaning both displacement components vanish\n\n\\[u = (u_x,u_y) = 0\\]\n\n\\(\\Gamma_{D_x}\\) (right): Only the horizontal displacement is constrained \\(u_x = 0\\), while the vertical displacement \\(u_y\\) is left free\n\\(\\Gamma_N\\) (left and top): These boundaries are traction-free, that is,\n\n\\[\\sigma(u)\\cdot n = 0\\]\nThe chosen combination of boundary conditions eliminates possible rigid body motions. This ensures that the variational problem admits a unique solution, i.e. the problem is well-posed\n\nimport numpy as np\nfrom mpi4py import MPI\nimport pyvista\n\nfrom dolfinx import default_scalar_type\nfrom dolfinx.mesh import CellType, create_rectangle, locate_entities_boundary\nfrom dolfinx.fem import (\n  Constant, dirichletbc, Function, functionspace, \n  locate_dofs_geometrical, locate_dofs_topological\n)\nfrom dolfinx.fem.petsc import LinearProblem\nfrom dolfinx.plot import vtk_mesh\n\nfrom petsc4py import PETSc\n\nfrom ufl import (\n  Identity, Measure, TestFunction, TrialFunction, \n  dot, dx, inner, grad, nabla_div, sym\n)\n\nL = 1\nH = 1.3\n\nlambda_ = 1.25\nmu = 1\nrho = 1\ng = 1\n\nAs in the previous demos, we begin by defining the computational mesh and the corresponding function space. The function space is chosen to represent vector-valued functions, since the unknown displacement field \\(u = (u_x, u_y)\\) has two components\n\nmesh = create_rectangle(\n    MPI.COMM_WORLD, \n    np.array([[0, 0], [L, H]]), \n    [30, 30], \n    cell_type=CellType.triangle\n)\nV = functionspace(mesh, (\"Lagrange\", 1, (mesh.geometry.dim,)))\n\nBoundary conditions\nNext, we define the boundary conditions for our problem\n\n# Define geometric tolerance\ntol = 1e-8\n\n# Locate boundary facets\nbottom_facets = locate_entities_boundary(\n  mesh, \n  mesh.topology.dim -1,\n  lambda x: np.isclose(x[1], 0.0, atol=tol)\n)\n\nright_facets = locate_entities_boundary(\n  mesh, \n  mesh.topology.dim -1,\n  lambda x: np.isclose(x[0], L, atol=tol)\n)\n\n# Define boundary conditions\nu_bc = Function(V)\n\n# Γ_D (bottom): u = (0,0)\nbc_bottom = dirichletbc(\n  PETSc.ScalarType((0.0, 0.0)), \n  locate_dofs_topological(V, mesh.topology.dim -1, bottom_facets), \n  V\n)\n\n# Γ_{Dx} (right): u_x = 0\nbc_right_x = dirichletbc(\n  PETSc.ScalarType(0.0),\n  locate_dofs_topological(V.sub(0), mesh.topology.dim -1, right_facets),\n  V.sub(0)\n)\n\n# Collect Dirichlet boundary conditions\nbcs = [bc_bottom, bc_right_x]\n\nNote that the traction-free boundaries \\(\\Gamma_N\\) (left and top) do not require explicit constraints in the variational formulation, since they are naturally included through the weak form of the problem\nVariational formulation\nWe now turn to the weak form of the elasticity problem. Multiplying the PDE by a test function \\(v\\) and integrating by parts yields the variational form\n\\[a(u,v) = L(v)\\]\nwhere\n\\[\\begin{aligned}\na(u,v) &= \\int_\\Omega \\sigma(u):\\epsilon(v) \\,\\mathrm{d}x \\\\\nL(v) &= \\int_\\Omega f \\cdot v \\,\\mathrm{d}x\n\\end{aligned}\\]\nNote that the Neumann boundary conditions on \\(\\Gamma_N\\) (traction-free boundaries) are naturally included in the weak formulation, and therefore do not need to be imposed explicitly\n\n# Strain and stress\ndef epsilon(u):\n  return sym(grad(u))\n\n# linear problem: tr(epsilon(u)) = nabla_div(u)\ndef sigma(u):\n  return lambda_ *nabla_div(u) *Identity(len(u)) + 2 *mu *epsilon(u)\n\n# Body force\nf = Constant(mesh, default_scalar_type((0, -rho *g)))\n\n# Define trial and test functions\nu = TrialFunction(V)\nv = TestFunction(V)\n\n# Bilinear form and linear form\na = inner(sigma(u), epsilon(v)) *dx\nL = dot(f, v) *dx  # +dot(T, v) *ds\n\nAs in the previous demos, we now assemble the system matrix and right-hand side vector, and then use PETSc to solve the resulting variational problem\n\n# Define linear problem using PETSc\nproblem = LinearProblem(\n  a, \n  L, \n  bcs=bcs,\n  petsc_options={\n    \"ksp_type\": \"preonly\",\n    \"pc_type\": \"lu\"\n  }\n)\n\n# Solve the system\nuh = problem.solve()\n\n# Print solver information\nPETSc.Sys.Print(\"Linear system solved.\")\n\nLinear system solved.\n\n\nVisualization\n\nfrom pathlib import Path\n\nresults_folder = Path(\"fenicsx/bcs_subdomains\")\nresults_folder.mkdir(exist_ok=True, parents=True)\n\ntopology, cell_types, x = vtk_mesh(V)\ngrid = pyvista.UnstructuredGrid(topology, cell_types, x)\n\nvals = np.zeros((x.shape[0], 3))\nvals[:, :len(uh)] = uh.x.array.reshape((x.shape[0], len(uh)))\ngrid[\"u\"] = vals\n\n# Create plotter and pyvista grid\nplotter = pyvista.Plotter(off_screen=False)\n\nactor_0 = plotter.add_mesh(grid, style=\"wireframe\", color=\"k\")\nwarped = grid.warp_by_vector(\"u\", factor=1.1)\nactor_1 = plotter.add_mesh(warped, opacity=0.8)\nplotter.view_xy()\n\nif not pyvista.OFF_SCREEN:\n  ##p.show()\n  plotter.export_html(results_folder/\"component.html\")\nelse:\n  fig_array = plotter.screenshot(results_folder/\"component.png\")\n\n        \n\n\n\nK.8.6 Electromagnetics example\nAuthor: Hans Petter Langtangen, Anders Logg and Jørgen S. Dokken\nIn this example, we consider an iron cylinder with copper wires wound around it, as illustrated below\n\nA static current of \\(J = 1\\,\\text{A}\\) flows through the copper wires. Our goal is to compute the magnetic field B in the iron cylinder, the copper wires, and the surrounding vacuum\nTo simplify the problem, we note that the cylinder extends far in the \\(z\\)-direction, so the field can be assumed to be independent of the \\(z\\)-coordinate. This reduces the problem to a two-dimensional cross-section in the \\(x\\)–\\(y\\) plane\nWe begin with Maxwell’s equations in magnetostatics:\n\\[\\nabla \\cdot \\mathbf{B} = 0, \\quad \\nabla \\times \\mathbf{H} = \\mathbf{J}\n\\]\nwhere \\(\\mathbf{B}\\) is the magnetic flux density, \\(\\mathbf{H}\\) is the magnetic field intensity, and \\(\\mathbf{J}\\) is the current density. The constitutive relation connects \\(\\mathbf{B}\\) and \\(\\mathbf{H}\\):\n\\[\\mathbf{B} = \\mu \\mathbf{H}\\]\nwith \\(\\mu\\) denoting the magnetic permeability of the medium\nStep 1: Introducing the Vector Potential\nSince \\(\\nabla \\cdot \\mathbf{B} = 0\\), we may represent the magnetic flux density using a vector potential \\(\\mathbf{A}\\):\n\\[\\mathbf{B} = \\nabla \\times \\mathbf{A}\\]\nStep 2: Substituting into Ampère’s Law\nUsing \\(\\mathbf{B} = \\mu \\mathbf{H}\\), Ampère’s law becomes\n\\[\\nabla \\times \\left(\\frac{1}{\\mu} \\nabla \\times \\mathbf{A}\\right) = \\mathbf{J}\\]\nStep 3: Reduction to 2D\nFor a long cylinder aligned with the \\(z\\)-axis and a current flowing along \\(z\\), symmetry implies that the vector potential has only a \\(z\\)-component:\n\\[\\mathbf{A}(x,y) = (0,0, A_z(x,y))\\]\nThus, the magnetic flux density lies in the \\(x\\)-\\(y\\) plane\nStep 4: Scalar Poisson Equation\nWith this reduction, the vector equation simplifies to a scalar Poisson equation for \\(A_z\\):\n\\[-\\nabla \\cdot \\left(\\frac{1}{\\mu} \\nabla A_z\\right) = J_z\\]\n\\[\\lim_{\\vert(x,y)\\vert\\to \\infty}A_z = 0\\]\nwhere \\(J_z\\) is the \\(z\\)-component of the current density. Once \\(A_z\\) is known, the magnetic field can be recovered as\n\\[\\mathbf{B} = \\nabla \\times \\mathbf{A} =\n\\left(\\frac{\\partial A_z}{\\partial y}, \\; -\\frac{\\partial A_z}{\\partial x}, \\; 0 \\right)\\]\nSince we cannot compute on an infinite domain, we truncate the problem by surrounding the cylinder with a sufficiently large disk. On the boundary of this disk, we impose the condition \\(A_z = 0\\)\nThe current density \\(J_z\\) is prescribed inside the circular cross-sections of the copper wires. In the interior set of circles, we assign a current of \\(+1\\,\\mathrm{A}\\), while in the exterior set of circles, we assign a current of \\(-1\\,\\mathrm{A}\\). This ensures that the net current in the domain is balanced, making the problem well-posed and allowing for a consistent computation of the magnetic field throughout the iron, copper, and vacuum regions\nVariational formulation\nTo derive the variational problem, we multiply the governing PDE by a test function \\(v\\) and integrate over the computational domain \\(\\Omega\\):\n\\[-\\int_\\Omega \\nabla \\cdot \\left( \\mu^{-1} \\nabla A_z \\right) v \\,\\mathrm{d}x = \\int_\\Omega J_z v \\,\\mathrm{d}x\\]\nApplying integration by parts (Green’s theorem) gives\n\\[\\int_\\Omega \\mu^{-1} \\nabla A_z \\cdot \\nabla v \\,\\mathrm{d}x\n    -\\int_{\\partial \\Omega} \\mu^{-1} \\frac{\\partial A_z}{\\partial n} v \\,\\mathrm{d}s = \\int_\\Omega J_z v \\,\\mathrm{d}x\\]\nOn the boundary \\(\\partial \\Omega\\), we have prescribed \\(A_z = 0\\), which implies that the test function \\(v\\) also vanishes there. Consequently, the boundary integral disappears, leaving us with the variational formulation:\n\\[a(A_z, v) = L(v)\\]\nwhere\n\\[ a(A_z, v) = \\int_\\Omega \\mu^{-1} \\nabla A_z \\cdot \\nabla v \\,\\mathrm{d}x, \\qquad\nL(v) = \\int_\\Omega J_z v \\,\\mathrm{d}x\\]\nThus, the problem reduces to finding \\(A_z \\in V\\), where \\(V\\) is the appropriate function space with homogeneous Dirichlet boundary conditions, such that\n\\[a(A_z, v) = L(v) \\quad \\text{for all } v \\in V\\]\nMeshing a complex structure with subdomains\nWe now turn to the practical implementation of the problem. The first step is to create the computational mesh. We can construct this geometry in GMSH. Each physical region (iron, copper wires, vacuum, and outer boundary) is tagged with unique markers. These markers are preserved when the mesh is imported into dolfinx, and they allow us to assign different material parameters and source terms\n\n\"\"\"\nElectromagnetic coil geometry generation using Gmsh and DOLFINx.\nThis script creates a 2D geometry consisting of:\n- Iron cylinder (magnetic core)\n- Copper wire windings (inner and outer)\n- Vacuum/air background\n\"\"\"\n\n# Import required libraries\nimport numpy as np\nfrom mpi4py import MPI\n\nimport gmsh\nimport pyvista\n\nfrom dolfinx import default_scalar_type\nfrom dolfinx.fem import (dirichletbc, Expression, Function, functionspace, \n                        locate_dofs_topological)\nfrom dolfinx.fem.petsc import LinearProblem\nfrom dolfinx.io import XDMFFile\nfrom dolfinx.io.gmshio import model_to_mesh\nfrom dolfinx.mesh import compute_midpoints, locate_entities_boundary\nfrom dolfinx.plot import vtk_mesh\n\nfrom ufl import TestFunction, TrialFunction, as_vector, dot, dx, grad, inner\n\n# MPI configuration\nrank = MPI.COMM_WORLD.rank\n\n# Initialize Gmsh\ngmsh.initialize()\n\n# ============================================================================\n# GEOMETRY PARAMETERS\n# ============================================================================\nR = 5      # Radius of computational domain\n\na = 1      # Inner radius of iron cylinder\nb = 1.2    # Outer radius of iron cylinder\n\nN = 8      # Number of copper wire windings\nc_1 = 0.8  # Radius of inner copper wire circle\nc_2 = 1.4  # Radius of outer copper wire circle\nr = 0.1    # Radius of individual copper wires\n\n# Mesh parameters\ngdim = 2           # Geometric dimension (2D problem)\nmodel_rank = 0     # MPI rank responsible for geometry creation\nmesh_comm = MPI.COMM_WORLD\n\n# ============================================================================\n# GEOMETRY CREATION\n# ============================================================================\nif mesh_comm.rank == model_rank:\n\n    # ------------------------------------------------------------------------\n    # Create background domain (vacuum/air)\n    # ------------------------------------------------------------------------\n    print(\"Creating background domain...\")\n    background = gmsh.model.occ.addDisk(0, 0, 0, R, R)\n    gmsh.model.occ.synchronize()    \n    \n    # ------------------------------------------------------------------------\n    # Create iron cylinder (magnetic core)\n    # ------------------------------------------------------------------------\n    print(\"Creating iron cylinder geometry...\")\n    \n    # Create outer and inner boundaries of iron cylinder\n    outer_iron = gmsh.model.occ.addCircle(0, 0, 0, b)\n    inner_iron = gmsh.model.occ.addCircle(0, 0, 0, a)\n    \n    # Create curve loops and surface for iron cylinder (annular region)\n    gmsh.model.occ.addCurveLoop([outer_iron], 5)\n    gmsh.model.occ.addCurveLoop([inner_iron], 6)\n    iron = gmsh.model.occ.addPlaneSurface([5, 6])  # Surface with hole\n    gmsh.model.occ.synchronize()\n\n     # ------------------------------------------------------------------------\n    # Create copper wire windings\n    # ------------------------------------------------------------------------\n    print(f\"Creating {N} inner and {N} outer copper wire windings...\")\n    \n    # Inner copper wires (North polarity) - evenly distributed\n    angles_N = [i * 2 * np.pi / N for i in range(N)]\n    wires_N = []\n    for v in angles_N:\n        x_pos = c_1 * np.cos(v)\n        y_pos = c_1 * np.sin(v)\n        wire = gmsh.model.occ.addDisk(x_pos, y_pos, 0, r, r)\n        wires_N.append((2, wire))\n\n    # Outer copper wires (South polarity) - offset by half angle\n    angles_S = [(i + 0.5) * 2 * np.pi / N for i in range(N)]\n    wires_S = []\n    for v in angles_S:\n        x_pos = c_2 * np.cos(v)\n        y_pos = c_2 * np.sin(v)\n        wire = gmsh.model.occ.addDisk(x_pos, y_pos, 0, r, r)\n        wires_S.append((2, wire))\n    \n    gmsh.model.occ.synchronize()\n\n    # ------------------------------------------------------------------------\n    # Boolean operations to create final geometry\n    # ------------------------------------------------------------------------\n    print(\"Performing boolean operations...\")\n    \n    # Combine all surfaces for fragmentation\n    all_surfaces = [(2, iron)]\n    all_surfaces.extend(wires_S)\n    all_surfaces.extend(wires_N)\n    \n    # Fragment the background domain with all other surfaces\n    # This creates non-overlapping regions\n    whole_domain = gmsh.model.occ.fragment([(2, background)], all_surfaces)\n    gmsh.model.occ.synchronize()\n\n    # ------------------------------------------------------------------------\n    # Assign physical markers to different regions\n    # Physical markers are used for material properties:\n    # - Tag 0: Vacuum/air background\n    # - Tag 1: Iron cylinder\n    # - Tags 2 to N+1: Inner copper wires\n    # - Tags N+2 to 2*N+1: Outer copper wires\n    # ------------------------------------------------------------------------\n    print(\"Assigning physical markers...\")\n    \n    inner_tag = 2\n    outer_tag = 2 + N\n    background_surfaces = []\n    other_surfaces = []\n    \n    # Classify each surface based on geometric properties\n    for domain in whole_domain[0]:\n        # Get center of mass and total mass of the domain\n        com = gmsh.model.occ.getCenterOfMass(domain[0], domain[1])\n        mass = gmsh.model.occ.getMass(domain[0], domain[1])\n\n        # Identify iron cylinder by its characteristic mass\n        if np.isclose(mass, np.pi * (b**2 - a**2), rtol=1e-3):\n            gmsh.model.addPhysicalGroup(domain[0], [domain[1]], tag=1)\n            other_surfaces.append(domain)\n            print(f\"  Iron cylinder identified (tag=1)\")        \n\n        # Identify background surfaces by center of mass at origin\n        elif np.allclose(com, [0, 0, 0], atol=1e-6):\n            background_surfaces.append(domain[1])        \n        \n        # Identify inner copper wires by distance from origin\n        elif np.isclose(np.linalg.norm(com), c_1, rtol=1e-3):\n            gmsh.model.addPhysicalGroup(domain[0], [domain[1]], inner_tag)\n            print(f\"  Inner wire identified (tag={inner_tag})\")\n            inner_tag += 1\n            other_surfaces.append(domain)\n        \n        # Identify outer copper wires by distance from origin\n        elif np.isclose(np.linalg.norm(com), c_2, rtol=1e-3):\n            gmsh.model.addPhysicalGroup(domain[0], [domain[1]], outer_tag)\n            print(f\"  Outer wire identified (tag={outer_tag})\")\n            outer_tag += 1\n            other_surfaces.append(domain)\n    \n    # Assign physical marker for vacuum/air background\n    gmsh.model.addPhysicalGroup(2, background_surfaces, tag=0)\n    print(f\"  Background/vacuum regions identified (tag=0)\")\n\n    # ------------------------------------------------------------------------\n    # Configure adaptive mesh sizing\n    # ------------------------------------------------------------------------\n    print(\"Configuring mesh size fields...\")\n    \n    # Create distance field from wire and iron boundaries\n    gmsh.model.mesh.field.add(\"Distance\", 1)\n    edges = gmsh.model.getBoundary(other_surfaces, oriented=False)\n    gmsh.model.mesh.field.setNumbers(1, \"EdgesList\", [e[1] for e in edges])\n    \n    # Create threshold field for adaptive sizing\n    gmsh.model.mesh.field.add(\"Threshold\", 2)\n    gmsh.model.mesh.field.setNumber(2, \"IField\", 1)\n    gmsh.model.mesh.field.setNumber(2, \"LcMin\", r / 3)    # Fine mesh near wires\n    gmsh.model.mesh.field.setNumber(2, \"LcMax\", 6 * r)    # Coarse mesh far away\n    gmsh.model.mesh.field.setNumber(2, \"DistMin\", 4 * r)  # Distance for fine mesh\n    gmsh.model.mesh.field.setNumber(2, \"DistMax\", 10 * r) # Distance for coarse mesh\n    \n    # Set minimum field as background mesh\n    gmsh.model.mesh.field.add(\"Min\", 5)\n    gmsh.model.mesh.field.setNumbers(5, \"FieldsList\", [2])\n    gmsh.model.mesh.field.setAsBackgroundMesh(5)\n\n    # ------------------------------------------------------------------------\n    # Generate and optimize mesh\n    # ------------------------------------------------------------------------\n    print(\"Generating mesh...\")\n    \n    # Use Frontal-Delaunay algorithm for 2D meshing\n    gmsh.option.setNumber(\"Mesh.Algorithm\", 7)\n    \n    # Generate 2D mesh\n    gmsh.model.mesh.generate(gdim)\n    \n    # Optimize mesh quality using Netgen optimizer\n    print(\"Optimizing mesh quality...\")\n    gmsh.model.mesh.optimize(\"Netgen\")\n    \n    print(\"Geometry and mesh generation completed successfully!\")\n\n# Note: After this point, you would typically:\n# 1. Convert the Gmsh model to DOLFINx mesh using model_to_mesh()\n# 2. Set up the electromagnetic problem (Maxwell's equations)\n# 3. Apply boundary conditions\n# 4. Solve the finite element problem\n\nCreating background domain...\nCreating iron cylinder geometry...\nCreating 8 inner and 8 outer copper wire windings...\nPerforming boolean operations...\nInfo    : [  0%] Fragments                                                                                  Info    : [ 10%] Fragments                                                                                  Info    : [ 20%] Fragments                                                                                  Info    : [ 30%] Fragments                                                                                  Info    : [ 40%] Fragments                                                                                  Info    : [ 70%] Fragments                                                                                  Info    : [ 80%] Fragments - Making faces                                                                                Info    : [ 90%] Fragments - Adding holes                                                                                Assigning physical markers...\n  Iron cylinder identified (tag=1)\n  Outer wire identified (tag=10)\n  Outer wire identified (tag=11)\n  Outer wire identified (tag=12)\n  Outer wire identified (tag=13)\n  Outer wire identified (tag=14)\n  Outer wire identified (tag=15)\n  Outer wire identified (tag=16)\n  Outer wire identified (tag=17)\n  Inner wire identified (tag=2)\n  Inner wire identified (tag=3)\n  Inner wire identified (tag=4)\n  Inner wire identified (tag=5)\n  Inner wire identified (tag=6)\n  Inner wire identified (tag=7)\n  Inner wire identified (tag=8)\n  Inner wire identified (tag=9)\n  Background/vacuum regions identified (tag=0)\nConfiguring mesh size fields...\nGenerating mesh...\nInfo    : Meshing 1D...\nInfo    : [  0%] Meshing curve 1 (Ellipse)\nInfo    : [ 10%] Meshing curve 2 (Ellipse)\nInfo    : [ 20%] Meshing curve 3 (Ellipse)\nInfo    : [ 20%] Meshing curve 4 (Ellipse)\nInfo    : [ 30%] Meshing curve 5 (Ellipse)\nInfo    : [ 30%] Meshing curve 6 (Ellipse)\nInfo    : [ 40%] Meshing curve 7 (Ellipse)\nInfo    : [ 40%] Meshing curve 8 (Circle)\nInfo    : [ 50%] Meshing curve 9 (Ellipse)\nInfo    : [ 50%] Meshing curve 10 (Ellipse)\nInfo    : [ 60%] Meshing curve 11 (Circle)\nInfo    : [ 60%] Meshing curve 12 (Ellipse)\nInfo    : [ 70%] Meshing curve 13 (Ellipse)\nInfo    : [ 70%] Meshing curve 14 (Ellipse)\nInfo    : [ 80%] Meshing curve 15 (Ellipse)\nInfo    : [ 80%] Meshing curve 16 (Ellipse)\nInfo    : [ 90%] Meshing curve 17 (Ellipse)\nInfo    : [ 90%] Meshing curve 18 (Ellipse)\nInfo    : [100%] Meshing curve 19 (Ellipse)\nInfo    : Done meshing 1D (Wall 0.00747925s, CPU 0.012548s)\nInfo    : Meshing 2D...\nInfo    : [  0%] Meshing surface 2 (Plane, Bamg)\nInfo    : [  0%] BAMG succeeded 1668 vertices 2920 triangles\nInfo    : [ 10%] Meshing surface 3 (Plane, Bamg)\nInfo    : [ 10%] BAMG succeeded 47 vertices 73 triangles\nInfo    : [ 10%] BAMG succeeded 46 vertices 71 triangles\nInfo    : [ 10%] BAMG succeeded 46 vertices 71 triangles\nInfo    : [ 20%] Meshing surface 4 (Plane, Bamg)\nInfo    : [ 20%] BAMG succeeded 48 vertices 75 triangles\nInfo    : [ 20%] BAMG succeeded 47 vertices 73 triangles\nInfo    : [ 20%] BAMG succeeded 47 vertices 73 triangles\nInfo    : [ 20%] Meshing surface 5 (Plane, Bamg)\nInfo    : [ 20%] BAMG succeeded 50 vertices 79 triangles\nInfo    : [ 20%] BAMG succeeded 48 vertices 75 triangles\nInfo    : [ 20%] BAMG succeeded 48 vertices 75 triangles\nInfo    : [ 30%] Meshing surface 6 (Plane, Bamg)\nInfo    : [ 30%] BAMG succeeded 46 vertices 71 triangles\nInfo    : [ 30%] Meshing surface 7 (Plane, Bamg)\nInfo    : [ 30%] BAMG succeeded 47 vertices 73 triangles\nInfo    : [ 30%] BAMG succeeded 46 vertices 71 triangles\nInfo    : [ 30%] BAMG succeeded 46 vertices 71 triangles\nInfo    : [ 40%] Meshing surface 8 (Plane, Bamg)\nInfo    : [ 40%] BAMG succeeded 48 vertices 75 triangles\nInfo    : [ 40%] BAMG succeeded 47 vertices 73 triangles\nInfo    : [ 40%] BAMG succeeded 47 vertices 73 triangles\nInfo    : [ 40%] Meshing surface 9 (Plane, Bamg)\nInfo    : [ 40%] BAMG succeeded 45 vertices 69 triangles\nInfo    : [ 50%] Meshing surface 10 (Plane, Bamg)\nInfo    : [ 50%] BAMG succeeded 47 vertices 73 triangles\nInfo    : [ 50%] BAMG succeeded 46 vertices 71 triangles\nInfo    : [ 50%] BAMG succeeded 46 vertices 71 triangles\nInfo    : [ 50%] Meshing surface 11 (Plane, Bamg)\nInfo    : [ 50%] BAMG succeeded 47 vertices 73 triangles\nInfo    : [ 50%] BAMG succeeded 46 vertices 71 triangles\nInfo    : [ 50%] BAMG succeeded 46 vertices 71 triangles\nInfo    : [ 60%] Meshing surface 12 (Plane, Bamg)\nInfo    : [ 60%] BAMG succeeded 48 vertices 75 triangles\nInfo    : [ 60%] BAMG succeeded 47 vertices 73 triangles\nInfo    : [ 60%] BAMG succeeded 47 vertices 73 triangles\nInfo    : [ 60%] Meshing surface 13 (Plane, Bamg)\nInfo    : [ 60%] BAMG succeeded 47 vertices 73 triangles\nInfo    : [ 60%] BAMG succeeded 46 vertices 71 triangles\nInfo    : [ 60%] BAMG succeeded 46 vertices 71 triangles\nInfo    : [ 70%] Meshing surface 14 (Plane, Bamg)\nInfo    : [ 70%] BAMG succeeded 47 vertices 73 triangles\nInfo    : [ 70%] BAMG succeeded 47 vertices 73 triangles\nInfo    : [ 70%] Meshing surface 15 (Plane, Bamg)\nInfo    : [ 70%] BAMG succeeded 47 vertices 73 triangles\nInfo    : [ 70%] BAMG succeeded 46 vertices 71 triangles\nInfo    : [ 70%] BAMG succeeded 46 vertices 71 triangles\nInfo    : [ 80%] Meshing surface 16 (Plane, Bamg)\nInfo    : [ 80%] BAMG succeeded 47 vertices 73 triangles\nInfo    : [ 80%] BAMG succeeded 46 vertices 71 triangles\nInfo    : [ 80%] BAMG succeeded 45 vertices 69 triangles\nInfo    : [ 80%] BAMG succeeded 45 vertices 69 triangles\nInfo    : [ 80%] Meshing surface 17 (Plane, Bamg)\nInfo    : [ 80%] BAMG succeeded 48 vertices 75 triangles\nInfo    : [ 80%] BAMG succeeded 47 vertices 73 triangles\nInfo    : [ 80%] BAMG succeeded 47 vertices 73 triangles\nInfo    : [ 90%] Meshing surface 18 (Plane, Bamg)\nInfo    : [ 90%] BAMG succeeded 47 vertices 73 triangles\nInfo    : [ 90%] BAMG succeeded 47 vertices 73 triangles\nInfo    : [ 90%] Meshing surface 19 (Plane, Bamg)\nInfo    : [ 90%] BAMG succeeded 5937 vertices 11458 triangles\nInfo    : [ 90%] BAMG succeeded 5989 vertices 11562 triangles\nInfo    : [100%] Meshing surface 20 (Plane, Bamg)\nInfo    : [100%] BAMG succeeded 3288 vertices 6249 triangles\nInfo    : [100%] BAMG succeeded 3211 vertices 6095 triangles\nInfo    : [100%] BAMG succeeded 3199 vertices 6071 triangles\nInfo    : Done meshing 2D (Wall 2.14339s, CPU 3.36936s)\nInfo    : 10878 nodes 22493 elements\nOptimizing mesh quality...\nInfo    : Optimizing mesh (Netgen)...\nInfo    : Done optimizing mesh (Wall 1.58302e-06s, CPU 2e-06s)\nGeometry and mesh generation completed successfully!\n\n\nFollowing the Navier–Stokes tutorial, we load the mesh directly into DOLFINx without first writing it to a file\n\nmesh, ct, _ = model_to_mesh(gmsh.model, mesh_comm, model_rank, gdim=2)\ngmsh.finalize()\n\nTo inspect the mesh, we use ParaView and obtain the following result\n\nfrom pathlib import Path\n\nresults_folder = Path(\"fenicsx/bcs_subdomains\")\nresults_folder.mkdir(exist_ok=True, parents=True)\n\nwith XDMFFile(MPI.COMM_WORLD, results_folder/\"mt_electro.xdmf\", \"w\") as xdmf:\n    xdmf.write_mesh(mesh)\n    xdmf.write_meshtags(ct, mesh.geometry)\n\n\nWe can also visualize the subdomains using PyVista\n\nplotter = pyvista.Plotter(off_screen=False)\n\ntdim = mesh.topology.dim\nmesh.topology.create_connectivity(tdim, tdim)\nnum_local_cells = mesh.topology.index_map(tdim).size_local\n\ngrid = pyvista.UnstructuredGrid(*vtk_mesh(mesh, tdim))\ngrid.cell_data[\"Marker\"] = ct.values[ct.indices &lt; num_local_cells]\ngrid.set_active_scalars(\"Marker\")\n\nactor = plotter.add_mesh(grid, show_edges=True)\nplotter.view_xy()\n\nif not pyvista.OFF_SCREEN:\n    #plotter.show()\n    plotter.export_html(results_folder/\"cell_tags.html\")    \nelse:\n    cell_tag_fig = plotter.screenshot(results_folder/\"cell_tags.png\")\n\n        \n\nNext, we define discontinuous functions for permeability \\(\\mu\\) and current \\(J_z\\), based on the MeshTags as in “Defining subdomains for different materials”\n\n\"\"\"\nMaterial properties and current density setup for electromagnetic simulation.\nThis code assigns material properties (permeability) and current densities \nto different regions of the electromagnetic coil geometry.\n\"\"\"\n\n# ============================================================================\n# MATERIAL PROPERTIES AND CURRENT DENSITY SETUP\n# ============================================================================\n\ndef setup_material_properties(mesh, ct, N):\n    \"\"\"\n    Set up material properties and current densities for electromagnetic simulation.\n    \n    Parameters:\n    -----------\n    mesh : dolfinx.mesh.Mesh\n        The computational mesh\n    ct : dolfinx.mesh.MeshTags\n        Cell tags identifying different material regions\n    N : int\n        Number of copper wire windings\n        \n    Returns:\n    --------\n    mu : dolfinx.fem.Function\n        Magnetic permeability function\n    J : dolfinx.fem.Function\n        Current density function\n    \"\"\"\n    \n    # Create discontinuous Galerkin function space (piecewise constant)\n    # DG(0) is appropriate for material properties that are constant within each element\n    Q = functionspace(mesh, (\"DG\", 0))\n    \n    # Get unique material tags from the mesh\n    material_tags = np.unique(ct.values)\n    print(f\"Found material tags: {material_tags}\")\n    \n    # Initialize functions for magnetic permeability and current density\n    mu = Function(Q)  # Magnetic permeability [H/m]\n    J = Function(Q)   # Current density [A/m²]\n    \n    # Initialize current density to zero everywhere\n    # (only copper wires will have non-zero current)\n    J.x.array[:] = 0.0\n    \n    print(\"Assigning material properties...\")\n    \n    # ------------------------------------------------------------------------\n    # MATERIAL PROPERTY DEFINITIONS\n    # ------------------------------------------------------------------------\n    \n    # Physical constants and material properties\n    mu_0 = 4 * np.pi * 1e-7        # Vacuum permeability [H/m]\n    mu_copper = 1.26e-6            # Copper permeability ≈ μ₀ [H/m]\n    mu_iron = 1e-5                 # Iron permeability (simplified) [H/m]\n                                   # Note: Real iron μ ≈ 6.3e-3, but using 1e-5 here\n    \n    # Current density magnitude [A/m²]\n    current_density_positive = 1.0   # Inner wires (North polarity)\n    current_density_negative = -1.0  # Outer wires (South polarity)\n    \n    # ------------------------------------------------------------------------\n    # ASSIGN PROPERTIES TO EACH MATERIAL REGION\n    # ------------------------------------------------------------------------\n    \n    for tag in material_tags:\n        # Find all cells belonging to this material tag\n        cells = ct.find(tag)\n        num_cells = len(cells)\n        \n        # Assign magnetic permeability based on material type\n        if tag == 0:\n            # Vacuum/air background\n            mu_value = mu_0\n            material_name = \"Vacuum/Air\"\n            \n        elif tag == 1:\n            # Iron cylinder (magnetic core)\n            mu_value = mu_iron\n            material_name = \"Iron\"\n            \n        else:\n            # Copper wires (both inner and outer)\n            mu_value = mu_copper\n            material_name = \"Copper\"\n        \n        # Set permeability values for all cells in this region\n        mu.x.array[cells] = np.full_like(cells, mu_value, dtype=default_scalar_type)\n        \n        # ------------------------------------------------------------------------\n        # ASSIGN CURRENT DENSITIES TO COPPER WIRES\n        # ------------------------------------------------------------------------\n        \n        # Inner copper wires (tags 2 to N+1) - positive current (North polarity)\n        if tag in range(2, 2 + N):\n            J.x.array[cells] = np.full_like(cells, current_density_positive, \n                                          dtype=default_scalar_type)\n            current_info = f\" | J = +{current_density_positive} A/m²\"\n            \n        # Outer copper wires (tags N+2 to 2N+1) - negative current (South polarity)\n        elif tag in range(2 + N, 2 * N + 2):\n            J.x.array[cells] = np.full_like(cells, current_density_negative, \n                                          dtype=default_scalar_type)\n            current_info = f\" | J = {current_density_negative} A/m²\"\n            \n        else:\n            # Non-conducting materials (vacuum, iron)\n            current_info = \" | J = 0 A/m²\"\n        \n        # Print material assignment summary\n        print(f\"  Tag {tag:2d}: {material_name:10s} \"\n              f\"| μ = {mu_value:.2e} H/m{current_info} \"\n              f\"| {num_cells:4d} cells\")\n    \n    print(\"Material properties assignment completed.\")\n    \n    return mu, J\n\n# ============================================================================\n# USAGE EXAMPLE\n# ============================================================================\n\n# Assuming mesh, ct, and N are already defined from the geometry creation\nmu, J = setup_material_properties(mesh, ct, N)\n\nFound material tags: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17]\nAssigning material properties...\n  Tag  0: Vacuum/Air | μ = 1.26e-06 H/m | J = 0 A/m² | 17633 cells\n  Tag  1: Iron       | μ = 1.00e-05 H/m | J = 0 A/m² | 2920 cells\n  Tag  2: Copper     | μ = 1.26e-06 H/m | J = +1.0 A/m² |   71 cells\n  Tag  3: Copper     | μ = 1.26e-06 H/m | J = +1.0 A/m² |   73 cells\n  Tag  4: Copper     | μ = 1.26e-06 H/m | J = +1.0 A/m² |   75 cells\n  Tag  5: Copper     | μ = 1.26e-06 H/m | J = +1.0 A/m² |   71 cells\n  Tag  6: Copper     | μ = 1.26e-06 H/m | J = +1.0 A/m² |   71 cells\n  Tag  7: Copper     | μ = 1.26e-06 H/m | J = +1.0 A/m² |   73 cells\n  Tag  8: Copper     | μ = 1.26e-06 H/m | J = +1.0 A/m² |   69 cells\n  Tag  9: Copper     | μ = 1.26e-06 H/m | J = +1.0 A/m² |   71 cells\n  Tag 10: Copper     | μ = 1.26e-06 H/m | J = -1.0 A/m² |   71 cells\n  Tag 11: Copper     | μ = 1.26e-06 H/m | J = -1.0 A/m² |   73 cells\n  Tag 12: Copper     | μ = 1.26e-06 H/m | J = -1.0 A/m² |   71 cells\n  Tag 13: Copper     | μ = 1.26e-06 H/m | J = -1.0 A/m² |   73 cells\n  Tag 14: Copper     | μ = 1.26e-06 H/m | J = -1.0 A/m² |   71 cells\n  Tag 15: Copper     | μ = 1.26e-06 H/m | J = -1.0 A/m² |   69 cells\n  Tag 16: Copper     | μ = 1.26e-06 H/m | J = -1.0 A/m² |   73 cells\n  Tag 17: Copper     | μ = 1.26e-06 H/m | J = -1.0 A/m² |   73 cells\nMaterial properties assignment completed.\n\n\nIn the code above, a slightly less extreme value for the magnetic permeability of iron was chosen to make the solution more illustrative; otherwise, the field in the iron cylinder would dominate completely.\nWe now proceed to define the weak problem\n\n\"\"\"\nFinite Element Formulation for 2D Electromagnetic Problem (Magnetostatic).\nThis code sets up the weak form for solving ∇×(1/μ ∇×A) = J\nwhere A is the magnetic vector potential (z-component only in 2D)\n\"\"\"\n\ndef setup_electromagnetic_problem(mesh, mu, J):\n    \"\"\"\n    Set up finite element formulation for 2D magnetostatic problem.\n    \n    Parameters:\n    -----------\n    mesh : dolfinx.mesh.Mesh\n        Computational mesh\n    mu : dolfinx.fem.Function  \n        Magnetic permeability function\n    J : dolfinx.fem.Function\n        Current density function\n        \n    Returns:\n    --------\n    a : ufl.Form\n        Bilinear form (left-hand side)\n    L : ufl.Form  \n        Linear form (right-hand side)\n    bc : dolfinx.fem.DirichletBC\n        Boundary condition\n    V : dolfinx.fem.FunctionSpace\n        Function space\n    \"\"\"\n    \n    print(\"Setting up finite element formulation...\")\n    \n    # ------------------------------------------------------------------------\n    # Function Space Definition\n    # ------------------------------------------------------------------------\n    # Use first-order Lagrange elements (P1) for magnetic vector potential\n    # In 2D, we solve for A_z component only (out-of-plane)\n    V = functionspace(mesh, (\"Lagrange\", 1))\n    \n    print(f\"Function space: P1 Lagrange elements\")\n    print(f\"Total DOFs: {V.dofmap.index_map.size_global}\")\n    \n    # ------------------------------------------------------------------------\n    # Boundary Conditions\n    # ------------------------------------------------------------------------\n    # Apply homogeneous Dirichlet BC: A_z = 0 on entire boundary\n    # This assumes the magnetic vector potential vanishes at far-field\n    \n    # Get topological dimension for boundary identification\n    tdim = mesh.topology.dim  # Should be 2 for 2D problem\n    \n    # Locate all boundary facets (edges in 2D, faces in 3D)\n    # The lambda function returns True for all boundary points\n    facets = locate_entities_boundary(mesh, tdim - 1, \n                                    lambda x: np.full(x.shape[1], True))\n    \n    # Find degrees of freedom on boundary facets\n    dofs = locate_dofs_topological(V, tdim - 1, facets)\n    \n    # Create homogeneous Dirichlet boundary condition: A_z = 0\n    bc = dirichletbc(default_scalar_type(0), dofs, V)\n    \n    print(f\"Boundary condition: A_z = 0 on {len(facets)} boundary facets\")\n    print(f\"Constrained DOFs: {len(dofs)}\")\n    \n    # ------------------------------------------------------------------------\n    # Weak Form Definition\n    # ------------------------------------------------------------------------\n    # Define trial and test functions\n    u = TrialFunction(V)  # Trial function (A_z)\n    v = TestFunction(V)   # Test function\n    \n    # Bilinear form: a(u,v) = ∫ (1/μ) ∇u · ∇v dx\n    # This comes from the weak form of ∇×(1/μ ∇×A) = J\n    # In 2D: ∇×A = (∂A_z/∂y, -∂A_z/∂x), so |∇×A|² = |∇A_z|²\n    a = (1 / mu) * dot(grad(u), grad(v)) * dx\n    \n    # Linear form: L(v) = ∫ J·v dx  \n    # Current density source term\n    L = J * v * dx\n    \n    print(\"Weak form successfully defined:\")\n    print(\"  Bilinear form: a(u,v) = ∫ (1/μ) ∇u·∇v dx\")\n    print(\"  Linear form:   L(v)   = ∫ J v dx\")\n    print(\"  Boundary condition: u = 0 on ∂Ω\")\n    \n    return a, L, bc, V\n\na, L, bc, V = setup_electromagnetic_problem(mesh, mu, J)    \n\nSetting up finite element formulation...\nFunction space: P1 Lagrange elements\nTotal DOFs: 10878\nBoundary condition: A_z = 0 on 53 boundary facets\nConstrained DOFs: 53\nWeak form successfully defined:\n  Bilinear form: a(u,v) = ∫ (1/μ) ∇u·∇v dx\n  Linear form:   L(v)   = ∫ J v dx\n  Boundary condition: u = 0 on ∂Ω\n\n\nWe are now ready to solve the linear problem\n\ndef solve_electromagnetic_enhanced(V, a, L, bc, solver_options=None):\n    \"\"\"\n    Enhanced solver with better control and monitoring.\n    \"\"\"\n    \n    print(\"Setting up enhanced electromagnetic solver...\")\n    \n    # Create solution function\n    A_z = Function(V)\n    A_z.name = \"MagneticVectorPotential\"  # For visualization\n    \n    # Default solver options\n    if solver_options is None:\n        solver_options = {\n            \"ksp_type\": \"cg\",           # Conjugate Gradient\n            \"pc_type\": \"hypre\",         # Hypre preconditioner  \n            \"ksp_rtol\": 1e-8,           # Relative tolerance\n            \"ksp_atol\": 1e-12,          # Absolute tolerance\n            \"ksp_max_it\": 1000,         # Maximum iterations\n            \"ksp_monitor\": None,        # Monitor convergence\n        }\n    \n    # Create linear problem with custom options\n    problem = LinearProblem(\n        a, L, \n        u=A_z, \n        bcs=[bc],\n        petsc_options=solver_options\n    )\n    \n    print(\"Solver configuration:\")\n    print(f\"  Method: {solver_options.get('ksp_type', 'default')}\")\n    print(f\"  Preconditioner: {solver_options.get('pc_type', 'default')}\")\n    print(f\"  Tolerance: {solver_options.get('ksp_rtol', 'default')}\")\n    \n    # Solve with timing\n    import time\n    start_time = time.time()\n    \n    try:\n        problem.solve()\n        solve_time = time.time() -start_time\n        \n        print(\"✅ Solution completed successfully!\")\n        print(f\"  Solve time: {solve_time:.3f} seconds\")\n        print(f\"  Solution range: [{A_z.x.array.min():.6e}, {A_z.x.array.max():.6e}]\")\n        print(f\"  Solution norm: {np.linalg.norm(A_z.x.array):.6e}\")\n        \n        # Check for reasonable solution\n        if np.all(np.abs(A_z.x.array) &lt; 1e-15):\n            print(\"⚠️  WARNING: Solution is nearly zero - check source terms and materials\")\n            \n    except Exception as e:\n        print(f\"❌ Solver failed: {e}\")\n        raise\n    \n    return A_z\n\nA_z = solve_electromagnetic_enhanced(V, a, L, bc)\n\nSetting up enhanced electromagnetic solver...\nSolver configuration:\n  Method: cg\n  Preconditioner: hypre\n  Tolerance: 1e-08\n  Residual norms for dolfinx_solve_13468153488 solve.\n  0 KSP Residual norm 3.598619871356e-06\n  1 KSP Residual norm 3.122411945850e-07\n  2 KSP Residual norm 2.222388013708e-08\n  3 KSP Residual norm 1.268984359645e-09\n  4 KSP Residual norm 8.856908240429e-11\n  5 KSP Residual norm 9.774915584456e-12\n  6 KSP Residual norm 5.259661881310e-13\n✅ Solution completed successfully!\n  Solve time: 0.038 seconds\n  Solution range: [-7.260625e-09, 9.400419e-08]\n  Solution norm: 5.369828e-06\n\n\nAs we have computed the magnetic potential, we can now calculate the magnetic field by setting \\(\\mathbf{B} = \\nabla \\times A_z\\). Note that, since we have chosen a function space of first-order piecewise linear functions to represent the potential, the curl of a function in this space is a discontinuous zeroth-order function (i.e., a function that is constant on each cell). We use dolfinx.fem.Expression to interpolate the curl into the function space W\n\n\"\"\"\nMagnetic flux density calculation from magnetic vector potential.\nThis code computes B = ∇ × A where A = A_z ẑ in 2D.\nResult: B = (∂A_z/∂y, -∂A_z/∂x) = (B_x, B_y)\n\"\"\"\n\ndef calculate_magnetic_field(mesh, A_z):\n    \"\"\"\n    Magnetic field calculation\n    \n    Parameters:\n    -----------\n    mesh : dolfinx.mesh.Mesh\n        Computational mesh\n    A_z : dolfinx.fem.Function\n        Magnetic vector potential (z-component)\n        \n    Returns:\n    --------\n    B : dolfinx.fem.Function\n        Magnetic flux density vector field (B_x, B_y)\n    \"\"\"\n    \n    print(\"Calculating magnetic flux density B = ∇ × A...\")\n    \n    # Create vector function space for B field\n    # DG(0) = piecewise constant discontinuous elements\n    # (mesh.geometry.dim, ) creates vector space with 2 components in 2D\n    W = functionspace(mesh, (\"DG\", 0, (mesh.geometry.dim, )))\n    \n    # Create function to store magnetic flux density\n    B = Function(W)\n    B.name = \"MagneticFluxDensity\"\n    \n    # Calculate B = ∇ × A = (∂A_z/∂y, -∂A_z/∂x) in 2D\n    # A_z.dx(0) = ∂A_z/∂x (derivative w.r.t. first coordinate)  \n    # A_z.dx(1) = ∂A_z/∂y (derivative w.r.t. second coordinate)\n    B_expr = Expression(\n        as_vector((A_z.dx(1), -A_z.dx(0))), \n        W.element.interpolation_points()\n    )\n    \n    # Interpolate the expression onto the DG function space\n    B.interpolate(B_expr)\n    \n    # Calculate magnitude for reporting\n    B_magnitude = np.sqrt(B.x.array[0::2]**2 + B.x.array[1::2]**2)\n    \n    print(\"✅ Magnetic flux density calculated successfully!\")\n    print(f\"  B_x range: [{B.x.array[0::2].min():.6e}, {B.x.array[0::2].max():.6e}] T\")\n    print(f\"  B_y range: [{B.x.array[1::2].min():.6e}, {B.x.array[1::2].max():.6e}] T\") \n    print(f\"  |B| range: [{B_magnitude.min():.6e}, {B_magnitude.max():.6e}] T\")\n    \n    return B\n\nB = calculate_magnetic_field(mesh, A_z)\n\nCalculating magnetic flux density B = ∇ × A...\n✅ Magnetic flux density calculated successfully!\n  B_x range: [-3.938081e-07, 3.939646e-07] T\n  B_y range: [-3.951111e-07, 3.952070e-07] T\n  |B| range: [6.023721e-14, 3.952158e-07] T\n\n\nNote that we use ufl.as_vector to interpret the Python tuple (A_z.dx(1), -A_z.dx(0)) as a vector in UFL\nWe now plot the magnetic potential A_z and the magnetic field B. To do this, we first create a new plotter\n\nplotter = pyvista.Plotter(off_screen=False)\n\nAz_grid = pyvista.UnstructuredGrid(*vtk_mesh(V))\nAz_grid.point_data[\"A_z\"] = A_z.x.array\nAz_grid.set_active_scalars(\"A_z\")\n\nwarp = Az_grid.warp_by_scalar(\"A_z\", factor=1e7)\nactor = plotter.add_mesh(warp, show_edges=True)\n\nif not pyvista.OFF_SCREEN:\n    #plotter.show()\n    plotter.export_html(results_folder/\"Az.html\")    \nelse:\n    Az_fig = plotter.screenshot(results_folder/\"Az.png\")\n\n        \n\nVisualizing the magnetic field\nSince the magnetic field is a piecewise-constant vector field, we need to create a custom plotting function. We begin by computing the midpoints of each cell, which are the locations where we want to visualize the cell-wise constant vectors. Next, we extract the data from the function B and reshape it into 3D vectors. Finally, we associate each vector with its corresponding midpoint using pyvista.PolyData\n\n\"\"\"\nMagnetic field visualization using PyVista for 2D electromagnetic simulation.\nThis code creates vector glyphs to visualize the magnetic flux density B field.\n\"\"\"\n\ndef visualize_magnetic_field(mesh, B, scale_factor=2e6):\n    \"\"\"\n    Magnetic field visualization\n    \n    Parameters:\n    -----------\n    mesh : dolfinx.mesh.Mesh\n        The computational mesh\n    B : dolfinx.fem.Function\n        Magnetic flux density vector field\n    scale_factor : float\n        Scaling factor for vector glyphs\n        \n    Returns:\n    --------\n    plotter : pyvista.Plotter\n        Configured plotter object\n    \"\"\"\n    \n    print(\"Setting up magnetic field visualization...\")\n    \n    # Extract function space information from B\n    W = B.function_space  # Get function space from B field\n\n    # Create plotter\n    plotter = pyvista.Plotter(off_screen=False)\n    plotter.set_position([0, 0, 5])  # Camera position\n    \n    # -----------------------------------------------------------------------\n    # MESH SETUP (with ghost cells for parallel processing)\n    # -----------------------------------------------------------------------\n    \n    # Get topology information including ghost cells\n    top_imap = mesh.topology.index_map(mesh.topology.dim)\n    num_cells = top_imap.size_local + top_imap.num_ghosts\n    \n    print(f\"Total cells (including ghosts): {num_cells}\")\n    \n    # Ensure connectivity is created\n    mesh.topology.create_connectivity(mesh.topology.dim, mesh.topology.dim)\n    \n    # Compute cell midpoints for vector placement\n    cell_indices = np.arange(num_cells, dtype=np.int32)\n    midpoints = compute_midpoints(mesh, mesh.topology.dim, cell_indices)\n    \n    # -----------------------------------------------------------------------\n    # VECTOR FIELD DATA PREPARATION\n    # -----------------------------------------------------------------------\n    \n    # Get DOF information \n    num_dofs = W.dofmap.index_map.size_local + W.dofmap.index_map.num_ghosts\n    block_size = W.dofmap.index_map_bs  # Should be 2 for 2D vector field\n    \n    print(f\"DOFs: {num_dofs}, Block size: {block_size}\")\n    \n    # Verify consistency\n    assert num_cells == num_dofs, f\"Mismatch: {num_cells} cells vs {num_dofs} DOFs\"\n    \n    # Prepare 3D vector data (PyVista requires 3D vectors)\n    values = np.zeros((num_dofs, 3), dtype=np.float64)\n    \n    # Reshape B field data and assign to first 2 components\n    # B.x.array is [Bx0, By0, Bx1, By1, ...] for DG elements\n    B_reshaped = B.x.array.real.reshape(num_dofs, block_size)\n    values[:, :mesh.geometry.dim] = B_reshaped  # Fill (Bx, By, 0)\n    \n    print(f\"B field range: [{np.linalg.norm(values, axis=1).min():.3e}, \"\n          f\"{np.linalg.norm(values, axis=1).max():.3e}] T\")\n    \n    # -----------------------------------------------------------------------\n    # PYVISTA VISUALIZATION SETUP  \n    # -----------------------------------------------------------------------\n    \n    # Create point cloud at cell midpoints\n    cloud = pyvista.PolyData(midpoints)\n    cloud[\"B\"] = values  # Attach vector field data\n    \n    # Create vector glyphs (arrows) \n    glyphs = cloud.glyph(\"B\", factor=scale_factor)\n    \n    # Add wireframe mesh\n    try:\n        # Convert DOLFINx mesh to PyVista grid\n        grid = vtk_mesh(mesh, mesh.topology.dim)[0]\n        actor_mesh = plotter.add_mesh(grid, style=\"wireframe\", color=\"k\", \n                                     line_width=1, opacity=0.3)\n    except Exception as e:\n        print(f\"⚠️  Could not add mesh wireframe: {e}\")\n        actor_mesh = None\n    \n    # Add vector field glyphs\n    actor_vectors = plotter.add_mesh(glyphs, color=\"red\", \n                                   scalar_bar_args={\"title\": \"B [T]\"})\n    \n    # Set up camera and view\n    plotter.camera_position = \"xy\"  # Top-down view for 2D\n    plotter.add_axes()\n    plotter.show_grid()\n    \n    print(\"✅ Visualization setup complete!\")\n    \n    return plotter\n\nplotter = visualize_magnetic_field(mesh, B)\n\nSetting up magnetic field visualization...\nTotal cells (including ghosts): 21701\nDOFs: 21701, Block size: 2\nB field range: [6.024e-14, 3.952e-07] T\n⚠️  Could not add mesh wireframe: NumPy array could not be wrapped pyvista.\n✅ Visualization setup complete!\n\n\n\nif not pyvista.OFF_SCREEN:\n    #plotter.show()\n    plotter.export_html(results_folder/\"B.html\")    \nelse:\n    Az_fig = plotter.screenshot(results_folder/\"B.png\")",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>K</span>  <span class='chapter-title'>The FEniCS computing platform</span>"
    ]
  },
  {
    "objectID": "x_PDE_FEniCS.html#best-practices-for-fenicsx",
    "href": "x_PDE_FEniCS.html#best-practices-for-fenicsx",
    "title": "Appendix K — The FEniCS computing platform",
    "section": "K.9 Best Practices for FEniCSx",
    "text": "K.9 Best Practices for FEniCSx\n\nK.9.1 Solver configuration\nAuthor: Jørgen S. Dokken\nIn this section, we demonstrate how to specify the linear algebra solver to be used for solving our PDEs, and how to verify the implementation by examining convergence rates:\n\\[-\\Delta u = f \\quad \\text{in } \\Omega, \\qquad\nu = u_D \\quad \\text{on } \\partial \\Omega\\]\nUsing the manufactured solution \\(u_D = \\cos(2\\pi x)\\cos(2\\pi y)\\), we obtain the right-hand side function \\(f = 8\\pi^2 \\cos(2\\pi x)\\cos(2\\pi y)\\)\nWe begin by creating a generic module that evaluates the analytical solution at any point \\(x\\)\n\nimport numpy\nfrom mpi4py import MPI\nfrom petsc4py import PETSc\n\nfrom dolfinx.mesh import create_unit_square, locate_entities_boundary\nfrom dolfinx.fem import dirichletbc, functionspace, Function, locate_dofs_topological\nfrom dolfinx.fem.petsc import LinearProblem\n\nimport ufl\nfrom ufl import SpatialCoordinate, TestFunction, TrialFunction, div, dx, inner, grad\n\n\ndef u_ex(mod):\n    return lambda x: mod.cos(2 * mod.pi * x[0]) * mod.cos(2 * mod.pi * x[1])\n\nNote that the return type of u_ex is a lambda function. Therefore, we can define two separate lambda functions: one using NumPy (for interpolation) and one using UFL (for defining the source term)\n\nu_numpy = u_ex(numpy)\nu_ufl = u_ex(ufl)\n\nWe begin by defining the source term in UFL, using ufl.SpatialCoordinate as the input to u_ufl\n\nmesh = create_unit_square(MPI.COMM_WORLD, 30, 30)\nx = SpatialCoordinate(mesh)\nf = -div(grad(u_ufl(x)))\n\nNext, let us define our linear variational problem\n\nV = functionspace(mesh, (\"Lagrange\", 1))\nu = TrialFunction(V)\nv = TestFunction(V)\n\na = inner(grad(u), grad(v)) *dx\nL = f *v *dx\n\nu_bc = Function(V)\nu_bc.interpolate(u_numpy)\n\nfacets = locate_entities_boundary(\n  mesh, \n  mesh.topology.dim -1, \n  lambda x: numpy.full(x.shape[1], True)\n)\ndofs = locate_dofs_topological(V, mesh.topology.dim -1, facets)\nbcs = [dirichletbc(u_bc, dofs)]\n\nWe begin by solving the problem using LU factorization, a direct solver method similar to Gaussian elimination\n\ndefault_problem = LinearProblem(\n  a, \n  L, \n  bcs=bcs,\n  petsc_options={\"ksp_type\": \"preonly\", \"pc_type\": \"lu\"}\n)\nuh = default_problem.solve()\n\nWe now examine the solver process by inspecting the PETSc solver. Since the view options in PETSc are not adapted for notebooks (for example, solver.view() prints output to the terminal when used in a .py file), we instead write the solver output to a file, read it back, and then print it\n\nfrom pathlib import Path\n\nresults_folder = Path(\"fenicsx/best_practices\")\nresults_folder.mkdir(exist_ok=True, parents=True)\n\nlu_solver = default_problem.solver\nviewer = PETSc.Viewer().createASCII(str(results_folder/\"lu_output.txt\"))\nlu_solver.view(viewer)\nviewer.destroy()\n\nwith open(str(results_folder/\"lu_output.txt\"), \"r\") as solver_output:\n  for line in solver_output:\n      print(line.rstrip()) \n\nKSP Object: (dolfinx_solve_15391356336) 1 MPI process\n  type: preonly\n  maximum iterations=10000, initial guess is zero\n  tolerances: relative=1e-05, absolute=1e-50, divergence=10000.\n  left preconditioning\n  using NONE norm type for convergence test\nPC Object: (dolfinx_solve_15391356336) 1 MPI process\n  type: lu\n    out-of-place factorization\n    tolerance for zero pivot 2.22045e-14\n    matrix ordering: nd\n    factor fill ratio given 5., needed 5.08301\n      Factored matrix follows:\n        Mat Object: (dolfinx_solve_15391356336) 1 MPI process\n          type: seqaij\n          rows=961, cols=961\n          package used to perform factorization: petsc\n          total: nonzeros=32943, allocated nonzeros=32943\n            not using I-node routines\n  linear system matrix = precond matrix:\n  Mat Object: (dolfinx_solve_15391356336) 1 MPI process\n    type: seqaij\n    rows=961, cols=961\n    total: nonzeros=6481, allocated nonzeros=6481\n    total number of mallocs used during MatSetValues calls=0\n      not using I-node routines\n\n\nThis is a robust and straightforward method, and it is recommended for problems with up to a few thousand unknowns. It can be used efficiently for many 2D problems and smaller 3D problems. However, sparse LU decomposition quickly becomes inefficient, since for an \\(N \\times N\\) matrix the number of floating-point operations scales as \\(\\sim \\tfrac{2}{3}N^3\\)\nFor larger problems, we instead turn to iterative methods, which are faster and require less memory\nHow to choose a linear solver and preconditioner\nSince the Poisson equation leads to a symmetric, positive-definite system matrix, the optimal Krylov solver is the conjugate gradient (CG) method. By default, the preconditioner is incomplete LU factorization (ILU), a widely used and robust choice\nThe preconditioner can be changed by setting “pc_type” to any of the other PETSc preconditioners, as listed in the PETSc KSP solvers and PETSc preconditioners documentation\nAny PETSc option can be set through the petsc_options input, including the absolute tolerance (“ksp_atol”), relative tolerance (“ksp_rtol”), and maximum number of iterations (“ksp_max_it”)\n\ncg_problem = LinearProblem(\n  a, \n  L, \n  bcs=bcs,\n  petsc_options={\n    \"ksp_type\": \"cg\", \n    \"ksp_rtol\": 1e-6, \n    \"ksp_atol\": 1e-10, \n    \"ksp_max_it\": 1000\n  }\n)\nuh = cg_problem.solve()\n\ncg_solver = cg_problem.solver\nviewer = PETSc.Viewer().createASCII(str(results_folder/\"cg_output.txt\"))\ncg_solver.view(viewer)\nviewer.destroy()\n\nwith open(str(results_folder/\"cg_output.txt\"), \"r\") as solver_output:\n  for line in solver_output.readlines():\n      print(line.rstrip())    \n\nKSP Object: (dolfinx_solve_15391417360) 1 MPI process\n  type: cg\n  maximum iterations=1000, initial guess is zero\n  tolerances: relative=1e-06, absolute=1e-10, divergence=10000.\n  left preconditioning\n  using PRECONDITIONED norm type for convergence test\nPC Object: (dolfinx_solve_15391417360) 1 MPI process\n  type: ilu\n    out-of-place factorization\n    0 levels of fill\n    tolerance for zero pivot 2.22045e-14\n    matrix ordering: natural\n    factor fill ratio given 1., needed 1.\n      Factored matrix follows:\n        Mat Object: (dolfinx_solve_15391417360) 1 MPI process\n          type: seqaij\n          rows=961, cols=961\n          package used to perform factorization: petsc\n          total: nonzeros=6481, allocated nonzeros=6481\n            not using I-node routines\n  linear system matrix = precond matrix:\n  Mat Object: (dolfinx_solve_15391417360) 1 MPI process\n    type: seqaij\n    rows=961, cols=961\n    total: nonzeros=6481, allocated nonzeros=6481\n    total number of mallocs used during MatSetValues calls=0\n      not using I-node routines\n\n\nFor non-symmetric systems, it is preferable to use a Krylov solver designed for such problems, for example GMRES\n\ngmres_problem = LinearProblem(\n  a, \n  L, \n  bcs=bcs,\n  petsc_options={\n    \"ksp_type\": \"gmres\", \n    \"ksp_rtol\": 1e-6, \n    \"ksp_atol\": 1e-10, \n    \"ksp_max_it\": 1000, \n    \"pc_type\": \"none\"\n  }\n)\nuh = gmres_problem.solve()\n\ngmres_solver = gmres_problem.solver\nviewer = PETSc.Viewer().createASCII(str(results_folder/\"gmres_output.txt\"))\ngmres_solver.view(viewer)\nviewer.destroy()\n\nwith open(str(results_folder/\"gmres_output.txt\"), \"r\") as solver_output:\n  for line in solver_output.readlines():\n      print(line.rstrip())       \n\nKSP Object: (dolfinx_solve_15400916880) 1 MPI process\n  type: gmres\n    restart=30, using Classical (unmodified) Gram-Schmidt Orthogonalization with no iterative refinement\n    happy breakdown tolerance 1e-30\n  maximum iterations=1000, initial guess is zero\n  tolerances: relative=1e-06, absolute=1e-10, divergence=10000.\n  left preconditioning\n  using PRECONDITIONED norm type for convergence test\nPC Object: (dolfinx_solve_15400916880) 1 MPI process\n  type: none\n  linear system matrix = precond matrix:\n  Mat Object: (dolfinx_solve_15400916880) 1 MPI process\n    type: seqaij\n    rows=961, cols=961\n    total: nonzeros=6481, allocated nonzeros=6481\n    total number of mallocs used during MatSetValues calls=0\n      not using I-node routines\n\n\n\n\n\n\n\n\nNote\n\n\n\nWhen using manufactured solutions, we often expect the error to be close to machine precision. However, this becomes more complicated when iterative methods are employed. The key issue is ensuring that the error introduced by the iterative solver remains smaller than the tolerance used in the convergence test. For linear elements and small meshes, a tolerance in the range of \\(10^{-12}\\) to \\(10^{-14}\\) also works well when using Krylov solvers\n\n\n\n\nK.9.2 JIT options and visualization using Pandas\nAuthor: Jørgen S. Dokken\nIn this section, we explore how to optimize and inspect the integration kernels used in dolfinx. As seen in the previous demos, dolfinx uses the Unified Form Language (UFL) to describe variational problems\nThese UFL descriptions must be translated into code for assembling the right- and left-hand sides of the discrete variational problem\ndolfinx uses ffcx to generate efficient C code for assembling element matrices. This C code is then compiled via CFFI, and a variety of compilation options can be specified\nWe begin by specifying the current directory as the location to store the generated C files. The current directory is obtained using pathlib\n\nimport time\nfrom typing import Dict\nfrom pathlib import Path\n\nfrom mpi4py import MPI\nimport pandas as pd\n\nimport matplotlib.pyplot as plt\nimport seaborn\n\nfrom dolfinx.mesh import create_unit_cube\nfrom dolfinx.fem import functionspace, form\nfrom dolfinx.fem.petsc import assemble_matrix\n\nimport ufl\nfrom ufl import TestFunction, TrialFunction, dx, inner\n\ncache_dir = f\"{str(Path.cwd())}/.cache\"\nprint(f\"Directory to put C files in: {cache_dir}\")\n\nDirectory to put C files in: /Users/kyyoo/Documents/notebooks/SeoulTechPSE.github.io/.cache\n\n\nNext, we generate a general function to assemble the mass matrix for a unit cube. Note that we use dolfinx.fem.form to compile the variational form. For code that uses dolfinx.fem.petsc.LinearProblem, you can pass jit_options as a keyword argument\n\ndef compile_form(space: str, degree: int, jit_options: Dict):\n    N = 10\n    mesh = create_unit_cube(MPI.COMM_WORLD, N, N, N)\n    \n    V = functionspace(mesh, (space, degree))\n    u = TrialFunction(V)\n    v = TestFunction(V)\n    \n    a = inner(u, v) *dx\n    a_compiled = form(a, jit_options=jit_options)\n    \n    start = time.perf_counter()\n    assemble_matrix(a_compiled)\n    end = time.perf_counter()\n    \n    return end -start\n\nWe begin by examining the different levels of optimization that the C compiler can apply to the generated code. A list of available optimization options, along with their explanations, can be found here\n\noptimization_options = [\"-O1\", \"-O2\", \"-O3\", \"-Ofast\"]\n\nThe next option to consider is whether to compile the code with -march=native. This option enables CPU-specific instructions for the local machine, which can lead to different results on different systems. More information can be found here\n\nmarch_native = [True, False]\n\nWe select a subset of finite element spaces and vary the order of each space to examine how it affects the assembly time under different compilation option\n\nresults = {\"Space\": [], \"Degree\": [], \"Options\": [], \"Time\": []}\nfor space in [\"N1curl\", \"Lagrange\", \"RT\"]:\n  for degree in [1, 2, 3]:\n    for native in march_native:\n      for option in optimization_options:\n        if native:\n          cffi_options = [option, \"-march=native\"]\n        else:\n          cffi_options = [option]\n        jit_options = {\n          \"cffi_extra_compile_args\": cffi_options,\n          \"cache_dir\": cache_dir, \n          \"cffi_libraries\": [\"m\"]\n        }\n        \n        runtime = compile_form(space, degree, jit_options=jit_options)\n\n        results[\"Space\"].append(space)\n        results[\"Degree\"].append(str(degree))\n        results[\"Options\"].append(\"\\n\".join(cffi_options))\n        results[\"Time\"].append(runtime)\n\nWe have now stored all the results in a dictionary. To visualize them, we use pandas and its DataFrame class. In a Jupyter notebook, the data can be inspected as follows:\n\nresults_df = pd.DataFrame.from_dict(results)\nresults_df\n\n\n\n\n\n\n\n\nSpace\nDegree\nOptions\nTime\n\n\n\n\n0\nN1curl\n1\n-O1\\n-march=native\n0.016555\n\n\n1\nN1curl\n1\n-O2\\n-march=native\n0.014961\n\n\n2\nN1curl\n1\n-O3\\n-march=native\n0.014574\n\n\n3\nN1curl\n1\n-Ofast\\n-march=native\n0.015332\n\n\n4\nN1curl\n1\n-O1\n0.017051\n\n\n...\n...\n...\n...\n...\n\n\n67\nRT\n3\n-Ofast\\n-march=native\n0.453212\n\n\n68\nRT\n3\n-O1\n0.473607\n\n\n69\nRT\n3\n-O2\n0.519211\n\n\n70\nRT\n3\n-O3\n0.500359\n\n\n71\nRT\n3\n-Ofast\n0.476947\n\n\n\n\n72 rows × 4 columns\n\n\n\nWe can now create a plot for each element type to visualize how the results vary with different compilation options. A new column is created for each element type and degree\n\nseaborn.set(style=\"ticks\")\nseaborn.set_style(\"darkgrid\")\n\nresults_df[\"Element\"] = results_df[\"Space\"] +\" \" +results_df[\"Degree\"]\nhue_order = sorted(results_df[\"Space\"].unique())\nfor degree in [1, 2, 3]:\n  df_degree = results_df[results_df[\"Degree\"] == str(degree)]\n\n  g = seaborn.catplot(\n    x=\"Options\", \n    y=\"Time\", \n    hue=\"Space\",\n    hue_order=hue_order, \n    kind=\"bar\", \n    data=df_degree,\n    height=3,\n    aspect=4.0)\n  g.fig.suptitle(f\"Degree = {degree}\", y=1.02)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWe observe that the compile time increases as the degree of the function space grows, and that the greatest speedup is achieved when using -O3 or -Ofast in combination with -march=native\n\n\nK.9.3 Error control: Computing convergence rates\nAuthor: Jørgen S. Dokken, Hans Petter Langtangen, Anders Logg\nFor any numerical method, one of the most fundamental questions is its convergence rate: how fast the error decreases as the resolution is increased (i.e., as the mesh size is decreased)\nIn the finite element method, this typically involves proving—either theoretically or empirically—that the error \\(\\| u - u_h \\|\\) is bounded by a constant times the mesh size \\(h\\) raised to some power \\(p\\), that is,\n\\[\\| u - u_h \\| \\le C \\, h^p\\]\nfor some constant \\(C\\) independent of the mesh. The number \\(p\\) is called the convergence rate of the method. Note that different norms, such as the \\(L^2\\)-norm or the \\(H^1\\)-norm, generally yield different convergence rates\nComputing error norms\nWe first construct a manufactured problem based on the same configuration used in the solver\n\nimport numpy as np\nfrom mpi4py import MPI\n\nfrom dolfinx import default_scalar_type\nfrom dolfinx.mesh import create_unit_square, locate_entities_boundary\nfrom dolfinx.fem import (Expression, Function, functionspace,\n                         assemble_scalar, dirichletbc, form, locate_dofs_topological)\nfrom dolfinx.fem.petsc import LinearProblem\n\nimport ufl\nfrom ufl import (SpatialCoordinate, TestFunction, TrialFunction, \n                 div, dot, dx, grad, inner)\n\ndef u_ex(mod):\n  return lambda x: mod.cos(2 *mod.pi *x[0]) *mod.cos(2 *mod.pi *x[1])\n\nu_numpy = u_ex(np)\nu_ufl = u_ex(ufl)\n\ndef solve_poisson(N=10, degree=1):\n\n  mesh = create_unit_square(MPI.COMM_WORLD, N, N)\n    \n  x = SpatialCoordinate(mesh)\n  f = -div(grad(u_ufl(x)))\n    \n  V = functionspace(mesh, (\"Lagrange\", degree))\n  u = TrialFunction(V)\n  v = TestFunction(V)\n    \n  a = inner(grad(u), grad(v)) *dx\n  L = f *v *dx\n    \n  u_bc = Function(V)\n  u_bc.interpolate(u_numpy)\n    \n  facets = locate_entities_boundary(\n    mesh, \n    mesh.topology.dim -1, \n    lambda x: np.full(x.shape[1], True)\n  )\n  dofs = locate_dofs_topological(\n    V, \n    mesh.topology.dim -1, \n    facets\n  )\n  bcs = [dirichletbc(u_bc, dofs)]\n    \n  default_problem = LinearProblem(\n    a, \n    L, \n    bcs=bcs, \n    petsc_options={\"ksp_type\": \"preonly\", \"pc_type\": \"lu\"}\n  )\n\n  return default_problem.solve(), u_ufl(x)\n\nNow, we compute the error between the analytical solution \\(u_{\\text{ex}} = u_{\\text{ufl}}(x)\\) and the approximate solution \\(u_h\\). The \\(L^2\\)-norm of the error\n\\[\\| u_e - u_h \\|_{L^2(\\Omega)} = \\left( \\int_\\Omega |u_e - u_h|^2 \\, dx \\right)^{1/2}\\]\nmeasures the pointwise accuracy of the approximate solution in an average sense. It quantifies how close \\(u_h\\) is to \\(u_e\\) in terms of their values\n\nuh, u_ex = solve_poisson(10)\ncomm = uh.function_space.mesh.comm\n\nerror_L2 = form((uh -u_ex)**2 *ufl.dx)\nE_L2 = np.sqrt(comm.allreduce(assemble_scalar(error_L2), MPI.SUM))\n\nif comm.rank == 0:\n  print(f\"L2-error: {E_L2:.2e}\")\n\nL2-error: 5.28e-02\n\n\nSometimes it is of interest to compute the error of the gradient field, \\(\\| \\nabla (u_e - u_h) \\|\\), often referred to as the \\(H_0^1\\)-norm of the error. This can be expressed as\n\\[\\left( \\int_\\Omega \\lvert \\nabla (u_e - u_h) \\rvert^2 \\, dx \\right)^{1/2}\\]\nThe \\(H_0^1\\)-norm measures the error in the so-called energy space, which is particularly relevant for elliptic partial differential equations, as it reflects how well the numerical solution captures the energy of the exact solution\n\neh = uh -u_ex\nerror_H10 = form(dot(grad(eh), grad(eh)) *dx)\nE_H10 = np.sqrt(comm.allreduce(assemble_scalar(error_H10), op=MPI.SUM))\n\nif comm.rank == 0:\n  print(f\"H01-error: {E_H10:.2e}\")\n\nH01-error: 1.36e+00\n\n\nReliable error norm computation\nHowever, expanding the expression gives\n\\[(u_{\\text{ex}} - u_h)^2 = u_{\\text{ex}}^2 + u_h^2 - 2u_{\\text{ex}}u_h\\]\nIf the error is small (while the solutions themselves are of moderate size), this calculation involves subtracting two quantities of order one—namely \\(u_{\\text{ex}}^2 + u_h^2 \\sim 1\\) and \\(2u_{\\text{ex}}u_h \\sim 1\\)—to obtain a much smaller number. This subtraction is prone to round-off errors\nTo avoid this issue, we interpolate both the approximate and the exact solutions into a higher-order function space. We then subtract their degrees of freedom to form an explicit error function. Finally, we assemble (integrate) the squared error and take the square root to obtain the \\(L^2\\)-norm\n\ndef error_L2(uh, u_ex, degree_raise=3):\n\n  # Create higher order function space\n  mesh = uh.function_space.mesh\n  degree = uh.function_space.ufl_element().degree\n  family = uh.function_space.ufl_element().family_name\n    \n  W = functionspace(mesh, (family, degree +degree_raise))\n    \n  # Interpolate approximate solution\n  u_W = Function(W)\n  u_W.interpolate(uh)\n\n  # Interpolate exact solution, special handling if exact solution\n  # is a ufl expression or a python lambda function\n  u_ex_W = Function(W)\n  if isinstance(u_ex, ufl.core.expr.Expr):\n    u_expr = Expression(u_ex, W.element.interpolation_points())\n    u_ex_W.interpolate(u_expr)\n  else:\n    u_ex_W.interpolate(u_ex)\n\n  # Compute the error in the higher order function space\n  e_W = Function(W)\n  e_W.x.array[:] = u_W.x.array -u_ex_W.x.array\n\n  # Integrate the error\n  error = form(ufl.inner(e_W, e_W) *ufl.dx)\n  error_local = assemble_scalar(error)\n  error_global = mesh.comm.allreduce(error_local, op=MPI.SUM)\n  \n  return np.sqrt(error_global)\n\nComputing convergence rates\nLet us consider a sequence of mesh resolutions\n\\[h_0 &gt; h_1 &gt; h_2 &gt; \\cdots, \\qquad h_i = \\frac{1}{N_i}\\]\nwhere \\(N_i\\) denotes the number of subdivisions (or degrees of freedom) in the discretization. We compute the numerical error for a range of values of \\(N_i\\)\n\nNs = [4, 8, 16, 32, 64]\nEs = np.zeros(len(Ns), dtype=default_scalar_type)\nhs = np.zeros(len(Ns), dtype=np.float64)\n\nfor i, N in enumerate(Ns):\n  uh, u_ex = solve_poisson(N, degree=1)\n  comm = uh.function_space.mesh.comm\n\n  # Accepts either u_numpy or u_ex\n  # Use u_numpy for L2 error (no JIT needed)\n  hs[i] = 1. /Ns[i]\n  Es[i] = error_L2(uh, u_numpy)\n  if comm.rank == 0:\n    print(f\"h: {hs[i]:.2e} Error: {Es[i]:.2e}\")\n\nh: 2.50e-01 Error: 2.43e-01\nh: 1.25e-01 Error: 7.96e-02\nh: 6.25e-02 Error: 2.15e-02\nh: 3.12e-02 Error: 5.47e-03\nh: 1.56e-02 Error: 1.37e-03\n\n\nIf we assume that the error \\(E_i\\) can be expressed as\n\\(E_i = C h_i^r\\)\nwith unknown constants \\(C\\) and \\(r\\), then by comparing two consecutive experiments\n\\[E_{i-1} = C h_{i-1}^r, \\qquad E_i = C h_i^r\\]\nwe can solve for \\(r\\):\n\\[r = \\frac{\\ln(E_i / E_{i-1})}{\\ln(h_i / h_{i-1})}\\]\nAs \\(i\\) increases, the computed values of \\(r\\) should approach the expected convergence rate, which in the case of the \\(L^2\\)-error is typically the polynomial degree plus one. This calculation can be implemented compactly using NumPy\n\nrates = np.log(Es[1:] /Es[:-1]) /np.log(hs[1:] /hs[:-1])\nif comm.rank == 0:\n  print(f\"Rates: {rates}\")\n\nRates: [1.61228695 1.89147568 1.97191247 1.99290382]\n\n\nWe perform a similar study for different polynomial orders to verify the previous claim\n\ndegrees = [1, 2, 3, 4]\nfor degree in degrees:\n  hs = np.zeros(len(Ns), dtype=np.float64)\n  Es = np.zeros(len(Ns), dtype=default_scalar_type)\n    \n  for i, N in enumerate(Ns):\n    uh, u_ex = solve_poisson(N, degree=degree)\n    comm = uh.function_space.mesh.comm\n\n    hs[i] = 1. /Ns[i]    \n    Es[i] = error_L2(uh, u_numpy, degree_raise=3)\n\n    if comm.rank == 0:\n      print(f\"h: {hs[i]:.2e} Error: {Es[i]:.2e}\")\n    \n  rates = np.log(Es[1:] /Es[:-1]) /np.log(hs[1:] /hs[:-1])\n  if comm.rank == 0:\n    print(f\"Polynomial degree {degree:d}, Rates {rates}\\n\")\n\nh: 2.50e-01 Error: 2.43e-01\nh: 1.25e-01 Error: 7.96e-02\nh: 6.25e-02 Error: 2.15e-02\nh: 3.12e-02 Error: 5.47e-03\nh: 1.56e-02 Error: 1.37e-03\nPolynomial degree 1, Rates [1.61228695 1.89147568 1.97191247 1.99290382]\n\nh: 2.50e-01 Error: 3.52e-02\nh: 1.25e-01 Error: 4.39e-03\nh: 6.25e-02 Error: 5.50e-04\nh: 3.12e-02 Error: 6.88e-05\nh: 1.56e-02 Error: 8.60e-06\nPolynomial degree 2, Rates [3.00109101 2.99828073 2.99822291 2.99930786]\n\nh: 2.50e-01 Error: 5.54e-03\nh: 1.25e-01 Error: 3.35e-04\nh: 6.25e-02 Error: 1.99e-05\nh: 3.12e-02 Error: 1.21e-06\nh: 1.56e-02 Error: 7.49e-08\nPolynomial degree 3, Rates [4.04795047 4.07357659 4.03728992 4.01645269]\n\nh: 2.50e-01 Error: 7.20e-04\nh: 1.25e-01 Error: 2.42e-05\nh: 6.25e-02 Error: 7.75e-07\nh: 3.12e-02 Error: 2.44e-08\nh: 1.56e-02 Error: 7.64e-10\nPolynomial degree 4, Rates [4.896908   4.96084158 4.98926012 4.99766448]\n\n\n\nInfinity norm estimates\nWe first define a function to evaluate the infinity norm, i.e., the maximum pointwise error between the numerical and exact solutions\n\ndef error_infinity(u_h, u_ex):\n  # Interpolate exact solution (UFL or lambda handled separately)\n  u_ex_V = Function(u_h.function_space)\n  comm = u_h.function_space.mesh.comm\n    \n  if isinstance(u_ex, ufl.core.expr.Expr):\n    u_expr = Expression(u_ex, u_h.function_space.element.interpolation_points())\n    u_ex_V.interpolate(u_expr)\n  else:\n    u_ex_V.interpolate(u_ex)\n\n  # Compute global infinity norm from local maxima  \n  error_max_local = np.max(np.abs(u_h.x.array -u_ex_V.x.array))\n  error_max = comm.allreduce(error_max_local, op=MPI.MAX)\n\n  return error_max\n\nPerforming this procedure for various polynomial degrees produces the following results:\n\nfor degree in degrees:\n  hs = np.zeros(len(Ns), dtype=np.float64)    \n  Es = np.zeros(len(Ns), dtype=default_scalar_type)\n\n  for i, N in enumerate(Ns):\n    uh, u_ex = solve_poisson(N, degree=degree)\n    comm = uh.function_space.mesh.comm\n\n    hs[i] = 1. /Ns[i]    \n    Es[i] = error_infinity(uh, u_numpy)\n\n    if comm.rank == 0:\n       print(f\"h: {hs[i]:.2e} Error: {Es[i]:.2e}\")\n    \n  rates = np.log(Es[1:] /Es[:-1]) /np.log(hs[1:] /hs[:-1])\n  if comm.rank == 0:\n    print(f\"Polynomial degree {degree:d}, Rates {rates}\\n\")\n\nh: 2.50e-01 Error: 2.73e-01\nh: 1.25e-01 Error: 6.96e-02\nh: 6.25e-02 Error: 1.75e-02\nh: 3.12e-02 Error: 4.38e-03\nh: 1.56e-02 Error: 1.10e-03\nPolynomial degree 1, Rates [1.96828918 1.9917697  1.99791282 1.99947611]\n\nh: 2.50e-01 Error: 4.85e-02\nh: 1.25e-01 Error: 3.65e-03\nh: 6.25e-02 Error: 2.37e-04\nh: 3.12e-02 Error: 1.50e-05\nh: 1.56e-02 Error: 9.38e-07\nPolynomial degree 2, Rates [3.73213705 3.94227924 3.98612529 3.99656575]\n\nh: 2.50e-01 Error: 1.08e-02\nh: 1.25e-01 Error: 8.13e-04\nh: 6.25e-02 Error: 5.86e-05\nh: 3.12e-02 Error: 3.80e-06\nh: 1.56e-02 Error: 2.41e-07\nPolynomial degree 3, Rates [3.72903577 3.79406598 3.94800701 3.97888408]\n\nh: 2.50e-01 Error: 1.63e-03\nh: 1.25e-01 Error: 5.23e-05\nh: 6.25e-02 Error: 1.67e-06\nh: 3.12e-02 Error: 5.25e-08\nh: 1.56e-02 Error: 1.64e-09\nPolynomial degree 4, Rates [4.96133103 4.96807694 4.99161588 4.99787562]\n\n\n\nWe observe superconvergence for second-order polynomials, resulting in an observed fourth-order convergence\n\n\nK.9.4 Custom Newton solvers\nAuthor: Jørgen S. Dokken\nNewton’s Method, as used in the non-linear Poisson problem, is a technique for solving a non-linear equation by iteratively solving a sequence of linear equations. Given a function \\(F:\\mathbb{R}^M \\to \\mathbb{R}^M\\), the iterates \\(u_k, u_{k+1} \\in \\mathbb{R}^M\\) satisfy\n\\[u_{k+1} = u_k - J_F(u_k)^{-1} F(u_k)\\]\nwhere \\(J_F(u_k)\\) is the Jacobian matrix of \\(F\\) at \\(u_k\\)\nIntroducing the increment \\(\\delta u_k = u_{k+1} - u_k\\), this can be rewritten as the linear system\n\\[J_F(u_k) \\, \\delta u_k = - F(u_k)\\]\nwith the update\n\\[u_{k+1} = u_k + \\delta u_k\\]\nProblem specification\nWe start by importing the required libraries\n\nimport numpy as np\nfrom mpi4py import MPI\nfrom petsc4py import PETSc\n\nimport matplotlib.pyplot as plt\nimport pyvista\n\nimport dolfinx\nimport dolfinx.fem.petsc\n\nimport ufl\n\nplt.style.use('default')\n\nWe consider the following non-linear problem:\n\\[u^2 - 2u = x^2 + 4x + 3 \\quad \\text{for } x \\in [0,1]\\]\nThis problem has two solutions: \\(u = -x - 1\\) and \\(u = x + 3\\). We define these roots as Python functions and create an appropriate array of points for plotting the solutions\n\ndef root_0(x):\n  return 3 +x[0]\n\ndef root_1(x):\n  return -1 -x[0]\n\nN = 10\nx_spacing = np.linspace(0, 1, N)\nroots = [root_0, root_1]\n\nStarting with the initial guess \\(u_0 = 0\\), we then define the mesh, the function space, and the function uh for the numerical solution\n\nmesh = dolfinx.mesh.create_unit_interval(MPI.COMM_WORLD, N)\n\nV = dolfinx.fem.functionspace(mesh, (\"Lagrange\", 1))\nuh = dolfinx.fem.Function(V)\n\nDefinition of residual and Jacobian\nWe then formulate the variational problem by multiplying with a test function and integrating over the domain \\([0,1]\\)\n\nv = ufl.TestFunction(V)\n\nx = ufl.SpatialCoordinate(mesh)\nF = uh**2 *v *ufl.dx -2 *uh *v *ufl.dx -(x[0]**2 +4 *x[0] +3) *v *ufl.dx\nresidual = dolfinx.fem.form(F)\n\nThe Jacobian \\(J_F\\) is then obtained by applying ufl.derivative to the variational form\n\nJ = ufl.derivative(F, uh)\njacobian = dolfinx.fem.form(J)\n\nAs the problem will be solved iteratively, the sparse matrix and residual vector are assembled only once\nSetup of iteration-independent structures\n\nA = dolfinx.fem.petsc.create_matrix(jacobian)\nL = dolfinx.fem.petsc.create_vector(residual)\n\nWe then set up the linear solver together with a vector to hold the update du\n\nsolver = PETSc.KSP().create(mesh.comm)\nsolver.setOperators(A)\n\ndu = dolfinx.fem.Function(V)\n\nTo monitor the evolution of uh during the iterations, we obtain the DoF coordinates and sort them in ascending order\n\ncoords = V.tabulate_dof_coordinates()[:, 0]\nsort_order = np.argsort(coords)\n\nmax_iterations = 25\nsolutions = np.zeros((max_iterations +1, len(coords)))\nsolutions[0] = uh.x.array[sort_order]\n\nAt this stage, we are ready to solve the linearized problem. For each iteration, the Jacobian and residual are reassembled, and the norm of the update (dx) is used as the stopping criterion\n\ni = 0\nwhile i &lt; max_iterations:\n  # Assemble Jacobian and residual\n  with L.localForm() as loc_L:\n    loc_L.set(0)\n  A.zeroEntries()\n  dolfinx.fem.petsc.assemble_matrix(A, jacobian)\n  A.assemble()\n  dolfinx.fem.petsc.assemble_vector(L, residual)\n  L.ghostUpdate(addv=PETSc.InsertMode.ADD_VALUES, mode=PETSc.ScatterMode.REVERSE)\n\n  # Scale residual by -1\n  L.scale(-1)\n  L.ghostUpdate(addv=PETSc.InsertMode.INSERT_VALUES, mode=PETSc.ScatterMode.FORWARD)\n\n  # Solve linear problem\n  solver.solve(L, du.x.petsc_vec)\n  du.x.scatter_forward()\n  \n  # Update u_{i+1} = u_i + delta u_i\n  uh.x.array[:] += du.x.array\n  i += 1\n\n  # Compute norm of update\n  correction_norm = du.x.petsc_vec.norm(0)  # L^2 norm\n  print(f\"Iteration {i}: {correction_norm = :.6e}\")\n  if correction_norm &lt; 1e-10:\n    break\n  solutions[i, :] = uh.x.array[sort_order]\n\nIteration 1: correction_norm = 2.941583e+01\nIteration 2: correction_norm = 1.077679e+01\nIteration 3: correction_norm = 2.048893e+00\nIteration 4: correction_norm = 8.991947e-02\nIteration 5: correction_norm = 2.277570e-04\nIteration 6: correction_norm = 2.211497e-09\nIteration 7: correction_norm = 1.123544e-15\n\n\nAt this point, we compute the residual magnitude to monitor convergence\n\ndolfinx.fem.petsc.assemble_vector(L, residual)\nprint(f\"Final residual = {L.norm(0):.6e}\")\n\nFinal residual = 4.859835e-16\n\n\nVisualization of Newton iterations\nWe now study how the solution evolves and quantify its error with respect to the two exact roots of the problem\n\n# Plot solution for each of the iterations\nfig = plt.figure(figsize=(8, 6))\n\nfor j, solution in enumerate(solutions[:i]):\n  plt.plot(coords[sort_order], solution, label=f\"Iteration {j}\")\n\n# Plot each of the roots of the problem, \n# and compare the approximate solution with each of them\nfor j, root in enumerate(roots):\n  u_ex = root(x)\n  \n  L2_error = dolfinx.fem.form(ufl.inner(uh -u_ex, uh -u_ex) *ufl.dx)\n  global_L2 = mesh.comm.allreduce(\n    dolfinx.fem.assemble_scalar(L2_error), \n    op=MPI.SUM\n  )\n  print(f\"L2-error (root {j}) {np.sqrt(global_L2):.6e}\")\n\n  plt.plot(x_spacing, root(x_spacing.reshape(1, -1)), ':o', label=f'root_{j}')\n\nplt.xlabel('x')\nplt.ylabel('u')\nplt.grid()\nplt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n\nL2-error (root 0) 5.033223e+00\nL2-error (root 1) 1.110223e-16\n\n\n\n\n\n\n\n\n\nNewton’s method with DirichletBC\nIn the previous example, we did not incorporate Dirichlet boundary conditions. We now extend the formulation to the non-linear Poisson problem, where such boundary conditions play a central role. As a first step, we define the computational mesh, the analytical solution, and the corresponding forcing term f\n\ndef q(u):\n  return 1 +u**2\n\ndomain = dolfinx.mesh.create_unit_square(MPI.COMM_WORLD, 10, 10)\n\nx = ufl.SpatialCoordinate(domain)\nu_ufl = 1 +x[0] +2 *x[1]\nf = - ufl.div(q(u_ufl) *ufl.grad(u_ufl))\n\ndef u_exact(x):\n  return eval(str(u_ufl))\n\nNext, we specify the Dirichlet boundary condition bc, and define the variational residual F together with its Jacobian J\n\nV = dolfinx.fem.functionspace(domain, (\"Lagrange\", 1))\n\nu_D = dolfinx.fem.Function(V)\nu_D.interpolate(u_exact)\n\nfdim = domain.topology.dim -1\ndomain.topology.create_connectivity(fdim, fdim +1)\nboundary_facets = dolfinx.mesh.exterior_facet_indices(domain.topology)\nbc = dolfinx.fem.dirichletbc(\n  u_D, \n  dolfinx.fem.locate_dofs_topological(V, fdim, boundary_facets)\n)\n\nuh = dolfinx.fem.Function(V)\nv = ufl.TestFunction(V)\n\nF = q(uh) *ufl.dot(ufl.grad(uh), ufl.grad(v)) *ufl.dx -f *v *ufl.dx\nJ = ufl.derivative(F, uh)\n\nresidual = dolfinx.fem.form(F)\njacobian = dolfinx.fem.form(J)\n\nWe then set up the system matrix A, the right-hand side vector L, and the update function du\n\ndu = dolfinx.fem.Function(V)\n\nA = dolfinx.fem.petsc.create_matrix(jacobian)\nL = dolfinx.fem.petsc.create_vector(residual)\n\nsolver = PETSc.KSP().create(mesh.comm)\nsolver.setOperators(A)\n\nSince this problem has strong Dirichlet boundary conditions, we need to apply a lifting to the right-hand side of our Newton problem. Recall that our goal is to solve the system\n\\[\\begin{aligned}\nJ_F(u_k)\\,\\delta u_k &= - F(u_k),\\\\\nu_{k+1} &= u_k + \\delta u_k\n\\end{aligned}\\]\nWe require that\n\\[u_{k+1}\\vert_{\\text{bc}} = u_D\\]\nHowever, we do not know if the current iterate satisfies the boundary condition, i.e., whether\n\\[u_k\\vert_{\\text{bc}} = u_D\\]\nTo enforce the boundary condition, we therefore apply the following condition on the Newton correction \\(\\delta u_k\\):\n\\[\\delta u_k \\vert_{\\text{bc}} = u_D - u_k \\vert_{\\text{bc}}\\]\nThis leads to the following Newton scheme with strong Dirichlet conditions:\n\\[\n\\begin{aligned}\nJ_F(u_k)\\,\\delta u_k &= -F(u_k), & \\delta u_k \\vert_{\\text{bc}} &= u_D - u_k\\vert_{\\text{bc}},\\\\\nu_{k+1} &= u_k + \\delta u_k\n\\end{aligned}\\]\n\ni = 0\nerror = dolfinx.fem.form(\n  ufl.inner(uh -u_ufl, uh -u_ufl) *ufl.dx(metadata={\"quadrature_degree\": 4})\n)\nL2_error = []\ndu_norm = []\nwhile i &lt; max_iterations:\n  # Assemble Jacobian and residual\n  with L.localForm() as loc_L:\n    loc_L.set(0)\n  A.zeroEntries()\n  dolfinx.fem.petsc.assemble_matrix(A, jacobian, bcs=[bc])\n  A.assemble()\n  dolfinx.fem.petsc.assemble_vector(L, residual)\n  L.ghostUpdate(addv=PETSc.InsertMode.ADD, mode=PETSc.ScatterMode.REVERSE)\n  L.scale(-1)\n\n  # Compute b - J(u_D-u_(i-1))\n  dolfinx.fem.petsc.apply_lifting(\n    L, \n    [jacobian], \n    [[bc]], \n    x0=[uh.x.petsc_vec], \n    alpha=1\n  )\n  \n  # Set du|_bc = u_{i-1}-u_D\n  dolfinx.fem.petsc.set_bc(L, [bc], uh.x.petsc_vec, 1.0)\n  L.ghostUpdate(addv=PETSc.InsertMode.INSERT_VALUES, mode=PETSc.ScatterMode.FORWARD)\n\n  # Solve linear problem\n  solver.solve(L, du.x.petsc_vec)\n  du.x.scatter_forward()\n\n  # Update u_{i+1} = u_i + delta u_i\n  uh.x.array[:] += du.x.array\n  i += 1\n\n  # Compute norm of update\n  correction_norm = du.x.petsc_vec.norm(0)\n\n  # Compute L2 error comparing to the analytical solution\n  L2_error.append(np.sqrt(mesh.comm.allreduce(dolfinx.fem.assemble_scalar(error), op=MPI.SUM)))\n  du_norm.append(correction_norm)\n\n  print(f\"Iteration {i}: {correction_norm = :.6e}, L2_error = {L2_error[-1]:.6e}\")\n  if correction_norm &lt; 1e-10:\n    break\n\nIteration 1: correction_norm = 2.174258e+02, L2_error = 1.009449e+00\nIteration 2: correction_norm = 1.546085e+02, L2_error = 1.025886e+00\nIteration 3: correction_norm = 4.927247e+01, L2_error = 3.541886e-01\nIteration 4: correction_norm = 1.695612e+01, L2_error = 7.129374e-02\nIteration 5: correction_norm = 3.166798e+00, L2_error = 4.565047e-03\nIteration 6: correction_norm = 1.713648e-01, L2_error = 2.626998e-05\nIteration 7: correction_norm = 8.143265e-04, L2_error = 1.230202e-09\nIteration 8: correction_norm = 3.734667e-08, L2_error = 7.046012e-15\nIteration 9: correction_norm = 5.469294e-13, L2_error = 2.797439e-16\n\n\nWe visualize the convergence by plotting the \\(L^2\\)-error and the residual norm (\\(\\delta u\\)) across iterations\n\nfig = plt.figure(figsize=(12, 4))\n\nplt.subplot(121)\nplt.semilogy(np.arange(i), L2_error, 'bo')\nplt.grid()\n\nplt.xlabel(\"Iterations\")\nplt.ylabel(r\"$L^2$ - error\")\n\nplt.subplot(122)\nplt.semilogy(np.arange(i), du_norm, 'ro')\nplt.grid()\n\nplt.xlabel(\"Iterations\")\nplt.ylabel(r\"$\\vert\\vert \\delta u\\vert\\vert$\");\n\n\n\n\n\n\n\n\nWe compute the maximum error and plot the solution\n\nerror_max = domain.comm.allreduce(\n  np.max(np.abs(uh.x.array -u_D.x.array)), \n  op=MPI.MAX\n)\n\nif domain.comm.rank == 0:\n  print(f\"Error_max = {error_max:.6e}\")\n\nError_max = 8.881784e-16\n\n\n\nfrom pathlib import Path\n\nresults_folder = Path(\"fenicsx/best_practices\")\nresults_folder.mkdir(exist_ok=True, parents=True)\n\nu_topology, u_cell_types, u_geometry = dolfinx.plot.vtk_mesh(V)\nu_grid = pyvista.UnstructuredGrid(u_topology, u_cell_types, u_geometry)\n\nu_grid.point_data[\"u\"] = uh.x.array.real\nu_grid.set_active_scalars(\"u\")\n\nu_plotter = pyvista.Plotter(off_screen=False)\nu_plotter.add_mesh(u_grid, show_edges=True)\nu_plotter.view_xy()\n\nif not pyvista.OFF_SCREEN:\n  #u_plotter.show()\n  u_plotter.export_html(results_folder/\"newton_solution.html\")",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>K</span>  <span class='chapter-title'>The FEniCS computing platform</span>"
    ]
  },
  {
    "objectID": "x_Mandelbrot_set.html",
    "href": "x_Mandelbrot_set.html",
    "title": "Appendix M — Mandelbrot Set in Python",
    "section": "",
    "text": "M.1 The Boundary of Iterative Stability\n\\(~\\)\nOriginal Author: Bartosz Zaczyński\n\\(~\\)\n\\[\\begin{aligned}\nz_0 &= 0\\\\\nz_{n+1} &= z_n^2 + c\n\\end{aligned}\\]\ndef sequence(c):\n  z = 0\n  while True:\n    yield z\n    z = z**2 +c\nfor n, z in enumerate(sequence(c=1)):\n  print(f'z({n}) = {z}')\n  if n &gt;= 9:\n    break\n\nz(0) = 0\nz(1) = 1\nz(2) = 2\nz(3) = 5\nz(4) = 26\nz(5) = 677\nz(6) = 458330\nz(7) = 210066388901\nz(8) = 44127887745906175987802\nz(9) = 1947270476915296449559703445493848930452791205\nfor n, z in enumerate(sequence(c=0)):\n  print(f'z({n}) = {z}')\n  if n &gt;= 9:\n    break\n\nz(0) = 0\nz(1) = 0\nz(2) = 0\nz(3) = 0\nz(4) = 0\nz(5) = 0\nz(6) = 0\nz(7) = 0\nz(8) = 0\nz(9) = 0\nfor n, z in enumerate(sequence(c=-1)):\n  print(f'z({n}) = {z}')\n  if n &gt;= 9:\n    break\n\nz(0) = 0\nz(1) = -1\nz(2) = 0\nz(3) = -1\nz(4) = 0\nz(5) = -1\nz(6) = 0\nz(7) = -1\nz(8) = 0\nz(9) = -1",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>M</span>  <span class='chapter-title'>Mandelbrot Set in Python</span>"
    ]
  },
  {
    "objectID": "x_Mandelbrot_set.html#the-boundary-of-iterative-stability",
    "href": "x_Mandelbrot_set.html#the-boundary-of-iterative-stability",
    "title": "Appendix M — Mandelbrot Set in Python",
    "section": "",
    "text": "Formally, \\(\\,\\)the  Mandelbrot set is the set of complex numbers, \\(\\color{red}{c}\\), for which an infinite sequence of numbers, \\(z_0\\), \\(z_1\\), \\(\\cdots\\), \\(z_n\\), \\(\\cdots\\), remains bounded\n\n\n\nThe entire Mandelbrot set fits in a circle with a radius of two when depicted on the complex plane. This is a handy fact that’ll let you skip many unnecessary calculations for points that certainly don’t belong to the set\n\n\n\n\nMost numbers will make this sequence diverge to infinity. \\(\\,\\)However, \\(\\,\\)some will keep it stable by either converging the sequence to a single value or staying within a bounded range. Others will make the sequence periodically stable by cycling back and forth between the same few values. Stable and periodically stable values make up the Mandelbrot set\n\n\n\n\nIt’s not obvious which numbers are stable and which aren’t, \\(\\,\\)because the formula is sensitive to even the smallest change of the tested value, \\(c\\)\nThe fractal corresponding to the Mandelbrot set has a finite area estimated at 1.506484 square units. Mathematicians haven’t pinpointed the exact number yet and don’t know whether it’s rational or not. On the other hand, the perimeter of the Mandelbrot set is infinite",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>M</span>  <span class='chapter-title'>Mandelbrot Set in Python</span>"
    ]
  },
  {
    "objectID": "x_Mandelbrot_set.html#plotting-the-mandelbrot-set-using-pythons-matplotlib",
    "href": "x_Mandelbrot_set.html#plotting-the-mandelbrot-set-using-pythons-matplotlib",
    "title": "Appendix M — Mandelbrot Set in Python",
    "section": "M.2 Plotting the Mandelbrot Set Using Python’s Matplotlib",
    "text": "M.2 Plotting the Mandelbrot Set Using Python’s Matplotlib\n\nTo generate the initial set of candidate values, \\(~\\)you can take advantage of np.linspace(), \\(\\,\\)which creates evenly spaced numbers in a given range:\n\n\nimport numpy as np\n#np.warnings.filterwarnings('ignore')\n\ndef complex_matrix(xmin, xmax, ymin, ymax, pixel_density):\n    \n  re = np.linspace(xmin, xmax, int((xmax -xmin) *pixel_density))\n  im = np.linspace(ymin, ymax, int((ymax -ymin) *pixel_density))\n\n  return re[np.newaxis, :] + im[:, np.newaxis] *1j\n\n\ndef is_stable(c, num_iterations):\n  z = 0\n  for _ in range(num_iterations):\n    z = z**2 +c\n  return abs(z) &lt;= 2\n\n\nM.2.1 Low-Resolution Scatter Plot\n\nimport matplotlib.pyplot as plt\n\nc = complex_matrix(-2, 0.5, -1.5, 1.5, pixel_density=21)\nmembers = c[is_stable(c, num_iterations=20)]\n\ndef plot_low_resolution_scatter():\n\n  plt.figure(figsize=(6, 8))\n  plt.scatter(members.real, members.imag, color='black', marker='x', s=1)\n    \n  plt.gca().set_aspect('equal')\n  plt.axis('off')\n  plt.tight_layout()\n\n  plt.show()\n\n/var/folders/4x/8kn2nym12cn7x7qmg_6s4b8h0000gn/T/ipykernel_76102/2353567406.py:4: RuntimeWarning: overflow encountered in square\n  z = z**2 +c\n/var/folders/4x/8kn2nym12cn7x7qmg_6s4b8h0000gn/T/ipykernel_76102/2353567406.py:4: RuntimeWarning: invalid value encountered in square\n  z = z**2 +c\n\n\n\nplot_low_resolution_scatter()\n\n\n\n\n\n\n\n\n\n\nM.2.2 High-Resolution Black-and-White Visualization\n\nc = complex_matrix(-2, 0.5, -1.5, 1.5, pixel_density=512)\n\ndef plot_high_resolution_black_and_white():\n  \n  plt.figure(figsize=(6, 8))\n  plt.imshow(is_stable(c, num_iterations=20), cmap='binary')\n  \n  plt.gca().set_aspect('equal')\n  plt.axis('off')\n  plt.tight_layout()\n\n  plt.show()\n\n\nplot_high_resolution_black_and_white()\n\n/var/folders/4x/8kn2nym12cn7x7qmg_6s4b8h0000gn/T/ipykernel_76102/2353567406.py:4: RuntimeWarning: overflow encountered in square\n  z = z**2 +c\n/var/folders/4x/8kn2nym12cn7x7qmg_6s4b8h0000gn/T/ipykernel_76102/2353567406.py:4: RuntimeWarning: invalid value encountered in square\n  z = z**2 +c",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>M</span>  <span class='chapter-title'>Mandelbrot Set in Python</span>"
    ]
  },
  {
    "objectID": "x_Mandelbrot_set.html#drawing-the-mandelbrot-set-with-pillow",
    "href": "x_Mandelbrot_set.html#drawing-the-mandelbrot-set-with-pillow",
    "title": "Appendix M — Mandelbrot Set in Python",
    "section": "M.3 Drawing the Mandelbrot Set With Pillow",
    "text": "M.3 Drawing the Mandelbrot Set With Pillow\n\nBy replacing Matplotlib’s plt.imshow() with a very similar call to Pillow’s factory method:\n   image = Image.fromarray(~is_stable(c, num_iterations=20))\n   # image.show()  # for console\n   display(image)  # for jupyter notebook\nNotice the use of the bitwise not operator (~) in front of your stability matrix, \\(\\,\\)which inverts all of the Boolean values. \\(\\,\\)This is so that the Mandelbrot set appears in black on a white background since Pillow assumes a black background by default\n\n\nM.3.1 Finding Convergent Elements of the Set\n\nfrom dataclasses import dataclass\n\n@dataclass\nclass MandelbrotSet:\n  \n  max_iterations: int\n\n  def __contains__(self, c: complex) -&gt; bool:\n    z = 0\n    for _ in range(self.max_iterations):\n      z = z**2 +c\n      if abs(z) &gt; 2:\n        return False     \n    return True\n\n\nmandelbrot_set = MandelbrotSet(max_iterations=30)\n\n\n0.26 in mandelbrot_set\n\nFalse\n\n\n\n0.25 in mandelbrot_set\n\nTrue\n\n\n\n\nfrom PIL import Image\n\nwidth, height = 512, 512\nscale = 0.0055\nBLACK_AND_WHITE = '1'\n\nimage = Image.new(mode=BLACK_AND_WHITE, size=(width, height))\n\nfor y in range(height):\n  for x in range(width):\n    c = scale *complex(x -width /1.35, height /2 -y)\n    image.putpixel((x, y), c not in mandelbrot_set)\n\n\ndisplay(image)\n\n\n\n\n\n\n\n\n\n\nM.3.2 Measuring Divergence With the Escape Count\n\nThe number of iterations it takes to detect divergence is known as the escape count. \\(\\,\\)We can use the escape count to introduce multiple levels of gray\nHowever, \\(\\,\\)it’s usually more convenient to deal with normalized escape counts so that their values are on a scale from zero to one regardless of the maximum number of iterations\n\n\n@dataclass\nclass MandelbrotSet:\n\n  max_iterations: int\n\n  def __contains__(self, c: complex) -&gt; bool:\n    return self.stability(c) == 1\n\n  def stability(self, c: complex) -&gt; float:\n    return self.escape_count(c) /self.max_iterations\n\n  def escape_count(self, c: complex) -&gt; int:\n    z = 0\n    for iteration in range(self.max_iterations):\n      z = z**2 +c\n      if abs(z) &gt; 2:\n        return iteration\n    return self.max_iterations\n\n\nmandelbrot_set = MandelbrotSet(max_iterations=30)\n\n\nmandelbrot_set.escape_count(0.25)\n\n30\n\n\n\nmandelbrot_set.stability(0.25)\n\n1.0\n\n\n\n0.25 in mandelbrot_set\n\nTrue\n\n\n\nmandelbrot_set.escape_count(0.26)\n\n29\n\n\n\nmandelbrot_set.stability(0.26)\n\n0.9666666666666667\n\n\n\n0.26 in mandelbrot_set\n\nFalse\n\n\n\nThe updated implementation of the MandelbrotSet class allows for a grayscale visualization, which ties pixel intensity with stability\nBut you’ll need to change the pixel mode to L, \\(\\,\\)which stands for luminance. \\(\\,\\)In this mode, \\(\\,\\)each pixel takes an integer value between 0 and 255, \\(\\,\\)so you’ll also need to scale the fractional stability appropriately:\n\n\nwidth, height = 512, 512\nscale = 0.0055\nGRAYSCALE = 'L'\n\nimage = Image.new(mode=GRAYSCALE, size=(width, height))\n\nfor y in range(height):\n  for x in range(width):\n    c = scale *complex(x -width /1.35, height /2 -y)\n    instability = 1 -mandelbrot_set.stability(c)\n    image.putpixel((x, y), int(instability *255))\n\n\ndisplay(image)\n\n\n\n\n\n\n\n\n\n\nM.3.3 Smoothing Out the Banding Artifacts\n\nGetting rid of color banding from the Mandelbrot set’s exterior boils down to using fractional escape counts.\nOne way to interpolate their intermediate values is to use logarithms\n\n\nfrom math import log\n\n@dataclass\nclass MandelbrotSet:\n  \n  max_iterations: int\n  escape_radius: float = 2.0\n\n  def __contains__(self, c: complex) -&gt; bool:\n    return self.stability(c) == 1\n    \n  def stability(self, c: complex, smooth=False, clamp=True) -&gt; float:\n    value = self.escape_count(c, smooth) /self.max_iterations\n    return max(0.0, min(value, 1.0)) if clamp else value    \n\n  def escape_count(self, c: complex, smooth=False) -&gt; int or float:\n    z = 0\n    for iteration in range(self.max_iterations):\n      z = z**2 +c\n      if abs(z) &gt; self.escape_radius:\n        if smooth:\n          return iteration +1 -log(log(abs(z))) /log(2)\n        return iteration\n    return self.max_iterations\n\n\nmandelbrot_set = MandelbrotSet(max_iterations=20, escape_radius=1000.0)\n\nwidth, height = 512, 512\nscale = 0.0055\nGRAYSCALE = 'L'\n\nimage = Image.new(mode=GRAYSCALE, size=(width, height))\nfor y in range(height):\n  for x in range(width):\n    c = scale *complex(x -width /1.35, height /2 -y)\n    instability = 1 -mandelbrot_set.stability(c, smooth=True)\n    image.putpixel((x, y), int(instability *255))\n\n\ndisplay(image)\n\n\n\n\n\n\n\n\n\n\nM.3.4 Translating Between Set Elements and Pixels\n\nUnlike the logarithms before, \\(\\,\\)the math for scaling and translating the image isn’t terribly difficult. However, \\(\\,\\)it adds a bit of code complexity\nYou can build a smart pixel data type that’ll encapsulate the conversion between the coordinate systems, account for scaling, and handle the colors\n\n\n@dataclass\nclass Viewport:\n  \n  image: Image.Image\n  center: complex\n  width: float\n        \n  @property\n  def scale(self):\n    return self.width /self.image.width        \n\n  @property\n  def height(self):\n    return self.scale *self.image.height\n\n  @property\n  def offset(self):\n    return self.center +complex(-self.width, self.height) /2\n\n  def __iter__(self):\n    for y in range(self.image.height):\n      for x in range(self.image.width):\n        yield Pixel(self, x, y)\n\n\n@dataclass\nclass Pixel:\n\n  viewport: Viewport\n  x: int\n  y: int\n\n  @property\n  def color(self):\n    return self.viewport.image.getpixel((self.x, self.y))\n\n  @color.setter\n  def color(self, value):\n    self.viewport.image.putpixel((self.x, self.y), value)\n\n  def __complex__(self):\n    return complex(self.x, -self.y) *self.viewport.scale +self.viewport.offset\n\n\nmandelbrot_set = MandelbrotSet(max_iterations=256, escape_radius=1000.0)\n\nimage = Image.new(mode='L', size=(512, 512))\nfor pixel in Viewport(image, center=-0.7435 +0.1314j, width=0.002):\n  c = complex(pixel)\n  instability = 1 -mandelbrot_set.stability(c, smooth=True)\n  pixel.color = int(instability *255)\n\n\nThe viewport spans 0.002 world units and is centered at -0.7435 +0.1314j, \\(\\,\\)which is close to a Misiurewicz point that produces a beautiful spiral\n\n\ndisplay(image)\n\n\n\n\n\n\n\n\n\nfrom PIL import ImageEnhance\n\nenhancer = ImageEnhance.Brightness(image)\ndisplay(enhancer.enhance(1.4))\n\n\n\n\n\n\n\n\n\nWe can find many more unique points producing such spectacular results. \\(\\,\\)Wikipedia hosts an entire image gallery of various details of the Mandelbrot set that are worth exploring\n\n\nmandelbrot_set = MandelbrotSet(max_iterations=256, escape_radius=1000.0)\n\nimage = Image.new(mode='L', size=(512, 512))\nfor pixel in Viewport(image, center=-0.74364990 +0.13188204j, width=0.00073801):\n  c = complex(pixel)\n  instability = 1 -mandelbrot_set.stability(c, smooth=True)\n  pixel.color = int(instability *255)\n\n\nenhancer = ImageEnhance.Brightness(image)\ndisplay(enhancer.enhance(1.4))",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>M</span>  <span class='chapter-title'>Mandelbrot Set in Python</span>"
    ]
  },
  {
    "objectID": "x_Mandelbrot_set.html#making-an-artistic-representation-of-the-mandelbrot-set",
    "href": "x_Mandelbrot_set.html#making-an-artistic-representation-of-the-mandelbrot-set",
    "title": "Appendix M — Mandelbrot Set in Python",
    "section": "M.4 Making an Artistic Representation of the Mandelbrot Set",
    "text": "M.4 Making an Artistic Representation of the Mandelbrot Set\n\nWhile there are many algorithms for plotting the Mandelbrot set in aesthetically pleasing ways, \\(\\,\\)our imagination is the only limit!\n\n\nM.4.1 Color Palette\n\nTo use more colors, \\(\\,\\)you’ll need to create your image in the RGB mode first, \\(\\,\\)which will allocate 24 bits per pixel:\n   image = Image.new(mode='RGB', size=(width, height))\nFrom now on, \\(\\,\\)Pillow will represent every pixel as a tuple comprised of the red, green, and blue (RGB) color channels\nMatplotlib library includes several colormaps with normalized color channels. \\(\\,\\)Some colormaps are fixed lists of colors, \\(\\,\\)while others are able to interpolate values given as a parameter\n\n\nimport matplotlib as mpl\n\ncmap = mpl.colormaps['twilight']\ncolormap = cmap(np.linspace(0, 1, 256))\ncolormap[:5]\n\narray([[0.88575016, 0.85000925, 0.88797365, 1.        ],\n       [0.88172231, 0.85127594, 0.88638057, 1.        ],\n       [0.87724881, 0.85187028, 0.8843412 , 1.        ],\n       [0.87233134, 0.85180165, 0.88189704, 1.        ],\n       [0.86696016, 0.85108961, 0.87909767, 1.        ]])\n\n\n\nPillow only understands integers in the range of 0 through 255 for the color channels. \\(\\,\\)We need another function that’ll reverse the normalization process to make the Pillow library happy:\n\n\ndef denormalize(colormap):\n  return [tuple(int(channel *255) for channel in color) for color in colormap]\n\npalette = denormalize(colormap)\npalette[:5] \n\n[(225, 216, 226, 255),\n (224, 217, 226, 255),\n (223, 217, 225, 255),\n (222, 217, 224, 255),\n (221, 217, 224, 255)]\n\n\n\nThe twilight colormap is a list of 510 colors. \\(\\,\\)After calling denormalize() on it, \\(\\,\\)you’ll get a color palette suitable for your painting function\nIf you’d like to test out a couple of different palettes, \\(\\,\\)then it might be convenient to introduce a helper function to avoid retyping the same commands over and over again:\n\n\ndef paint(mandelbrot_set, viewport, palette, smooth):\n  for pixel in viewport:\n    stability = mandelbrot_set.stability(complex(pixel), smooth)\n    index = int(min(stability *len(palette), len(palette) -1))\n    pixel.color = palette[index % len(palette)]\n\n\nThe number of colors in your palette doesn’t necessarily have to equal the maximum number of iterations. \\(\\,\\)After all, it’s unknown how many stability values there’ll be until we run the recursive formula. \\(\\,\\)When we enable smoothing, \\(\\,\\)the number of fractional escape counts can be greater than the number of iterations!\n\n\nmandelbrot_set = MandelbrotSet(max_iterations=512, escape_radius=1000.0)\nimage = Image.new(mode='RGB', size=(512, 512))\nviewport = Viewport(image, center=-0.7435 +0.1314j, width=0.002)\npaint(mandelbrot_set, viewport, palette, smooth=True)\n\ndisplay(image)\n\n\n\n\n\n\n\n\n\nFeel free to try other color palettes included in Matplotlib or one of the third-party libraries that they mention in the documentation. \\(\\,\\)Additionally, \\(\\,\\)Matplotlib lets you reverse the color order by appending the _r suffix to a colormap’s name\n\n\ncmap = mpl.colormaps['twilight_r']\ncolormap = cmap(np.linspace(0, 1, 256))\n\npalette = denormalize(colormap)\npaint(mandelbrot_set, viewport, palette, smooth=True)\n\ndisplay(image)\n\n\n\n\n\n\n\n\n\nimage = Image.new(mode='RGB', size=(768, 768))\nviewport = Viewport(image, center=-0.743643135 +0.131825963j, width= 0.000014628)\n\ncmap = mpl.colormaps['plasma']\ncolormap = cmap(np.linspace(0, 1, 256))\n\npalette = denormalize(colormap)\npaint(mandelbrot_set, viewport, palette, smooth=True)\n\ndisplay(image)\n\n\n\n\n\n\n\n\n\nSuppose you wanted to emphasize the fractal’s edge. In such a case, you can divide the fractal into three parts and assign different colors to each:\n\n\nexterior = [(1, 1, 1)] *50\ninterior = [(1, 1, 1)] *5\ngray_area = [(1 - i /44,) *3 for i in range(45)]\npalette = denormalize(exterior +gray_area +interior)\n\nmandelbrot_set = MandelbrotSet(max_iterations=20, escape_radius=1000.0)\nviewport = Viewport(image, center=-0.75, width=2.5)\n\npaint(mandelbrot_set, viewport, palette, smooth=True)\n\ndisplay(image)\n\n\n\n\n\n\n\n\n\n\nM.4.2 Color Gradient\n\nimport numpy as np\nfrom scipy.interpolate import interp1d\n\ndef make_gradient(colors, interpolation='linear'):\n\n  X = [i /(len(colors) -1) for i in range(len(colors))]\n  Y = [[color[i] for color in colors] for i in range(3)]\n  channels = [interp1d(X, y, kind=interpolation) for y in Y]\n  \n  return lambda x: [np.clip(channel(x), 0, 1) for channel in channels]\n\n\nblack  = (0, 0, 0)\nblue   = (0, 0, 1)\nmaroon = (0.5, 0, 0)\nnavy   = (0, 0, 0.5)\nred    = (1, 0, 0)\n\ncolors = [black, navy, blue, maroon, red, black]\ngradient = make_gradient(colors, interpolation='cubic')\n\n\nnum_colors = 256\npalette = denormalize([gradient(i /num_colors) for i in range(num_colors)])\n\n\nimage = Image.new(mode='RGB', size=(768, 768))\nmandelbrot_set = MandelbrotSet(max_iterations=20, escape_radius=1000)\nviewport = Viewport(image, center=-0.75, width=2.5)\n\npaint(mandelbrot_set, viewport, palette, smooth=True)\n\ndisplay(image)\n\n\n\n\n\n\n\n\n\n\nM.4.3 Color Model\n\nThere are alternative color models that let you express the same concept. \\(\\,\\)One is the Hue, Saturation, Brightness (HSB) color model, also known as Hue, Saturation, Value (HSV)\n\n\n\n\nHue Saturation Brightness Cylinder\n\n\n\nThe three HSB coordinates are:\n\nHue: The angle measured counterclockwise between 0° and 360°\nSaturation: The radius of the cylinder between 0% and 100%\nBrightness: The height of the cylinder between 0% and 100%\n\nTo use such coordinates in Pillow, \\(~\\)we must translate them to a tuple of RGB values in the familiar range of 0 to 255:\n\n\nfrom PIL.ImageColor import getrgb\n\ndef hsb(hue_degrees: int, saturation: float, brightness: float):\n  return getrgb( \n    f\"hsv({hue_degrees % 360},\"\n    f\"{saturation *100}%,\"\n    f\"{brightness *100}%)\"\n  )\n\nimage = Image.new(mode='RGB', size=(768, 768))\nmandelbrot_set = MandelbrotSet(max_iterations=20, escape_radius=1000.0)\n\nfor pixel in Viewport(image, center=-0.75, width=2.5):\n  stability = mandelbrot_set.stability(complex(pixel), smooth=True)\n  pixel.color = (0, 0, 0) if stability == 1 else hsb(\n    hue_degrees=int((1 - stability) * 360),\n    saturation=1 - stability,\n    brightness=1,\n  )\n\ndisplay(image)",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>M</span>  <span class='chapter-title'>Mandelbrot Set in Python</span>"
    ]
  },
  {
    "objectID": "x_Mandelbrot_set.html#conclusions",
    "href": "x_Mandelbrot_set.html#conclusions",
    "title": "Appendix M — Mandelbrot Set in Python",
    "section": "M.5 Conclusions",
    "text": "M.5 Conclusions\n\nIn this appendix, \\(\\,\\)we learned how to:\n\nApply complex numbers to a practical problem\nFind members of the Mandelbrot set\nDraw these sets as fractals using Matplotlib and Pillow\nMake a colorful artistic representation of the fractals\n\nNow we know how to use Python to plot and draw the famous fractal discovered by Benoît Mandelbrot",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>M</span>  <span class='chapter-title'>Mandelbrot Set in Python</span>"
    ]
  },
  {
    "objectID": "x_matplotlib.html",
    "href": "x_matplotlib.html",
    "title": "Appendix L — Matplotlib: Plotting and Visualization",
    "section": "",
    "text": "L.1 Importing matplotlib\nimport numpy as np\nimport sympy\n\nimport matplotlib as mpl\nimport matplotlib.pyplot as plt\nmpl.rcParams['font.size'] = 12.0\nprint('matplotlib:', mpl.__version__)\nprint('numpy:     ', np.__version__)\nprint('sympy:     ', sympy.__version__)\n\nmatplotlib: 3.10.0\nnumpy:      2.3.1\nsympy:      1.14.0",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>L</span>  <span class='chapter-title'>Matplotlib: Plotting and Visualization</span>"
    ]
  },
  {
    "objectID": "x_matplotlib.html#importing-matplotlib",
    "href": "x_matplotlib.html#importing-matplotlib",
    "title": "Appendix L — Matplotlib: Plotting and Visualization",
    "section": "M.1 Importing matplotlib",
    "text": "M.1 Importing matplotlib\n\nimport numpy as np\nimport sympy\n\nimport matplotlib as mpl\nimport matplotlib.pyplot as plt\nmpl.rcParams['font.size'] = 12.0\n\n\nprint('matplotlib:', mpl.__version__)\nprint('numpy:     ', np.__version__)\nprint('sympy:     ', sympy.__version__)\n\nmatplotlib: 3.10.0\nnumpy:      2.3.1\nsympy:      1.14.0",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>L</span>  <span class='chapter-title'>Matplotlib: Plotting and Visualization</span>"
    ]
  },
  {
    "objectID": "x_matplotlib.html#getting-started",
    "href": "x_matplotlib.html#getting-started",
    "title": "Appendix L — Matplotlib: Plotting and Visualization",
    "section": "L.2 Getting started",
    "text": "L.2 Getting started\n\n\n\n\n\n\n\n\n\n\nx = np.linspace(-5, 2, 100)\n\ny1 = x**3 +5*x**2 +10\ny2 = 3*x**2 +10*x\ny3 = 6*x +10\n\n#---------------------------------------\n\nfig, ax = plt.subplots(figsize=(6, 4))\n\nax.plot(x, y1, color=\"blue\", label=\"$y(x)$\")\nax.plot(x, y2, color=\"red\", label=\"$y'(x)$\")\nax.plot(x, y3, color=\"green\", label=\"$y''(x)$\")\n\nax.set_xlabel(\"$x$\")\nax.set_ylabel(\"$y$\")\nax.legend()\n\nplt.show()\n\n\n\n\n\n\n\n\n\n\nfig, ax = plt.subplots(figsize=(6, 4))\n\nax.plot(x, y1, lw=1.5, color=\"blue\", label=r\"$y(x)$\")\nax.plot(x, y2, lw=1.5, color=\"red\", label=r\"$y'(x)$\")\nax.plot(x, y3, lw=1.5, color=\"green\", label=r\"$y''(x)$\")\n\nax.plot(x, np.zeros_like(x), lw=0.5, color=\"black\")\nax.plot([-3.33], [(-3.33)**3 + 5*(-3.33)**2 + 10], \n        lw=0.5, marker='o', color=\"blue\")\nax.plot([0], [10], lw=0.5, marker='o', color=\"blue\")\nax.plot([-3.33, -3.33], [0, (-3.33)**3 + 5*(-3.33)**2 + 10], \n        ls='--', lw=0.5, color=\"black\")\nax.plot([0, 0], [0, 10], ls='--', lw=0.5, color=\"black\")\n\nax.set_xlim([-5, 2.5])\nax.set_xticks([-5, -2.5, 0, 2.5])\nax.set_ylim(-20, 40)\nax.set_yticks([-20, -10, 0, 10, 20, 30, 40])\n\nax.set_xlabel(\"$x$\", fontsize=14)\nax.set_ylabel(\"$y$\", fontsize=14)\n\nax.legend(loc=2, ncol=3, fontsize=14, frameon=False)\n\nplt.show()",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>L</span>  <span class='chapter-title'>Matplotlib: Plotting and Visualization</span>"
    ]
  },
  {
    "objectID": "x_matplotlib.html#figure",
    "href": "x_matplotlib.html#figure",
    "title": "Appendix L — Matplotlib: Plotting and Visualization",
    "section": "L.3 Figure",
    "text": "L.3 Figure\n\nx = np.linspace(-2, 2, 1000)\n\ny1 = np.cos(40*x)\ny2 = np.exp(-x**2)\n\n#---------------------------------------\n\n# the width and height of the figure canvas in inches\nfig = plt.figure(figsize=(6, 3), facecolor=\"#f1f1f1\") \n\n# axes coordinates as fractions of \n# the canvas width and height\nleft, bottom, width, height = 0.1, 0.1, 0.8, 0.8\nax = fig.add_axes((left, bottom, width, height), \n                    facecolor=\"#e1e1e1\")\n\nax.plot(x, y1 *y2)\nax.plot(x, y2, 'g')\nax.plot(x,-y2, 'g')\n\nax.set_xlabel(\"x\")\nax.set_ylabel(\"y\")\n\nplt.show()\n\n\n\n\n\n\n\n\n\nfig.savefig(\"./figures/matplotlib_savefig.png\", dpi=100, facecolor=\"#f1f1f1\")\nfig.savefig(\"./figures/matplotlib_savefig.pdf\", dpi=300, facecolor=\"#f1f1f1\")",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>L</span>  <span class='chapter-title'>Matplotlib: Plotting and Visualization</span>"
    ]
  },
  {
    "objectID": "x_matplotlib.html#axes",
    "href": "x_matplotlib.html#axes",
    "title": "Appendix L — Matplotlib: Plotting and Visualization",
    "section": "L.4 Axes",
    "text": "L.4 Axes\n\nMatplotlib provides several different Axes layout managers, which create and place Axes instances within a figure canvas following different strategies\nTo facilitate the forthcoming examples, \\(\\,\\)we here briefly look at one of these layout managers:\n   plt.subplots\nEarlier in this appendix, \\(\\,\\)we already used this function to conveniently generate new Figure and Axes objects in one function call\nHowever, \\(\\,\\)the plt.subplots function is also capable of filling a figure with a grid of Axes instances, which is specified using the first and the second arguments, or alternatively with the nrows and ncols arguments, which, as the names implies, creates a grid of Axes objects, \\(\\,\\)with the given number of rows and columns\n   fig, axes = plt.subplots(nrows=3, ncols=2)\nHere, the function plt.subplots returns a tuple (fig, axes), where fig is a figure and axes is a numpy array of size (ncols, nrows), \\(\\,\\)in which each element is an Axes instance that has been appropriately placed in the corresponding figure canvas. \\(\\,\\)At this point we can also specify that columns and/or rows should share x and y axes, using the sharex and sharey arguments, which can be set to True or False\nThe plt.subplots function also takes two special keyword arguments fig_kw and subplot_kw, which are dictionaries with keyword arguments that are used when creating the Figure and Axes instances, respectively. This allows us to set and retain full control of the properties of the Figure and Axes objects with plt.subplots, \\(\\,\\)a similar way is possible when directly using plt.figure and the add_axes method",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>L</span>  <span class='chapter-title'>Matplotlib: Plotting and Visualization</span>"
    ]
  },
  {
    "objectID": "x_matplotlib.html#plot-types",
    "href": "x_matplotlib.html#plot-types",
    "title": "Appendix L — Matplotlib: Plotting and Visualization",
    "section": "L.5 Plot types",
    "text": "L.5 Plot types\n\nMatplotlib implements many types of plotting techniques as methods of the Axes object\n\n\n\n\n\nFor details, see Matplotlib’s plot types\n\n\ndef hide_labels(fig, ax, fignum):\n       \n  ax.set_xticks([])\n  ax.set_yticks([])\n\n  ax.xaxis.set_ticks_position('none')\n  ax.yaxis.set_ticks_position('none')\n\n  ax.axis('tight')\n\n  fignum += 1\n  #fig.savefig(f\"./figures/plot_types_{fignum}.pdf\")\n\n\nx = np.linspace(-3, 3, 25)\ny1 = x**3 +3*x**2 +10\ny2 = -1.5*x**3 +10*x**2 -15\n\n\nfig, ax = plt.subplots(figsize=(6, 4))\n\nax.plot(x, y1)\nax.plot(x, y2)\n\nhide_labels(fig, ax, 1)\n\n\n\n\n\n\n\n\n\nfig, ax = plt.subplots(figsize=(6, 4))\n\nax.step(x, y1)\nax.step(x, y2)\n\nhide_labels(fig, ax, 2)\n\n\n\n\n\n\n\n\n\nfig, ax = plt.subplots(figsize=(6, 4))\n\nwidth = 6/50\n\nax.bar(x - width/2, y1, width=width, color=\"blue\")\nax.bar(x + width/2, y2, width=width, color=\"green\")\n\nhide_labels(fig, ax, 3)\n\n\n\n\n\n\n\n\n\nfig, ax = plt.subplots(figsize=(6, 4))\n\nax.hist(y2, bins=15, edgecolor=\"black\")\nax.hist(y1, bins=15, edgecolor=\"black\")\n\nhide_labels(fig, ax, 4)\n\n\n\n\n\n\n\n\n\nfig, ax = plt.subplots(figsize=(6, 4))\n\nax.errorbar(x, y2, yerr=y1, lw=0.6, fmt='--o', capsize=5)\n\nhide_labels(fig, ax, 5)\n\n\n\n\n\n\n\n\n\nfig, ax = plt.subplots(figsize=(6, 4))\n\nax.fill_between(x, y1, y2, edgecolor=\"black\")\n\nhide_labels(fig, ax, 6)\n\n\n\n\n\n\n\n\n\nfig, ax = plt.subplots(figsize=(6, 4))\n\nax.stem(x, y1, linefmt='b', markerfmt='bs')\nax.stem(x, y2, linefmt='r', markerfmt='ro')\n\nhide_labels(fig, ax, 7)\n\n\n\n\n\n\n\n\n\nfig, ax = plt.subplots(figsize=(6, 4))\n\nx = np.linspace(0, 5, 50)\n\nax.scatter(x, -1 +x +0.25*x**2 +2*np.random.rand(len(x)))\nax.scatter(x, np.sqrt(x) +2*np.random.rand(len(x)), color=\"green\")\n\nhide_labels(fig, ax, 8)\n\n\n\n\n\n\n\n\n\n\nx = y = np.linspace(-np.pi, np.pi, 16)\nX, Y = np.meshgrid(x, y)\nU = np.sin(X)\nV = np.cos(Y)\n\nfig, ax = plt.subplots(figsize=(5, 5))\n\nax.quiver(X, Y, U, V)\nhide_labels(fig, ax, 9)\n\n\n\n\n\n\n\n\n\nfig, ax = plt.subplots(figsize=(5, 5))\n\nax.streamplot(X, Y, U, V)\nhide_labels(fig, ax, 9)\n\n\n\n\n\n\n\n\n\nplt.style.use('_mpl-gallery')\n\n# make data:\nnp.random.seed(10)\nD = np.random.normal((3, 5, 4), (1.25, 1.00, 1.25), (100, 3))\n\n# plot\nfig, ax = plt.subplots(figsize=(5, 5))\nVP = ax.boxplot(D, \n                positions=[2, 4, 6], \n                widths=1.5, \n                patch_artist=True,\n                showmeans=False, \n                showfliers=False,\n                medianprops={\"color\": \"white\", \"linewidth\": 0.5},\n                boxprops={\"facecolor\": \"C0\", \"edgecolor\": \"white\",\n                          \"linewidth\": 0.5},\n                whiskerprops={\"color\": \"C0\", \"linewidth\": 1.5},\n                capprops={\"color\": \"C0\", \"linewidth\": 1.5})\n\nax.set(xlim=(0, 8), xticks=[],\n       ylim=(0, 8), yticks=[])\n\nplt.show()\n\n\n\n\n\n\n\n\n\nplt.style.use('_mpl-gallery-nogrid')\n\n# make data\nx = [1, 2, 3, 4]\ncolors = plt.get_cmap('Blues')(np.linspace(0.2, 0.7, len(x)))\n\n# plot\nfig, ax = plt.subplots(figsize=(5, 5))\nax.pie(x, colors=colors, radius=3, center=(4, 4),\n       wedgeprops={\"linewidth\": 1, \"edgecolor\": \"white\"}, frame=True)\n\nax.set(xlim=(0, 8), xticks=[],\n       ylim=(0, 8), yticks=[])\n\nplt.show()",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>L</span>  <span class='chapter-title'>Matplotlib: Plotting and Visualization</span>"
    ]
  },
  {
    "objectID": "x_matplotlib.html#line-properties",
    "href": "x_matplotlib.html#line-properties",
    "title": "Appendix L — Matplotlib: Plotting and Visualization",
    "section": "L.6 Line properties",
    "text": "L.6 Line properties\n\nIn matplotlib, \\(\\,\\)we set the line properties with keyword arguments to the plot methods, such as for example plot, step, bar\n\n\ndef axes_settings(fig, ax, title, ymax):\n     \n  ax.set_title(title)\n  ax.set_ylim(0, ymax +1)\n  ax.set_xticks([])\n  ax.set_yticks([])\n    \nx = np.linspace(-5, 5, 5)\ny = np.ones_like(x)\n\n\nfig, ax = plt.subplots(figsize=(6, 4))\n\n# Line width\nlinewidths = [0.5, 1.0, 2.0, 4.0]\nfor n, linewidth in enumerate(linewidths):\n  ax.plot(x, y + n, color=\"blue\", linewidth=linewidth)\n    \naxes_settings(fig, ax, \"linewidth\", len(linewidths))\n\n\n\n\n\n\n\n\n\nfig, ax = plt.subplots(figsize=(6, 4))  \n\n# Line style\nlinestyles = ['-', '--', '-.', ':']\nfor n, linestyle in enumerate(linestyles):\n  ax.plot(x, y + n, color=\"blue\", linestyle=linestyle)\n\n# Custom dash style\nline, = ax.plot(x, y + len(linestyles), color=\"red\", lw=2)\nlength1, gap1, length2, gap2 = 5, 2, 10, 2\nline.set_dashes([length1, gap1, length2, gap2])\n    \naxes_settings(fig, ax, \"linestyle\", len(linestyles) + 1)\n\n\n\n\n\n\n\n\n\nfig, ax = plt.subplots(figsize=(6, 4))\n \n# Marker types\nmarkers = ['+', 'o', '*', 's', '.', '1', '2', '3', '4']\nfor n, marker in enumerate(markers):\n  \n  ax.plot(x, y + n, color=\"blue\", lw=0, marker=marker)\n    \naxes_settings(fig, ax, \"markers\", len(markers))\n\n\n\n\n\n\n\n\n\nfig, ax = plt.subplots(figsize=(6, 4))\n\n# Marker size and color\nmarkersizeandcolors = [(4, \"white\"), (8, \"red\"), (12, \"yellow\"), (16, \"lightgreen\")]\nfor n, (markersize, markercolor) in enumerate(markersizeandcolors):\n  ax.plot(x, y + n, color=\"blue\", lw=1, ls='-', \n          marker='o',\n          markersize=markersize,\n          markerfacecolor=markercolor, \n          markeredgewidth=2)\n    \naxes_settings(fig, ax, \"marker size/color\", len(markersizeandcolors))\n\n\n\n\n\n\n\n\n\n\n# a symboloc variable for x, and a numerical array with specific values of x\nsym_x = sympy.Symbol(\"x\")\nx = np.linspace(-2*np.pi, 2*np.pi, 100)\n\ndef sin_expansion(x, n):\n  \"\"\"\n  Evaluate the n-th order Taylor series expansion of sin(x)\n  for numerical values in the numpy array x\n  \"\"\"\n    \n  return sympy.lambdify(sym_x, sympy.sin(sym_x).series(n=n).removeO(), 'numpy')(x)\n\n\nfig, ax = plt.subplots(figsize=(6, 4))\n\nax.plot(x, np.sin(x), lw=4, color=\"red\", label='exact')\n\ncolors = [\"blue\", \"black\"]\nlinestyles =[':', '-.', '--']\n\nfor idx, n in enumerate(range(2, 13, 2)):\n    \n  ax.plot(x, sin_expansion(x, n), \n          color=colors[idx //3], \n          ls=linestyles[idx %3], \n          lw=3, \n          label=f'O({n})')\n    \nax.set_xlim(-1.5*np.pi, 1.5*np.pi)\nax.set_ylim(-1.25, 1.25) \n\n# place a legend outside of the Axes\nax.legend(bbox_to_anchor=(1.02, 1), loc=2, borderaxespad=0.0)\n\n\n\n\n\n\n\n\n\ndata1 = np.random.randn(200, 2) *np.array([3, 1])\ndata2 = np.random.randn(200, 2) *np.array([1, 3])\n\nfig, axes = plt.subplots(2, 1, figsize=(6, 8), sharey=True)\n\naxes[0].scatter(data1[:, 0], data1[:, 1], color=\"g\", marker=\"s\", s=30, alpha=0.5)\naxes[0].scatter(data2[:, 0], data2[:, 1], color=\"b\", marker=\"o\", s=30, alpha=0.5)\n\naxes[1].hist([data1[:, 1], data2[:, 1]], \n    bins=15, \n    color=[\"g\", \"b\"], \n    alpha=0.5, \n    orientation='horizontal')\n\nplt.show()",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>L</span>  <span class='chapter-title'>Matplotlib: Plotting and Visualization</span>"
    ]
  },
  {
    "objectID": "x_matplotlib.html#legends",
    "href": "x_matplotlib.html#legends",
    "title": "Appendix L — Matplotlib: Plotting and Visualization",
    "section": "L.7 Legends",
    "text": "L.7 Legends\n\nSee help(plt.legend) for details. \\(\\,\\)The loc argument allows us to specify where in the Axes area the legend is to be added: loc=1 for upper right corner, loc=2 for upper left corner, loc=3 for the lower-left corner, and loc=4 for lower right corner\nIn the example of the previous section, \\(\\,\\)we used the bbox_to_anchor, \\(\\,\\)which helps the legend be placed at an arbitrary location within the figure canvas. The bbox_to_anchor argument takes the value of a tuple on the form (x, y), \\(~\\)where x and y are the canvas coordinates within the Axes object. That is, \\(\\,\\)the point (0, 0) corresponds to the lower-left corner, and (1, 1) corresponds to the upper right corner. \\(\\,\\)Note that x and y can be smaller than 0 and larger than 1, in this case, which indicates that the legend is to be placed outside the Axes area, \\(\\,\\)as was used in the previous section\nBy default, all lines in the legend are shown in a vertical arrangement. Using the ncols argument, it is possible to split the legend labels into multiple columns\n\n\nx = np.linspace(0, 1, 100)\n\nfig, axes = plt.subplots(4, 1, figsize=(6, 12), sharex=True)\n\nfor n in range(4):\n  axes[n].plot(x, x, label=\"$y(x) = x$\")\n  axes[n].plot(x, x +x**2, label=\"$y(x) = x + x^2$\")\n  axes[n].legend(loc=n + 1)\n  axes[n].set_title(f'legend: loc={n + 1}')\n\n\n\n\n\n\n\n\n\nx = np.linspace(-1, 1, 100)\n\ndef linear_eqn(x, slope):\n  return slope *x\n\nfig, ax = plt.subplots(figsize=(6, 3))\n\ncolors = ['blue', 'green', 'red', 'cyan',\n          'magenta', 'yellow', 'black', 'orange']\n\nfor slope in range(1, 9):\n  ax.plot(x, \n          linear_eqn(x, slope), \n          color=colors[slope -1],\n          label=f\"$y(x)={slope:d}x$\")\n    \nax.set_xlim(-1, 1)\nax.set_ylim(-8, 8)\n\nax.tick_params(axis='x', pad=10)\n    \nax.legend(bbox_to_anchor=(-0.01, 1.05), ncol=4, loc=3, borderaxespad=0.0)",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>L</span>  <span class='chapter-title'>Matplotlib: Plotting and Visualization</span>"
    ]
  },
  {
    "objectID": "x_matplotlib.html#text-formatting-and-annotations",
    "href": "x_matplotlib.html#text-formatting-and-annotations",
    "title": "Appendix L — Matplotlib: Plotting and Visualization",
    "section": "L.8 Text formatting and annotations",
    "text": "L.8 Text formatting and annotations\n\nMatplotlib provides several ways of configuring fonts properties. \\(\\,\\)The default values can be set in the matplotlib resource file. \\(\\,\\)To display where the currently active matplotlib file is loaded from, \\(\\,\\)one can do the following:\nmpl.matplotlib_fname()\nAnd session-wide configuration can be set in the mpl.rcParams dictionary: \\(~\\)for example,\nmpl.rcParams['font.size'] = 12.0\nTry print(mpl.rcParams) to get a list of possible configuration parameters and their current values\nMatplotlib provides excellent support for LaTeX markup within its text labels: Any text label in matplotlib can include LaTeX math by enclosing it within $ signs: \\(\\,\\) for example 'Regular text: $f(x)=1-x^2$'\nBy default, \\(\\,\\) matplotlib uses an internal LaTeX rendering, \\(\\,\\)which supports a subset of LaTeX language. However, by setting the configuration parameter mpl.rcParams['text.usetex']=True, \\(~\\)it is also possible to use an external full-featured LaTeX engine\nWhen embedding LaTeX code in strings there is a common stumbling block: Python uses \\ as escape character, \\(\\,\\)while in LaTeX it is used to denote the start of commands. \\(\\,\\)To prevent the Python interpreter from escaping characters in strings containing LaTeX expressions, \\(\\,\\)it is convenient to use raw strings, \\(\\,\\)which are literal string expressions that are prepended with an r, \\(\\,\\)for example: r\"$\\int f(x) dx$\" and r'$x_{\\rm A}$'\n\n\nx = np.linspace(-20, 20, 100)\ny = np.sin(x) /x\n\nfig, ax = plt.subplots(figsize=(6, 4))\n\nax.plot(x, y)\n\nax.set_xlabel(\"x label\")\nax.set_ylabel(\"y label\")\n\nfor label in ax.get_xticklabels() +ax.get_yticklabels():\n  label.set_rotation(45)\n\n\n\n\n\n\n\n\n\nfig, ax = plt.subplots(figsize=(6, 3))\n\nax.set_xticks([])\nax.set_yticks([])\nax.set_xlim(-0.50, 3.50)\nax.set_ylim(-0.05, 0.25)\nax.axhline(0)\n\n# text label\nax.text(0, 0.1, \"Text label\", fontsize=14, family=\"serif\")\n\n# annotation\nax.plot(1, 0, \"o\")\nax.annotate(\"Annotation\", fontsize=14, family=\"serif\",\n            xy=(1, 0), \n            xycoords=\"data\",\n            xytext=(20, 50), \n            textcoords=\"offset points\",\n            arrowprops=dict(arrowstyle=\"-&gt;\", connectionstyle=\"arc3, rad=.5\"))\n\n# equation\nax.text(2, 0.1, \n        r\"Equation: $i\\hbar\\partial_t \\Psi = \\hat{H}\\Psi$\", \n        fontsize=14, family=\"serif\")\n\nplt.show()\n\n\n\n\n\n\n\n\n\nmpl.rc('font', family='NanumGothic')\nmpl.rcParams['axes.unicode_minus'] = False\n\n\ndata = 1000 +np.random.randint(-100, 100, 50).cumsum()\n\nfig, ax = plt.subplots(figsize=(6, 4))\n\nax.plot(range(50), data, 'r')\n\nax.set_title('시간대별 가격 추이')\nax.set_ylabel('주식 가격, [원]')\nax.set_xlabel('시간, [분]')\nplt.show()\n\n\n\n\n\n\n\n\n\nmpl.rc('font', family='serif')\nmpl.rcParams['axes.unicode_minus'] = True",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>L</span>  <span class='chapter-title'>Matplotlib: Plotting and Visualization</span>"
    ]
  },
  {
    "objectID": "x_matplotlib.html#axis-properties",
    "href": "x_matplotlib.html#axis-properties",
    "title": "Appendix L — Matplotlib: Plotting and Visualization",
    "section": "L.9 Axis properties",
    "text": "L.9 Axis properties\n\nL.9.1 Axis labels and titles\n\nx = np.linspace(0, 50, 500)\ny = np.sin(x) *np.exp(-x /10)\n\nfig, ax = plt.subplots(figsize=(6, 2), subplot_kw={'facecolor': \"#ebf5ff\"})\n\nax.plot(x, y, lw=2)\n\nax.set_xlim([0, 50])\nax.set_ylim([-1.0, 1.0])\n\nax.set_xlabel(\"x\", labelpad=5, fontsize=14, fontname='serif', color='blue')\nax.set_ylabel(\"f(x)\", labelpad=5, fontsize=14, fontname='serif', color='blue')\n\nax.set_title(\"Axis labels and Title example\", \n             fontsize=14, fontname='serif', color='blue', loc='center')\n\nplt.show()\n\n\n\n\n\n\n\n\n\n\nL.9.2 Axis range\n\nx = np.linspace(0, 30, 500)\ny = np.sin(x) *np.exp(-x /10)\n\nfig, axes = plt.subplots(4, 1, figsize=(6, 12), \n                         subplot_kw={'facecolor': '#ebf5ff'})\n\naxes[0].plot(x, y, lw=2)\naxes[0].set_xlim(-5, 35)\naxes[0].set_ylim(-1, 1)\naxes[0].set_title(\"set_[x/y]lim\")\n\naxes[1].plot(x, y, lw=2)\naxes[1].axis('tight')\naxes[1].set_title(\"axis('tight')\")\n\naxes[2].plot(x, y, lw=2)\naxes[2].axis('equal')\naxes[2].set_title(\"axis('equal')\")\n\naxes[3].plot(x, y, lw=2)\naxes[3].autoscale(True)\naxes[3].set_title(\"autoscale(True)\")\n\nplt.show()\n\n\n\n\n\n\n\n\n\n\nL.9.3 Axis ticks, tick labels, and grids\n\\(~\\)\n\n\n\n\n\n\n\n\n\n\nx = np.linspace(-2*np.pi, 2*np.pi, 500)\ny = np.sin(x) *np.exp(-x**2/20)\n\nfig, axes = plt.subplots(2, 1, figsize=(6, 8))\n\naxes[0].plot(x, y, lw=2)\naxes[0].set_title(\"default ticks\")\naxes[0].tick_params(which='both', direction='in')\n\naxes[1].plot(x, y, lw=2)\naxes[1].set_title(\"set_[x/y]ticks\")\naxes[1].set_xticks([-5, 0, 5])\naxes[1].set_yticks([-1, 0, 1])\naxes[1].tick_params(which='both', direction='in')\n\n\n\n\n\n\n\n\n\nfig, axes = plt.subplots(2, 1, figsize=(6, 8))\n\naxes[0].plot(x, y, lw=2)\naxes[0].set_title(\"set_major/minor_locator\")\naxes[0].xaxis.set_major_locator(mpl.ticker.MaxNLocator(4))\naxes[0].xaxis.set_minor_locator(mpl.ticker.MaxNLocator(8))\naxes[0].yaxis.set_major_locator(mpl.ticker.MaxNLocator(4))\naxes[0].yaxis.set_minor_locator(mpl.ticker.MaxNLocator(8))\naxes[0].tick_params(which='both', direction='in')\n\naxes[1].plot(x, y, lw=2)\naxes[1].set_title(\"set_[x/y]ticklabels\")\naxes[1].set_xticks([-2*np.pi, -np.pi, 0, np.pi, 2*np.pi])\naxes[1].set_xticklabels(['$-2\\pi$', '$-\\pi$', '$0$', '$\\pi$', '$2\\pi$'])\naxes[1].xaxis.set_minor_locator(\n  mpl.ticker.FixedLocator([-3*np.pi/2, -np.pi/2, 0, np.pi/2, 3*np.pi/2]))\naxes[1].set_yticks([-1, 0, 1])\naxes[1].yaxis.set_minor_locator(mpl.ticker.MaxNLocator(8))\naxes[1].tick_params(which='both', direction='in')\n\n&lt;&gt;:14: SyntaxWarning: invalid escape sequence '\\p'\n&lt;&gt;:14: SyntaxWarning: invalid escape sequence '\\p'\n&lt;&gt;:14: SyntaxWarning: invalid escape sequence '\\p'\n&lt;&gt;:14: SyntaxWarning: invalid escape sequence '\\p'\n&lt;&gt;:14: SyntaxWarning: invalid escape sequence '\\p'\n&lt;&gt;:14: SyntaxWarning: invalid escape sequence '\\p'\n&lt;&gt;:14: SyntaxWarning: invalid escape sequence '\\p'\n&lt;&gt;:14: SyntaxWarning: invalid escape sequence '\\p'\n/var/folders/4x/8kn2nym12cn7x7qmg_6s4b8h0000gn/T/ipykernel_76769/2527598018.py:14: SyntaxWarning: invalid escape sequence '\\p'\n  axes[1].set_xticklabels(['$-2\\pi$', '$-\\pi$', '$0$', '$\\pi$', '$2\\pi$'])\n/var/folders/4x/8kn2nym12cn7x7qmg_6s4b8h0000gn/T/ipykernel_76769/2527598018.py:14: SyntaxWarning: invalid escape sequence '\\p'\n  axes[1].set_xticklabels(['$-2\\pi$', '$-\\pi$', '$0$', '$\\pi$', '$2\\pi$'])\n/var/folders/4x/8kn2nym12cn7x7qmg_6s4b8h0000gn/T/ipykernel_76769/2527598018.py:14: SyntaxWarning: invalid escape sequence '\\p'\n  axes[1].set_xticklabels(['$-2\\pi$', '$-\\pi$', '$0$', '$\\pi$', '$2\\pi$'])\n/var/folders/4x/8kn2nym12cn7x7qmg_6s4b8h0000gn/T/ipykernel_76769/2527598018.py:14: SyntaxWarning: invalid escape sequence '\\p'\n  axes[1].set_xticklabels(['$-2\\pi$', '$-\\pi$', '$0$', '$\\pi$', '$2\\pi$'])\n\n\n\n\n\n\n\n\n\n\nfig, axes = plt.subplots(3, 1, figsize=(6, 12))\n\nfor ax in axes:\n  ax.plot(x, y, lw=2)\n  ax.xaxis.set_major_locator(mpl.ticker.MultipleLocator(4))\n  ax.xaxis.set_minor_locator(mpl.ticker.MultipleLocator(0.5))\n  ax.yaxis.set_major_locator(mpl.ticker.MultipleLocator(1))\n  ax.yaxis.set_minor_locator(mpl.ticker.MultipleLocator(0.25))\n    \naxes[0].set_title(\"default grid\")\naxes[0].grid()\n\naxes[1].set_title(\"major/minor grid\")\naxes[1].grid(color=\"blue\", which=\"both\", ls=':', lw=0.5)\n\naxes[2].set_title(\"individual x/y major/minor grid\")\naxes[2].grid(color=\"gray\", which=\"major\", axis='x', ls='-', lw=0.5)\naxes[2].grid(color=\"gray\", which=\"minor\", axis='x', ls=':', lw=0.25)\naxes[2].grid(color=\"gray\", which=\"major\", axis='y', ls='-', lw=0.5)\n\n\n\n\n\n\n\n\n\nx = np.linspace(0, 1e5, 100)\ny = x**2\n\nfig, axes = plt.subplots(2, 1, figsize=(6, 8))\n\naxes[0].plot(x, y, 'b.')\naxes[0].set_title(\"default labels\", loc='right')\n\naxes[1].plot(x, y, 'b')\naxes[1].set_title(\"scientific notation labels\", loc='right')\n\nformatter = mpl.ticker.ScalarFormatter(useMathText=True)\nformatter.set_powerlimits((-1, 2))\n\naxes[1].xaxis.set_major_formatter(formatter)\naxes[1].yaxis.set_major_formatter(formatter)\n\n\n\n\n\n\n\n\n\n\nL.9.4 Log lots\n\nx = np.linspace(0, 1e3, 100)\ny1, y2 = x**3, x**4\n\nfig, axes = plt.subplots(3, 1, figsize=(6, 12))\n\naxes[0].set_title('loglog')\naxes[0].loglog(x, y1, 'b', x, y2, 'r')\n\naxes[1].set_title('semilogy')\naxes[1].semilogy(x, y1, 'b', x, y2, 'r')\n\naxes[2].set_title('plot set_[x/y]scale')\naxes[2].plot(x, y1, 'b', x, y2, 'r')\naxes[2].set_xscale('log')\naxes[2].set_yscale('log')\n\n\n\n\n\n\n\n\n\n\nL.9.5 Twin axes\n\nradius = np.linspace(0, 5, 100)\narea = 4 *np.pi *radius**2         # area\nvolume = (4 *np.pi /3) *radius**3  # volume\n\n\nig, ax1 = plt.subplots(figsize=(6, 4))\n\nax1.set_title(\"Surface area and Volume of a sphere\", fontsize=16)\nax1.set_xlabel(\"radius, [m]\", fontsize=16)\n\nax1.plot(radius, area, lw=2, color=\"blue\")\nax1.set_ylabel(r\"Surface area, [$m^2$]\", fontsize=16, color=\"blue\")\nax1.set_xlim(0, 5)\nax1.set_ylim(0, 350)\n\nfor label in ax1.get_yticklabels():\n  label.set_color(\"blue\")\n    \nax2 = ax1.twinx()\nax2.plot(radius, volume, lw=2, color=\"red\")\nax2.set_ylabel(r\"Volume, [$m^3$]\", fontsize=16, color=\"red\")\nax2.set_ylim(0, 600)\nfor label in ax2.get_yticklabels():\n  label.set_color(\"red\")\n\n\n\n\n\n\n\n\n\n\nL.9.6 Spines\n\nx = np.linspace(-10, 10, 500)\ny = np.sin(x) /x\n\nfig, ax = plt.subplots(figsize=(6, 4))\n\nax.plot(x, y, lw=2)\n\nax.set_xticks([-10, -5, 5, 10])\nax.set_yticks([0.5, 1])\n\n# remove top and right spines\nax.spines['top'].set_color('none')\nax.spines['right'].set_color('none')\n\n# set only bottom and left spine ticks\nax.xaxis.set_ticks_position('bottom')\nax.yaxis.set_ticks_position('left')\n\n# move bottom and left spines to x = 0 and y = 0\nax.spines['bottom'].set_position(('data', 0))\nax.spines['left'].set_position(('data', 0))",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>L</span>  <span class='chapter-title'>Matplotlib: Plotting and Visualization</span>"
    ]
  },
  {
    "objectID": "x_matplotlib.html#advanced-axes-layouts",
    "href": "x_matplotlib.html#advanced-axes-layouts",
    "title": "Appendix L — Matplotlib: Plotting and Visualization",
    "section": "L.10 Advanced axes layouts",
    "text": "L.10 Advanced axes layouts\n\nL.10.1 Insets\n\ndef f(x):\n  return 1 /(1 +x**2) +0.1 /(1 +((3 -x) /0.1)**2)\n\ndef plot_and_format_axes(ax, x, f, fontsize):\n  ax.plot(x, f(x), lw=2)\n  ax.xaxis.set_major_locator(mpl.ticker.MaxNLocator(5))\n  ax.yaxis.set_major_locator(mpl.ticker.MaxNLocator(4))\n\n  ax.set_xlabel(\"$x$\", fontsize=fontsize)\n  ax.set_ylabel(\"$f(x)$\", fontsize=fontsize)\n\n  for label in ax.get_xticklabels() +ax.get_yticklabels():\n    label.set_fontsize(fontsize)\n\n  ax.tick_params(which='both', direction='in')\n\n\nx = np.linspace(-4, 14, 1000)    \n    \nfig = plt.figure(figsize=(6.5, 4))    \n\n# main graph\nax = fig.add_axes([0.1, 0.15, 0.8, 0.8], facecolor=\"#f5f5f5\")\nplot_and_format_axes(ax, x, f, 12)\n\nx0, x1 = 2.5, 3.5\nx = np.linspace(x0, x1, 1000)\n\nax.axvline(x0, ymax=0.3, color=\"gray\", ls=':')\nax.axvline(x1, ymax=0.3, color=\"gray\", ls=':')\n\n# inset\nax1 = fig.add_axes([0.5, 0.5, 0.38, 0.42], facecolor='none') \nplot_and_format_axes(ax1, x, f, 10)\n\n\n\n\n\n\n\n\n\n\nL.10.2 Subplots\n\nx1 = np.random.randn(100)\nx2 = np.random.randn(100)\n\n# squeeze=False, the axes is always a two-dimensional array\nfig, axes = plt.subplots(2, 2, figsize=(7, 7), \n                        sharex=True, \n                        sharey=True, \n                        squeeze=False) \n\naxes[0, 0].set_title(\"Uncorrelated\", fontsize=12)             \naxes[0, 0].scatter(x1, x2)\n\naxes[0, 1].set_title(\"Weakly positively correlated\", fontsize=12)\naxes[0, 1].scatter(x1, x1 +x2)\n\naxes[1, 0].set_title(\"Weakly negatively correlated\", fontsize=12)\naxes[1, 0].scatter(x1,-x1 +x2)\n\naxes[1, 1].set_title(\"Strongly correlated\", fontsize=12)\naxes[1, 1].scatter(x1, x1 +0.15*x2)\n\naxes[1, 1].set_xlabel(\"x\")\naxes[1, 0].set_xlabel(\"x\")\naxes[0, 0].set_ylabel(\"y\")\naxes[1, 0].set_ylabel(\"y\")\n\nplt.subplots_adjust(left=0.1, \n                    right=0.95, \n                    bottom=0.1, \n                    top=0.95, \n                    wspace=0.1, \n                    hspace=0.2)\n\n\n\n\n\n\n\n\n\n\nL.10.3 Subplot2grid\n\ndef plot_and_format_axes(ax, ax_number):\n  ax.set_xlim(0, 6)\n  ax.set_ylim(0, 6)    \n  \n  ax.xaxis.set_major_locator(mpl.ticker.MaxNLocator(6))\n  ax.yaxis.set_major_locator(mpl.ticker.MaxNLocator(6))\n  \n  ax.tick_params(which='both', direction='in', top=True, right=True)\n  ax.set_xticklabels([])\n  ax.set_yticklabels([])\n  ax.text(3, 3, f'ax{ax_number}', \n    horizontalalignment='center', \n    verticalalignment='center')\n\n\nplt.figure(figsize=(6, 6))\n\nax0 = plt.subplot2grid((3, 3), (0, 0))\nax1 = plt.subplot2grid((3, 3), (0, 1))\nax2 = plt.subplot2grid((3, 3), (1, 0), colspan=2)\nax3 = plt.subplot2grid((3, 3), (2, 0), colspan=3)\nax4 = plt.subplot2grid((3, 3), (0, 2), rowspan=2)\n\naxes = [ax0, ax1, ax2, ax3, ax4]\n\nfor i in range(5):\n  plot_and_format_axes(axes[i], i)\n\n\n\n\n\n\n\n\n\n\nL.10.4 GridSpec\n\nfig = plt.figure(figsize=(6, 6))\n\ngs = mpl.gridspec.GridSpec(4, 4)\n\nax0, ax1 = fig.add_subplot(gs[0, 0]),  fig.add_subplot(gs[1, 1])\nax2, ax3 = fig.add_subplot(gs[2, 2]),  fig.add_subplot(gs[3, 3])\nax4, ax5 = fig.add_subplot(gs[0, 1:]), fig.add_subplot(gs[1:, 0])\nax6, ax7 = fig.add_subplot(gs[1, 2:]), fig.add_subplot(gs[2:, 1])\nax8, ax9 = fig.add_subplot(gs[2, 3]),  fig.add_subplot(gs[3, 2])\n\naxes = [ax0, ax1, ax2, ax3, ax4, ax5, ax6, ax7, ax8, ax9]\n\nfor i in range(10):\n  plot_and_format_axes(axes[i], i)\n\n\n\n\n\n\n\n\n\nfig = plt.figure(figsize=(6, 6))\n\ngs = mpl.gridspec.GridSpec(2, 2, \n                    width_ratios=[4, 1], \n                    height_ratios=[1, 4],\n                    wspace=0.05, \n                    hspace=0.05)\n\nax0 = fig.add_subplot(gs[1, 0])\nax1 = fig.add_subplot(gs[0, 0])\nax2 = fig.add_subplot(gs[1, 1])\n\naxes = [ax0, ax1, ax2]\n\nfor i in range(3):\n  plot_and_format_axes(axes[i], i)",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>L</span>  <span class='chapter-title'>Matplotlib: Plotting and Visualization</span>"
    ]
  },
  {
    "objectID": "x_matplotlib.html#colormap-plots",
    "href": "x_matplotlib.html#colormap-plots",
    "title": "Appendix L — Matplotlib: Plotting and Visualization",
    "section": "L.11 Colormap plots",
    "text": "L.11 Colormap plots\n\nx = y = np.linspace(-2, 2, 150)\nX, Y = np.meshgrid(x, y)\n\nR1 = np.sqrt((X +0.5)**2 +(Y +0.5)**2)\nR2 = np.sqrt((X +0.5)**2 +(Y -0.5)**2)\nR3 = np.sqrt((X -0.5)**2 +(Y +0.5)**2)\nR4 = np.sqrt((X -0.5)**2 +(Y -0.5)**2)\n\n\nfig, ax = plt.subplots(figsize=(7, 6))\n\nZ = np.sin(10 *R1) /(10 *R1) +np.sin(20 *R4) /(20 *R4)\nZ = Z[:-1, :-1]\n\np = ax.pcolor(X, Y, Z, cmap='seismic', vmin=-abs(Z).max(), vmax=abs(Z).max())\n\nax.axis('tight')\n\nax.set_xlabel('x')\nax.set_ylabel('y')\nax.set_title('pcolor')\n\nax.xaxis.set_major_locator(mpl.ticker.MaxNLocator(4))\nax.yaxis.set_major_locator(mpl.ticker.MaxNLocator(4))\n\ncb = fig.colorbar(p, ax=ax)\ncb.set_label('z')\ncb.set_ticks([-1, -.5, 0, .5, 1])\n\n\n\n\n\n\n\n\n\nfig, ax = plt.subplots(figsize=(7, 6))\n\nZ = 1/R1 -1/R2 -1/R3 +1/R4\n\nim = ax.imshow(Z, vmin=-1, vmax=1, cmap=mpl.cm.bwr,\n               extent=[x.min(), x.max(), y.min(), y.max()])\nim.set_interpolation('bilinear')\n\nax.axis('tight')\n\nax.set_xlabel('x')\nax.set_ylabel('y')\nax.set_title('imshow')\n\nax.xaxis.set_major_locator(mpl.ticker.MaxNLocator(4))\nax.yaxis.set_major_locator(mpl.ticker.MaxNLocator(4))\n\ncb = fig.colorbar(im, ax=ax)\ncb.set_label('z')\ncb.set_ticks([-1, -.5, 0, .5, 1])\n\n\n\n\n\n\n\n\n\n\nx = y = np.linspace(0, 1, 75)\nX, Y = np.meshgrid(x, y)\nZ = (-2 *np.cos(2 *np.pi *X) *np.cos(2 *np.pi *Y) \n     -0.7 *np.cos(np.pi -4 *np.pi *X))\n\n\nfig, ax = plt.subplots(figsize=(6, 6))\n\nc = ax.contour(X, Y, Z, 15, cmap=mpl.cm.RdBu, vmin=-1, vmax=1)\n\nax.axis('tight')\n\nax.set_xlabel('x')\nax.set_ylabel('y')\nax.set_title(\"contour\")\n\nax.xaxis.set_major_locator(mpl.ticker.MaxNLocator(4))\nax.yaxis.set_major_locator(mpl.ticker.MaxNLocator(4))\n\n\n\n\n\n\n\n\n\nfig, ax = plt.subplots(figsize=(6, 6))\n\nc = ax.contourf(X, Y, Z, 15, cmap=mpl.cm.RdBu, vmin=-1, vmax=1)\n\nax.axis('tight')\n\nax.set_xlabel('x')\nax.set_ylabel('y')\nax.set_title(\"contourf\")\n\nax.xaxis.set_major_locator(mpl.ticker.MaxNLocator(4))\nax.yaxis.set_major_locator(mpl.ticker.MaxNLocator(4))\n\nfig.tight_layout()\n\n\n\n\n\n\n\n\n\n\nx = y = np.linspace(-10, 10, 150)\n\nX, Y = np.meshgrid(x, y)\nZ = np.cos(X) *np.cos(Y) *np.exp(-(X/5)**2 -(Y/5)**2)\nP = Z[:-1, :-1]\n\nnorm = mpl.colors.Normalize(-abs(Z).max(), abs(Z).max())\n\ndef plot_and_format_axes(fig, ax, p):\n  ax.axis('tight')\n  ax.set_xlabel(r\"$x$\", fontsize=12)\n  ax.set_ylabel(r\"$y$\", fontsize=12)\n\n  ax.xaxis.set_major_locator(mpl.ticker.MaxNLocator(4))\n  ax.yaxis.set_major_locator(mpl.ticker.MaxNLocator(4))\n    \n  ax.tick_params(which='both', direction='in', top=True, right=True)\n\n  cb0 = fig.colorbar(p, ax=ax)\n  cb0.set_label(r\"$z$\", fontsize=14)\n  cb0.set_ticks([-1, -0.5, 0, 0.5, 1])\n\n\nfig, axes = plt.subplots(2, 1, figsize=(7, 12))\n\np = axes[0].pcolor(X, Y, P, norm=norm, cmap=mpl.cm.bwr)\nplot_and_format_axes(fig, axes[0], p)\n\naxes[1].contour(X, Y, Z, np.linspace(-1.0, 1.0, 20), norm=norm, cmap=mpl.cm.bwr)\nplot_and_format_axes(fig, axes[1], p)\n\nplt.subplots_adjust(hspace=0.15)\n\n\n\n\n\n\n\n\n\n\nX, Y = np.meshgrid(np.linspace(-2, 2, 100), np.linspace(-2, 2, 100))\n\n# A low hump with a spike coming out\n# needs to have z/colour axis on a log scale so we see both hump and spike\n\nZ1 = np.exp(-X**2 -Y**2)\nZ2 = np.exp(-(10 *X)**2 -(10 *Y)**2)\nZ = Z1 +50 *Z2\n\nfig, axes = plt.subplots(2, 1, figsize=(7, 12))\n\nc = axes[0].pcolor(X, Y, Z, \n      norm=mpl.colors.LogNorm(vmin=Z.min(), vmax=Z.max()), cmap='PuBu_r')\nfig.colorbar(c, ax=axes[0])\n\n# linear scale only shows the spike\nc = axes[1].pcolor(X, Y, Z, cmap='PuBu_r') \nfig.colorbar(c, ax=axes[1])\n\nplt.subplots_adjust(hspace=0.1)\n\n\n\n\n\n\n\n\n\nMatplotlib has a number of built-in colormaps accessible via matplotlib.colormaps. To get a list of all registered colormaps, you can do:\n   from matplotlib import colormaps\n   list(colormaps)",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>L</span>  <span class='chapter-title'>Matplotlib: Plotting and Visualization</span>"
    ]
  },
  {
    "objectID": "x_matplotlib.html#d-plots",
    "href": "x_matplotlib.html#d-plots",
    "title": "Appendix L — Matplotlib: Plotting and Visualization",
    "section": "L.12 3D plots",
    "text": "L.12 3D plots\n\nIn matplotlib, \\(\\,\\)drawing 3D graphs requires using a different axes object, namely the Axes3D object that is available from the mpl_toolkits.mplot3d.axes3d module. \\(\\,\\)We can create a 3D-ware axes instance explicitly using the constructor of the Axes3D class, \\(\\,\\)by passing a Figure instance as argument:\n   ax = Axes3D(fig)\nAlternatively, \\(\\,\\)we can use the add_subplot function with the projection='3d' argument:\n   ax = fig.add_subplot(1, 1, 1, projection='3d')\nor use plt.subplots with the subplot_kw={'projection':'3d'} argument:\n   fig, ax = plt.subplots(1, 1, \n                 figsize=(6, 6), subplot_kw={'projection':'3d'})\n\n\nfrom mpl_toolkits.mplot3d.axes3d import Axes3D\n\nx = y = np.linspace(-3, 3, 74)\nX, Y = np.meshgrid(x, y)\nR = np.sqrt(X**2 +Y**2)\nZ = np.sin(4 *R) /R\n\nnorm = mpl.colors.Normalize(-abs(Z).max(), abs(Z).max())\n\ndef title_and_labels(ax, title):\n  ax.set_title(title)\n  \n  ax.set_xlabel(\"$x$\", labelpad=0.05, fontsize=12)\n  ax.set_ylabel(\"$y$\", labelpad=0.05, fontsize=12)\n  ax.set_zlabel(\"$z$\", labelpad=0.05, fontsize=12)\n  ax.set_box_aspect(None, zoom=0.85)\n  ax.tick_params(axis='both', pad=0.01)  \n\n\nfig, axes = plt.subplots(3, 1, figsize=(6, 16), subplot_kw={'projection': '3d'})\n\np = axes[0].plot_surface(X, Y, Z, rstride=1, cstride=1, lw=0, \n                  antialiased=False, norm=norm, cmap=mpl.cm.Blues)\ntitle_and_labels(axes[0], \"plot_surface\") \n\naxes[1].plot_wireframe(X, Y, Z, rstride=3, cstride=3, color=\"darkgrey\")\ntitle_and_labels(axes[1], \"plot_wireframe\")\n\naxes[2].contour(X, Y, Z, zdir='z', offset=0, norm=norm, cmap=mpl.cm.Blues)\naxes[2].contour(X, Y, Z, zdir='y', offset=3, norm=norm, cmap=mpl.cm.Blues)\ntitle_and_labels(axes[2], \"contour\") \n\nplt.subplots_adjust(left=0.05, right=0.95, bottom=0.1, top=0.95, hspace=0.1) \n\n\n\n\n\n\n\n\n\nfig, axes = plt.subplots(3, 1, figsize=(6, 16), subplot_kw={'projection': '3d'})\n\nr = np.linspace(0, 10, 100)\n\np = axes[0].plot(np.cos(r), np.sin(r), 6 -r)\ntitle_and_labels(axes[0], \"plot\")\n\np = axes[1].scatter(np.cos(r), np.sin(r), 6 -r)\ntitle_and_labels(axes[1], \"scatter\")\n\nr = np.linspace(0, 6, 30)\n\np = axes[2].bar3d(np.cos(r), np.sin(r), np.zeros_like(r), \n                  0.05*np.ones_like(r), 0.05*np.ones_like(r), 6 -r)\ntitle_and_labels(axes[2], \"bar3d\")\n\nplt.subplots_adjust(left=0.05, right=0.95, bottom=0.1, top=0.95, hspace=0.1)",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>L</span>  <span class='chapter-title'>Matplotlib: Plotting and Visualization</span>"
    ]
  },
  {
    "objectID": "x_matplotlib.html#cheatsheets-and-handouts",
    "href": "x_matplotlib.html#cheatsheets-and-handouts",
    "title": "Appendix L — Matplotlib: Plotting and Visualization",
    "section": "L.13 CheatSheets and Handouts",
    "text": "L.13 CheatSheets and Handouts",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>L</span>  <span class='chapter-title'>Matplotlib: Plotting and Visualization</span>"
    ]
  },
  {
    "objectID": "x_sympy_symbolic_computing.html",
    "href": "x_sympy_symbolic_computing.html",
    "title": "Appendix B — Sympy: Symbolic Computing",
    "section": "",
    "text": "B.1 Importing sympy\n\\(~\\)\n\\(~\\)\nimport sympy\nfrom sympy import I, pi, oo\nsympy.init_printing()\n\nfrom IPython.display import display\n\nprint(\"sympy: \", sympy.__version__)\n\nsympy:  1.14.0",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>B</span>  <span class='chapter-title'>Sympy: Symbolic Computing</span>"
    ]
  },
  {
    "objectID": "x_sympy_symbolic_computing.html#importing-sympy",
    "href": "x_sympy_symbolic_computing.html#importing-sympy",
    "title": "Appendix B — Sympy: Symbolic Computing",
    "section": "C.1 Importing sympy",
    "text": "C.1 Importing sympy\n\nimport sympy\nfrom sympy import I, pi, oo\nsympy.init_printing()\n\nfrom IPython.display import display\n\nprint(\"sympy: \", sympy.__version__)\n\nsympy:  1.14.0",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>B</span>  <span class='chapter-title'>Sympy: Symbolic Computing</span>"
    ]
  },
  {
    "objectID": "x_sympy_symbolic_computing.html#symbols",
    "href": "x_sympy_symbolic_computing.html#symbols",
    "title": "Appendix B — Sympy: Symbolic Computing",
    "section": "B.2 Symbols",
    "text": "B.2 Symbols\n\nx = sympy.Symbol('x')\nprint(x.is_real)\n\nNone\n\n\n\ny = sympy.Symbol('y', real=True)\ny.is_real\n\nTrue\n\n\n\nz = sympy.Symbol('z', complex=True)\nz.is_complex\n\nTrue\n\n\n\nprint(z.is_real)\n\nNone\n\n\n\n\nx = sympy.Symbol('x')\ny = sympy.Symbol('y', positive=True)\nz = sympy.Symbol('z', negative=True)\n\n\nsympy.sqrt(x**2)\n\n\\(\\displaystyle \\sqrt{x^{2}}\\)\n\n\n\nsympy.sqrt(y**2)\n\n\\(\\displaystyle y\\)\n\n\n\nsympy.sqrt(z**2)\n\n\\(\\displaystyle - z\\)\n\n\n\n\nn1 = sympy.Symbol('n')\nn2 = sympy.Symbol('n', integer=True)\nn3 = sympy.Symbol('n', odd=True)\nn4 = sympy.Symbol('n', even=True)\n\n\nsympy.cos(n1*pi)\n\n\\(\\displaystyle \\cos{\\left(\\pi n \\right)}\\)\n\n\n\nsympy.cos(n2*pi)\n\n\\(\\displaystyle \\left(-1\\right)^{n}\\)\n\n\n\nsympy.cos(n3*pi)\n\n\\(\\displaystyle -1\\)\n\n\n\nsympy.cos(n4*pi)\n\n\\(\\displaystyle 1\\)\n\n\n\n\na, b, c = sympy.symbols('a, b, c', negative=True)\n\n\nd, e, f = sympy.symbols('d, e, f', positive=True)",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>B</span>  <span class='chapter-title'>Sympy: Symbolic Computing</span>"
    ]
  },
  {
    "objectID": "x_sympy_symbolic_computing.html#numbers",
    "href": "x_sympy_symbolic_computing.html#numbers",
    "title": "Appendix B — Sympy: Symbolic Computing",
    "section": "B.3 Numbers",
    "text": "B.3 Numbers\n\nWe cannot directly use the built-in Python objects for integers, int, and floating-point numbers, float, and so on. Instead, sympy provides the classes sympy.Integer and sympy.Float for representing integers and floating-point numbers within the sympy framework\nThis distinction is important to be aware of when working with sympy, but fortunately we rarely need to concern ourselves with creating objects of type sympy.Integer and sympy.Float to representing specific numbers, \\(~\\)since sympy automatically promotes Python numbers to instances of these classes when they occur in sympy expressions\n\n\ni = sympy.Integer(18)\ntype(i)\n\nsympy.core.numbers.Integer\n\n\n\ni.is_Integer, i.is_real, i.is_odd, i.is_even\n\n(True, True, False, True)\n\n\n\nf = sympy.Float(2.3)\ntype(f)\n\nsympy.core.numbers.Float\n\n\n\nf.is_Float, f.is_real, f.is_complex\n\n(True, True, True)\n\n\n\n\ni, f = sympy.sympify(19), sympy.sympify(2.3)\ntype(i), type(f)\n\n(sympy.core.numbers.Integer, sympy.core.numbers.Float)\n\n\n\nB.3.1 Integer\n\nWhile the Symbol with integer=True represents some integer, \\(\\,\\)the Integer instance represents a specific integer\nFor both cases, \\(\\,\\)the is_integer attribute is True, \\(\\,\\)but there is also an attribute is_Integer (note the capital I), \\(\\,\\)which is only True for Integer instances\n\n\nn = sympy.Symbol('n', integer=True)\nn.is_integer, n.is_Integer, n.is_positive, n.is_Symbol\n\n(True, False, None, True)\n\n\n\ni = sympy.Integer(19)\ni.is_integer, i.is_Integer, i.is_positive, i.is_Symbol\n\n(True, True, True, False)\n\n\n\nsympy.Integer('19' *20)\n\n\\(\\displaystyle 1919191919191919191919191919191919191919\\)\n\n\n\nsympy.Integer('12_345_678'), sympy.Integer(12_345_678)\n\n\\(\\displaystyle \\left( 12345678, \\  12345678\\right)\\)\n\n\n\n# great common division, leat common multiple\nsympy.igcd(36, 15), sympy.ilcm(7, 34)\n\n\\(\\displaystyle \\left( 3, \\  238\\right)\\)\n\n\n\nIntegers in sympy are arbitrary precision, \\(\\,\\)meaning that they have no fixed lower and upper bounds, \\(\\,\\)which is the case when representing integers with a specific bit-size, as, for example, in numpy\n\n\ni = sympy.Integer(19)\ni**100\n\n\\(\\displaystyle 75051624198251984443456989853061891539043939434909537798332873934101480896578056472849915762891214746171016655874432115640378001\\)\n\n\n\nsympy.factorial(100)\n\n\\(\\displaystyle 93326215443944152681699238856266700490715968264381621468592963895217599993229915608941463976156518286253697920827223758251185210916864000000000000000000000000\\)\n\n\n\n\nB.3.2 Float\n\nLike Integer, \\(\\,\\)Float is arbitrary precision, \\(\\,\\)in contrast to Python’s built-in float type and the float types in numpy. \\(\\,\\)This means that any Float can represent a float with arbitrary number of decimals\nWhen a Float instance is created using its constructor, \\(\\,\\)there are two arguments: the first argument is a Python float or a string representing a floating-point number, and the second (optional) argument is the precision (number of significant decimal digits) of the Float object\n\n\n# create a string representation with 25 decimals\nf'{0.3:.25f}'  \n\n'0.2999999999999999888977698'\n\n\n\nsympy.Float(0.3, 25) \n\n\\(\\displaystyle 0.2999999999999999888977698\\)\n\n\n\nsympy.Float('0.3', 25)\n\n\\(\\displaystyle 0.3\\)\n\n\n\nsympy.Float('123 456 789.123_456', '')\n\n\\(\\displaystyle 123456789.123456\\)\n\n\n\n\nB.3.3 Rational\n\nr0 = sympy.Rational(11, 13)\nr0\n\n\\(\\displaystyle \\frac{11}{13}\\)\n\n\n\nr0.p, r0.q\n\n\\(\\displaystyle \\left( 11, \\  13\\right)\\)\n\n\n\nr1 = sympy.Rational(2, 3)\nr2 = sympy.Rational(4, 5)\n\n\nr1 * r2\n\n\\(\\displaystyle \\frac{8}{15}\\)\n\n\n\nr1 / r2\n\n\\(\\displaystyle \\frac{5}{6}\\)\n\n\n\nsympy.Rational(0.2), sympy.Rational('0.2')\n\n\\(\\displaystyle \\left( \\frac{3602879701896397}{18014398509481984}, \\  \\frac{1}{5}\\right)\\)\n\n\n\n\nB.3.4 Constants and special symbols\n\\(\\displaystyle \\gamma = \\lim_{n \\to \\infty} \\left( \\sum_{k=1}^n \\frac{1}{k} -\\ln n\\right)\\)\n\nsympy.pi, sympy.E, sympy.EulerGamma, sympy.I, sympy.oo\n\n\\(\\displaystyle \\left( \\pi, \\  e, \\  \\gamma, \\  i, \\  \\infty\\right)\\)\n\n\n\n\nB.3.5 Functions\n\nx, y, z = sympy.symbols('x, y, z')\n\n\nf = sympy.Function('f')\ntype(f)\n\nsympy.core.function.UndefinedFunction\n\n\n\nf(x)\n\n\\(\\displaystyle f{\\left(x \\right)}\\)\n\n\n\ng = sympy.Function('g')(x, y, z)\ng\n\n\\(\\displaystyle g{\\left(x,y,z \\right)}\\)\n\n\n\ng.free_symbols\n\n\\(\\displaystyle \\left\\{x, y, z\\right\\}\\)\n\n\n\n\nsympy.sin\n\nsin\n\n\n\nsympy.sin(x)\n\n\\(\\displaystyle \\sin{\\left(x \\right)}\\)\n\n\n\nsympy.sin(pi *1.5)\n\n\\(\\displaystyle -1\\)\n\n\n\nn = sympy.Symbol('n', integer=True)\nsympy.sin(pi * n)\n\n\\(\\displaystyle 0\\)\n\n\n\nh = sympy.Lambda(x, x**2)\nh\n\n\\(\\displaystyle \\left( x \\mapsto x^{2} \\right)\\)\n\n\n\nh(5)\n\n\\(\\displaystyle 25\\)\n\n\n\nh(1 + x)\n\n\\(\\displaystyle \\left(x + 1\\right)^{2}\\)",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>B</span>  <span class='chapter-title'>Sympy: Symbolic Computing</span>"
    ]
  },
  {
    "objectID": "x_sympy_symbolic_computing.html#expressions",
    "href": "x_sympy_symbolic_computing.html#expressions",
    "title": "Appendix B — Sympy: Symbolic Computing",
    "section": "B.4 Expressions",
    "text": "B.4 Expressions\n\nx = sympy.Symbol('x')\n\nexpr = 1 +2*x**2 +3*x**3\nexpr\n\n\\(\\displaystyle 3 x^{3} + 2 x^{2} + 1\\)\n\n\n\nexpr.args\n\n\\(\\displaystyle \\left( 1, \\  2 x^{2}, \\  3 x^{3}\\right)\\)\n\n\n\nexpr.args[2]\n\n\\(\\displaystyle 3 x^{3}\\)\n\n\n\nexpr.args[2].args\n\n\\(\\displaystyle \\left( 3, \\  x^{3}\\right)\\)\n\n\n\nexpr.args[2].args[1]\n\n\\(\\displaystyle x^{3}\\)\n\n\n\nexpr.args[2].args[1].args\n\n\\(\\displaystyle \\left( x, \\  3\\right)\\)\n\n\n\nexpr.args[2].args[1].args[1]\n\n\\(\\displaystyle 3\\)\n\n\n\nexpr.args[2].args[1].args[1].args\n\n\\(\\displaystyle \\left( \\right)\\)",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>B</span>  <span class='chapter-title'>Sympy: Symbolic Computing</span>"
    ]
  },
  {
    "objectID": "x_sympy_symbolic_computing.html#manipulating-expressions",
    "href": "x_sympy_symbolic_computing.html#manipulating-expressions",
    "title": "Appendix B — Sympy: Symbolic Computing",
    "section": "B.5 Manipulating expressions",
    "text": "B.5 Manipulating expressions\n\nB.5.1 Simplication\n\nexpr = 2 * (x**2 - x) - x * (x + 1)\nexpr\n\n\\(\\displaystyle 2 x^{2} - x \\left(x + 1\\right) - 2 x\\)\n\n\n\nsympy.simplify(expr)\n\n\\(\\displaystyle x \\left(x - 3\\right)\\)\n\n\n\nexpr.simplify()\n\n\\(\\displaystyle x \\left(x - 3\\right)\\)\n\n\n\nexpr\n\n\\(\\displaystyle 2 x^{2} - x \\left(x + 1\\right) - 2 x\\)\n\n\n\n\nexpr = 2 * sympy.cos(x) * sympy.sin(x)\nexpr\n\n\\(\\displaystyle 2 \\sin{\\left(x \\right)} \\cos{\\left(x \\right)}\\)\n\n\n\nsympy.simplify(expr)\n\n\\(\\displaystyle \\sin{\\left(2 x \\right)}\\)\n\n\n\nexpr = sympy.exp(x) * sympy.exp(y)\nexpr\n\n\\(\\displaystyle e^{x} e^{y}\\)\n\n\n\nsympy.simplify(expr)\n\n\\(\\displaystyle e^{x + y}\\)\n\n\n\n\nB.5.2 Expand\n\nexpr = (x + 1) * (x + 2)\nexpr\n\n\\(\\displaystyle \\left(x + 1\\right) \\left(x + 2\\right)\\)\n\n\n\nexpr.expand()\n\n\\(\\displaystyle x^{2} + 3 x + 2\\)\n\n\n\n\nsympy.sin(x + y).expand(trig=True)\n\n\\(\\displaystyle \\sin{\\left(x \\right)} \\cos{\\left(y \\right)} + \\sin{\\left(y \\right)} \\cos{\\left(x \\right)}\\)\n\n\n\na, b = sympy.symbols('a, b', positive=True)\nsympy.log(a * b).expand(log=True)\n\n\\(\\displaystyle \\log{\\left(a \\right)} + \\log{\\left(b \\right)}\\)\n\n\n\nsympy.exp(I*a + b).expand(complex=True)\n\n\\(\\displaystyle i e^{b} \\sin{\\left(a \\right)} + e^{b} \\cos{\\left(a \\right)}\\)\n\n\n\nsympy.expand((a * b)**x, power_base=True)\n\n\\(\\displaystyle a^{x} b^{x}\\)\n\n\n\nsympy.exp(I*(a -b)*x).expand(power_exp=True)\n\n\\(\\displaystyle e^{i a x} e^{- i b x}\\)\n\n\n\n\nB.5.3 Factor, collect and combine\n\nsympy.factor(x**2 - 1)\n\n\\(\\displaystyle \\left(x - 1\\right) \\left(x + 1\\right)\\)\n\n\n\nsympy.factor(x *sympy.cos(y) + x *sympy.sin(z))\n\n\\(\\displaystyle x \\left(\\sin{\\left(z \\right)} + \\cos{\\left(y \\right)}\\right)\\)\n\n\n\n\nexpr = x + y + x * y * z\nexpr\n\n\\(\\displaystyle x y z + x + y\\)\n\n\n\nexpr.collect(x)\n\n\\(\\displaystyle x \\left(y z + 1\\right) + y\\)\n\n\n\nexpr.collect(y)\n\n\\(\\displaystyle x + y \\left(x z + 1\\right)\\)\n\n\n\n\nexpr = sympy.cos(x + y) + sympy.sin(x - y)\n\n\nexpr1 = expr.expand(trig=True)\nexpr1\n\n\\(\\displaystyle - \\sin{\\left(x \\right)} \\sin{\\left(y \\right)} + \\sin{\\left(x \\right)} \\cos{\\left(y \\right)} - \\sin{\\left(y \\right)} \\cos{\\left(x \\right)} + \\cos{\\left(x \\right)} \\cos{\\left(y \\right)}\\)\n\n\n\nexpr2 = expr1.collect([sympy.cos(x), sympy.sin(x)])\nexpr2\n\n\\(\\displaystyle \\left(- \\sin{\\left(y \\right)} + \\cos{\\left(y \\right)}\\right) \\sin{\\left(x \\right)} + \\left(- \\sin{\\left(y \\right)} + \\cos{\\left(y \\right)}\\right) \\cos{\\left(x \\right)}\\)\n\n\n\nexpr3 = expr2.collect(sympy.cos(y) - sympy.sin(y))\nexpr3\n\n\\(\\displaystyle \\left(\\sin{\\left(x \\right)} + \\cos{\\left(x \\right)}\\right) \\left(- \\sin{\\left(y \\right)} + \\cos{\\left(y \\right)}\\right)\\)\n\n\n\n\nsympy.logcombine(sympy.log(a) - sympy.log(b))\n\n\\(\\displaystyle \\log{\\left(\\frac{a}{b} \\right)}\\)\n\n\n\n\nB.5.4 Apart, together and cancel\n\nexpr1 = 1/(x**2 + 3*x + 2)\nexpr1\n\n\\(\\displaystyle \\frac{1}{x^{2} + 3 x + 2}\\)\n\n\n\nsympy.apart(expr1, x)\n\n\\(\\displaystyle - \\frac{1}{x + 2} + \\frac{1}{x + 1}\\)\n\n\n\nexpr2 = 1 / (y * x + y) + 1 / (1+x)\nexpr2\n\n\\(\\displaystyle \\frac{1}{x y + y} + \\frac{1}{x + 1}\\)\n\n\n\nsympy.together(expr2)\n\n\\(\\displaystyle \\frac{y + 1}{y \\left(x + 1\\right)}\\)\n\n\n\nexpr3 = y / (y * x + y)\nexpr3\n\n\\(\\displaystyle \\frac{y}{x y + y}\\)\n\n\n\nsympy.cancel(expr3)\n\n\\(\\displaystyle \\frac{1}{x + 1}\\)\n\n\n\n\nB.5.5 Substitutions\n\n(x + y).subs(x, y)\n\n\\(\\displaystyle 2 y\\)\n\n\n\nsympy.sin(x * sympy.exp(x)).subs(x, y)\n\n\\(\\displaystyle \\sin{\\left(y e^{y} \\right)}\\)\n\n\n\nsympy.sin(x * z).subs({z: sympy.exp(y), x: y, sympy.sin: sympy.cos})\n\n\\(\\displaystyle \\cos{\\left(y e^{y} \\right)}\\)\n\n\n\nexpr = x * y + z**2 *x\nexpr\n\n\\(\\displaystyle x y + x z^{2}\\)\n\n\n\nvalues = {x: 1.25, y: 0.4, z: 3.2}\nexpr.subs(values)\n\n\\(\\displaystyle 13.3\\)\n\n\n\n\nB.5.6 Numerical evaluation\n\nsympy.N(1 + pi)\n\n\\(\\displaystyle 4.14159265358979\\)\n\n\n\nsympy.N(1 + pi, 50)\n\n\\(\\displaystyle 4.1415926535897932384626433832795028841971693993751\\)\n\n\n\n\n(x + 1/pi).evalf(10)\n\n\\(\\displaystyle x + 0.3183098862\\)\n\n\n\n\nexpr = sympy.sin(pi * x * sympy.exp(x))\nexpr\n\n\\(\\displaystyle \\sin{\\left(\\pi x e^{x} \\right)}\\)\n\n\n\n[expr.subs(x, i).evalf(3) for i in range(0, 10)]  # rather slow\n\n\\(\\displaystyle \\left[ 0, \\  0.774, \\  0.642, \\  0.722, \\  0.944, \\  0.205, \\  0.974, \\  0.977, \\  -0.87, \\  -0.695\\right]\\)\n\n\n\n\nexpr_func = sympy.lambdify(x, expr)\nexpr_func(1.0)\n\n\\(\\displaystyle 0.773942685266709\\)\n\n\n\n\nimport numpy as np\n\nxvalues = np.arange(0, 10)\n\nexpr_func = sympy.lambdify(x, expr, 'numpy')\nexpr_func(xvalues)  # efficient method\n\narray([ 0.        ,  0.77394269,  0.64198244,  0.72163867,  0.94361635,\n        0.20523391,  0.97398794,  0.97734066, -0.87034418, -0.69512687])",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>B</span>  <span class='chapter-title'>Sympy: Symbolic Computing</span>"
    ]
  },
  {
    "objectID": "x_sympy_symbolic_computing.html#calculus",
    "href": "x_sympy_symbolic_computing.html#calculus",
    "title": "Appendix B — Sympy: Symbolic Computing",
    "section": "B.6 Calculus",
    "text": "B.6 Calculus\n\nB.6.1 Derivatives\n\nf = sympy.Function('f')(x)\nsympy.diff(f, x)  # equivalent to f.diff(x)\n\n\\(\\displaystyle \\frac{d}{d x} f{\\left(x \\right)}\\)\n\n\n\nsympy.diff(f, x, x)\n\n\\(\\displaystyle \\frac{d^{2}}{d x^{2}} f{\\left(x \\right)}\\)\n\n\n\nsympy.diff(f, x, 3)  # equivalent to sympy.diff(f, x, x, x)\n\n\\(\\displaystyle \\frac{d^{3}}{d x^{3}} f{\\left(x \\right)}\\)\n\n\n\n\ng = sympy.Function('g')(x, y)\ng.diff(x, y) # equivalent to sympy.diff(g, x, y)\n\n\\(\\displaystyle \\frac{\\partial^{2}}{\\partial y\\partial x} g{\\left(x,y \\right)}\\)\n\n\n\ng.diff(x, 3, y, 2) # equivalent to sympy.diff(g, x, x, x, y, y)\n\n\\(\\displaystyle \\frac{\\partial^{5}}{\\partial y^{2}\\partial x^{3}} g{\\left(x,y \\right)}\\)\n\n\n\n\nexpr = x**4 + x**3 + x**2 + x + 1\nexpr\n\n\\(\\displaystyle x^{4} + x^{3} + x^{2} + x + 1\\)\n\n\n\nexpr.diff(x)\n\n\\(\\displaystyle 4 x^{3} + 3 x^{2} + 2 x + 1\\)\n\n\n\nexpr.diff(x, x)\n\n\\(\\displaystyle 2 \\left(6 x^{2} + 3 x + 1\\right)\\)\n\n\n\n\nexpr = (x + 1)**3 * y ** 2 * (z - 1)\nexpr\n\n\\(\\displaystyle y^{2} \\left(x + 1\\right)^{3} \\left(z - 1\\right)\\)\n\n\n\nexpr.diff(x, y, z)\n\n\\(\\displaystyle 6 y \\left(x + 1\\right)^{2}\\)\n\n\n\n\nexpr = sympy.sin(x * y) * sympy.cos(x / 2)\nexpr\n\n\\(\\displaystyle \\sin{\\left(x y \\right)} \\cos{\\left(\\frac{x}{2} \\right)}\\)\n\n\n\nexpr.diff(x)\n\n\\(\\displaystyle y \\cos{\\left(\\frac{x}{2} \\right)} \\cos{\\left(x y \\right)} - \\frac{\\sin{\\left(\\frac{x}{2} \\right)} \\sin{\\left(x y \\right)}}{2}\\)\n\n\n\n\nexpr = sympy.functions.special.polynomials.hermite(x, 0)\nexpr\n\n\\(\\displaystyle \\frac{2^{x} \\sqrt{\\pi}}{\\Gamma\\left(\\frac{1}{2} - \\frac{x}{2}\\right)}\\)\n\n\n\nexpr.diff(x)\n\n\\(\\displaystyle \\frac{2^{x} \\sqrt{\\pi} \\operatorname{polygamma}{\\left(0,\\frac{1}{2} - \\frac{x}{2} \\right)}}{2 \\Gamma\\left(\\frac{1}{2} - \\frac{x}{2}\\right)} + \\frac{2^{x} \\sqrt{\\pi} \\log{\\left(2 \\right)}}{\\Gamma\\left(\\frac{1}{2} - \\frac{x}{2}\\right)}\\)\n\n\n\n\nd = sympy.Derivative(sympy.exp(sympy.cos(x)), x)\nd\n\n\\(\\displaystyle \\frac{d}{d x} e^{\\cos{\\left(x \\right)}}\\)\n\n\n\nd.doit()\n\n\\(\\displaystyle - e^{\\cos{\\left(x \\right)}} \\sin{\\left(x \\right)}\\)\n\n\n\n\nB.6.2 Integrals\n\na, b, x, y = sympy.symbols('a, b, x, y')\nf = sympy.Function('f')(x)\nsympy.integrate(f)\n\n\\(\\displaystyle \\int f{\\left(x \\right)}\\, dx\\)\n\n\n\nsympy.integrate(f, (x, a, b))\n\n\\(\\displaystyle \\int\\limits_{a}^{b} f{\\left(x \\right)}\\, dx\\)\n\n\n\n\nsympy.integrate(sympy.sin(x))\n\n\\(\\displaystyle - \\cos{\\left(x \\right)}\\)\n\n\n\nsympy.integrate(sympy.sin(x), (x, a, b))\n\n\\(\\displaystyle \\cos{\\left(a \\right)} - \\cos{\\left(b \\right)}\\)\n\n\n\nsympy.integrate(sympy.exp(-x**2), (x, 0, oo))\n\n\\(\\displaystyle \\frac{\\sqrt{\\pi}}{2}\\)\n\n\n\na, b, c = sympy.symbols('a, b, c', positive=True)\nsympy.integrate(a * sympy.exp(-((x -b)/c)**2), (x, -oo, oo))\n\n\\(\\displaystyle \\sqrt{\\pi} a c\\)\n\n\n\nsympy.integrate(sympy.sin(x * sympy.cos(x))) # No analytic integration\n\n\\(\\displaystyle \\int \\sin{\\left(x \\cos{\\left(x \\right)} \\right)}\\, dx\\)\n\n\n\n\nexpr = sympy.sin(x*sympy.exp(y))\nexpr\n\n\\(\\displaystyle \\sin{\\left(x e^{y} \\right)}\\)\n\n\n\nsympy.integrate(expr, x)\n\n\\(\\displaystyle - e^{- y} \\cos{\\left(x e^{y} \\right)}\\)\n\n\n\n\nexpr = (x + y)**2\nexpr\n\n\\(\\displaystyle \\left(x + y\\right)^{2}\\)\n\n\n\nsympy.integrate(expr, x)\n\n\\(\\displaystyle \\frac{x^{3}}{3} + x^{2} y + x y^{2}\\)\n\n\n\nsympy.integrate(expr, x, y)\n\n\\(\\displaystyle \\frac{x^{3} y}{3} + \\frac{x^{2} y^{2}}{2} + \\frac{x y^{3}}{3}\\)\n\n\n\nsympy.integrate(expr, (x, 0, 1), (y, 0, 1))\n\n\\(\\displaystyle \\frac{7}{6}\\)\n\n\n\n\nB.6.3 Series\n\nsympy.limit(sympy.sin(x) / x, x, 0)\n\n\\(\\displaystyle 1\\)\n\n\n\n\nx, h = sympy.symbols('x, h')\nf = sympy.Function('f')\ndiff_limit = (f(x + h) - f(x)) / h\n\n\nsympy.limit(diff_limit.subs(f, sympy.cos), h, 0)\n\n\\(\\displaystyle - \\sin{\\left(x \\right)}\\)\n\n\n\nsympy.limit(diff_limit.subs(f, sympy.sin), h, 0)\n\n\\(\\displaystyle \\cos{\\left(x \\right)}\\)\n\n\n\nexpr = (x**2 - 3*x) / (2*x - 2)\nexpr\n\n\\(\\displaystyle \\frac{x^{2} - 3 x}{2 x - 2}\\)\n\n\n\np = sympy.limit(expr/x, x, sympy.oo)\n\n\nq = sympy.limit(expr - p*x, x, sympy.oo)\n\n\np, q\n\n\\(\\displaystyle \\left( \\frac{1}{2}, \\  -1\\right)\\)\n\n\n\n\nB.6.4 Sums and products\n\nn = sympy.symbols('n', integer=True)\nx = sympy.Sum(1/(n**2), (n, 1, oo))\nx\n\n\\(\\displaystyle \\sum_{n=1}^{\\infty} \\frac{1}{n^{2}}\\)\n\n\n\nx.doit()\n\n\\(\\displaystyle \\frac{\\pi^{2}}{6}\\)\n\n\n\nx = sympy.Product(n, (n, 1, 7))\nx\n\n\\(\\displaystyle \\prod_{n=1}^{7} n\\)\n\n\n\nx.doit()\n\n\\(\\displaystyle 5040\\)\n\n\n\n\nx = sympy.Symbol('x')\nsympy.Sum((x)**n/(sympy.factorial(n)), (n, 1, oo)).doit().simplify()\n\n\\(\\displaystyle e^{x} - 1\\)",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>B</span>  <span class='chapter-title'>Sympy: Symbolic Computing</span>"
    ]
  },
  {
    "objectID": "x_sympy_symbolic_computing.html#equations",
    "href": "x_sympy_symbolic_computing.html#equations",
    "title": "Appendix B — Sympy: Symbolic Computing",
    "section": "B.7 Equations",
    "text": "B.7 Equations\n\nx = sympy.Symbol('x')\nsympy.solve(x**2 +2*x -3)\n\n\\(\\displaystyle \\left[ -3, \\  1\\right]\\)\n\n\n\na, b, c = sympy.symbols('a, b, c')\nsympy.solve(a *x**2 +b *x +c, x)\n\n\\(\\displaystyle \\left[ \\frac{- b - \\sqrt{- 4 a c + b^{2}}}{2 a}, \\  \\frac{- b + \\sqrt{- 4 a c + b^{2}}}{2 a}\\right]\\)\n\n\n\nsympy.solve(sympy.sin(x) - sympy.cos(x), x)\n\n\\(\\displaystyle \\left[ \\frac{\\pi}{4}\\right]\\)\n\n\n\n\nsympy.solve(sympy.exp(x) + 2 *x, x)\n\n\\(\\displaystyle \\left[ - W\\left(\\frac{1}{2}\\right)\\right]\\)\n\n\nThe value of LambertW function \\(W(z)\\) is such that \\(z = W(z)\\exp(W(z))\\) for any complex number \\(z\\)\n\n-sympy.LambertW(1/2)\n\n\\(\\displaystyle -0.351733711249196\\)\n\n\n\n\nsols = sympy.solve(x**5 - x**2 + 1, x)\nfor i in range(5):\n    display(sols[i])\n\n\\(\\displaystyle \\operatorname{CRootOf} {\\left(x^{5} - x^{2} + 1, 0\\right)}\\)\n\n\n\\(\\displaystyle \\operatorname{CRootOf} {\\left(x^{5} - x^{2} + 1, 1\\right)}\\)\n\n\n\\(\\displaystyle \\operatorname{CRootOf} {\\left(x^{5} - x^{2} + 1, 2\\right)}\\)\n\n\n\\(\\displaystyle \\operatorname{CRootOf} {\\left(x^{5} - x^{2} + 1, 3\\right)}\\)\n\n\n\\(\\displaystyle \\operatorname{CRootOf} {\\left(x^{5} - x^{2} + 1, 4\\right)}\\)\n\n\n\n#sympy.solve(sympy.tan(x) + x, x)\n\nNotImplementedError: multiple generators [x, tan(x)] No algorithms are implemented to solve equation x + tan(x)\n\n\neq1 = x +2 *y -1 \neq2 = x -y +1\n\n\nsympy.solve([eq1, eq2], [x, y], dict=True)\n\n\\(\\displaystyle \\left[ \\left\\{ x : - \\frac{1}{3}, \\  y : \\frac{2}{3}\\right\\}\\right]\\)\n\n\n\neq1 = x**2 -y\neq2 = y**2 -x\n\n\nsols = sympy.solve([eq1, eq2], [x, y], dict=True)\nfor i in range(4):\n    display(sols[i])\n\n\\(\\displaystyle \\left\\{ x : 0, \\  y : 0\\right\\}\\)\n\n\n\\(\\displaystyle \\left\\{ x : 1, \\  y : 1\\right\\}\\)\n\n\n\\(\\displaystyle \\left\\{ x : \\left(- \\frac{1}{2} - \\frac{\\sqrt{3} i}{2}\\right)^{2}, \\  y : - \\frac{1}{2} - \\frac{\\sqrt{3} i}{2}\\right\\}\\)\n\n\n\\(\\displaystyle \\left\\{ x : \\left(- \\frac{1}{2} + \\frac{\\sqrt{3} i}{2}\\right)^{2}, \\  y : - \\frac{1}{2} + \\frac{\\sqrt{3} i}{2}\\right\\}\\)\n\n\n\n[eq1.subs(sol).simplify() == 0 and \n eq2.subs(sol).simplify() == 0 for sol in sols]\n\n[True, True, True, True]",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>B</span>  <span class='chapter-title'>Sympy: Symbolic Computing</span>"
    ]
  },
  {
    "objectID": "x_sympy_symbolic_computing.html#linear-algebra",
    "href": "x_sympy_symbolic_computing.html#linear-algebra",
    "title": "Appendix B — Sympy: Symbolic Computing",
    "section": "B.8 Linear algebra",
    "text": "B.8 Linear algebra\n\nsympy.Matrix([1, 2])\n\n\\(\\displaystyle \\left[\\begin{matrix}1\\\\2\\end{matrix}\\right]\\)\n\n\n\nsympy.Matrix([[1, 2]])\n\n\\(\\displaystyle \\left[\\begin{matrix}1 & 2\\end{matrix}\\right]\\)\n\n\n\nsympy.Matrix([[1, 2], [3, 4]])\n\n\\(\\displaystyle \\left[\\begin{matrix}1 & 2\\\\3 & 4\\end{matrix}\\right]\\)\n\n\n\nsympy.Matrix(3, 4, lambda m, n: 10 * m + n)\n\n\\(\\displaystyle \\left[\\begin{matrix}0 & 1 & 2 & 3\\\\10 & 11 & 12 & 13\\\\20 & 21 & 22 & 23\\end{matrix}\\right]\\)\n\n\n\n\na, b, c, d = sympy.symbols('a, b, c, d')\n\nM = sympy.Matrix([[a, b], [c, d]])\nM\n\n\\(\\displaystyle \\left[\\begin{matrix}a & b\\\\c & d\\end{matrix}\\right]\\)\n\n\n\nM * M  # Matrix multiplication\n\n\\(\\displaystyle \\left[\\begin{matrix}a^{2} + b c & a b + b d\\\\a c + c d & b c + d^{2}\\end{matrix}\\right]\\)\n\n\n\nx = sympy.Matrix(sympy.symbols('x_1, x_2'))\nx\n\n\\(\\displaystyle \\left[\\begin{matrix}x_{1}\\\\x_{2}\\end{matrix}\\right]\\)\n\n\n\nM * x\n\n\\(\\displaystyle \\left[\\begin{matrix}a x_{1} + b x_{2}\\\\c x_{1} + d x_{2}\\end{matrix}\\right]\\)\n\n\n\n\np, q = sympy.symbols('p, q')\nM = sympy.Matrix([[1, p], [q, 1]])\nM\n\n\\(\\displaystyle \\left[\\begin{matrix}1 & p\\\\q & 1\\end{matrix}\\right]\\)\n\n\n\nb = sympy.Matrix(sympy.symbols('b_1, b_2'))\nb\n\n\\(\\displaystyle \\left[\\begin{matrix}b_{1}\\\\b_{2}\\end{matrix}\\right]\\)\n\n\n\nx = M.LUsolve(b)\nx\n\n\\(\\displaystyle \\left[\\begin{matrix}b_{1} - \\frac{p \\left(- b_{1} q + b_{2}\\right)}{- p q + 1}\\\\\\frac{- b_{1} q + b_{2}}{- p q + 1}\\end{matrix}\\right]\\)\n\n\n\nx = M.inv() *b\nx\n\n\\(\\displaystyle \\left[\\begin{matrix}- \\frac{b_{1}}{p q - 1} + \\frac{b_{2} p}{p q - 1}\\\\\\frac{b_{1} q}{p q - 1} - \\frac{b_{2}}{p q - 1}\\end{matrix}\\right]\\)",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>B</span>  <span class='chapter-title'>Sympy: Symbolic Computing</span>"
    ]
  },
  {
    "objectID": "x_sympy_symbolic_computing.html#ode",
    "href": "x_sympy_symbolic_computing.html#ode",
    "title": "Appendix B — Sympy: Symbolic Computing",
    "section": "B.9 ODE",
    "text": "B.9 ODE\n\nx = sympy.Symbol('x')\nf = sympy.Function('f')\n\n\nsympy.dsolve(sympy.Derivative(f(x), x, x) +9 *f(x), f(x))\n\n\\(\\displaystyle f{\\left(x \\right)} = C_{1} \\sin{\\left(3 x \\right)} + C_{2} \\cos{\\left(3 x \\right)}\\)\n\n\n\neq = (sympy.sin(x) *sympy.cos(f(x)) \n    + sympy.cos(x) *sympy.sin(f(x)) *f(x).diff(x))\neq\n\n\\(\\displaystyle \\sin{\\left(x \\right)} \\cos{\\left(f{\\left(x \\right)} \\right)} + \\sin{\\left(f{\\left(x \\right)} \\right)} \\cos{\\left(x \\right)} \\frac{d}{d x} f{\\left(x \\right)}\\)\n\n\n\nsympy.dsolve(eq, hint='1st_exact')\n\n\\(\\displaystyle \\left[ f{\\left(x \\right)} = - \\operatorname{acos}{\\left(\\frac{C_{1}}{\\cos{\\left(x \\right)}} \\right)} + 2 \\pi, \\  f{\\left(x \\right)} = \\operatorname{acos}{\\left(\\frac{C_{1}}{\\cos{\\left(x \\right)}} \\right)}\\right]\\)\n\n\n\n\nt = sympy.Symbol('t')\nx, y = sympy.Function('x'), sympy.Function('y')\n\neq = (sympy.Eq(sympy.Derivative(x(t),t), 12 *t *x(t) + 8 *y(t)),\n      sympy.Eq(sympy.Derivative(y(t),t), 21 *x(t) + 7 *t *y(t)))\neq\n\n\\(\\displaystyle \\left( \\frac{d}{d t} x{\\left(t \\right)} = 12 t x{\\left(t \\right)} + 8 y{\\left(t \\right)}, \\  \\frac{d}{d t} y{\\left(t \\right)} = 7 t y{\\left(t \\right)} + 21 x{\\left(t \\right)}\\right)\\)\n\n\n\nsols = sympy.dsolve(eq)\nfor i in [0, 1]:\n    display(sols[i])\n\n\\(\\displaystyle x{\\left(t \\right)} = C_{1} x_{0}{\\left(t \\right)} + C_{2} x_{0}{\\left(t \\right)} \\int \\frac{8 \\left(e^{\\int 7 t\\, dt}\\right) e^{\\int 12 t\\, dt}}{x_{0}^{2}{\\left(t \\right)}}\\, dt\\)\n\n\n\\(\\displaystyle y{\\left(t \\right)} = C_{1} y_{0}{\\left(t \\right)} + C_{2} \\left(y_{0}{\\left(t \\right)} \\int \\frac{8 \\left(e^{\\int 7 t\\, dt}\\right) e^{\\int 12 t\\, dt}}{x_{0}^{2}{\\left(t \\right)}}\\, dt + \\frac{\\left(e^{\\int 7 t\\, dt}\\right) e^{\\int 12 t\\, dt}}{x_{0}{\\left(t \\right)}}\\right)\\)\n\n\n\n\neq = (sympy.Eq(sympy.Derivative(x(t),t), x(t) *y(t) *sympy.sin(t)), \n      sympy.Eq(sympy.Derivative(y(t),t), y(t)**2 *sympy.sin(t)))\neq\n\n\\(\\displaystyle \\left( \\frac{d}{d t} x{\\left(t \\right)} = x{\\left(t \\right)} y{\\left(t \\right)} \\sin{\\left(t \\right)}, \\  \\frac{d}{d t} y{\\left(t \\right)} = y^{2}{\\left(t \\right)} \\sin{\\left(t \\right)}\\right)\\)\n\n\n\nsympy.dsolve(eq)\n\n\\(\\displaystyle \\left\\{x{\\left(t \\right)} = - \\frac{e^{C_{1}}}{C_{2} e^{C_{1}} - \\cos{\\left(t \\right)}}, y{\\left(t \\right)} = - \\frac{1}{C_{1} - \\cos{\\left(t \\right)}}\\right\\}\\)",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>B</span>  <span class='chapter-title'>Sympy: Symbolic Computing</span>"
    ]
  },
  {
    "objectID": "x_sympy_symbolic_computing.html#plot",
    "href": "x_sympy_symbolic_computing.html#plot",
    "title": "Appendix B — Sympy: Symbolic Computing",
    "section": "B.10 Plot",
    "text": "B.10 Plot\n\nfrom sympy.plotting import plot\n\nx = sympy.symbols('x')\np1 = plot(x**2, (x - 1) * x * (x + 1), (x, -1.2, 1.2))\n\n\n\n\n\n\n\n\n\nfrom sympy.plotting import plot_parametric\n\nu = sympy.symbols('u')\np2 = plot_parametric((sympy.cos(u), sympy.sin(u)), \n                     (u, sympy.cos(u)), (u, -3, 3))\n\n\n\n\n\n\n\n\n\nexpr1 = (u, sympy.cos(2 *pi *u)/2 + 1/2)\nexpr2 = (u, sympy.sin(2 *pi *u)/2 + 1/2)\n\np3 = plot_parametric(expr1, expr2, (u, 0, 1), line_color='blue')\n\n\n\n\n\n\n\n\n\np3[0].line_color = 'red'\np3.show()\n\n\n\n\n\n\n\n\n\nfrom sympy.plotting import plot3d\n\nx, y = sympy.symbols('x y')\np4 = plot3d((x**2 + y**2, (x, -5, 5), (y, -5, 5)),\n    (x*y, (x, -3, 3), (y, -3, 3)))\n\n\n\n\n\n\n\n\n\nfrom sympy.plotting import plot3d_parametric_line\n\nu = sympy.symbols('u')\np5 = plot3d_parametric_line(sympy.cos(u), sympy.sin(u), u, (u, -5, 5))\n\n\n\n\n\n\n\n\n\nfrom sympy.plotting import plot3d_parametric_surface\n\nu, v = sympy.symbols('u v')\np6 = plot3d_parametric_surface(sympy.cos(u + v), \n    sympy.sin(u - v), u - v, (u, -5, 5), (v, -5, 5))\n\n\n\n\n\n\n\n\n\nfrom sympy import plot_implicit, Eq, And\n\np7 = plot_implicit(\n    Eq(x**2 + y**2, 3), (x, -3, 3), (y, -3, 3))\n\n\n\n\n\n\n\n\n\np8 = plot_implicit(And(y &gt; x, y &gt; -x))\n\n\n\n\n\n\n\n\n\nfrom sympy.plotting import PlotGrid\n\np9 = PlotGrid(2, 1, p1, p2)",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>B</span>  <span class='chapter-title'>Sympy: Symbolic Computing</span>"
    ]
  },
  {
    "objectID": "x_sympy_Laplace_transform.html",
    "href": "x_sympy_Laplace_transform.html",
    "title": "Appendix C — Sympy: Laplace Transform",
    "section": "",
    "text": "C.1 Importing sympy\nimport sympy\nfrom sympy import laplace_transform, inverse_laplace_transform, pi\nsympy.init_printing()\n\nfrom IPython.display import display\n\nprint(\"sympy: \", sympy.__version__)\n\nsympy:  1.14.0",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>C</span>  <span class='chapter-title'>Sympy: Laplace Transform</span>"
    ]
  },
  {
    "objectID": "x_sympy_Laplace_transform.html#importing-sympy",
    "href": "x_sympy_Laplace_transform.html#importing-sympy",
    "title": "Appendix C — Sympy: Laplace Transform",
    "section": "",
    "text": "_laplace_trabsform_expansion.py\n\nlaplace_tranform_() is the expanded version of laplace_transform()\nsubs_() is the modified function of sub() method",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>C</span>  <span class='chapter-title'>Sympy: Laplace Transform</span>"
    ]
  },
  {
    "objectID": "x_sympy_Laplace_transform.html#symbols-and-functions",
    "href": "x_sympy_Laplace_transform.html#symbols-and-functions",
    "title": "Appendix C — Sympy: Laplace Transform",
    "section": "C.2 Symbols and Functions",
    "text": "C.2 Symbols and Functions\n\na, b = sympy.symbols(\"a, b\", positive=True, constant=True)\nn, m = sympy.symbols(\"n, m\", positive=True, constant=True, integer=True)\n\nt, tau, tau1, tau2 = sympy.symbols(\"t, tau, tau1, tau2\", positive=True)\n\ny = sympy.Function(\"y\")\nf = sympy.Function(\"f\")\ng = sympy.Function(\"g\")\n\ns = sympy.symbols(\"s\")\n\nY = sympy.Function(\"Y\")\nF = sympy.Function(\"F\")\nG = sympy.Function(\"G\")\n\nLy = laplace_transform_(y(t), t, s)\nLf = laplace_transform_(f(t), t, s)\nLg = laplace_transform_(g(t), t, s)",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>C</span>  <span class='chapter-title'>Sympy: Laplace Transform</span>"
    ]
  },
  {
    "objectID": "x_sympy_Laplace_transform.html#laplace-transform",
    "href": "x_sympy_Laplace_transform.html#laplace-transform",
    "title": "Appendix C — Sympy: Laplace Transform",
    "section": "C.3 Laplace transform",
    "text": "C.3 Laplace transform\n\neq = 1\nLeq = laplace_transform_(eq, t, s)\nLeq\n\n\\(\\displaystyle \\frac{1}{s}\\)\n\n\n\neq = t\nLeq = laplace_transform_(eq, t, s)\nLeq\n\n\\(\\displaystyle \\frac{1}{s^{2}}\\)\n\n\n\neq = sympy.exp(-3 *t)\nLeq = laplace_transform_(eq, t, s)\nLeq\n\n\\(\\displaystyle \\frac{1}{s + 3}\\)\n\n\n\neq = 2 *sympy.Heaviside(t -3)\nLeq = laplace_transform_(eq, t, s)\nLeq\n\n\\(\\displaystyle \\frac{2 e^{- 3 s}}{s}\\)\n\n\n\neq = sympy.sin(2 *t)**2\nLeq = laplace_transform_(eq, t, s)\nLeq\n\n\\(\\displaystyle \\frac{2}{s^{2} + 4} + \\frac{1}{s}\\)",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>C</span>  <span class='chapter-title'>Sympy: Laplace Transform</span>"
    ]
  },
  {
    "objectID": "x_sympy_Laplace_transform.html#inverse-laplace-transform",
    "href": "x_sympy_Laplace_transform.html#inverse-laplace-transform",
    "title": "Appendix C — Sympy: Laplace Transform",
    "section": "C.4 Inverse Laplace transform",
    "text": "C.4 Inverse Laplace transform\n\nLeq = 1 / s**3\neq = inverse_laplace_transform(Leq, s, t)\neq\n\n\\(\\displaystyle \\frac{t^{2}}{2}\\)\n\n\n\nLeq = (-2 *s +6) / (s**2 + 4)\neq = inverse_laplace_transform(Leq, s, t)\neq\n\n\\(\\displaystyle 3 \\sin{\\left(2 t \\right)} - 2 \\cos{\\left(2 t \\right)}\\)",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>C</span>  <span class='chapter-title'>Sympy: Laplace Transform</span>"
    ]
  },
  {
    "objectID": "x_sympy_Laplace_transform.html#laplace-transform-of-derivatives",
    "href": "x_sympy_Laplace_transform.html#laplace-transform-of-derivatives",
    "title": "Appendix C — Sympy: Laplace Transform",
    "section": "C.5 Laplace transform of derivatives",
    "text": "C.5 Laplace transform of derivatives\n\neq = y(t).diff(t, 2) +2 *y(t).diff(t) +10 *y(t)\neq\n\n\\(\\displaystyle 10 y{\\left(t \\right)} + 2 \\frac{d}{d t} y{\\left(t \\right)} + \\frac{d^{2}}{d t^{2}} y{\\left(t \\right)}\\)\n\n\n\nLeq = subs_(laplace_transform_(eq, t, s), Ly, Y(s))\nLeq\n\n\\(\\displaystyle s^{2} Y{\\left(s \\right)} + 2 s Y{\\left(s \\right)} - s y{\\left(0 \\right)} + 10 Y{\\left(s \\right)} - 2 y{\\left(0 \\right)} - \\left. \\frac{d}{d t} y{\\left(t \\right)} \\right|_{\\substack{ t=0 }}\\)\n\n\n\nics = {y(0): 1, y(t).diff(t).subs(t, 0): 1}\n\nLeq = Leq.subs(ics)\nLeq\n\n\\(\\displaystyle s^{2} Y{\\left(s \\right)} + 2 s Y{\\left(s \\right)} - s + 10 Y{\\left(s \\right)} - 3\\)\n\n\n\nsol = sympy.solve(Leq, Y(s))[0]\nsol\n\n\\(\\displaystyle \\frac{s + 3}{s^{2} + 2 s + 10}\\)",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>C</span>  <span class='chapter-title'>Sympy: Laplace Transform</span>"
    ]
  },
  {
    "objectID": "x_sympy_Laplace_transform.html#laplace-transform-of-integrals",
    "href": "x_sympy_Laplace_transform.html#laplace-transform-of-integrals",
    "title": "Appendix C — Sympy: Laplace Transform",
    "section": "C.6 Laplace transform of integrals",
    "text": "C.6 Laplace transform of integrals\n\neq = sympy.integrate(g(tau1), (tau1, 0, t)) +sympy.integrate(sympy.integrate(f(tau2), (tau2, 0, tau1)), (tau1, 0, t))\neq\n\n\\(\\displaystyle \\int\\limits_{0}^{t} g{\\left(\\tau_{1} \\right)}\\, d\\tau_{1} + \\int\\limits_{0}^{t}\\int\\limits_{0}^{\\tau_{1}} f{\\left(\\tau_{2} \\right)}\\, d\\tau_{2}\\, d\\tau_{1}\\)\n\n\n\nLeq = subs_(laplace_transform_(eq, t, s), Lf, F(s))\nLeq = subs_(Leq, Lg, G(s))\nLeq\n\n\\(\\displaystyle \\frac{G{\\left(s \\right)}}{s} + \\frac{F{\\left(s \\right)}}{s^{2}}\\)",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>C</span>  <span class='chapter-title'>Sympy: Laplace Transform</span>"
    ]
  },
  {
    "objectID": "x_sympy_Laplace_transform.html#convolution",
    "href": "x_sympy_Laplace_transform.html#convolution",
    "title": "Appendix C — Sympy: Laplace Transform",
    "section": "C.7 Convolution",
    "text": "C.7 Convolution\n\neq = sympy.integrate(f(tau)*g(t -tau), (tau, 0, t)) +sympy.integrate(f(t -tau)*g(tau), (tau, 0, t))\neq\n\n\\(\\displaystyle \\int\\limits_{0}^{t} f{\\left(\\tau \\right)} g{\\left(t - \\tau \\right)}\\, d\\tau + \\int\\limits_{0}^{t} f{\\left(t - \\tau \\right)} g{\\left(\\tau \\right)}\\, d\\tau\\)\n\n\n\nLeq = subs_(laplace_transform_(eq, t, s), Lf, F(s))\nLeq = subs_(Leq, Lg, G(s))\nLeq\n\n\\(\\displaystyle 2 F{\\left(s \\right)} G{\\left(s \\right)}\\)",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>C</span>  <span class='chapter-title'>Sympy: Laplace Transform</span>"
    ]
  },
  {
    "objectID": "x_sympy_Laplace_transform.html#first-translation-theorem",
    "href": "x_sympy_Laplace_transform.html#first-translation-theorem",
    "title": "Appendix C — Sympy: Laplace Transform",
    "section": "C.8 First translation theorem",
    "text": "C.8 First translation theorem\n\neq = y(t) *sympy.exp(-a *t)\neq\n\n\\(\\displaystyle y{\\left(t \\right)} e^{- a t}\\)\n\n\n\nLeq = subs_(laplace_transform_(eq, t, s), Ly, Y(s))\nLeq\n\n\\(\\displaystyle Y{\\left(a + s \\right)}\\)",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>C</span>  <span class='chapter-title'>Sympy: Laplace Transform</span>"
    ]
  },
  {
    "objectID": "x_sympy_Laplace_transform.html#second-translation-theorem",
    "href": "x_sympy_Laplace_transform.html#second-translation-theorem",
    "title": "Appendix C — Sympy: Laplace Transform",
    "section": "C.9 Second translation theorem",
    "text": "C.9 Second translation theorem\n\neq = f(t -a) *sympy.Heaviside(t -a)\neq\n\n\\(\\displaystyle f{\\left(- a + t \\right)} \\theta\\left(- a + t\\right)\\)\n\n\n\nLeq = subs_(laplace_transform_(eq, t, s), Lf, F(s))\nLeq\n\n\\(\\displaystyle F{\\left(s \\right)} e^{- a s}\\)\n\n\n\neq = g(t) *sympy.Heaviside(t -a)\neq\n\n\\(\\displaystyle g{\\left(t \\right)} \\theta\\left(- a + t\\right)\\)\n\n\n\nLeq = laplace_transform_(eq, t, s)\nLeq\n\n\\(\\displaystyle \\mathcal{L}_{t}\\left[g{\\left(a + t \\right)}\\right]\\left(s\\right) e^{- a s}\\)\n\n\n\neq = sympy.cos(t) *sympy.Heaviside(t -pi)\neq\n\n\\(\\displaystyle \\cos{\\left(t \\right)} \\theta\\left(t - \\pi\\right)\\)\n\n\n\nLeq = laplace_transform_(eq, t, s)\nLeq\n\n\\(\\displaystyle - \\frac{s e^{- \\pi s}}{s^{2} + 1}\\)",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>C</span>  <span class='chapter-title'>Sympy: Laplace Transform</span>"
    ]
  },
  {
    "objectID": "x_sympy_Laplace_transform.html#derivatives-of-transforms",
    "href": "x_sympy_Laplace_transform.html#derivatives-of-transforms",
    "title": "Appendix C — Sympy: Laplace Transform",
    "section": "C.10 Derivatives of Transforms",
    "text": "C.10 Derivatives of Transforms\n\neq = 3 *f(t) *t**m *t**n\neq\n\n\\(\\displaystyle 3 t^{m} t^{n} f{\\left(t \\right)}\\)\n\n\n\nLeq = subs_(laplace_transform_(eq, t, s), Lf, F(s))\nLeq\n\n\\(\\displaystyle 3 \\left(-1\\right)^{n} \\sum_{k_{1}=0}^{n} \\frac{n! \\frac{d^{k_{1}}}{d s^{k_{1}}} \\left(-1\\right)^{m} \\frac{d^{m + \\max\\left(0, - k_{1} + n\\right)}}{d s^{m + \\max\\left(0, - k_{1} + n\\right)}} \\mathcal{L}_{t}\\left[f{\\left(t \\right)}\\right]\\left(s\\right)}{k_{1}! \\left(- k_{1} + n\\right)!}\\)\n\n\n\neq =  t**n *t**m *f(t)*sympy.exp(-2*t)\neq\n\n\\(\\displaystyle t^{m} t^{n} f{\\left(t \\right)} e^{- 2 t}\\)\n\n\n\nLeq = subs_(laplace_transform_(eq, t, s), Lf, F(s))\nLeq\n\n\\(\\displaystyle \\left(-1\\right)^{n} \\sum_{k_{1}=0}^{n} \\frac{n! \\frac{d^{k_{1}}}{d s^{k_{1}}} \\left(-1\\right)^{m} \\left. \\frac{d^{m + \\max\\left(0, - k_{1} + n\\right)}}{d s^{m + \\max\\left(0, - k_{1} + n\\right)}} \\mathcal{L}_{t}\\left[f{\\left(t \\right)}\\right]\\left(s\\right) \\right|_{\\substack{ s=s + 2 }}}{k_{1}! \\left(- k_{1} + n\\right)!}\\)\n\n\n\n\neq = t**n *sympy.diff(f(t), t, t)\neq\n\n\\(\\displaystyle t^{n} \\frac{d^{2}}{d t^{2}} f{\\left(t \\right)}\\)\n\n\n\nLeq = subs_(laplace_transform_(eq, t, s), Lf, F(s))\nLeq\n\n\\(\\displaystyle \\left(-1\\right)^{n} \\frac{d^{n}}{d s^{n}} \\left(s^{2} F{\\left(s \\right)} - s f{\\left(0 \\right)} - \\left. \\frac{d}{d t} f{\\left(t \\right)} \\right|_{\\substack{ t=0 }}\\right)\\)\n\n\n\neq = t *sympy.integrate(f(tau), (tau, 0, t))\neq\n\n\\(\\displaystyle t \\int\\limits_{0}^{t} f{\\left(\\tau \\right)}\\, d\\tau\\)\n\n\n\nLeq = subs_(laplace_transform_(eq, t, s), Lf, F(s))\nLeq.doit()\n\n\\(\\displaystyle - \\frac{\\frac{d}{d s} F{\\left(s \\right)}}{s} + \\frac{F{\\left(s \\right)}}{s^{2}}\\)\n\n\n\n\neq = t *f(t) *sympy.exp(-t)\neq\n\n\\(\\displaystyle t f{\\left(t \\right)} e^{- t}\\)\n\n\n\nLeq = subs_(laplace_transform_(eq, t, s), Lf, F(s))\nLeq\n\n\\(\\displaystyle - \\left. \\frac{d}{d s} F{\\left(s \\right)} \\right|_{\\substack{ s=s + 1 }}\\)\n\n\n\n\neq = t *sympy.diff(f(t), t) *sympy.exp(-4*t)\neq\n\n\\(\\displaystyle t e^{- 4 t} \\frac{d}{d t} f{\\left(t \\right)}\\)\n\n\n\nLeq = subs_(laplace_transform_(eq, t, s), Lf, F(s))\nLeq\n\n\\(\\displaystyle - \\left(s + 4\\right) \\left. \\frac{d}{d s} F{\\left(s \\right)} \\right|_{\\substack{ s=s + 4 }} - F{\\left(s + 4 \\right)}\\)",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>C</span>  <span class='chapter-title'>Sympy: Laplace Transform</span>"
    ]
  },
  {
    "objectID": "x_sympy_Laplace_transform.html#various-transform-examples",
    "href": "x_sympy_Laplace_transform.html#various-transform-examples",
    "title": "Appendix C — Sympy: Laplace Transform",
    "section": "C.11 Various Transform Examples",
    "text": "C.11 Various Transform Examples\n\neq = sympy.exp(-a*t) *sympy.diff(f(t), t, t)\neq\n\n\\(\\displaystyle e^{- a t} \\frac{d^{2}}{d t^{2}} f{\\left(t \\right)}\\)\n\n\n\nLeq = subs_(laplace_transform_(eq, t, s), Lf, F(s))\nLeq\n\n\\(\\displaystyle \\left(a + s\\right)^{2} F{\\left(a + s \\right)} - \\left(a + s\\right) f{\\left(0 \\right)} - \\left. \\frac{d}{d t} f{\\left(t \\right)} \\right|_{\\substack{ t=0 }}\\)\n\n\n\n\neq = sympy.exp(-4*t) *sympy.integrate(f(tau), (tau, 0, t))\neq\n\n\\(\\displaystyle e^{- 4 t} \\int\\limits_{0}^{t} f{\\left(\\tau \\right)}\\, d\\tau\\)\n\n\n\nLeq = subs_(laplace_transform_(eq, t, s), Lf, F(s))\nLeq\n\n\\(\\displaystyle \\frac{F{\\left(s + 4 \\right)}}{s + 4}\\)\n\n\n\n\neq = f(3*a*t)\neq\n\n\\(\\displaystyle f{\\left(3 a t \\right)}\\)\n\n\n\nLeq = subs_(laplace_transform_(eq, t, s), Lf, F(s))\nLeq\n\n\\(\\displaystyle \\frac{F{\\left(\\frac{s}{3 a} \\right)}}{3 a}\\)\n\n\n\\(~\\)",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>C</span>  <span class='chapter-title'>Sympy: Laplace Transform</span>"
    ]
  },
  {
    "objectID": "x_sympy_Laplace_transform.html#example-1",
    "href": "x_sympy_Laplace_transform.html#example-1",
    "title": "Appendix C — Sympy: Laplace Transform",
    "section": "C.12 Example 1",
    "text": "C.12 Example 1\nA technique that can be used to solve certain ODE problems is to Laplace transform the ODE, which for many problems results in an algebraic equation that is easier to solve. The solution to the algebraic equation can then be transformed back to the original domain with an inverse Laplace transform, to obtain the solution to the original problem. For example, consider the following differential equation for a driven harmonic oscillator:\n\\[ \\frac{d^2 y}{dt^2} + 2\\frac{dy}{dt} +10 y = 2\\sin 3t \\]\n\\(~\\)\n\nt = sympy.symbols('t', positive=True)\ny = sympy.Function('y')\n\ns = sympy.symbols('s', real=True)\nY = sympy.Function('Y')\n\nLy = laplace_transform_(y(t), t, s) \n\n\node = y(t).diff(t, 2) +2 *y(t).diff(t) +10 *y(t) -2 *sympy.sin(3*t)\node\n\n\\(\\displaystyle 10 y{\\left(t \\right)} - 2 \\sin{\\left(3 t \\right)} + 2 \\frac{d}{d t} y{\\left(t \\right)} + \\frac{d^{2}}{d t^{2}} y{\\left(t \\right)}\\)\n\n\n\nLode = subs_(laplace_transform_(ode, t, s), Ly, Y(s))\nLode\n\n\\(\\displaystyle s^{2} Y{\\left(s \\right)} + 2 s Y{\\left(s \\right)} - s y{\\left(0 \\right)} + 10 Y{\\left(s \\right)} - 2 y{\\left(0 \\right)} - \\left. \\frac{d}{d t} y{\\left(t \\right)} \\right|_{\\substack{ t=0 }} - \\frac{6}{s^{2} + 9}\\)\n\n\n\nAt this point, we need to specify the initial conditions for the ODE problem. Here we use \\(y(0)=1\\) and \\(y'(0)=0\\), and after creating dictionary that contains these initial conditions, we use it to substitute the values into the Laplace-transformed ODE equation:\n\n\nics = {y(0): 1, y(t).diff(t).subs(t, 0): 0}\nLode = Lode.subs(ics)\nLode\n\n\\(\\displaystyle s^{2} Y{\\left(s \\right)} + 2 s Y{\\left(s \\right)} - s + 10 Y{\\left(s \\right)} - 2 - \\frac{6}{s^{2} + 9}\\)\n\n\n\nThis is an algebraic equation that can be solved for \\(Y(s)\\)\n\n\nYsol = sympy.solve(Lode, Y(s))\nYsol[0]\n\n\\(\\displaystyle \\frac{s^{3} + 2 s^{2} + 9 s + 24}{s^{4} + 2 s^{3} + 19 s^{2} + 18 s + 90}\\)\n\n\n\nThe result is a list of solutions, which in this case contains only one element. Performing the inverse Laplace transformation on this expression gives the solution to the original problem in the time domain:\n\n\nYp = sympy.apart(Ysol[0])\nYp\n\n\\(\\displaystyle - \\frac{6 \\left(2 s - 1\\right)}{37 \\left(s^{2} + 9\\right)} + \\frac{49 s + 92}{37 \\left(s^{2} + 2 s + 10\\right)}\\)\n\n\n\nysol = sympy.inverse_laplace_transform(Yp.args[0], s, t) +sympy.inverse_laplace_transform(Yp.args[1], s, t)\nysol\n\n\\(\\displaystyle \\frac{2 \\sin{\\left(3 t \\right)}}{37} - \\frac{12 \\cos{\\left(3 t \\right)}}{37} + \\frac{43 e^{- t} \\sin{\\left(3 t \\right)}}{111} + \\frac{49 e^{- t} \\cos{\\left(3 t \\right)}}{37}\\)",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>C</span>  <span class='chapter-title'>Sympy: Laplace Transform</span>"
    ]
  },
  {
    "objectID": "x_sympy_Laplace_transform.html#example-2",
    "href": "x_sympy_Laplace_transform.html#example-2",
    "title": "Appendix C — Sympy: Laplace Transform",
    "section": "C.13 Example 2",
    "text": "C.13 Example 2\nIn some instances, the Laplace transform can be used to solve linear differential equations with variable monomial coefficients\n\\[ty'' + y' +ty =0, \\;\\;y(0)=1, \\;y'(0)=0\\]\n\\(~\\)\n\ns, t = sympy.symbols('s, t', positive=True)\n\ny = sympy.Function('y')\nY  = sympy.Function('Y')\nLy = laplace_transform_(y(t), t, s)\n\node = t *y(t).diff(t, 2) +y(t).diff(t) +t *y(t)\node\n\n\\(\\displaystyle t y{\\left(t \\right)} + t \\frac{d^{2}}{d t^{2}} y{\\left(t \\right)} + \\frac{d}{d t} y{\\left(t \\right)}\\)\n\n\n\\(~\\)\n\nReduce the given differential equation to a linear first-order DE in the transformed function \\(Y(s)=\\mathcal{L}_t\\left[y(t)\\right]\\)\n\n\nics={y(0): 1, y(t).diff(t).subs(t, 0): 0}\n\nLode = subs_(laplace_transform_(ode, t, s).subs(ics), Ly, Y(s))\nLode.doit().collect(Y(s).diff(s))\n\n\\(\\displaystyle - s Y{\\left(s \\right)} + \\left(- s^{2} - 1\\right) \\frac{d}{d s} Y{\\left(s \\right)}\\)\n\n\n\nSolve the first-order ODE for \\(Y(s)\\) and then find \\(y(t)=\\mathcal{L}_t^{-1} \\left[Y(s) \\right]\\)\n\n\nsol = sympy.dsolve(Lode, Y(s), hint='separable')\nsol\n\n\\(\\displaystyle Y{\\left(s \\right)} = \\frac{C_{1}}{\\sqrt{s^{2} + 1}}\\)\n\n\n\ny = inverse_laplace_transform(sol.rhs, s, t)\ny\n\n\\(\\displaystyle C_{1} J_{0}\\left(t\\right)\\)\n\n\n\nc = sympy.Eq(y.subs(t, 0), 1)\nc\n\n\\(\\displaystyle C_{1} = 1\\)\n\n\n\ny = y.subs(c.lhs, c.rhs)\ny\n\n\\(\\displaystyle J_{0}\\left(t\\right)\\)",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>C</span>  <span class='chapter-title'>Sympy: Laplace Transform</span>"
    ]
  },
  {
    "objectID": "x_equation_solving.html",
    "href": "x_equation_solving.html",
    "title": "Appendix D — Equation Solving",
    "section": "",
    "text": "D.1 Importing modules\n\\(~\\)\nimport numpy as np\nimport sympy\nsympy.init_printing()\n\nfrom scipy import linalg as la\nfrom scipy import optimize\n\nimport matplotlib as mpl\nimport matplotlib.pyplot as plt\nprint('numpy: ', np.__version__)\nprint('sympy: ', sympy.__version__)\n\nfrom scipy import __version__\nprint('scipy: ', __version__)\n\nprint('matplotlib: ', mpl.__version__)\n\nnumpy:  2.3.1\nsympy:  1.14.0\nscipy:  1.16.0\nmatplotlib:  3.10.0",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>D</span>  <span class='chapter-title'>Equation Solving</span>"
    ]
  },
  {
    "objectID": "x_equation_solving.html#importing-modules",
    "href": "x_equation_solving.html#importing-modules",
    "title": "Appendix D — Equation Solving",
    "section": "E.1 Importing modules",
    "text": "E.1 Importing modules\n\nimport numpy as np\nimport sympy\nsympy.init_printing()\n\nfrom scipy import linalg as la\nfrom scipy import optimize\n\nimport matplotlib as mpl\nimport matplotlib.pyplot as plt\n\n\nprint('numpy: ', np.__version__)\nprint('sympy: ', sympy.__version__)\n\nfrom scipy import __version__\nprint('scipy: ', __version__)\n\nprint('matplotlib: ', mpl.__version__)\n\nnumpy:  2.3.1\nsympy:  1.14.0\nscipy:  1.16.0\nmatplotlib:  3.10.0",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>D</span>  <span class='chapter-title'>Equation Solving</span>"
    ]
  },
  {
    "objectID": "x_equation_solving.html#system-of-linear-equations",
    "href": "x_equation_solving.html#system-of-linear-equations",
    "title": "Appendix D — Equation Solving",
    "section": "D.2 System of Linear equations",
    "text": "D.2 System of Linear equations\n\\(~\\)\n\\[\n\\begin{pmatrix}\n    a_{11} & a_{12} & \\cdots & a_{1n} \\\\\n    a_{21} & a_{22} & \\cdots & a_{2n} \\\\\n    \\vdots & \\vdots & \\ddots & \\vdots \\\\\n    a_{m1} & a_{m2} & \\cdots & a_{mn}\n\\end{pmatrix}\n\\begin{pmatrix}\n    x_1\\\\\n    x_2\\\\\n    \\vdots\\\\\n    x_n\n\\end{pmatrix}=\n\\begin{pmatrix}\n    b_1\\\\\n    b_2\\\\\n    \\vdots\\\\\n    b_m\n\\end{pmatrix}\\]\n\\[\\text{or}\\]\n\\[\\text{simply}~\\mathbf{A}\\mathbf{x}=\\mathbf{b}\\]\n\\(~\\)\n\nD.2.1 Square system\n\\[\n\\begin{aligned}\n2 x_1 + 3 x_2 &= 4\\\\\n5 x_1 + 4 x_2 &= 3\n\\end{aligned}\n\\]\n\nA = np.array([[2, 3], [5, 4]])\nb = np.array([4, 3])\nx = la.solve(A, b)\nx\n\narray([-1.,  2.])\n\n\n\nx1 = np.linspace(-4, 2, 100)\n\nx2_1 = (4 -2 *x1) /3\nx2_2 = (3 -5 *x1) /4\n\n\nfig, ax = plt.subplots(figsize=(6, 4))\n\nax.plot(x1, x2_1, 'r', lw=2, label=r\"$2x_1+3x_2=4$\")\nax.plot(x1, x2_2, 'b', lw=2, label=r\"$5x_1+4x_2=3$\")\n\nax.plot(x[0], x[1], 'ko', lw=2)\nax.annotate(\"\"\"The intersection point of\nthe two lines is the solution\nof the equation system\"\"\",\n            xy=(x[0], x[1]), \n            xycoords='data',\n            xytext=(-120, -75), \n            textcoords='offset points', \n            arrowprops=dict(arrowstyle=\"-&gt;\", \n            connectionstyle=\"arc3, rad=-0.3\"))\n\nax.set_xlim([-4, 2])\nax.set_ylim([-2, 6])\nax.tick_params(which='both', direction='in')\nax.set_xlabel(r\"$x_1$\", fontsize=16)\nax.set_ylabel(r\"$x_2$\", fontsize=16)\nax.legend()\n\n\n\n\n\n\n\n\n\nD.2.1.1 Symbolic approach\n\nA = sympy.Matrix([[2, 3], [5, 4]])\nb = sympy.Matrix([4, 3])\n\n\nA.condition_number()\n\n\\(\\displaystyle \\frac{\\sqrt{2 \\sqrt{170} + 27}}{\\sqrt{27 - 2 \\sqrt{170}}}\\)\n\n\n\nsympy.N(_)\n\n\\(\\displaystyle 7.58240137440151\\)\n\n\n\n\nL, U, _ = A.LUdecomposition()\n\n\nL, U, L * U\n\n\\(\\displaystyle \\left( \\left[\\begin{matrix}1 & 0\\\\\\frac{5}{2} & 1\\end{matrix}\\right], \\  \\left[\\begin{matrix}2 & 3\\\\0 & - \\frac{7}{2}\\end{matrix}\\right], \\  \\left[\\begin{matrix}2 & 3\\\\5 & 4\\end{matrix}\\right]\\right)\\)\n\n\n\nx = A.solve(b); x  # equivalent to A.LUsolve(b)\n\n\\(\\displaystyle \\left[\\begin{matrix}-1\\\\2\\end{matrix}\\right]\\)\n\n\n\n\nD.2.1.2 Numerical approach\n\nA = np.array([[2, 3], [5, 4]])\nb = np.array([4, 3])\n\n\nnp.linalg.cond(A)\n\n\\(\\displaystyle 7.58240137440151\\)\n\n\n\nP, L, U = la.lu(A)\n\n\nP, L, U, P @ (L @ U)\n\n(array([[0., 1.],\n        [1., 0.]]),\n array([[1. , 0. ],\n        [0.4, 1. ]]),\n array([[5. , 4. ],\n        [0. , 1.4]]),\n array([[2., 3.],\n        [5., 4.]]))\n\n\n\nla.solve(A, b)\n\narray([-1.,  2.])\n\n\n\n\nThe advantage of using sympy is of course that we may obtain exact results and we can also include symbolic variables in the matrices. However, not all problems are solvable symbolically, or it may give exceedingly lengthy results\nThe advantage of using a numerical approach with numpy/scipy, on the other hand, is that we are guaranteed to obtain a result, although it will be an approximate solution due to floating-point errors\nSee the code below for an example that illustrates the differences between the symbolic and numerical approaches, and for an example that show the numerical approaches can be sensitive for equation systems with large condition numbers\nIn this example, we solve the equation system\n\\[\n  \\begin{pmatrix}\n  1 & \\sqrt{p}\\\\\n  1 & \\frac{1}{\\sqrt{p}}\n  \\end{pmatrix}\n  \\begin{pmatrix}\n  x_1 \\\\ x_2\n  \\end{pmatrix}=\n  \\begin{pmatrix}\n  1 \\\\ 2\n  \\end{pmatrix}\n  \\]\nwhich for \\(p=1\\) is singular and for \\(p\\) in the vicinity of one is ill-conditioned\nA comparison between this symbolic solution and the numerical solution is shown in Figure below. Here the errors in the numerical solution are due to numerical floating-point errors, and the numerical errors are significantly larger in the vicinity of \\(p=1\\), where the system has a large condition number. Also, if there are other sources of errors in either \\(\\mathbf{A}\\) or \\(\\mathbf{b}\\), the corresponding errors in \\(\\mathbf{x}\\) can be even more severe\n\n\n# Symbolic problem specification\np = sympy.symbols(\"p\", positive=True)\nA = sympy.Matrix([[1, sympy.sqrt(p)], [1, 1/sympy.sqrt(p)]])\nb = sympy.Matrix([1, 2])\n\n# Solve symbolically\nx_sym_sol = A.solve(b)\nx_sym_sol.simplify()\nx_sym_sol\n\n\\(\\displaystyle \\left[\\begin{matrix}\\frac{2 p - 1}{p - 1}\\\\- \\frac{\\sqrt{p}}{p - 1}\\end{matrix}\\right]\\)\n\n\n\nAcond = A.condition_number().simplify()\nAcond\n\n\\(\\displaystyle \\frac{\\max\\left(\\frac{\\sqrt{2} \\sqrt{\\left(p + 1\\right)^{2} - \\sqrt{p^{4} + 14 p^{2} + 1}}}{2 \\sqrt{p}}, \\frac{\\sqrt{2} \\sqrt{\\left(p + 1\\right)^{2} + \\sqrt{p^{4} + 14 p^{2} + 1}}}{2 \\sqrt{p}}\\right)}{\\min\\left(\\frac{\\sqrt{2} \\sqrt{\\left(p + 1\\right)^{2} - \\sqrt{p^{4} + 14 p^{2} + 1}}}{2 \\sqrt{p}}, \\frac{\\sqrt{2} \\sqrt{\\left(p + 1\\right)^{2} + \\sqrt{p^{4} + 14 p^{2} + 1}}}{2 \\sqrt{p}}\\right)}\\)\n\n\n\n# Function for solving numerically\nAA = lambda p: np.array([[1, np.sqrt(p)], [1, 1/np.sqrt(p)]])\nbb = np.array([1, 2])\nx_num_sol = lambda p: np.linalg.solve(AA(p), bb)\n\n\n# Graph the difference between the symbolic (exact) \n# and numerical results.\np_vec = np.linspace(0.9, 1.1, 200)\n\nfig, axes = plt.subplots(2, 1, figsize=(6, 8))\n\nfor n in range(2):\n    x_sym = np.array([x_sym_sol[n].subs(p, pp).evalf() \n                     for pp in p_vec])\n    x_num = np.array([x_num_sol(pp)[n] for pp in p_vec])\n    axes[0].plot(p_vec, (x_num - x_sym) /x_sym, 'k')\n    \naxes[0].set_title(\"Error in solution\\n\"\n                  \"(numerical - symbolic) /symbolic\")\n# axes[0].set_xlabel(r'$p$', fontsize=12)\naxes[0].set_xlim([0.9, 1.1])\naxes[0].set_ylim([-2.0e-13, 4.0e-13])\naxes[0].tick_params(which='both', direction='in')\naxes[0].tick_params(axis='x', pad=7)\naxes[0].set_xticks(np.arange(0.9, 1.1, 0.05))\n\naxes[1].plot(p_vec, [Acond.subs(p, pp).evalf() \n                     for pp in p_vec])\n\naxes[1].set_title(\"Condition number\")\naxes[1].set_xlabel(r'$p$', fontsize=12)\naxes[1].set_xlim([0.9, 1.1])\naxes[1].set_ylim([0, 9000])\naxes[1].tick_params(which='both', direction='in')\naxes[1].tick_params(axis='x', pad=7)\naxes[1].set_xticks(np.arange(0.9, 1.1, 0.05))\naxes[1].set_yticks([3000, 6000, 9000])\n\n\n\n\n\n\n\n\n\n\n\nD.2.2 Rectangular system\n\nD.2.2.1 Under-determined\n\nRectangular systems, with \\(m \\times n\\), can be either under-determined or over-determined\nUnder-determined systems have more variables than equations, so the solution cannot be fully determined. Therefore, for such a system, the solution must be given in terms of the remaining free variables. This makes it difficult to treat this type of problem numerically, but a symbolic approach can often be used instead. For example, consider the underdetermined linear equation system\n\\[\n  \\begin{pmatrix}\n  1 & 2 & 3\\\\\n  4 & 5 & 6\n  \\end{pmatrix}\n  \\begin{pmatrix}\n  x_1 \\\\ x_2 \\\\ x_3\n  \\end{pmatrix}=\n  \\begin{pmatrix}\n  7 \\\\ 8\n  \\end{pmatrix}\n\\]\n\n\nx_vars = sympy.symbols(\"x_1, x_2, x_3\")\nx = sympy.Matrix(x_vars)\nA = sympy.Matrix([[1, 2, 3], [4, 5, 6]])\nb = sympy.Matrix([7, 8])\nsympy.solve(A*x - b, x_vars)\n\n\\(\\displaystyle \\left\\{ x_{1} : x_{3} - \\frac{19}{3}, \\  x_{2} : \\frac{20}{3} - 2 x_{3}\\right\\}\\)\n\n\n\n\nD.2.2.2 Over-determined: Least squares\n\nIt is often interesting to find an approximate solution to an over-determined system. An example of when this situation arises is data fitting: Say we have a model where a variable \\(y\\) is a quadratic polynomial in the variable \\(x\\), so that \\(y = a_0 +a_1 x +a_2 x^2\\), and that we would like to fit this model to experimental data\nHere \\(y\\) is nonlinear in \\(x\\), but \\(y\\) is linear in the three unknown coefficients \\(a_0\\), \\(a_1\\) and \\(a_2\\), and this fact can be used to write the model as a linear equation system. If we collect data for \\(m\\) pairs \\(\\{ x_i, y_i \\}_{i=1}^m\\) of the variables \\(x\\) and \\(y\\), we can write the model as an \\(m \\times 3\\) equation system:\n\\[\n  \\begin{pmatrix}\n  1 & x_1 & x_1^2\\\\\n  \\vdots & \\vdots & \\vdots \\\\\n  1 & x_m & x_m^2\n  \\end{pmatrix}\n  \\begin{pmatrix}\n  a_0 \\\\[4pt] a_1 \\\\[4pt] a_3\n  \\end{pmatrix}=\n  \\begin{pmatrix}\n  y_1 \\\\ \\vdots \\\\ y_m\n  \\end{pmatrix}\n\\]\nFor \\(m &gt; 3\\), there is in general no exact solution, and we need to introduce an approximate solution that give a best fit for the over-determined system\nA natural definition of best fit for the over-determined system \\(\\mathbf{Ax} \\approx \\mathbf{b}\\), is to minimize the sum of square error,\n\\[ \\min_x \\sum_{i=1}^m r_i^2 \\]\nwhere \\(\\mathbf{r} = \\mathbf{b} -\\mathbf{Ax}\\) is the residual vector. This leads to the least square solution of the problem \\(\\mathbf{Ax} \\approx \\mathbf{b}\\), which minimizes the distances between the data points\nIn sympy, we can solve for the least square solution of an over-determined system using the solve_least_squares method, and for numerical problems we can use the scipy function la.lstsq\n\n\nnp.random.seed(1234)\n\n# define true model parameters\nm = 100\nx = np.linspace(-1, 1, m)\na, b, c = 1, 2, 3\ny_exact = a +b*x +c*x**2\n\n# simulate noisy data points\nX = 1 -2*np.random.rand(m)\nY = a +b*X +c*X**2 +np.random.randn(m)\n\n# fit the data to the model using linear least square\n# see np.vander for alternative\nA = np.vstack([X**0, X**1, X**2])  \nsol, r, rank, sv = la.lstsq(A.T, Y)\ny_fit = sol[0] +sol[1] *x +sol[2] *x**2\n\n\nfig, ax = plt.subplots(figsize=(6, 4))\n\nax.plot(X, Y, 'go', alpha=0.5, \n        label='Simulated data')\nax.plot(x, y_exact, 'r', lw=2, \n        label='True value $y = 1 + 2x + 3x^2$')\nax.plot(x, y_fit, 'b', lw=2, \n        label='Least square fit')\nax.set_xlabel(r\"$x$\", fontsize=12)\nax.set_ylabel(r\"$y$\", fontsize=12)\nax.legend(loc=2)\n\n\n\n\n\n\n\n\n\n# fit the data to the model using linear least square: \n\n# 1st order polynomial\nA = np.vstack([X**n for n in range(2)])\nsol, r, rank, sv = la.lstsq(A.T, Y)\ny_fit1 = sum([s*x**n for n, s in enumerate(sol)])\n\n# 15th order polynomial\nA = np.vstack([X**n for n in range(16)])\nsol, r, rank, sv = la.lstsq(A.T, Y)\ny_fit15 = sum([s*x**n for n, s in enumerate(sol)])\n\n\nfig, ax = plt.subplots(figsize=(6, 4))\n\nax.plot(X, Y, 'go', alpha=0.5, \n        label='Simulated data')\nax.plot(x, y_exact, 'r', lw=2, \n        label='True value $y = 1 + 2x + 3x^2$')\nax.plot(x, y_fit1, 'b', lw=2, \n        label='Least square fit [1st order]')\nax.plot(x, y_fit15, 'm', lw=2, \n        label='Least square fit [15th order]')\nax.set_xlabel(r\"$x$\", fontsize=12)\nax.set_ylabel(r\"$y$\", fontsize=12)\nax.legend(loc=2)",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>D</span>  <span class='chapter-title'>Equation Solving</span>"
    ]
  },
  {
    "objectID": "x_equation_solving.html#eigenvalue-problems",
    "href": "x_equation_solving.html#eigenvalue-problems",
    "title": "Appendix D — Equation Solving",
    "section": "D.3 Eigenvalue problems",
    "text": "D.3 Eigenvalue problems\n\neps, delta = sympy.symbols(\"epsilon, Delta\")\nH = sympy.Matrix([[eps, delta], [delta, -eps]])\nH \n\n\\(\\displaystyle \\left[\\begin{matrix}\\epsilon & \\Delta\\\\\\Delta & - \\epsilon\\end{matrix}\\right]\\)\n\n\n\nH.eigenvals()\n\n\\(\\displaystyle \\left\\{ - \\sqrt{\\Delta^{2} + \\epsilon^{2}} : 1, \\  \\sqrt{\\Delta^{2} + \\epsilon^{2}} : 1\\right\\}\\)\n\n\n\nH.eigenvects()[0]\n\n\\(\\displaystyle \\left( - \\sqrt{\\Delta^{2} + \\epsilon^{2}}, \\  1, \\  \\left[ \\left[\\begin{matrix}\\frac{\\epsilon}{\\Delta} - \\frac{\\sqrt{\\Delta^{2} + \\epsilon^{2}}}{\\Delta}\\\\1\\end{matrix}\\right]\\right]\\right)\\)\n\n\n\nH.eigenvects()[1]\n\n\\(\\displaystyle \\left( \\sqrt{\\Delta^{2} + \\epsilon^{2}}, \\  1, \\  \\left[ \\left[\\begin{matrix}\\frac{\\epsilon}{\\Delta} + \\frac{\\sqrt{\\Delta^{2} + \\epsilon^{2}}}{\\Delta}\\\\1\\end{matrix}\\right]\\right]\\right)\\)\n\n\n\n(_, _, evec1), (_, _, evec2) = H.eigenvects()\n\n\nsympy.simplify(evec1[0].T*evec2[0])\n\n\\(\\displaystyle \\left[\\begin{matrix}0\\end{matrix}\\right]\\)\n\n\n\nObtaining analytical expressions for eigenvalues and eigenvectors using these methods is often very desirable indeed, but unfortunately it only works for small matrices\nThus, for larger systems we must resort to a fully numerical approach. For this, we can use the la.eigvals and la.eig functions in the scipy linear algebra package\nMatrices that are either Hermitian or real symmetric have real-valued eigenvalues, and for such matrices, it is advantageous to instead use the functions la.eigvalsh and la.eigh, which guarantees that the eigenvalues returned by the function is stored in a numpy array with real values\n\n\nA = np.array([[1, 3, 5], [3, 5, 3], [5, 3, 9]])\nA\n\narray([[1, 3, 5],\n       [3, 5, 3],\n       [5, 3, 9]])\n\n\n\nla.eigvals(A)\n\narray([13.35310908+0.j, -1.75902942+0.j,  3.40592034+0.j])\n\n\n\nevals, evecs = la.eig(A)\n\n\nevecs\n\narray([[ 0.42663918,  0.90353276, -0.04009445],\n       [ 0.43751227, -0.24498225, -0.8651975 ],\n       [ 0.79155671, -0.35158534,  0.49982569]])\n\n\n\nla.eigvalsh(A)\n\narray([-1.75902942,  3.40592034, 13.35310908])\n\n\n\nevals, evecs = la.eigh(A)\n\n\nevecs\n\narray([[ 0.90353276, -0.04009445, -0.42663918],\n       [-0.24498225, -0.8651975 , -0.43751227],\n       [-0.35158534,  0.49982569, -0.79155671]])",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>D</span>  <span class='chapter-title'>Equation Solving</span>"
    ]
  },
  {
    "objectID": "x_equation_solving.html#nonlinear-equation",
    "href": "x_equation_solving.html#nonlinear-equation",
    "title": "Appendix D — Equation Solving",
    "section": "D.4 Nonlinear equation",
    "text": "D.4 Nonlinear equation\n\nA nonlinear equation can always be written on the form \\(f(x)=0\\), where \\(f(x)\\) is a nonlinear function and we seek the value of \\(x\\) (which can be a scalar or a vector) such that \\(f(x)\\) is zero. This \\(x\\) is called the root of \\(f(x)=0\\), and equation solving is therefore often referred to as root finding\n\n\nD.4.1 Univariate equation\n\nIn sympy, we can solve many analytically solvable univariate and nonlinear equations using the sympy.solve function. For example,\n\n\nx, a, b, c = sympy.symbols(\"x, a, b, c\")\n\n\nsympy.solve(a + b*x + c*x**2, x)\n\n\\(\\displaystyle \\left[ \\frac{- b - \\sqrt{- 4 a c + b^{2}}}{2 c}, \\  \\frac{- b + \\sqrt{- 4 a c + b^{2}}}{2 c}\\right]\\)\n\n\n\nsympy.solve(a*sympy.cos(x) - b*sympy.sin(x), x)\n\n\\(\\displaystyle \\left[ - 2 \\operatorname{atan}{\\left(\\frac{b - \\sqrt{a^{2} + b^{2}}}{a} \\right)}, \\  - 2 \\operatorname{atan}{\\left(\\frac{b + \\sqrt{a^{2} + b^{2}}}{a} \\right)}\\right]\\)\n\n\n```{python}\nsympy.solve(sympy.sin(x)-x, x)\n```\n\nNotImplementedError: multiple generators [x, sin(x)]\nNo algorithms are implemented to solve equation -x + sin(x)\n\nIn this type of situation, we need to resort to various numerical techniques. As a first step, it is often very useful to graph the function. This can give important clues about the number of solutions to the equation, and their approximate locations\n\n\nx = np.linspace(-2, 2, 400)\n\n# four examples of nonlinear functions\nf1 = x**2 -x -1\nf2 = x**3 -3*np.sin(x)\nf3 = np.exp(x) -2\nf4 = 1 -x**2 +np.sin(50 /(1 +x**2))\n\n\n# plot each function\nfig, axes = plt.subplots(4, 1, figsize=(4, 12), sharex=True)\n\nfor n, f in enumerate([f1, f2, f3, f4]):\n    axes[n].plot(x, f, lw=1.5)\n    axes[n].axhline(0, ls=':', color='k')\n    axes[n].set_xlim(-2, 2)\n    axes[n].set_ylim(-5, 5)\n    axes[n].set_xticks([-2, -1, 0, 1, 2])\n    axes[n].tick_params(which='both', direction='in')\n    axes[n].set_ylabel(r'$f(x)$', fontsize=12)\n    \naxes[0].set_xlabel(r'$x$', fontsize=12)\n\ntitles = [r'$f(x) = x^2 -x -1$', \n          r'$f(x) = x^3 -3 \\sin(x)$',\n          r'$f(x) = \\exp(x) -2$', \n          r'$f(x) = \\sin\\left( 50/(1 +x^2) \\right) +1 -x^2$']\nfor n, title in enumerate(titles):\n    axes[n].set_title(title)\n    \nfig.tight_layout()\n\n\n\n\n\n\n\n\n\n\nD.4.2 Bisection method\n\n# define a function, desired tolerance \n# and starting interval [a, b]\nx = np.linspace(-2.1, 2.1, 1000)\nf = lambda x: np.exp(x) - 2\ntol = 0.1\na, b = -2, 2\n\n\n# graph the function f\nfig, ax = plt.subplots(1, 1, figsize=(6.8, 4))\n\nax.plot(x, f(x), lw=1.5)\nax.axhline(0, ls=':', color='k')\nax.set_xlabel(r'$x$', fontsize=12)\nax.set_ylabel(r'$f(x)$', fontsize=12)\nax.set_ylim(-3, 7)\nax.set_xticks([-2, -1, 0, 1, 2])\nax.tick_params(which='both', direction='in')\n\n# find the root using the bisection method and visualize\n# the steps in the method in the graph\nfa, fb = f(a), f(b)\n\nax.plot(a, fa, 'ko'), ax.vlines(a, 0, fa, ls=':')\nax.plot(b, fb, 'ko'), ax.vlines(b, 0, fb, ls=':')\nax.text(a, fa - 0.8, r\"$a$\", ha='center', fontsize=12)\nax.text(b, fb + 0.5, r\"$b$\", ha='center', fontsize=12)\n\nn = 1\nwhile b - a &gt; tol:\n    m = a + (b - a)/2\n    fm = f(m)\n\n    ax.plot(m, fm, 'ko')\n    ax.plot([m, m], [0, fm], color='g', ls=':')\n    ax.text(m, fm -0.5, rf\"$m_{n}$\", ha='center', fontsize=8)\n    n += 1\n    \n    if np.sign(fa) == np.sign(fm):\n        a, fa = m, fm\n    else:\n        b, fb = m, fm\n\nax.plot(m, fm, 'r*', markersize=10)\nax.annotate(f\"Root approximately at {m: .3f}\",\n            fontsize=12, \n            family=\"serif\",\n            xy=(a, fm), \n            xycoords='data',\n            xytext=(-150, +50), \n            textcoords='offset points', \n            arrowprops=dict(arrowstyle=\"-&gt;\", \n            connectionstyle=\"arc3, rad=-.5\"))\n\nax.set_title(\"Bisection method\")\nplt.show()\n\n\n\n\n\n\n\n\n\n\nD.4.3 Newton’s method\n\ns_x = sympy.symbols(\"x\")\ns_f = sympy.exp(s_x) -2\n\nf = lambda x: sympy.lambdify(s_x, s_f, 'numpy')(x)\nfp = lambda x: sympy.lambdify(s_x, sympy.diff(s_f, s_x), 'numpy')(x)\n\nx = np.linspace(-1.9, 2.1, 1000)\n\n# define a function, desired tolerance and starting point xk\ntol = 0.01\nxk = 2\n\n\n# setup a graph for visualizing the root finding steps\nfig, ax = plt.subplots(1, 1, figsize=(6.8, 4))\n\nax.plot(x, f(x))\nax.axhline(0, ls=':', color='k')\nax.set_xticks([-2, -1, 0, 1, 2])\nax.set_ylim(-3, 7)\nax.set_xlabel(r'$x$', fontsize=12)\nax.set_ylabel(r'$f(x)$', fontsize=12)\nax.tick_params(which='both', direction='in')\n\n# repeat Newton's method \n# until convergence to the desired tolerance has been reached\n\nn = 0\nwhile f(xk) &gt; tol:\n    xk_new = xk -f(xk) /fp(xk)\n\n    ax.plot([xk, xk], [0, f(xk)], color='k', ls=':')\n    ax.plot(xk, f(xk), 'ko')\n    ax.text(xk, -.5, rf'$x_{n}$', ha='center')\n    ax.plot([xk, xk_new], [f(xk), 0], 'g:')\n\n    xk = xk_new\n    n += 1\n\nax.plot(xk, f(xk), 'ro', markersize=10)\nax.annotate(f\"Root approximately at {xk: .3f}\",\n            fontsize=12, \n            family=\"serif\",\n            xy=(xk, f(xk)), \n            xycoords='data',\n            xytext=(-150, +50), \n            textcoords='offset points', \n            arrowprops=dict(arrowstyle=\"-&gt;\", \n            connectionstyle=\"arc3, rad=-.5\"))\n\nax.set_title(\"Newton's method\")\nplt.show()\n\n\n\n\n\n\n\n\n\nThe scipy optimize module provides multiple functions for numerical root finding. The optimize.bisect and optimize.newton functions implement variants of bisection and Newton methods\n\n\nf = lambda x: np.exp(x) -2\n\n\noptimize.bisect(f, -2, 2)\n\n\\(\\displaystyle 0.693147180560118\\)\n\n\n\nx_root_guess = 2\nfprime = lambda x: np.exp(x)\n\n\noptimize.newton(f, x_root_guess)\n\n\\(\\displaystyle 0.693147180559946\\)\n\n\n\noptimize.newton(f, x_root_guess, fprime=fprime)\n\n\\(\\displaystyle 0.693147180559945\\)\n\n\n\nThe scipy optimize module provides additional functions for root finding. In particular, the optimize.brentq and optimize.brenth functions, which are variants of the bisection method, and also work on an interval where the function changes sign\n\n\noptimize.brentq(f, -2, 2)\n\n\\(\\displaystyle 0.693147180559945\\)\n\n\n\noptimize.brenth(f, -2, 2)\n\n\\(\\displaystyle 0.693147180559938\\)\n\n\n\noptimize.ridder(f, -2, 2)\n\n\\(\\displaystyle 0.69314718055904\\)",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>D</span>  <span class='chapter-title'>Equation Solving</span>"
    ]
  },
  {
    "objectID": "x_equation_solving.html#system-of-nonlinear-equations",
    "href": "x_equation_solving.html#system-of-nonlinear-equations",
    "title": "Appendix D — Equation Solving",
    "section": "D.5 System of nonlinear equations",
    "text": "D.5 System of nonlinear equations\n\nFor example, consider the following system of two multivariate and nonlinear equations:\n\\[\n  \\begin{aligned}\n  y - x^3 -2x^2 +1 &= 0\\\\\n  y + x^2 -1 &= 0\n  \\end{aligned}\n\\]\n\n\ndef f(x):\n  return [x[1] -x[0]**3 -2*x[0]**2 +1, x[1] +x[0]**2 -1]\n\n\noptimize.fsolve(f, [1, 1])\n\narray([0.73205081, 0.46410162])\n\n\n\noptimize.broyden1(f, [1, 1])\n\narray([0.73205046, 0.46410254])\n\n\n\noptimize.broyden2(f, [1, 1])\n\narray([0.73205083, 0.4641017 ])\n\n\n\n\nx, y = sympy.symbols(\"x, y\")\nf_mat = sympy.Matrix([y - x**3 -2*x**2 + 1, y + x**2 - 1])\nf_mat.jacobian(sympy.Matrix([x, y]))\n\n\\(\\displaystyle \\left[\\begin{matrix}- 3 x^{2} - 4 x & 1\\\\2 x & 1\\end{matrix}\\right]\\)\n\n\n\ndef f_jacobian(x):\n  return [[-3*x[0]**2 - 4*x[0], 1], [2*x[0], 1]]\n\n\noptimize.fsolve(f, [1, 1], fprime=f_jacobian)\n\narray([0.73205081, 0.46410162])\n\n\n\n\n#def f(x):\n#  return [x[1] -x[0]**3 -2*x[0]**2 +1, x[1] +x[0]**2 -1]\n\nx = np.linspace(-3, 2, 5000)\ny1 = x**3 +2*x**2 -1\ny2 = -x**2 +1\n \nfig, ax = plt.subplots(figsize=(6, 4))\n\nax.plot(x, y1, 'b', lw=1.5, label=r'$y = x^3 +2x^2 -1$')\nax.plot(x, y2, 'g', lw=1.5, label=r'$y =-x^2 +1$')\n\nx_guesses = [[-2, 2], [1, -1], [-2, -5]]\nfor x_guess in x_guesses:\n    sol = optimize.fsolve(f, x_guess)\n    ax.plot(sol[0], sol[1], 'r*', markersize=10)\n\n    ax.plot(x_guess[0], x_guess[1], 'ko')\n    ax.annotate(\"\", \n                xy=(sol[0], sol[1]), \n                xytext=(x_guess[0], x_guess[1]),\n                arrowprops=dict(arrowstyle=\"-&gt;\", \n                linewidth=1.5, linestyle=':'))\n    \nax.legend(loc=0)\nax.set_xlabel(r'$x$', fontsize=12)\nax.set_ylabel(r'$y$', fontsize=12)\n\nplt.show()\n\n\n\n\n\n\n\n\n\nimport warnings\nwarnings.filterwarnings(\"error\")\n\n#def f(x):\n#    return [x[1] -x[0]**3 -2*x[0]**2 +1, x[1] +x[0]**2 -1]\n\nfig, ax = plt.subplots(figsize=(6, 4))\n\nax.plot(x, y1, 'k', lw=1.5, label=r'$y = x^3 +2x^2 -1$')\nax.plot(x, y2, 'k', lw=1.5, label=r'$y = -x^2 +1$')\n\nsol1 = optimize.fsolve(f, [-2,  2])\nsol2 = optimize.fsolve(f, [ 1, -1])\nsol3 = optimize.fsolve(f, [-2, -5])\n\ncolors = ['r', 'b', 'g']\nfor m in np.linspace(-4, 3, 80):\n    for n in np.linspace(-15, 15, 40):\n        x_guess = [m, n]\n\n        try: \n            sol = optimize.fsolve(f, x_guess)\n\n            for idx, s in enumerate([sol1, sol2, sol3]):\n                if abs(s -sol).max() &lt; 1e-8:\n                    ax.plot(sol[0], sol[1], \n                            colors[idx]+'*', markersize=10)\n                    ax.plot(x_guess[0], x_guess[1], \n                            colors[idx]+'.')\n        except:\n            pass\n\n\nax.set_xlim(-4, 3)\nax.set_ylim(-15, 15)\nax.set_xlabel(r'$x$', fontsize=12)\nax.set_ylabel(r'$y$', fontsize=12)\n\nplt.show()",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>D</span>  <span class='chapter-title'>Equation Solving</span>"
    ]
  },
  {
    "objectID": "x_interpolation.html",
    "href": "x_interpolation.html",
    "title": "Appendix F — Interpolation",
    "section": "",
    "text": "F.1 Importing modules\n\\(~\\)\nInterpolation is a mathematical method for constructing a function from a discrete set of data points. To perform interpolation in python, we use the polynomial module from numpy and the interpolate module from scipy\nimport numpy as np\nfrom numpy import polynomial as P\nfrom scipy import interpolate\nfrom scipy import linalg\n\nimport matplotlib as mpl\nimport matplotlib.pyplot as plt",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>F</span>  <span class='chapter-title'>Interpolation</span>"
    ]
  },
  {
    "objectID": "x_interpolation.html#importing-modules",
    "href": "x_interpolation.html#importing-modules",
    "title": "Appendix F — Interpolation",
    "section": "G.1 Importing modules",
    "text": "G.1 Importing modules\n\nimport numpy as np\nfrom numpy import polynomial as P\nfrom scipy import interpolate\nfrom scipy import linalg\n\nimport matplotlib as mpl\nimport matplotlib.pyplot as plt",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>F</span>  <span class='chapter-title'>Interpolation</span>"
    ]
  },
  {
    "objectID": "x_interpolation.html#interpolation",
    "href": "x_interpolation.html#interpolation",
    "title": "Appendix F — Interpolation",
    "section": "F.2 Interpolation",
    "text": "F.2 Interpolation\n\nOne-dimensional interpolation can be formulated as follows:\nFor a given set of \\(n\\) data point \\(\\{ (x_i, y_i) \\}_{i=1}^n\\), find a function \\(f(x)\\) such that \\(f(x_i) = y_i\\) for \\(i \\in [1, n]\\). The function \\(f(x)\\) is known as the interpolant, and it is not unique\nIn fact, there are an infinite number of functions that satisfy the interpolation criteria. Typically we can write the interpolant as a linear combination of some basis functions \\(\\phi_j(x)\\), such that\n\\[ f(x) = \\sum_{j=1}^n c_j \\phi_j(x) \\]\nwhere \\(c_j\\) are unknown coefficients\nSubstituting the given data points into this linear combination results in a linear equation system for the unknown coefficients: \\(\\sum_{j=1}^n c_j \\phi_j(x_i) = y_i\\)\n\\[\n  \\begin{pmatrix}\n  \\phi_1(x_1) & \\phi_2(x_1) & \\cdots & \\phi_n(x_1) \\\\\n  \\phi_1(x_2) & \\phi_2(x_2) & \\cdots & \\phi_n(x_2) \\\\\n  \\vdots & \\vdots & \\ddots & \\vdots\\\\\n  \\phi_1(x_n) & \\phi_2(x_n) & \\cdots & \\phi_n(x_n) \\\\\n  \\end{pmatrix}\n  \\begin{pmatrix}\n  c_1 \\\\ c_2 \\\\ \\vdots \\\\c_n\n  \\end{pmatrix} =\n  \\begin{pmatrix}\n  y_1 \\\\ y_2 \\\\ \\vdots \\\\y_n\n  \\end{pmatrix}\n\\]\nor in a more compact implicit matrix form as \\(\\Phi(\\mathbf{x}) \\mathbf{c} = \\mathbf{y}\\), where the elements of the matrix \\(\\Phi(\\mathbf{x})\\) are \\(\\Phi(\\mathbf{x})_{ij} = \\phi_j(x_i)\\)\nCommon choices of basis functions for interpolation are various types of polynomials, for example, the power basis \\(\\phi_j(x_i) = x_i^{j-1}\\), or orthogonal polynomials such as Legendre polynomials \\(\\phi_j(x_i) = P_{j-1}(x_i)\\), Chebyshev polynomials \\(\\phi_j(x_i) = T_{j-1}(x_i)\\), or piecewise polynomials\nNote that in general \\(f(x)\\) is not unique, but for \\(n\\) data points there is a unique interpolating polynomial of order \\(n-1\\), regardless of which polynomial basis we use. The structure of the matrix \\(\\Phi(\\mathbf{x})\\) is different for different polynomial bases, and its condition number and the computational cost of solving the interpolation problem varies correspondingly",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>F</span>  <span class='chapter-title'>Interpolation</span>"
    ]
  },
  {
    "objectID": "x_interpolation.html#polynomials",
    "href": "x_interpolation.html#polynomials",
    "title": "Appendix F — Interpolation",
    "section": "F.3 Polynomials",
    "text": "F.3 Polynomials\n\nThe numpy library contains the submodule polynomial (here imported as P), which provides functions and classes for working with polynomials. In particular, it provides implementations of many standard orthogonal polynomials\nFor example, we can create a representation of the polynomial\n\\[1 +2x +3x^2\\]\nby passing the list [1, 2, 3] to the Polynomial class:\n\n\np1 = P.Polynomial([1,2,3])\np1\n\n\\(x \\mapsto \\text{1.0} + \\text{2.0}\\,x + \\text{3.0}\\,x^{2}\\)\n\n\n\nIn the examples above, the first of the lists coef is the coefficient array and the second and third lists are the domain and window attributes\n\n\np1.coef, p1.domain, p1.window\n\n(array([1., 2., 3.]), array([-1.,  1.]), array([-1.,  1.]))\n\n\n\nAlternatively, we can also initialize a polynomial by specifying its roots using the class method P.Polynomial.fromroots. The polynomial with roots at \\(x=-1\\) and \\(x=1\\), for example, can be created using:\n\n\np2 = P.Polynomial.fromroots([-1, 1])\np2\n\n\\(x \\mapsto \\text{-1.0}\\color{LightGray}{ + \\text{0.0}\\,x} + \\text{1.0}\\,x^{2}\\)\n\n\n\n\nThe roots of a polynomial can be computed using the roots method\n\n\np1.roots()\n\narray([-0.33333333-0.47140452j, -0.33333333+0.47140452j])\n\n\n\np2.roots()\n\narray([-1.,  1.])\n\n\n\nTo evaluate the polynomial p1 at the points \\(x=[1.5, 2.5, 3.5]\\), we simply call the p1 class instance with a list of \\(x\\) values as this argument:\n\n\np1(np.array([1.5, 2.5, 3.5]))\n\narray([10.75, 24.75, 44.75])\n\n\n\nConsider addition, subtraction, multiplication, and division of the polynomial \\(p_1(x) = (x -3)(x -2)(x -1)\\) with the polynomial \\(p_2(x) = (x -2)\\)\n\n\np1 = P.Polynomial.fromroots([1, 2, 3])\np2 = P.Polynomial.fromroots([2])\n\n\np3 = p1 + p2\np3\n\n\\(x \\mapsto \\text{-8.0} + \\text{12.0}\\,x - \\text{6.0}\\,x^{2} + \\text{1.0}\\,x^{3}\\)\n\n\n\np4 = p1 - p2\np4\n\n\\(x \\mapsto \\text{-4.0} + \\text{10.0}\\,x - \\text{6.0}\\,x^{2} + \\text{1.0}\\,x^{3}\\)\n\n\n\np5 = p1 * p2\np5\n\n\\(x \\mapsto \\text{12.0} - \\text{28.0}\\,x + \\text{23.0}\\,x^{2} - \\text{8.0}\\,x^{3} + \\text{1.0}\\,x^{4}\\)\n\n\n\np6 = p1 // p2\np6\n\n\\(x \\mapsto \\text{3.0} - \\text{4.0}\\,x + \\text{1.0}\\,x^{2}\\)\n\n\n\np6.roots()\n\narray([1., 3.])\n\n\n\nIn addition to the Polynomial class for polynomials in the standard power basis, the polynomial module also has classes for representing polynomials in Chebyshev, Legendre, Laguerre and Hermite bases, with the names Chebyshev, Legendre, Laguerre, Hermite (Physicists’) and HermiteE (Probabilists’), respectively\nFor example, the Chebyshev polynomial with coefficient list [1, 2, 3], that is, the polynomial\n\\[ T_0(x) +2T_1(x) +3T_2(x) \\]\nwhere \\(T_i(x)\\) is the Chebyshev polynomial of order \\(i\\), can be created using:\n\n\nc1 = P.Chebyshev([1, 2, 3])\nc1\n\n\\(x \\mapsto \\text{1.0}\\,{T}_{0}(x) + \\text{2.0}\\,{T}_{1}(x) + \\text{3.0}\\,{T}_{2}(x)\\)\n\n\n\nc1.roots()\n\narray([-0.76759188,  0.43425855])\n\n\n\n\nc1 = P.Chebyshev.fromroots([-1, 1])\nc1\n\n\\(x \\mapsto \\text{-0.5}\\,{T}_{0}(x)\\color{LightGray}{ + \\text{0.0}\\,{T}_{1}(x)} + \\text{0.5}\\,{T}_{2}(x)\\)\n\n\n\nl1 = P.Legendre.fromroots([-1, 1])\nl1\n\n\\(x \\mapsto \\text{-0.66666667}\\,{P}_{0}(x)\\color{LightGray}{ + \\text{0.0}\\,{P}_{1}(x)} + \\text{0.66666667}\\,{P}_{2}(x)\\)\n\n\n\nc1(np.array([-0.5, 0, 1]))\n\narray([-0.75, -1.  ,  0.  ])\n\n\n\nl1(np.array([-0.5, 0, 1]))\n\narray([-0.75, -1.  ,  0.  ])",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>F</span>  <span class='chapter-title'>Interpolation</span>"
    ]
  },
  {
    "objectID": "x_interpolation.html#polynomial-interpolation",
    "href": "x_interpolation.html#polynomial-interpolation",
    "title": "Appendix F — Interpolation",
    "section": "F.4 Polynomial interpolation",
    "text": "F.4 Polynomial interpolation\n\nTo solve the interpolation problem, we need to first evaluate the matrix \\(\\Phi(\\mathbf{x})\\) for a given basis, and then solve the resulting linear equation system \\(\\Phi(\\mathbf{x}) \\mathbf{c} = \\mathbf{y}\\). Each of the polynomial classes in polynomial provides a function for computing the (generalized) Vandermonde matrix for the corresponding basis\nFor example, consider the data points \\((1,1)\\), \\((2,3)\\), \\((3,5)\\) and \\((4,4)\\)\n\n\nx = np.array([1, 2, 3, 4])\ny = np.array([1, 3, 5, 4])\n\n\nTo interpolate a polynomial through these points, we need to use a polynomial of third degree (number of data points minus one)\nFor interpolation in the power basis, we seek the coefficients \\(c_i\\)’s such that\n\\[ f(x) = c_1 +c_2 x +c_3 x^2 +c_4 x^3 \\]\nand to find these coefficients, we evaluate the Vandermonde matrix and solve the interpolation equation system\n\n\ndeg = len(x) -1\nA = P.polynomial.polyvander(x, deg)\nA\n\narray([[ 1.,  1.,  1.,  1.],\n       [ 1.,  2.,  4.,  8.],\n       [ 1.,  3.,  9., 27.],\n       [ 1.,  4., 16., 64.]])\n\n\n\nc = linalg.solve(A, y)\nc\n\narray([ 2. , -3.5,  3. , -0.5])\n\n\n\nThe interpolation polynomial is thus \\(f(x) = 2 -3.5x + 3x^2 -0.5x^3\\)\n\n\nf1 = P.Polynomial(c)\n\n\nf1(2.5)\n\nnp.float64(4.1875)\n\n\n\n\nTo perform this polynomial interpolation in another polynomial basis, all that we need to change is the name of the function that was used to generate the Vandermonde matrix \\(\\mathbf{A}\\) in the previous example\n\n\nA = P.chebyshev.chebvander(x, deg)\nA\n\narray([[  1.,   1.,   1.,   1.],\n       [  1.,   2.,   7.,  26.],\n       [  1.,   3.,  17.,  99.],\n       [  1.,   4.,  31., 244.]])\n\n\n\nc = linalg.solve(A, y)\nc\n\narray([ 3.5  , -3.875,  1.5  , -0.125])\n\n\n\nThe interpolation polynomial in the Chebyshev basis is\n\\[ f(x) = 3.5 T_0(x) -3.875 T_1(x) +1.5 T_2(x) -0.125 T_3(x) \\]\n\n\nf2 = P.Chebyshev(c)\n\n\nf2(2.5)\n\nnp.float64(4.1875)\n\n\n\nxx = np.linspace(x.min(), x.max(), 100)\n\nfig, ax = plt.subplots(1, 1, figsize=(6, 4))\n\nax.plot(xx, f1(xx), 'b', lw=2, label='Power basis interp.')\nax.plot(xx, f2(xx), 'r--', lw=2, label='Chebyshev basis interp.')\nax.scatter(x, y, label='data points')\n\nax.legend(loc=4)\nax.tick_params(which='both', direction='in')\nax.set_xlim(0.5, 4.5)\nax.set_ylim(0, 6)\nax.set_xticks(x)\nax.set_ylabel(r\"$y$\", fontsize=12)\nax.set_xlabel(r\"$x$\", fontsize=12)\n\nplt.show()\n\n\n\n\n\n\n\n\n\nf1b = P.Polynomial.fit(x, y, deg)\nf1b\n\n\\(x \\mapsto \\text{4.1875} + \\text{3.1875}\\,\\left(\\text{-1.66666667} + \\text{0.66666667}x\\right) - \\text{1.6875}\\,\\left(\\text{-1.66666667} + \\text{0.66666667}x\\right)^{2} - \\text{1.6875}\\,\\left(\\text{-1.66666667} + \\text{0.66666667}x\\right)^{3}\\)\n\n\n\nf2b = P.Chebyshev.fit(x, y, deg)\nf2b\n\n\\(x \\mapsto \\text{3.34375}\\,{T}_{0}(\\text{-1.66666667} + \\text{0.66666667}x) + \\text{1.921875}\\,{T}_{1}(\\text{-1.66666667} + \\text{0.66666667}x) - \\text{0.84375}\\,{T}_{2}(\\text{-1.66666667} + \\text{0.66666667}x) - \\text{0.421875}\\,{T}_{3}(\\text{-1.66666667} + \\text{0.66666667}x)\\)\n\n\n\\(~\\)\n\nMapping the interpolation data into the range that is most suitable for a specific basis can significantly improve the numerical stability of the interpolation. For example, using the Chebyshev basis with \\(x\\) values that are scaled such that \\(x \\in [-1, 1]\\), rather than the original \\(x\\) values in the previous example, reduces the condition number from almost \\(4660\\) to about \\(1.85\\):\n\n\nnp.linalg.cond(P.chebyshev.chebvander(x, deg))\n\nnp.float64(4659.7384241404425)\n\n\n\nnp.linalg.cond(P.chebyshev.chebvander((2*x -5)/3.0, deg))\n\nnp.float64(1.8542033440472898)\n\n\n\nNote that with fit method, the domain attribute of the resulting instances are automatically set to the appropriate \\(x\\) values of the data points (in this example, the input range is [1, 4]), and the coefficients are adjusted accordingly. To illustrate the problem, the values of the Chebyshev polynomials up to degree 5 are plotted below\n\n\nx = np.linspace(-1, 1, 100)\n\nfig = plt.figure(figsize=(5.5, 4))\nfor i in range(6): \n    ax = plt.plot(x, P.Chebyshev.basis(i)(x), lw=2, \n                  label=f'$T_{i}$')\nplt.legend(bbox_to_anchor=(1.01, 1.0), loc=2)\nplt.xlim(-1, 1)\nplt.ylim(-1, 1)\nplt.show()\n\n\n\n\n\n\n\n\n\nIn the range \\(-1 \\geq x \\geq 1\\), they are nice, equiripple functions. The same plots over the range \\(-2 \\geq x \\geq 2\\) look very different:\n\n\nx = np.linspace(-2, 2, 100)\n\nfig = plt.figure(figsize=(5.5, 4))\nfor i in range(6): \n    ax = plt.plot(x, P.Chebyshev.basis(i)(x), lw=2, \n                  label=f'$T_{i}$')\nplt.legend(bbox_to_anchor=(1.01, 1.0), loc=2)\nplt.xlim(-2, 2)\nplt.ylim(-400, 400)\nplt.show()\n\n\n\n\n\n\n\n\n\n\nHigh-order polynomial interpolation can have undesirable behavior between the interpolation points. Although the interpolation is exact at the given data points, a high-order polynomial can vary wildly between the specified points\nThis is famously illustrated by polynomial interpolation of Runge’s function\n\\[f(x) = \\frac{1}{1 + 25x^2}\\]\nusing evenly spaced sample points in the interval \\([-1, 1]\\). The result is an interpolant that nearly diverges between the data points near the end of the interval\n\n\ndef runge(x):\n  return 1/(1 + 25*x**2)\n\ndef runge_interpolate(n):\n  x = np.linspace(-1, 1, n +1)\n  p = P.Polynomial.fit(x, runge(x), deg=n)\n  return x, p\n\n\nxx = np.linspace(-1, 1, 250)\n\nfig, ax = plt.subplots(1, 1, figsize=(6, 4))\n\nax.plot(xx, runge(xx), 'k', lw=2, label=\"Runge's function\")\n\n# 13th order interpolation of the Runge function\nn = 13\nx, p = runge_interpolate(n)\nax.plot(x, runge(x), 'ro')\nax.plot(xx, p(xx), 'r', label=f'interp. order {n}')\n\n# 14th order interpolation of the Runge function\nn = 14\nx, p = runge_interpolate(n)\nax.plot(x, runge(x), 'go')\nax.plot(xx, p(xx), 'g', label=f'interp. order {n}')\n\nax.legend(loc=8)\nax.set_xlim(-1.1, 1.1)\nax.set_ylim(-1, 2)\nax.set_xticks([-1, -0.5, 0, 0.5, 1])\nax.set_xlabel(r\"$x$\", fontsize=12)\nax.set_ylabel(r\"$y$\", fontsize=12)\n\nax.tick_params(which='both', direction='in')",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>F</span>  <span class='chapter-title'>Interpolation</span>"
    ]
  },
  {
    "objectID": "x_interpolation.html#spline-interpolation",
    "href": "x_interpolation.html#spline-interpolation",
    "title": "Appendix F — Interpolation",
    "section": "F.5 Spline interpolation",
    "text": "F.5 Spline interpolation\n\nFor a set of \\(n\\) data points \\(\\{ x_i, y_i\\}\\), there are \\(n-1\\) subintervals \\([x_i, x_{i+1}]\\) in the full range of the data \\([x_0, x_n]\\). To interpolate the \\(n\\) data points using piecewise polynomials of degree \\(k\\) on each of the subintervals, we must determine \\((k+1)(n-1)\\) unknown parameters. The values at the knots give \\(2(n-1)\\) equations. Additional equations can be obtained by requiring also that derivatives and higher-order derivatives are continuous at the knots\nA spline is a special type of piecewise polynomial interpolant: a piecewise polynomial of degree \\(k\\) is a spline if it is continuously differentiable \\(k-1\\) times. The most popular choice is the third-order spline, \\(k=3\\), which requires \\(4(n-1)\\) parameters\nFor this case, the continuity of two derivatives at the \\(n-2\\) knots gives \\(2(n-2)\\) additional equations, bringing the total number of equations to \\(2(n-1) +2(n-2) = 4(n-1) -2\\)\nThere are therefore two remaining undetermined parameters, which must be determined by other means. A common approach is to additionally require that the second order derivatives at the end points are zero (resulting in the natural spline). This gives two more equations, which closes the equation system\n\n\nx = np.linspace(-1, 1, 11)\nxx = np.linspace(-1, 1, 100)\ny = runge(x)\nf_i = interpolate.interp1d(x, y, kind=3)\n\nfig, ax = plt.subplots(figsize=(6, 4))\n\nax.plot(x, y, 'ro', label='Sample points')\nax.plot(xx, runge(xx), 'k', lw=1, label=\"Runge's function\")\nax.plot(xx, f_i(xx), 'r--', lw=2, label='Spline order 3')\nax.legend()\n\nax.set_xlim(-1, 1)\nax.set_ylim(0, 1.2)\nax.set_xticks([-1, -0.5, 0, 0.5, 1])\nax.set_xlabel(r\"$x$\", fontsize=12)\nax.set_ylabel(r\"$y$\", fontsize=12)\nax.tick_params(which='both', direction='in')\n\nplt.show()\n\n\n\n\n\n\n\n\n\nTo illustrate the effect of the order of a spline interpolation, consider the problem of interpolating the data\n\\[ (0,3), (1,4), (2,3.5), (3,2), (4,1), (5,1.5), (6,1.25) \\text{ and } (7,0.7) \\]\nwith splines of increasing order\n\n\nx = np.array([0, 1, 2, 3, 4, 5, 6, 7])\ny = np.array([3, 4, 3.5, 2, 1, 1.5, 1.25, 0.7])\n\nxx = np.linspace(x.min(), x.max(), 100)\n\nfig, ax = plt.subplots(figsize=(6, 4))\n\nax.scatter(x, y)\nfor n in [1, 2, 3, 7]:\n    f = interpolate.interp1d(x, y, kind=n)\n    ax.plot(xx, f(xx), label='order %d' % n)\n    \nax.legend()\nax.set_xlabel(r\"$x$\", fontsize=12)\nax.set_ylabel(r\"$y$\", fontsize=12)\n\nax.tick_params(which='both', direction='in')",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>F</span>  <span class='chapter-title'>Interpolation</span>"
    ]
  },
  {
    "objectID": "x_interpolation.html#multivariate-interpolation",
    "href": "x_interpolation.html#multivariate-interpolation",
    "title": "Appendix F — Interpolation",
    "section": "F.6 Multivariate interpolation",
    "text": "F.6 Multivariate interpolation\n\ndef f(x, y):\n  return (np.exp(-(x +0.5)**2 -2*(y +0.5)**2) \n         -np.exp(-(x -0.5)**2 -2*(y -0.5)**2))\n\nx = y = np.linspace(-2, 2, 20)\nX, Y = np.meshgrid(x, y)\n\n# simulate noisy data at fixed grid points X, Y\nZ = f(X, Y) +0.01 *np.random.randn(*X.shape)\n\nf_i = interpolate.RectBivariateSpline(x, y, Z)\n\nxx = yy = np.linspace(x.min(), x.max(), 100)\nXX, YY = np.meshgrid(xx, yy)\n\nZZi = f_i(xx, yy)\n\n\nfig, axes = plt.subplots(2, 1, figsize=(6, 10), sharex=True)\n\nc = axes[0].contourf(XX, YY, f(XX, YY), 10, cmap=plt.cm.RdBu)\naxes[0].set_ylabel(r\"$y$\", fontsize=12)\naxes[0].set_title(\"Exact/High sampling\")\ncb = fig.colorbar(c, ax=axes[0])\ncb.set_label(r\"$z$\", fontsize=12)\n\nc = axes[1].contourf(XX, YY, ZZi, 10, cmap=plt.cm.RdBu)\naxes[1].set_ylim(-2, 2)\naxes[1].set_xlim(-2, 2)\naxes[1].set_xlabel(r\"$x$\", fontsize=12)\naxes[1].set_ylabel(r\"$y$\", fontsize=12)\naxes[1].set_title(\"Interpolation of noisy data/Low sampling\")\ncb = fig.colorbar(c, ax=axes[1])\ncb.set_label(r\"$z$\", fontsize=12)\n\n\n\n\n\n\n\n\n\nFor higher-dimensional problems, there is a function interpolate.interpnd, which is a generalization to \\(n\\)-dimensional problems\nAnother typical situation that requires multivariate interpolation occurs when sampled data is given on an irregular coordinate grid. To be able to easily plot and analyze such data with existing tools, it may be desirable to interpolate it onto a regular coordinate grid. In scipy, we can use the interpolate.griddata for exactly this task\n\n\ndef f(x, y):\n  return np.exp(-x**2 -y**2) *np.cos(4*x) *np.sin(6*y)\n\nx = y = np.linspace(-1, 1, 100)\nX, Y = np.meshgrid(x, y)\nZ = f(X, Y)\n\nnp.random.seed(115925231)\nN = 500\nxdata = np.random.uniform(-1, 1, N)\nydata = np.random.uniform(-1, 1, N)\nzdata = f(xdata, ydata)\n\n\nfig, ax = plt.subplots(figsize=(6, 5))\n\nc = ax.contourf(X, Y, Z, 15, cmap=plt.cm.RdBu);\nax.scatter(xdata, ydata, marker='.')\nax.set_ylim(-1, 1)\nax.set_xlim(-1, 1)\nax.set_xlabel(r\"$x$\", fontsize=12)\nax.set_ylabel(r\"$y$\", fontsize=12)\n\ncb = fig.colorbar(c, ax=ax)\ncb.set_label(r\"$z$\", fontsize=12)\n\n\n\n\n\n\n\n\n\ndef z_interpolate(xdata, ydata, zdata):\n    Zi_0 = interpolate.griddata((xdata, ydata), \n                        zdata, (X, Y), method='nearest')\n    Zi_1 = interpolate.griddata((xdata, ydata), \n                        zdata, (X, Y), method='linear')\n    Zi_3 = interpolate.griddata((xdata, ydata), \n                        zdata, (X, Y), method='cubic')\n    return Zi_0, Zi_1, Zi_3\n\nfig, axes = plt.subplots(3, 3, figsize=(6, 6), \n                         sharex=True, sharey=True)\n\nn_vec = [50, 150, 500]\n\nfor idx, n in enumerate(n_vec):\n    Zi_0, Zi_1, Zi_3 = z_interpolate(xdata[:n], \n                                     ydata[:n], \n                                     zdata[:n])\n    axes[idx, 0].contourf(X, Y, Zi_0, 15, cmap=plt.cm.RdBu)\n    axes[idx, 0].set_ylabel(f'{n} data points\\ny', fontsize=10)\n    axes[idx, 0].set_title(\"nearest\", fontsize=10)\n    axes[idx, 1].contourf(X, Y, Zi_1, 15, cmap=plt.cm.RdBu)\n    axes[idx, 1].set_title(\"linear\", fontsize=10)\n    axes[idx, 2].contourf(X, Y, Zi_3, 15, cmap=plt.cm.RdBu)\n    axes[idx, 2].set_title(\"cubic\", fontsize=10)\n\nfor m in range(len(n_vec)):\n    axes[idx, m].set_xlabel(\"x\", fontsize=10)",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>F</span>  <span class='chapter-title'>Interpolation</span>"
    ]
  },
  {
    "objectID": "x_ordinary_differential_equations.html",
    "href": "x_ordinary_differential_equations.html",
    "title": "Appendix H — Ordinary Differential Equations",
    "section": "",
    "text": "H.1 Importing modules\nIn this appendix, we will explore both symbolic and numerical approaches to solving ODE problems. For symbolic methods, we use the sympy module, and for numerical integration of ODEs, we use functions from the integrate module in scipy\nimport numpy as np\nimport sympy\nfrom sympy import pi\nsympy.init_printing()\n\nimport matplotlib.pyplot as plt\nimport matplotlib as mpl\nfrom mpl_toolkits.mplot3d.axes3d import Axes3D",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>H</span>  <span class='chapter-title'>Ordinary Differential Equations</span>"
    ]
  },
  {
    "objectID": "x_ordinary_differential_equations.html#importing-modules",
    "href": "x_ordinary_differential_equations.html#importing-modules",
    "title": "Appendix H — Ordinary Differential Equations",
    "section": "I.1 Importing modules",
    "text": "I.1 Importing modules\n\nimport numpy as np\nimport sympy\nfrom sympy import pi\nsympy.init_printing()\n\nimport matplotlib.pyplot as plt\nimport matplotlib as mpl\nfrom mpl_toolkits.mplot3d.axes3d import Axes3D",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>H</span>  <span class='chapter-title'>Ordinary Differential Equations</span>"
    ]
  },
  {
    "objectID": "x_ordinary_differential_equations.html#ordinary-differential-equations",
    "href": "x_ordinary_differential_equations.html#ordinary-differential-equations",
    "title": "Appendix H — Ordinary Differential Equations",
    "section": "H.2 Ordinary differential equations",
    "text": "H.2 Ordinary differential equations\n\nThe simplest form of an ordinary differential equation is\n\\[ \\frac{dy(x)}{dx} = f(x,y(x))\\]\nwhere \\(y(x)\\) is the unknown function and \\(f(x, y(x))\\) is known. \\(\\,\\)Only the first derivative occurs in the equation, and it is therefore an example of a first-order ODE\nMore generally, \\(~\\)we can write an \\(n\\)-th order ODE in explicit form as\n\\[ \\frac{d^ny}{dx^n} = f\\left(x,y,\\frac{dy}{dx},\\cdots,\\frac{d^{n-1}y}{dx^{n-1}}\\right)\\]\nor in implicit form as\n\\[ F\\left(x,y,\\frac{dy}{dx},\\cdots,\\frac{d^{n}y}{dx^{n}}\\right)=0\\]\nwhere \\(f\\) and \\(F\\) are known functions.\nThe general solution of an \\(n\\)-th order ODE have \\(n\\) free parameters that we need to specify, for example, as initial conditions for the unknown function and \\(n -1\\) of its derivatives\n\\[y(0)=y_0, y'(0)=y_1, \\cdots, y^{n-1}(0)=y_{n-1}\\]\nAn ODE can always be rewritten as a system of first-order ODEs. Specifically, the \\(n\\)-th order ODE on the explicit form can be written in the standard form by introducing \\(n\\) new functions\n\\[y_1 = y,\\, y_2=\\frac{dy}{dx},\\, \\cdots,\\, y_n = \\frac{dy^{n-1}}{dx^{n-1}}\\]\nThis gives the following system of first-order ODEs:\n\\[\\frac{d}{dx}\n  \\begin{bmatrix}\n  y_1\\\\\n  \\vdots\\\\\n  y_{n-1}\\\\\n  y_n\n  \\end{bmatrix}\n  =\n  \\begin{bmatrix}\n  y_2\\\\\n  \\vdots\\\\\n  y_n\\\\\n  f(x,y_1,\\cdots, y_n)\n  \\end{bmatrix}\n\\]\nwhich also can be written in a more compact vector form:\n\\[\\frac{d\\mathbf{y}}{dx} = \\mathbf{f}(x,\\mathbf{y}(x))\\]\nThis canonical form is particularly useful for numerical solutions of ODEs, and it is common that numerical methods for solving ODEs take the function \\(\\mathbf{f} = (f_1, f_2,\\cdots,f_n)\\)\nIf the functions \\(f_1, f_2, \\cdots, f_n\\) are all linear, then the corresponding system of ODEs can be written on the simple form\n\\[ \\frac{d\\mathbf{y}}{dx} = \\mathbf{A}(\\mathbf{x})\\mathbf{y}(\\mathbf{x}) +\\mathbf{r}(\\mathbf{x}),\\]\nwhere \\(\\mathbf{A}(\\mathbf{x})\\) is an \\(n\\times n\\) matrix, and \\(\\mathbf{r}(\\mathbf{x})\\) is an \\(n\\)-vector, that only depends on \\(\\mathbf{x}\\). In this form, the \\(\\mathbf{r}(\\mathbf{x})\\) is known as the source term, and the linear system is known as homogeneous if \\(\\mathbf{r}(\\mathbf{x})=\\mathbf{0}\\), and nonhomogeneous otherwise\nFor certain properties and forms of the function \\(\\mathbf{f}(\\mathbf{x}, \\mathbf{y}(\\mathbf{x}))\\), there may be known solutions and special methods for solving the corresponding ODE problem, but there is no general method for an arbitrary \\(\\mathbf{f}(\\mathbf{x}, \\mathbf{y}(\\mathbf{x}))\\), other than approximate numerical methods\nIn addition to the properties of \\(\\mathbf{f}(\\mathbf{x}, \\mathbf{y}(\\mathbf{x}))\\), the boundary conditions for an ODE also influence the solvability of the ODE problem, as well as which numerical approaches are available. There are two main types of boundary conditions for ODE problems: initial value conditions and boundary value conditions",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>H</span>  <span class='chapter-title'>Ordinary Differential Equations</span>"
    ]
  },
  {
    "objectID": "x_ordinary_differential_equations.html#symbolic-solution-to-odes",
    "href": "x_ordinary_differential_equations.html#symbolic-solution-to-odes",
    "title": "Appendix H — Ordinary Differential Equations",
    "section": "H.3 Symbolic solution to ODEs",
    "text": "H.3 Symbolic solution to ODEs\n\nSympy provides a generic ODE solver sympy.dsolve, which is able to find analytical solutions to many elementary ODEs. The sympy.dsolve function attempts to automatically classify a given ODE, and it may attempt a variety of techniques to find its solution\nWhile dsolve can be used to solve many simple ODEs symbolically, as we will see in the following, it is worth keeping in mind that most ODEs cannot be solved analytically. Typical examples of ODEs where one can hope to find a symbolic solution are ODEs of first or second-order, or linear systems of first-order ODEs with only a few unknown functions\nThe first example is the simple first-order ODE for Newton’s cooling law,\n\\[\\frac{dT(t)}{dt} = -k(T(t) -T_a)\\]\nwith the initial value \\(T(0) = T_0\\)\n\n\nt, k, T0, Ta = sympy.symbols('t, k, T_0, T_a')\nT = sympy.Function('T')\n\node = T(t).diff(t) +k *(T(t) - Ta)\n\n# display the equation including the equality sign \n# and a right-hand side that is zero\n\nsympy.Eq(ode, 0) \n\n\\(\\displaystyle k \\left(- T_{a} + T{\\left(t \\right)}\\right) + \\frac{d}{d t} T{\\left(t \\right)} = 0\\)\n\n\n\node_sol = sympy.dsolve(ode)\node_sol\n\n\\(\\displaystyle T{\\left(t \\right)} = C_{1} e^{- k t} + T_{a}\\)\n\n\n\nThe return value from sympy.dsolve is an instance of sympy.Eq, which is a symbolic representation of an equality\n\n\node_sol.lhs, ode_sol.rhs\n\n\\(\\displaystyle \\left( T{\\left(t \\right)}, \\  C_{1} e^{- k t} + T_{a}\\right)\\)\n\n\n\nOnce the general solution has been found, \\(\\,\\)we need to use the initial conditions to find the values of the yet-to-be-determined integration constants\nWe first create a dictionary that describes the initial condition, ics = {T(0): T0}, which we can use with sympy’s subs method to apply the initial condition to the solution of the ODE. This results in an equation for the unknown integration constant \\(C_1\\):\n\n\nics = {T(0): T0}\nC_eq = sympy.Eq(ode_sol.lhs.subs(t, 0).subs(ics), \n                ode_sol.rhs.subs(t, 0))\nC_eq\n\n\\(\\displaystyle T_{0} = C_{1} + T_{a}\\)\n\n\n\nIn the present example, the equation for \\(C_1\\) is trivial to solve, but for the sake of generality, here we solve it using sympy.solve:\n\n\nC_sol = sympy.solve(C_eq)\nC_sol\n\n\\(\\displaystyle \\left[ \\left\\{ C_{1} : T_{0} - T_{a}\\right\\}\\right]\\)\n\n\n\node_sol.subs(C_sol[0])\n\n\\(\\displaystyle T{\\left(t \\right)} = T_{a} + \\left(T_{0} - T_{a}\\right) e^{- k t}\\)\n\n\n\nApplying the initial conditions and solving for the undetermined integration constants can be slightly tedious, and it worth while to collect these steps in a reusable function. The following function apply_ics is a basic implementation that generalizes these steps to a differential equation of arbitrary order\n\n\ndef apply_ics(sol, ics, x, known_params):\n    \"\"\"\n    Apply the initial conditions (ics), \n    given as a dictionary on the form \n    \n    ics = {y(0): y0, y(x).diff(x).subs(x, 0): y1, ...}\n    \n    to the solution of the ODE with independent variable x\n\n    The undetermined integration constants \n    \n    C1, C2, ... \n    \n    are extracted from the free symbols of the ODE solution, \n    excluding symbols in the known_params list\n    \"\"\"\n    free_params = sol.free_symbols - set(known_params)\n    eqs = [(sol.lhs.diff(x, n) \n           - sol.rhs.diff(x, n)).subs(x, 0).subs(ics)\n           for n in range(len(ics))]\n    sol_params = sympy.solve(eqs, free_params)\n    \n    return sol.subs(sol_params)\n\n\nT_sol = apply_ics(ode_sol, ics, t, [k, Ta])\nT_sol\n\n\\(\\displaystyle T{\\left(t \\right)} = T_{a} + \\left(T_{0} - T_{a}\\right) e^{- k t}\\)\n\n\n\nx = np.linspace(0, 4, 100)\ny = sympy.lambdify((t, k), \n                   T_sol.rhs.subs({T0: 5, Ta: 1}), \n                   'numpy')\n\nfig, ax = plt.subplots(figsize=(6, 4))\n\nfor k in [1, 2, 3]:\n  ax.plot(x, y(x, k), label=rf'$k={k}$')\n\nax.set_title(rf'${sympy.latex(T_sol)}$', fontsize=12)\nax.set_xlabel(r\"$x$\", fontsize=12)\nax.set_ylabel(r\"$y$\", fontsize=12)\nax.legend()\nax.set_xlim(0, 4)\nax.set_ylim(1, 5)\nax.tick_params(which='both', direction='in', axis='x', pad=7)\n\n\n\n\n\n\n\n\n\nAs an example of a slightly more complicated problem, \\(\\,\\)consider the ODE for a damped harmonic oscillator, which is a second-order ODE on the form\n\\[ \\frac{d^2x(t)}{dt^2} +2\\gamma \\omega_0 \\frac{dx(t)}{dt} +\\omega_0^2 x(t) = 0\\]\nwhere \\(x(t)\\) is the position of the oscillator at time \\(t\\), \\(\\,\\omega_0\\) is the frequency for the undamped case, and \\(\\,\\gamma\\) is the damping ratio\n\n\nt, omega0, gamma = sympy.symbols('t, omega_0, gamma', \n                                 positive=True)\nx = sympy.Function(\"x\")\n\node = (x(t).diff(t, 2) \n      +2 *gamma* omega0 *x(t).diff(t) \n      +omega0**2 *x(t))\nsympy.Eq(ode, 0)\n\n\\(\\displaystyle 2 \\gamma \\omega_{0} \\frac{d}{d t} x{\\left(t \\right)} + \\omega_{0}^{2} x{\\left(t \\right)} + \\frac{d^{2}}{d t^{2}} x{\\left(t \\right)} = 0\\)\n\n\n\node_sol = sympy.dsolve(ode)\node_sol\n\n\\(\\displaystyle x{\\left(t \\right)} = C_{1} e^{\\omega_{0} t \\left(- \\gamma + \\sqrt{\\gamma - 1} \\sqrt{\\gamma + 1}\\right)} + C_{2} e^{- \\omega_{0} t \\left(\\gamma + \\sqrt{\\gamma - 1} \\sqrt{\\gamma + 1}\\right)}\\)\n\n\n\nSince this is a second-order ODE, there are two undetermined integration constants in the general solution. We need to specify initial conditions for both the position \\(x(0)\\) and the velocity \\(x'(0)\\) to single out a particular solution to the ODE\n\n\nics = {x(0): 1, x(t).diff(t).subs(t, 0): 0}\nx_sol = apply_ics(ode_sol, ics, t, [omega0, gamma])\nC1 = x_sol.rhs.args[0].args[0]\nC1\n\n\\(\\displaystyle \\frac{\\gamma}{2 \\sqrt{\\gamma - 1} \\sqrt{\\gamma + 1}} + \\frac{1}{2}\\)\n\n\n\nC2 = x_sol.rhs.args[1].args[0]\nC2\n\n\\(\\displaystyle - \\frac{\\gamma}{2 \\sqrt{\\gamma - 1} \\sqrt{\\gamma + 1}} + \\frac{1}{2}\\)\n\n\n\nThis is the solution for the dynamics of the oscillator for arbitrary values of \\(t\\), \\(\\omega_0\\) and \\(\\gamma\\). \\(~\\)However, substituting \\(\\gamma = 1\\), \\(\\,\\)which corresponds to critical damping, directly into this expression results in a division by zero error, and for this particular choice of \\(\\gamma\\), we need to be careful and compute the limit where \\(\\gamma \\rightarrow 1\\)\n\n\nx_critical = sympy.limit(x_sol.rhs, gamma, 1)\nx_critical\n\n\\(\\displaystyle \\left(\\omega_{0} t + 1\\right) e^{- \\omega_{0} t}\\)\n\n\n\nFinally, \\(\\,\\)we plot the solutions for \\(\\omega_0 = 2\\pi\\) and a sequence of different values of the damping ratio \\(\\gamma\\)\n\n\nfig, ax = plt.subplots(figsize=(6, 4))\n\ntt = np.linspace(0, 3, 250)\nfor g in [0.1, 0.5, 1, 2.0, 5.0]:\n    if g == 1:\n        x_t = sympy.lambdify(t, \n          x_critical.subs({omega0: 2.0 *pi}), 'numpy')\n    else:\n        x_t = sympy.lambdify(t, \n          x_sol.rhs.subs({omega0: 2.0 *pi, gamma: g}), 'numpy')\n    ax.plot(tt, x_t(tt).real, label=rf'$\\gamma = {g: .1f}$')\n\nax.set_xlabel(r\"$t$\", fontsize=12)\nax.set_ylabel(r\"$x(t)$\", fontsize=12)\nax.legend()\nax.set_xlim(0, 3)\nax.set_ylim(-1, 1.5)\nax.tick_params(which='both', direction='in', axis='x', pad=7)\n\n\n\n\n\n\n\n\n\n\nEven many first-order ODEs cannot be solved exactly in terms of elementary functions. For example, consider \\(\\displaystyle\\frac{dy}{dx} = x +y^2\\), \\(\\,\\)which is an example of an ODE that does not have any closed-form solution\n\n\nx = sympy.symbols('x', positive=True)\ny = sympy.Function('y')\nu = sympy.Function('u')\n\nf = y(x)**2 +x \n                    \node = y(x).diff(x) -f\node\n\n\\(\\displaystyle - x - y^{2}{\\left(x \\right)} + \\frac{d}{d x} y{\\left(x \\right)}\\)\n\n\n\nsympy.classify_ode(ode)\n\n('1st_rational_riccati', '1st_power_series', 'lie_group')\n\n\n\nSee Section 2.5\n\n\n# sympy bug: '1st_rational_riccati' not working\nq0 = x\nq1 = sympy.Integer('0')\nq2 = sympy.Integer('1')\n\nS = q2 *q0\nR = q1 +q2.diff(x) /q2\n\node_ = u(x).diff(x, 2) -R *u(x).diff(x) +S *u(x)\nu_sol = sympy.dsolve(ode_).rhs\nu_sol\n\n\\(\\displaystyle C_{1} Ai\\left(- x\\right) + C_{2} Bi\\left(- x\\right)\\)\n\n\n\ny_sol = -u_sol.diff(x) /(q2 *u_sol)\ny_sol\n\n\\(\\displaystyle \\frac{C_{1} Ai^\\prime\\left(- x\\right) + C_{2} Bi^\\prime\\left(- x\\right)}{C_{1} Ai\\left(- x\\right) + C_{2} Bi\\left(- x\\right)}\\)\n\n\n\nC1 = sympy.Symbol('C1')\nC1_ = sympy.solve(y_sol.subs({x: 0}), C1)[0]\nC1_\n\n\\(\\displaystyle \\sqrt{3} C_{2}\\)\n\n\n\ny_sol = y_sol.subs({C1: C1_}).cancel()\ny_sol\n\n\\(\\displaystyle \\frac{\\sqrt{3} Ai^\\prime\\left(- x\\right) + Bi^\\prime\\left(- x\\right)}{\\sqrt{3} Ai\\left(- x\\right) + Bi\\left(- x\\right)}\\)\n\n\n\n\nFor many other types of equations, sympy outright fails to produce any solution at all. \\(\\,\\)For example, if we attempt to solve the second-order\n\\[\\frac{d^2y}{dx^2} =x +y^2\\]\n\n\nsympy.Eq(y(x).diff(x, 2), f)\n\n\\(\\displaystyle \\frac{d^{2}}{d x^{2}} y{\\left(x \\right)} = x + y^{2}{\\left(x \\right)}\\)\n\n\n\ntry:\n  sympy.dsolve(y(x).diff(x, 2) -f)\nexcept Exception as e:\n  print(e)\n\nsolve: Cannot solve -x - y(x)**2 + Derivative(y(x), (x, 2))\n\n\n\nH.3.1 Direction fields\n\nA direction field graph is a simple but useful technique to visualize possible solutions to arbitrary first-order ODEs. It is made up of short lines that show the slope of the unknown function on a grid in the \\(x\\)–\\(y\\) plane. This graph can be easily produced because the slope of \\(y(x)\\) at arbitrary points of the \\(x\\)–\\(y\\) plane is given by the definition of the ODE:\n\\[\\frac{dy}{dx}=f(x,y(x))\\]\nThe reason why the direction field graph is useful is that smooth and continuous curves that tangent the slope lines (at every point) in the direction field graph are possible solutions to the ODE\n\n\ndef plot_direction_field(x, y_x, f_xy, \n        x_lim=(-5, 5), y_lim=(-5, 5), ax=None):\n    \n    f_np = sympy.lambdify((x, y_x), f_xy, 'numpy')\n    \n    x_vec = np.linspace(x_lim[0], x_lim[1], 20)\n    y_vec = np.linspace(y_lim[0], y_lim[1], 20)\n    \n    if ax is None:\n      _, ax = plt.subplots(figsize=(4, 4))\n\n    dx = x_vec[1] -x_vec[0]\n    dy = y_vec[1] -y_vec[0]\n    scale = 0.8 *dx\n\n    for m, x_ in enumerate(x_vec):\n      for n, y_ in enumerate(y_vec):\n        Dy = f_np(x_, y_) *dx\n        Dx = scale *dx /np.sqrt(dx**2 +Dy**2)\n        Dy = scale *Dy /np.sqrt(dx**2 +Dy**2)\n        ax.plot([x_ -Dx/2, x_ +Dx/2], \n                [y_ -Dy/2, y_ +Dy/2], 'b', lw=1)\n        ax.plot(x_ +Dx/2, y_ +Dy/2, 'bo', ms=2)  \n \n    ax.set_xlim(x_lim[0], x_lim[1])\n    ax.set_ylim(y_lim[0], y_lim[1])\n    ax.tick_params(which='both', direction='in')\n    title_ = sympy.latex(sympy.Eq(y(x).diff(x), f_xy))\n    ax.set_title(rf'${title_}$', fontsize=12)\n    \n    return ax\n\n\nWith this function, we can produce the direction field graphs for the ODEs. The direction lines in the graphs suggest how the curves that are solutions to the corresponding ODE behave, and direction field graphs are therefore a useful tool for visualizing solutions to ODEs that cannot be solved analytically\n\n\nx = sympy.symbols('x')\ny = sympy.Function('y')\n\nf = y(x)**2 +x\n\n\nfig, ax = plt.subplots(1, 1, figsize=(5, 5))\n\n# left panel\nplot_direction_field(x, y(x), f, ax=ax)\n\nx_vec = np.linspace(-5, 5, 300)\ny_vec = sympy.lambdify(x, y_sol)(x_vec)\n\nthreshold = 20\ny_vec[y_vec &gt; threshold] = np.inf\ny_vec[y_vec &lt;-threshold] = np.inf\n\nax.plot(x_vec, y_vec, 'r', lw=2)  \n\n\n\n\n\n\n\n\n\nThe following code generates the direction field graphs for\n\\(f(x,y(x))=y^2(x) +x\\)\n\\(f(x,y(x))=-x/y(x)\\)\n\\(f(x,y(x))=y^2(x)/x\\)\n\n\nx = sympy.symbols('x')\ny = sympy.Function('y')\n\nfig, axes = plt.subplots(3, 1, figsize=(4, 13))\n\nplot_direction_field(x, y(x), y(x)**2 +x, ax=axes[0])\nplot_direction_field(x, y(x), -x /y(x), ax=axes[1])\nplot_direction_field(x, y(x), y(x)**2 /x, ax=axes[2])\n\n\n\n\n\n\n\n\n\n\nH.3.2 Solving ODEs using Laplace Transformation\n\nA technique that can be used to solve certain ODE problems is to do Laplace transform on the ODE, which for many problems results in an algebraic equation that is easier to solve. The solution to the algebraic equation can then be transformed back to the original domain with an inverse Laplace transform, to obtain the solution to the original problem. For example, consider the following differential equation for a driven harmonic oscillator:\n\\[ \\frac{d^2 y}{dt^2} +2\\frac{dy}{dt} +10y =2\\sin 3t\\]\n_laplace_trabsform_expansion.py\n\nlaplace_tranform_() is the expanded version of laplace_transform()\nsubs_() is the modified function of sub() method\n\n\n\nt = sympy.symbols(\"t\", positive=True)\ny = sympy.Function(\"y\")\n\node = (y(t).diff(t, 2) \n      +2 *y(t).diff(t) \n      +10 *y(t) \n      -2 *sympy.sin(3*t))\nics = {y(0): 1, y(t).diff(t).subs(t, 0): 0}\n\node\n\n\\(\\displaystyle 10 y{\\left(t \\right)} - 2 \\sin{\\left(3 t \\right)} + 2 \\frac{d}{d t} y{\\left(t \\right)} + \\frac{d^{2}}{d t^{2}} y{\\left(t \\right)}\\)\n\n\n\ns = sympy.symbols('s', real=True)\nY = sympy.Function('Y')\n\nLy = laplace_transform_(y(t), t, s)\n\n\nLode = subs_(laplace_transform_(ode, t, s), Ly, Y(s)).subs(ics)\nLode\n\n\\(\\displaystyle s^{2} Y{\\left(s \\right)} + 2 s Y{\\left(s \\right)} - s + 10 Y{\\left(s \\right)} - 2 - \\frac{6}{s^{2} + 9}\\)\n\n\n\nY_sol = sympy.solve(Lode, Y(s))[0]\nY_sol \n\n\\(\\displaystyle \\frac{s^{3} + 2 s^{2} + 9 s + 24}{s^{4} + 2 s^{3} + 19 s^{2} + 18 s + 90}\\)\n\n\n\ny_sol = sympy.inverse_laplace_transform(Y_sol, s, t)\ny_sol\n\n\\(\\displaystyle \\frac{2 \\sin{\\left(3 t \\right)}}{37} - \\frac{12 \\cos{\\left(3 t \\right)}}{37} + \\frac{43 e^{- t} \\sin{\\left(3 t \\right)}}{111} + \\frac{49 e^{- t} \\cos{\\left(3 t \\right)}}{37}\\)\n\n\n\ny_t = sympy.lambdify(t, y_sol, modules=['numpy'])\n\n\nfig, ax = plt.subplots(figsize=(6, 4))\n\ntt = np.linspace(0, 10, 500)\nax.plot(tt, y_t(tt))\nax.set_xlabel(r\"$t$\", fontsize=12)\nax.set_ylabel(r\"$y(t)$\", fontsize=12)\nfig.tight_layout()",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>H</span>  <span class='chapter-title'>Ordinary Differential Equations</span>"
    ]
  },
  {
    "objectID": "x_ordinary_differential_equations.html#numerical-methods-for-solving-odes",
    "href": "x_ordinary_differential_equations.html#numerical-methods-for-solving-odes",
    "title": "Appendix H — Ordinary Differential Equations",
    "section": "H.4 Numerical methods for solving ODEs",
    "text": "H.4 Numerical methods for solving ODEs\n\nIn practice, ODE problems are therefore mainly solved with numerical methods. There are many approaches to solving ODEs numerically, and most of them are designed for problems that are formulated as a system of first-order ODEs on the standard form:\n\\[ \\frac{d\\mathbf{y}}{dx} = \\mathbf{f}(x, \\mathbf{y}(x))\\]\nThe basic idea of many numerical methods for ODEs is captured in Euler’s method. This method can, for example, be derived from a Taylor-series expansion of \\(y(x)\\) around the point \\(x\\):\n\\[ y(x+h) = y(x) +\\frac{dy(x)}{dx}h +\\frac{d^2 y(x)}{dx^2} h^2 + \\cdots, \\]\nwhere for notational simplicity we consider the case when \\(y(x)\\) is a scalar function\nBy dropping terms of second order or higher, \\(\\,\\)we get the approximate equation\n\\[ y(x+h) \\approx y(x) +\\frac{dy(x)}{dx}h, \\]\nwhich is accurate to first order in the step size \\(h\\). This equation can be turned into an iteration formula by discretizing the \\(x\\) variable, \\(x_0\\), \\(x_1\\), \\(\\cdots\\), \\(x_k\\), \\(\\,\\)choosing the step size \\(h_k =x_{k+1} -x_{k}\\), \\(\\,\\)and denoting \\(y_k = y(x_k)\\). \\(\\,\\)The resulting iteration formula:\n\\[\\color{red}{y_{k+1} \\approx y_{k} +f(x_k, y_k) h_k},\\]\nis known as the forward Euler method, and it is said to be an explicit form because given the value of the \\(y_k\\), we can directly compute \\(y_{k+1}\\) using the formula\nThere are two types of errors involved in this approach: First, the truncation of the Taylor series gives error that limits the accuracy of the method. Second, using the approximation of \\(y_k\\) given by the previous iteration when computing \\(y_{k+1}\\) gives an additional error that may accumulate over successive iterations, and that can affect the stability of the method\nAn alternative form, which can be derived in a similar manner, is the backward Euler method, \\(\\,\\)given by the iteration formula\n\\[\\color{red}{y_{k+1} \\approx y_{k} +f(x_{k+1}, y_{k+1}) h_k}\\]\nThis is an example of a backward differentiation formula(BDF), which is implicit, \\(\\,\\)because \\(y_{k+1}\\) occurs on both sides of the equation. \\(~\\)To compute \\(y_{k+1}\\), \\(\\text{ }\\)we therefore need to solve an algebraic equation (for example using Newton’s method)\nImplicit methods are more complicated to implement than explicit methods, and each iteration requires more computational work. However, the advantage is that implicit methods generally have larger stability region and better accuracy, which means that larger step size \\(h_k\\) can be used while still obtaining an accurate and stable solution\nImplicit methods are often particularly useful for stiff problems, which loosely speaking are ODE problems that describe dynamics with multiple disparate time scales\nThere are several methods to improve upon the first-order Euler forward and backward methods. One strategy is to keep higher-order terms in the Taylor-series expansion of \\(y(x + h)\\). \\(\\,\\)However, such methods require evaluating higher-order derivatives of \\(y(x)\\), which may be a problem\nWays around this problem include to approximate the higher-order derivatives using finite-difference approximations of the derivatives, or by sampling the function \\(f(x, y(x))\\) at intermediary points in the interval \\([x_k, x_{k+1}]\\)\nAn example of this type of method is the well-known Runge-Kutta method, which is a single-step method that uses additional evaluations of \\(f(x, y(x))\\). The most well-known Runge-Kutta method is the 4th-order scheme:\n\\[\\color{red}{y_{k+1} = y_k +\\frac{h_k}{6} (k_1 +2k_2 +2k_3 +k_4)}\\]\nwhere\n\\[\n\\begin{aligned}\n  k_1 &= f(t_k, \\,y_k)\\\\\n  k_2 &= f\\left(t_k + \\frac{h_k}{2},\\, y_k +h_k \\frac{k_1}{2} \\right)\\\\\n  k_3 &= f\\left(t_k + \\frac{h_k}{2},\\, y_k +h_k \\frac{k_2}{2} \\right)\\\\\n  k_4 &= f(t_k +h_k, \\,y_k +h_k k_3)\n\\end{aligned}\n\\]\nHere, \\(k_1\\) to \\(k_4\\) are four different evaluations of the ODE function \\(f(x, y(x))\\) that are used in the explicit formula for \\(y_{k+1}\\) given above. The resulting estimate of \\(y_{k+1}\\) is accurate to 4-th order, with an error of 5-th order.\nHigher-order schemes that use more function evaluations can also be constructed. By combining two methods of different order, it can be possible to also estimate the error in the approximation.\nA popular combination is the Runge-Kutta 4-th and 5-th order schemes, which results in a 4-th order accurate method with error estimates. It is known as RK45 or the Runge-Kutta-Fehlberg method. The Dormand-Prince method is another example of a higher-order method, which additionally uses adaptive step size control. For example, the 8-5-3 method combines 3rd and 5-th order schemes to produce an 8-th order method\nAn alternative method is to use more than one previous value of \\(y_k\\) to compute \\(y_{k+1}\\). Such methods are known as multistep methods, and can in general be written on the form\n\\[\\color{red}{y_{k + s} = \\sum_{n=0}^{s-1} a_n y_{k + n} + h \\sum_{n=0}^{s} b_n f(x_{k + n}, y_{k + n})}\\]\nThis formula means that to compute \\(y_{k+s}\\), the previous \\(s\\) values of \\(y_k\\) and \\(f(x_k, y_k)\\) are used (known as an \\(s\\)-step method). The choices of the coefficients \\(a_n\\) and \\(b_n\\) give rise to different multistep methods. Note that if \\(b_s = 0\\), then the method is explicit, and if \\(b_s \\neq 0\\) it is implicit\nFor example, \\(b_0 = b_1 = \\cdots = b_{s-1} = 0\\) gives the general formula for an \\(s\\)-step BDF formula, where \\(a_n\\) and \\(b_s\\) are chosen to maximize the order of the accuracy. For example, the one-step BDF method with \\(b_1 = a_0 = 1\\) reduces to the backward Euler method, and the two-step BDF method,\n\\[y_{k+2} = a_0 y_k +a_1 y_{k+1} +hb_2 f(x_{k+2}, y_{k+2})\\]\nwhen solved for the coefficients \\((a_0, a_1, b_2)\\) becomes \\((-\\frac{1}{3},\\,\\frac{4}{3},\\,\\frac{2}{3})\\)\nAnother family of multistep methods are the Adams methods, which result from the choice\n\\[\\color{red}{a_0 = a_1 = \\cdots = a_{s-2} = 0, \\text{ and } a_{s-1}=1}\\]\nwhere again the remaining unknown coefficients are chosen to maximize the order of the method\nSpecifically, the explicit method with \\(b_s = 0\\) are known as Adams-Bashforth methods, and the implicit methods with \\(b_s \\neq 0\\) are known as Adams-Moulton methods. For example, the one-step Adams-Bashforth and Adams-Moulton methods reduce to the forward and backward Euler methods, respectively\nAnd the two-step methods are\n\\[y_{k+2} = y_{k+1} + h \\left[ -\\frac{1}{2} f(x_{k}, y_{k}) + \\frac{3}{2} f(x_{k+1}, y_{k+1}) \\right]\\]\nand\n\\[y_{k+1} = y_{k} + \\frac{h}{2} \\left[ f(x_{k}, y_{k}) + f(x_{k+1}, y_{k+1}) \\right]\\]\nIn general, explicit methods are more convenient to implement and less computationally demanding to iterate than implicit methods, which in principle requires solving (a potentially nonlinear) equation in each iteration with an initial guess for the unknown \\(y_{k+1}\\)\nHowever, as mentioned earlier, implicit methods often are more accurate and have superior stability properties. A compromise that retain some of the advantages of both methods is to combine explicit and implicit methods in the following way:\n\nFirst compute \\(y_{k+1}\\) using an explicit method,\nthen use this \\(y_{k+1}\\) as an initial guess for solving the equation for \\(y_{k+1}\\) given by an implicit method.\n\nThis equation does not need to be solved exactly, and since the initial guess from the explicit method should be quite good, a fixed number of iterations, using for example Newton’s method, could be sufficient. Methods like these, where an explicit method is used to predict \\(y_{k+1}\\) and an implicit method is used to correct the prediction, are called predictor-corrector methods",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>H</span>  <span class='chapter-title'>Ordinary Differential Equations</span>"
    ]
  },
  {
    "objectID": "x_ordinary_differential_equations.html#numerical-integration-of-odes-using-scipy",
    "href": "x_ordinary_differential_equations.html#numerical-integration-of-odes-using-scipy",
    "title": "Appendix H — Ordinary Differential Equations",
    "section": "H.5 Numerical integration of ODEs using scipy",
    "text": "H.5 Numerical integration of ODEs using scipy\n\nThe integrate module of scipy provides the ODE solver interface: integrate.solve_ivp. This function numerically integrates a system of ordinary differential equations given an initial value:\n\\[\\begin{aligned}\n\\frac{d\\mathbf{y}}{dt}&= \\mathbf{f}(t, \\mathbf{y})\\\\\n\\mathbf{y}(t_0)&= \\mathbf{y}_0\n\\end{aligned}\\]\nExplicit Runge-Kutta methods ('RK23','RK45','DOP853') should be used for non-stiff problems and implicit methods ('Radau','BDF') for stiff problems. Among Runge-Kutta methods, 'DOP853' is recommended for solving with high precision (low values of rtol and atol)\nIf not sure, first try to run 'RK45'. If it makes unusually many iterations, diverges, or fails, your problem is likely to be stiff and you should use 'Radau' or 'BDF'. 'LSODA' can also be a good universal choice, but it might be somewhat less convenient to work with as it wraps old Fortran code\nFor example, \\(\\,\\)consider again the scalar ODE\n\\[y'(x) = f(x, y(x)) = x + y^2(x)\\]\n\n\nfrom scipy import integrate\n\ndef dydx(x, y):   \n    return x +y*y\n\ny0 = [0]\n\nsol_p = integrate.solve_ivp(dydx, [0, 1.9], y0)\nsol_m = integrate.solve_ivp(dydx, [0, -5], y0)\n\n\nfig, ax = plt.subplots(1, 1, figsize=(6, 6))\nplot_direction_field(x, y(x), f, ax=ax)\nax.plot(sol_p.t, sol_p.y.T, 'g', lw=2)\nax.plot(sol_m.t, sol_m.y.T, 'r', lw=2)\n\n\n\n\n\n\n\n\n\nIn the previous example, we solved a scalar ODE problem. More often we are interested in vector-valued ODE problems (systems of ODEs). To see how we can solve that kind of problems using solve_ivp, consider the Lokta-Volterra equations for the dynamics of a population of predator and prey animals (a classic example of coupled ODEs). The equations are\n\\[\nx'(t) = a x - b x y\n\\]\n\\[\ny'(t) = c x y - d y\n\\]\nwhere \\(x(t)\\) is the number of prey animals and \\(y(t)\\) is the number of predator animals, and the coefficients \\(a\\), \\(b\\), \\(c\\), and \\(d\\) describe the rates of the processes in the model\n\n\na, b, c, d = 0.4, 0.002, 0.001, 0.7\ndef f(t, xy):\n  x, y = xy\n  return [a *x -b *x *y, c *x *y -d *y]\n\n\nxy0 = [600, 400]\nsol = integrate.solve_ivp(f, [0, 50], xy0, \n        t_eval=np.linspace(0, 50, 400))\n\nt = sol.t\nxy_t = sol.y.T\n\n\nfig, axes = plt.subplots(2, 1, figsize=(6, 12))\n\naxes[0].plot(t, xy_t[:,0], 'r', label=\"Prey\")\naxes[0].plot(t, xy_t[:,1], 'b', label=\"Predator\")\naxes[0].set_xlabel(\"Time\")\naxes[0].set_ylabel(\"Number of animals\")\naxes[0].legend()\naxes[0].set_xlim(0, 50)\naxes[0].set_ylim(0, 1400)\naxes[0].tick_params(which='both', direction='in')\n\naxes[1].plot(xy_t[:,0], xy_t[:,1], 'k')\naxes[1].set_xlabel(\"Number of prey\")\naxes[1].set_ylabel(\"Number of predators\")\naxes[1].set_xlim(300, 1300)\naxes[1].set_ylim(50, 450)\naxes[1].tick_params(which='both', direction='in')\n\n\n\n\n\n\n\n\n\nIn the previous two examples, the function for the right-hand side of the ODE was implemented without additional arguments. Rather than using global variables, it is often convenient and elegant to implement the f function in such a way that it takes arguments for all its coefficient or parameters. To illustrate this point, let’s consider another famous ODE problem: the Lorenz equations, which is the following system of three coupled nonlinear ODEs,\n\\[\n  \\begin{aligned}\n  x'(t) &= \\sigma(y - x)\\\\\n  y'(t) &= x(\\rho - z) - y \\\\\n  z'(t) &= x y - \\beta z\n  \\end{aligned}\n\\]\n\n\nrho, sigma, beta = 28, 8, 8/3.0\n\ndef f(t, xyz, rho, sigma, beta):\n  x, y, z = xyz\n  return [sigma *(y -x), x *(rho -z) -y, x *y -beta *z]\n\n\nxyz0 = [1.0, 1.0, 1.0]\n\nt_span = [0, 25]\nt_eval = np.linspace(0, 25, 10000)\n\nsol1 = integrate.solve_ivp(f, t_span, xyz0, \n          t_eval=t_eval, args=(rho, sigma, beta))\nsol2 = integrate.solve_ivp(f, t_span, xyz0, \n          t_eval=t_eval, args=(rho, sigma, 0.6*beta))\nsol3 = integrate.solve_ivp(f, t_span, xyz0, \n          t_eval=t_eval, args=(rho, 2*sigma, 0.6*beta))\n\n\nfig, (ax1, ax2, ax3) = plt.subplots(3, 1, figsize=(7, 15), \n  subplot_kw={'projection': '3d'})\n\nxyz1 = sol1.y\nxyz2 = sol2.y\nxyz3 = sol3.y\n\nfor ax, xyz, c in [(ax1, xyz1, 'r'), \n                   (ax2, xyz2, 'b'), \n                   (ax3, xyz3, 'g')]:\n    ax.plot(xyz[0,:], xyz[1,:], xyz[2,:], c, alpha=0.5)\n\n    ax.set_xlabel('$x$', fontsize=12)\n    ax.set_ylabel('$y$', fontsize=12)\n    ax.set_zlabel('$z$', fontsize=12)\n    ax.set_xticks([-15, 0, 15])\n    ax.set_yticks([-20, 0, 20])\n    ax.set_zticks([0, 20, 40])\n\n\n\n\n\n\n\n\n\nTo illustrate how to use the integrate.ode interface, \\(\\,\\)we first look at the following sets of coupled second-order ODEs:\n\\[\n\\begin{aligned}\nm_1 x_1''(t) + \\gamma_1 x_1'(t) + k_1 x_1(t) - k_2 (x_2(t) - x_1(t)) &= 0\\\\\nm_2 x_2''(t) + \\gamma_2 x_2'(t) + k_2 (x_2(t) - x_1(t)) &= 0\n\\end{aligned}\n\\]\nThese equations describe the dynamics of two coupled springs, where \\(x_1(t)\\) and \\(x_2(t)\\) are the displacement of two objects, with masses \\(m_1\\) and \\(m_2\\), from their equilibrium positions. The object at \\(x_1\\) is connected to a fixed wall via a spring with spring constant \\(k_1\\), and connected to the object at \\(x_2\\) via a spring with spring constant \\(k_2\\). Both objects are subject to damping forces characterized by \\(\\gamma_1\\) and \\(\\gamma_2\\), respectively\nTo solve this kind of problem with scipy, we first have to write it in standard form, which we can do by introducing \\(y_0(t) =x_1(t)\\), \\(\\,y_1(t) =x_1'(t)\\), \\(\\,y_2(t) =x_2(t)\\), \\(\\,\\)and \\(\\,y_3(t) =x_2'(t)\\), which results in four coupled first-order equations:\n\\[\n  \\frac{d}{dt}\n  \\begin{bmatrix}\n    y_0(t)\\\\\n    y_1(t)\\\\\n    y_2(t)\\\\\n    y_3(t)\n    \\end{bmatrix} =\n    \\begin{bmatrix}\n    y_1(t) \\\\\n    \\left[-\\gamma_1 y_1(t) -k_1 y_0(t) -k_2 y_0(t) +k_2 y_2(t)\\right]/m_1 \\\\\n    y_3(t) \\\\\n    \\left[-\\gamma_2 y_3(t) -k_2 y_2(t) +k_2 y_0(t)\\right]/m_2\n  \\end{bmatrix}\n\\]\n\n\ndef f(t, y, args):\n  m1, k1, g1, m2, k2, g2 = args\n    \n  return [y[1], \n        - k1/m1 *y[0] +k2/m1 *(y[2] -y[0]) -g1/m1 *y[1], \n          y[3], \n        - k2/m2 *(y[2] -y[0]) -g2/m2 *y[3]]\n\n\nm1, k1, g1, m2, k2, g2 = 1.0, 10.0, 0.5, 2.0, 40.0, 0.25\nargs = (m1, k1, g1, m2, k2, g2)\n\ny0 = [1.0, 0, 0.5, 0]\nt = np.linspace(0, 20, 1000)\n\n\nInstead of calling the solve_ivp function, \\(\\,\\)we now need to create an instance of the class integrate.ode, \\(\\,\\)passing the ODE function \\(f\\) as an argument:\n\n\nr = integrate.ode(f)\n\n\n# the possible first argument: \n# vode, zvode, lsoda, dopri5 and dop853\nr.set_integrator('lsoda') \nr.set_initial_value(y0, t[0])\nr.set_f_params(args)\n\ndt = t[1] -t[0]\ny = np.zeros((len(t), len(y0)))\nidx = 0\nwhile r.successful() and r.t &lt; t[-1]:\n  y[idx, :] = r.y\n  r.integrate(r.t +dt)\n  idx += 1\n\n\nThis is arguably not as convenient as simply calling the solive_ivp, but it offers extra flexibility that sometimes is exactly what is needed\n\n\nfig = plt.figure(figsize=(5, 10))\n\nax1 = plt.subplot2grid((10, 4), (0, 0), colspan=4, rowspan=3)\nax2 = plt.subplot2grid((10, 4), (3, 0), colspan=4, rowspan=3)\nax3 = plt.subplot2grid((10, 4), (6, 0), colspan=4, rowspan=4)\n\n# x_1 vs time plot\nax1.plot(t, y[:, 0], 'r')\nax1.set_ylabel('$x_1$', fontsize=12)\nax1.set_yticks([-1, -.5, 0, .5, 1])\nax1.set_xlim(0, 20)\nax1.set_ylim(-1.0, 1.0)\nax1.tick_params(which='both', direction='in', axis='x', pad=7)\n\n# x2 vs time plot\nax2.plot(t, y[:, 2], 'b')\nax2.set_xlabel('$t$', fontsize=12)\nax2.set_ylabel('$x_2$', fontsize=12)\nax2.set_yticks([-1, -.5, 0, .5, 1])\nax2.set_xlim(0, 20)\nax2.set_ylim(-1.0, 1.0)\nax2.tick_params(which='both', direction='in', axis='x', pad=7)\n\n# x1 and x2 phase space plot\nax3.plot(y[:, 0], y[:, 2], 'g')\nax3.set_xlabel('$x_1$', fontsize=12)\nax3.set_ylabel('$x_2$', fontsize=12)\nax3.set_xticks([-1, -.5, 0, .5, 1])\nax3.set_yticks([-1, -.5, 0, .5, 1])\nax3.set_xlim(-1.0, 1.00)\nax3.set_ylim(-1.0, 1.0)\nax3.tick_params(which='both', direction='in', axis='x', pad=7)\n\nfig.tight_layout()\n\n\n\n\n\n\n\n\n\nPython functions for both \\(f(t, y(t))\\) and its Jacobian can conveniently be generated using sympy’s lambdify, provided that the ODE problem first can be defined as a sympy expression. This symbolic-numeric hybrid approach is a powerful method to solving ODE problems(See Appendix I)",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>H</span>  <span class='chapter-title'>Ordinary Differential Equations</span>"
    ]
  },
  {
    "objectID": "x_double_pendulum.html",
    "href": "x_double_pendulum.html",
    "title": "Appendix I — Double Pendulum",
    "section": "",
    "text": "\\(~\\)\n\nConsider the rather complicated system of two coupled 2nd-order nonlinear ODEs for a double pendulum\n\n\n\n\n\nNonlinear governing equations\n\\[\\scriptsize\n\\begin{aligned}\n   (m_1+m_2) l_1\\color{red}{\\ddot{\\theta_1}} + m_2l_2\\color{red}{\\ddot{\\theta_2}\\cos(\\theta_1-\\theta_2)} &  \n    + m_2l_2\\color{red}{\\left(\\dot{\\theta_2}\\right)^2\\sin(\\theta_1-\\theta_2)}+g(m_1+m_2)\\color{red}{\\sin(\\theta_1)} = 0\\\\\n   m_2l_2\\color{red}{\\ddot{\\theta_2}} + m_2l_1\\color{red}{\\ddot{\\theta_1}\\cos(\\theta_1-\\theta_2)} &  \n  - m_2l_1 \\color{red}{\\left(\\dot{\\theta_1}\\right)^2 \\sin(\\theta_1-\\theta_2)} +m_2g\\color{red}{\\sin(\\theta_2)} = 0\n\\end{aligned}\\]\n\n\\(~\\)\n\nimport numpy as np\nfrom scipy import integrate\n\nimport sympy\nfrom sympy import init_printing\ninit_printing()\n\nimport matplotlib.pyplot as plt\nfrom matplotlib.animation import FuncAnimation\nfrom IPython.display import HTML\n\n\nth1, th2 = sympy.symbols(\"theta_1, theta_2\", cls=sympy.Function)\nt, g, m1, l1, m2, l2 = sympy.symbols(\"t, g, m_1, l_1, m_2, l_2\")\n\node1 = sympy.Eq((m1 +m2) *l1 *th1(t).diff(t, t) +\n                m2 *l2 *th2(t).diff(t, t) *sympy.cos(th1(t) -th2(t)) +\n                m2 *l2 *th2(t).diff(t)**2 *sympy.sin(th1(t) -th2(t)) + \n                g *(m1 +m2) *sympy.sin(th1(t)), 0)\n\node2 = sympy.Eq(m2 *l2 *th2(t).diff(t, t) +\n                m2 *l1 *th1(t).diff(t, t) *sympy.cos(th1(t) -th2(t)) -\n                m2 *l1 *th1(t).diff(t)**2 *sympy.sin(th1(t) -th2(t)) +\n                m2 *g *sympy.sin(th2(t)), 0)\n\n\\(~\\)\n\nWe first have to write the system of two 2nd-order ODEs as a system of \\(\\,\\)four 1st-order ODEs on standard form. \\(\\,\\)To this end, \\(\\,\\)we need to introduce new functions\n\\[\n\\begin{aligned}\ny_1(t) &= \\theta_1(t) \\\\\ny_2(t) &= \\dot{\\theta_1}(t) \\\\\ny_3(t) &= \\theta_2(t) \\\\\ny_4(t) &=\\dot{\\theta_2}(t)\n\\end{aligned}\\]\nand rewrite the ODEs in terms of these functions\n\n\\(~\\)\n\ny1, y2, y3, y4 = sympy.symbols(\"y_1, y_2, y_3, y_4\", cls=sympy.Function)\n\nvarchange = {th1(t): y1(t),\n             th1(t).diff(t, t): y2(t).diff(t),\n             th2(t): y3(t),\n             th2(t).diff(t, t): y4(t).diff(t)}\n\node1_vc = ode1.subs(varchange)\node2_vc = ode2.subs(varchange)\node3 = sympy.Eq(y1(t).diff(t) -y2(t), 0)\node4 = sympy.Eq(y3(t).diff(t) -y4(t), 0)\n\n\\(~\\)\n\nAt this point, \\(\\,\\)we have four coupled 1st-order ODEs for the functions \\(\\,y_1\\,\\) to \\(\\,y_4\\). \\(\\,\\)It only remains to solve for the derivatives of these functions to obtain the ODEs in standard form\n\n\\(~\\)\n\ny = sympy.Matrix([y1(t), y2(t), y3(t), y4(t)])\nvcsol = sympy.solve((ode1_vc, ode2_vc, ode3, ode4), y.diff(t), dict=True)\n\nf = y.diff(t).subs(vcsol[0])\nf\n\n\\(\\displaystyle \\left[\\begin{matrix}y_{2}{\\left(t \\right)}\\\\\\frac{g m_{1} \\sin{\\left(y_{1}{\\left(t \\right)} \\right)} + \\frac{g m_{2} \\sin{\\left(y_{1}{\\left(t \\right)} - 2 y_{3}{\\left(t \\right)} \\right)}}{2} + \\frac{g m_{2} \\sin{\\left(y_{1}{\\left(t \\right)} \\right)}}{2} + \\frac{l_{1} m_{2} y_{2}^{2}{\\left(t \\right)} \\sin{\\left(2 y_{1}{\\left(t \\right)} - 2 y_{3}{\\left(t \\right)} \\right)}}{2} + l_{2} m_{2} y_{4}^{2}{\\left(t \\right)} \\sin{\\left(y_{1}{\\left(t \\right)} - y_{3}{\\left(t \\right)} \\right)}}{l_{1} \\left(- m_{1} + m_{2} \\cos^{2}{\\left(y_{1}{\\left(t \\right)} - y_{3}{\\left(t \\right)} \\right)} - m_{2}\\right)}\\\\y_{4}{\\left(t \\right)}\\\\\\frac{g m_{1} \\sin{\\left(2 y_{1}{\\left(t \\right)} - y_{3}{\\left(t \\right)} \\right)} - g m_{1} \\sin{\\left(y_{3}{\\left(t \\right)} \\right)} + g m_{2} \\sin{\\left(2 y_{1}{\\left(t \\right)} - y_{3}{\\left(t \\right)} \\right)} - g m_{2} \\sin{\\left(y_{3}{\\left(t \\right)} \\right)} + 2 l_{1} m_{1} y_{2}^{2}{\\left(t \\right)} \\sin{\\left(y_{1}{\\left(t \\right)} - y_{3}{\\left(t \\right)} \\right)} + 2 l_{1} m_{2} y_{2}^{2}{\\left(t \\right)} \\sin{\\left(y_{1}{\\left(t \\right)} - y_{3}{\\left(t \\right)} \\right)} + l_{2} m_{2} y_{4}^{2}{\\left(t \\right)} \\sin{\\left(2 y_{1}{\\left(t \\right)} - 2 y_{3}{\\left(t \\right)} \\right)}}{2 l_{2} \\left(m_{1} - m_{2} \\cos^{2}{\\left(y_{1}{\\left(t \\right)} - y_{3}{\\left(t \\right)} \\right)} + m_{2}\\right)}\\end{matrix}\\right]\\)\n\n\n\njac = sympy.Matrix([[f_i.diff(y_j) for y_j in y] for f_i in f])\n\njac[:, 0]\n\n\\(\\displaystyle \\left[\\begin{matrix}0\\\\\\frac{2 m_{2} \\left(g m_{1} \\sin{\\left(y_{1}{\\left(t \\right)} \\right)} + \\frac{g m_{2} \\sin{\\left(y_{1}{\\left(t \\right)} - 2 y_{3}{\\left(t \\right)} \\right)}}{2} + \\frac{g m_{2} \\sin{\\left(y_{1}{\\left(t \\right)} \\right)}}{2} + \\frac{l_{1} m_{2} y_{2}^{2}{\\left(t \\right)} \\sin{\\left(2 y_{1}{\\left(t \\right)} - 2 y_{3}{\\left(t \\right)} \\right)}}{2} + l_{2} m_{2} y_{4}^{2}{\\left(t \\right)} \\sin{\\left(y_{1}{\\left(t \\right)} - y_{3}{\\left(t \\right)} \\right)}\\right) \\sin{\\left(y_{1}{\\left(t \\right)} - y_{3}{\\left(t \\right)} \\right)} \\cos{\\left(y_{1}{\\left(t \\right)} - y_{3}{\\left(t \\right)} \\right)}}{l_{1} \\left(- m_{1} + m_{2} \\cos^{2}{\\left(y_{1}{\\left(t \\right)} - y_{3}{\\left(t \\right)} \\right)} - m_{2}\\right)^{2}} + \\frac{g m_{1} \\cos{\\left(y_{1}{\\left(t \\right)} \\right)} + \\frac{g m_{2} \\cos{\\left(y_{1}{\\left(t \\right)} - 2 y_{3}{\\left(t \\right)} \\right)}}{2} + \\frac{g m_{2} \\cos{\\left(y_{1}{\\left(t \\right)} \\right)}}{2} + l_{1} m_{2} y_{2}^{2}{\\left(t \\right)} \\cos{\\left(2 y_{1}{\\left(t \\right)} - 2 y_{3}{\\left(t \\right)} \\right)} + l_{2} m_{2} y_{4}^{2}{\\left(t \\right)} \\cos{\\left(y_{1}{\\left(t \\right)} - y_{3}{\\left(t \\right)} \\right)}}{l_{1} \\left(- m_{1} + m_{2} \\cos^{2}{\\left(y_{1}{\\left(t \\right)} - y_{3}{\\left(t \\right)} \\right)} - m_{2}\\right)}\\\\0\\\\- \\frac{m_{2} \\left(g m_{1} \\sin{\\left(2 y_{1}{\\left(t \\right)} - y_{3}{\\left(t \\right)} \\right)} - g m_{1} \\sin{\\left(y_{3}{\\left(t \\right)} \\right)} + g m_{2} \\sin{\\left(2 y_{1}{\\left(t \\right)} - y_{3}{\\left(t \\right)} \\right)} - g m_{2} \\sin{\\left(y_{3}{\\left(t \\right)} \\right)} + 2 l_{1} m_{1} y_{2}^{2}{\\left(t \\right)} \\sin{\\left(y_{1}{\\left(t \\right)} - y_{3}{\\left(t \\right)} \\right)} + 2 l_{1} m_{2} y_{2}^{2}{\\left(t \\right)} \\sin{\\left(y_{1}{\\left(t \\right)} - y_{3}{\\left(t \\right)} \\right)} + l_{2} m_{2} y_{4}^{2}{\\left(t \\right)} \\sin{\\left(2 y_{1}{\\left(t \\right)} - 2 y_{3}{\\left(t \\right)} \\right)}\\right) \\sin{\\left(y_{1}{\\left(t \\right)} - y_{3}{\\left(t \\right)} \\right)} \\cos{\\left(y_{1}{\\left(t \\right)} - y_{3}{\\left(t \\right)} \\right)}}{l_{2} \\left(m_{1} - m_{2} \\cos^{2}{\\left(y_{1}{\\left(t \\right)} - y_{3}{\\left(t \\right)} \\right)} + m_{2}\\right)^{2}} + \\frac{2 g m_{1} \\cos{\\left(2 y_{1}{\\left(t \\right)} - y_{3}{\\left(t \\right)} \\right)} + 2 g m_{2} \\cos{\\left(2 y_{1}{\\left(t \\right)} - y_{3}{\\left(t \\right)} \\right)} + 2 l_{1} m_{1} y_{2}^{2}{\\left(t \\right)} \\cos{\\left(y_{1}{\\left(t \\right)} - y_{3}{\\left(t \\right)} \\right)} + 2 l_{1} m_{2} y_{2}^{2}{\\left(t \\right)} \\cos{\\left(y_{1}{\\left(t \\right)} - y_{3}{\\left(t \\right)} \\right)} + 2 l_{2} m_{2} y_{4}^{2}{\\left(t \\right)} \\cos{\\left(2 y_{1}{\\left(t \\right)} - 2 y_{3}{\\left(t \\right)} \\right)}}{2 l_{2} \\left(m_{1} - m_{2} \\cos^{2}{\\left(y_{1}{\\left(t \\right)} - y_{3}{\\left(t \\right)} \\right)} + m_{2}\\right)}\\end{matrix}\\right]\\)\n\n\n\njac[:, 1]\n\n\\(\\displaystyle \\left[\\begin{matrix}1\\\\\\frac{m_{2} y_{2}{\\left(t \\right)} \\sin{\\left(2 y_{1}{\\left(t \\right)} - 2 y_{3}{\\left(t \\right)} \\right)}}{- m_{1} + m_{2} \\cos^{2}{\\left(y_{1}{\\left(t \\right)} - y_{3}{\\left(t \\right)} \\right)} - m_{2}}\\\\0\\\\\\frac{4 l_{1} m_{1} y_{2}{\\left(t \\right)} \\sin{\\left(y_{1}{\\left(t \\right)} - y_{3}{\\left(t \\right)} \\right)} + 4 l_{1} m_{2} y_{2}{\\left(t \\right)} \\sin{\\left(y_{1}{\\left(t \\right)} - y_{3}{\\left(t \\right)} \\right)}}{2 l_{2} \\left(m_{1} - m_{2} \\cos^{2}{\\left(y_{1}{\\left(t \\right)} - y_{3}{\\left(t \\right)} \\right)} + m_{2}\\right)}\\end{matrix}\\right]\\)\n\n\n\njac[:, 2]\n\n\\(\\displaystyle \\left[\\begin{matrix}0\\\\- \\frac{2 m_{2} \\left(g m_{1} \\sin{\\left(y_{1}{\\left(t \\right)} \\right)} + \\frac{g m_{2} \\sin{\\left(y_{1}{\\left(t \\right)} - 2 y_{3}{\\left(t \\right)} \\right)}}{2} + \\frac{g m_{2} \\sin{\\left(y_{1}{\\left(t \\right)} \\right)}}{2} + \\frac{l_{1} m_{2} y_{2}^{2}{\\left(t \\right)} \\sin{\\left(2 y_{1}{\\left(t \\right)} - 2 y_{3}{\\left(t \\right)} \\right)}}{2} + l_{2} m_{2} y_{4}^{2}{\\left(t \\right)} \\sin{\\left(y_{1}{\\left(t \\right)} - y_{3}{\\left(t \\right)} \\right)}\\right) \\sin{\\left(y_{1}{\\left(t \\right)} - y_{3}{\\left(t \\right)} \\right)} \\cos{\\left(y_{1}{\\left(t \\right)} - y_{3}{\\left(t \\right)} \\right)}}{l_{1} \\left(- m_{1} + m_{2} \\cos^{2}{\\left(y_{1}{\\left(t \\right)} - y_{3}{\\left(t \\right)} \\right)} - m_{2}\\right)^{2}} + \\frac{- g m_{2} \\cos{\\left(y_{1}{\\left(t \\right)} - 2 y_{3}{\\left(t \\right)} \\right)} - l_{1} m_{2} y_{2}^{2}{\\left(t \\right)} \\cos{\\left(2 y_{1}{\\left(t \\right)} - 2 y_{3}{\\left(t \\right)} \\right)} - l_{2} m_{2} y_{4}^{2}{\\left(t \\right)} \\cos{\\left(y_{1}{\\left(t \\right)} - y_{3}{\\left(t \\right)} \\right)}}{l_{1} \\left(- m_{1} + m_{2} \\cos^{2}{\\left(y_{1}{\\left(t \\right)} - y_{3}{\\left(t \\right)} \\right)} - m_{2}\\right)}\\\\0\\\\\\frac{m_{2} \\left(g m_{1} \\sin{\\left(2 y_{1}{\\left(t \\right)} - y_{3}{\\left(t \\right)} \\right)} - g m_{1} \\sin{\\left(y_{3}{\\left(t \\right)} \\right)} + g m_{2} \\sin{\\left(2 y_{1}{\\left(t \\right)} - y_{3}{\\left(t \\right)} \\right)} - g m_{2} \\sin{\\left(y_{3}{\\left(t \\right)} \\right)} + 2 l_{1} m_{1} y_{2}^{2}{\\left(t \\right)} \\sin{\\left(y_{1}{\\left(t \\right)} - y_{3}{\\left(t \\right)} \\right)} + 2 l_{1} m_{2} y_{2}^{2}{\\left(t \\right)} \\sin{\\left(y_{1}{\\left(t \\right)} - y_{3}{\\left(t \\right)} \\right)} + l_{2} m_{2} y_{4}^{2}{\\left(t \\right)} \\sin{\\left(2 y_{1}{\\left(t \\right)} - 2 y_{3}{\\left(t \\right)} \\right)}\\right) \\sin{\\left(y_{1}{\\left(t \\right)} - y_{3}{\\left(t \\right)} \\right)} \\cos{\\left(y_{1}{\\left(t \\right)} - y_{3}{\\left(t \\right)} \\right)}}{l_{2} \\left(m_{1} - m_{2} \\cos^{2}{\\left(y_{1}{\\left(t \\right)} - y_{3}{\\left(t \\right)} \\right)} + m_{2}\\right)^{2}} + \\frac{- g m_{1} \\cos{\\left(2 y_{1}{\\left(t \\right)} - y_{3}{\\left(t \\right)} \\right)} - g m_{1} \\cos{\\left(y_{3}{\\left(t \\right)} \\right)} - g m_{2} \\cos{\\left(2 y_{1}{\\left(t \\right)} - y_{3}{\\left(t \\right)} \\right)} - g m_{2} \\cos{\\left(y_{3}{\\left(t \\right)} \\right)} - 2 l_{1} m_{1} y_{2}^{2}{\\left(t \\right)} \\cos{\\left(y_{1}{\\left(t \\right)} - y_{3}{\\left(t \\right)} \\right)} - 2 l_{1} m_{2} y_{2}^{2}{\\left(t \\right)} \\cos{\\left(y_{1}{\\left(t \\right)} - y_{3}{\\left(t \\right)} \\right)} - 2 l_{2} m_{2} y_{4}^{2}{\\left(t \\right)} \\cos{\\left(2 y_{1}{\\left(t \\right)} - 2 y_{3}{\\left(t \\right)} \\right)}}{2 l_{2} \\left(m_{1} - m_{2} \\cos^{2}{\\left(y_{1}{\\left(t \\right)} - y_{3}{\\left(t \\right)} \\right)} + m_{2}\\right)}\\end{matrix}\\right]\\)\n\n\n\njac[:, 3]\n\n\\(\\displaystyle \\left[\\begin{matrix}0\\\\\\frac{2 l_{2} m_{2} y_{4}{\\left(t \\right)} \\sin{\\left(y_{1}{\\left(t \\right)} - y_{3}{\\left(t \\right)} \\right)}}{l_{1} \\left(- m_{1} + m_{2} \\cos^{2}{\\left(y_{1}{\\left(t \\right)} - y_{3}{\\left(t \\right)} \\right)} - m_{2}\\right)}\\\\1\\\\\\frac{m_{2} y_{4}{\\left(t \\right)} \\sin{\\left(2 y_{1}{\\left(t \\right)} - 2 y_{3}{\\left(t \\right)} \\right)}}{m_{1} - m_{2} \\cos^{2}{\\left(y_{1}{\\left(t \\right)} - y_{3}{\\left(t \\right)} \\right)} + m_{2}}\\end{matrix}\\right]\\)\n\n\n\nparams = {m1: 5.0, l1: 2.0, m2: 5.0, l2: 1.0, g: 9.8}\n\nf_np = sympy.lambdify((t, y), f.subs(params), 'numpy')\njac_np = sympy.lambdify((t, y), jac.subs(params), 'numpy')\n\n\ny0 = [2.0, 0.0, 0.0, 0.0]\n\nt = np.linspace(0, 20, 1000)\nr = integrate.ode(f_np, jac_np).set_initial_value(y0, t[0])\n\n\ndt = t[1] - t[0]\ny = np.zeros((len(t), len(y0)))\nidx = 0\nwhile r.successful() and r.t &lt; t[-1]:\n    y[idx, :] = r.y\n    r.integrate(r.t + dt)\n    idx += 1\n\n\\(~\\)\n\nWhen visualizing this solution, \\(\\,\\)it is more intuitive to animate the positions of the pendulums in the \\(\\,x–y\\,\\) plane rather than their angular deflections\n\n\\(~\\)\n\nth1_np, th2_np = y[:, 0], y[:, 2]\n\nx1 = params[l1] *np.sin(th1_np)\ny1 = -params[l1] *np.cos(th1_np)\nx2 = x1 +params[l2] *np.sin(th2_np)\ny2 = y1 -params[l2] *np.cos(th2_np)\n\n\nfig, ax = plt.subplots(figsize=(6, 6))\n\nax.set_xlabel('$x$', fontsize=16)\nax.set_ylabel('$y$', fontsize=16)\nax.set_xticks([-4, -2, 0, 2, 4])\nax.set_yticks([-4, -2, 0, 2, 4])\nax.set_xticklabels([-4, -2, 0, 2, 4], fontsize=14)\nax.set_yticklabels([-4, -2, 0, 2, 4], fontsize=14)\nax.set_xlim(-4, 4)\nax.set_ylim(-4, 4)\nax.tick_params(which='both', direction='in')\n\nplt.close()\n\n\nline1, = ax.plot([], [], 'o-', color='r', markersize=4.0, lw=2)\nline2, = ax.plot([], [], 'o-', color='b', markersize=20.0, lw=2)\n\ntime_text = ax.text(0.05, 0.9, '', fontsize=18, transform=ax.transAxes)\n\n\ndef init():\n    line1.set_data([], [])\n    line2.set_data([], [])\n    time_text.set_text('')\n\n    return line1, line2, time_text\n\n\ndef animate(i):\n    t_x1 = [0, x1[i]]\n    t_y1 = [0, y1[i]]\n    t_x2 = [x1[i], x2[i]]\n    t_y2 = [y1[i], y2[i]]\n\n    line1.set_data(t_x1, t_y1)\n    line2.set_data(t_x2, t_y2)\n    time_text.set_text(f'time = {i*dt:.1f}s')\n\n    return line1, line2, time_text\n\n\nanim = FuncAnimation(fig, animate, range(1, len(y)),\n        interval=dt*1000, blit=True, init_func=init)\nHTML('&lt;center&gt;' + anim.to_html5_video() + '&lt;/center&gt;')\n\n\n  \n  Your browser does not support the video tag.\n\n\n Double Pendulum Animation",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>I</span>  <span class='chapter-title'>Double Pendulum</span>"
    ]
  },
  {
    "objectID": "x_PDE_dedalus.html",
    "href": "x_PDE_dedalus.html",
    "title": "Appendix J — DEDALUS",
    "section": "",
    "text": "J.1 Coordinates, Distributors, and Bases\nDedalus solves differential equations using spectral methods. It’s open-source, written in Python, and MPI-parallelized\nThis tutorial walks through the basics of setting up and using coordinate, distributor, and basis objects in Dedalus. In Dedalus, we represent fields and solve PDEs using spectral discretizations. To set these up, we choose spectral bases for the spatial coordinates in the problem. Once the coordinates are defined, they are collected into a distributor object, which takes care of how fields and problems are split up and distributed in parallel\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport dedalus.public as d3",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>J</span>  <span class='chapter-title'>DEDALUS</span>"
    ]
  },
  {
    "objectID": "ch_07_Vectors.html",
    "href": "ch_07_Vectors.html",
    "title": "9  Vectors",
    "section": "",
    "text": "9.1 Vectors in 2-Space\n\\(~\\)\nExample \\(\\,\\) Use the given figure to prove the given result\n\\[\\mathbf{a} +\\mathbf{b} +\\mathbf{c}=\\mathbf{0}\\;~\\text{and}\\;~\\mathbf{a} +\\mathbf{b} +\\mathbf{c} +\\mathbf{d}=\\mathbf{0}\\]\n\\(~\\)",
    "crumbs": [
      "**Second Semester**",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Vectors</span>"
    ]
  },
  {
    "objectID": "ch_07_Vectors.html#sec-7-1",
    "href": "ch_07_Vectors.html#sec-7-1",
    "title": "9  Vectors",
    "section": "",
    "text": "We distinguish two important quantities: \\(~\\) scalars and vectors\n\nA scalar is simply a real number or a quantity that has magnitude\nA vector is usually described as a quantity that has both magnitude and direction\n\nEqual, Scalar Multiplication\n\\(~\\) \n\n\n\n\n\nAddition and Subtraction\n\\(~\\) \nVectors in a Coordinate Plane\n\\[\\mathbf{a}=\\left \\langle a_1, a_2 \\right \\rangle\\]\n\n\n\n\n\nLet \\(~\\)\\(\\mathbf{a}=\\left \\langle a_1, a_2 \\right \\rangle~\\) and \\(~\\)\\(\\mathbf{b}=\\left \\langle b_1, b_2 \\right \\rangle~\\) be vectors in \\(\\mathbb{R}^2\\)\n\nEquality: \\(\\text{ }\\) \\(\\mathbf{a} =\\mathbf{b}~\\) if and only if \\(~a_1 =b_1, \\;a_2=b_2\\)\nAddition: \\(\\text{ }\\) \\(\\mathbf{a} +\\mathbf{b} =\\left \\langle a_1 +b_1, a_2 +b_2 \\right \\rangle\\)\nScalar multiplication: \\(\\text{ }\\) \\(k\\mathbf{a} =\\left \\langle k a_1, k a_2 \\right \\rangle\\)\n\nProperties of Vectors\n\n\\(\\mathbf{a} +\\mathbf{b} = \\mathbf{b} +\\mathbf{a}\\)\n\\(\\mathbf{a} +(\\mathbf{b} +\\mathbf{c}) = (\\mathbf{a} +\\mathbf{b}) +\\mathbf{c}\\)\n\\(\\mathbf{a} +\\mathbf{0} = \\mathbf{a}\\)\n\\(\\mathbf{a} +(-\\mathbf{a}) = \\mathbf{0}\\)\n\\(k(\\mathbf{a} +\\mathbf{b}) = k\\mathbf{a} +k\\mathbf{b}\\)\n\\((k_1 +k_2)\\mathbf{a} = k_1\\mathbf{a} +k_2\\mathbf{a}\\)\n\\(k_1(k_2 \\mathbf{a}) = (k_1 k_2) \\mathbf{a}\\)\n\\(1\\mathbf{a} = \\mathbf{a}\\)\n\\(0\\mathbf{a} = \\mathbf{0}\\)\n\nMagnitude, \\(\\,\\) Unit Vector\n\\(\\begin{aligned}\n  \\left \\| \\mathbf{a} \\right \\| &= \\sqrt{a_1^2 +a_2^2} \\;\\;\\text{ for }\\; \\mathbf{a} =\\left \\langle a_1, a_2 \\right \\rangle \\\\\n  \\mathbf{u} &= \\frac{\\mathbf{a}}{\\left \\| \\mathbf{a} \\right \\|}\n\\end{aligned}\\)\n\\(\\mathbf{i}\\), \\(~\\mathbf{j}\\) vectors\n\\(\\begin{aligned}\n\\mathbf{a}\n  &= \\left \\langle a_1, a_2 \\right \\rangle \\\\\n  &=\\left \\langle a_1, 0 \\right \\rangle +\\left \\langle 0, a_2 \\right \\rangle  \\\\\n  &= a_1 \\left \\langle 1, 0 \\right \\rangle +a_2 \\left \\langle 0, 1 \\right \\rangle \\\\\n  &= a_1 \\mathbf{i} +a_2 \\mathbf{j}\n\\end{aligned}\\)",
    "crumbs": [
      "**Second Semester**",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Vectors</span>"
    ]
  },
  {
    "objectID": "ch_07_Vectors.html#sec-7-2",
    "href": "ch_07_Vectors.html#sec-7-2",
    "title": "9  Vectors",
    "section": "9.2 Vectors in 3-Space",
    "text": "9.2 Vectors in 3-Space\n\\(~\\)\n\n\n\n\n\n\n\\(\\mathbf{i}\\), \\(~\\mathbf{j}\\), and \\(~\\mathbf{k}\\) vectors\n\\(\\begin{aligned}\n  \\mathbf{a} &= \\left \\langle a_1, a_2, a_3 \\right \\rangle \\\\\n  &= \\left \\langle a_1, 0, 0 \\right \\rangle +\\left \\langle 0, a_2, 0 \\right \\rangle\n   +\\left \\langle 0, 0, a_3 \\right \\rangle \\\\\n  &= a_1 \\left \\langle 1, 0, 0 \\right \\rangle +a_2 \\left \\langle 0, 1, 0 \\right \\rangle\n   +a_3 \\left \\langle 0, 0, 1 \\right \\rangle \\\\\n  &= a_1 \\mathbf{i} +a_2 \\mathbf{j} +a_3 \\mathbf{k}\n\\end{aligned}\\)\nOctants\n\n\n\n\n\n\n\n\n\\(~\\)\n\n\n\n\nAxes\nCoordinates\nPlane\nCoordinates\n\n\n\n\n\\(x\\)\n\\((a, 0, 0)\\)\n\\(xy\\)\n\\((a, b, 0)\\)\n\n\n\\(y\\)\n\\((0, b, 0)\\)\n\\(yz\\)\n\\((0, b, c)\\)\n\n\n\\(z\\)\n\\((0, 0, c)\\)\n\\(xz\\)\n\\((a, 0, c)\\)\n\n\n\n\n\\(~\\)\n\n\n\\(~\\)\n\nLet \\(\\,\\mathbf{a}=\\left \\langle a_1, a_2, a_3 \\right \\rangle\\,\\) and \\(\\,\\mathbf{b}=\\left \\langle b_1, b_2, b_3 \\right \\rangle\\) be vectors in \\(\\mathbb{R}^3\\)\n\nEquality: \\(~\\mathbf{a} =\\mathbf{b}~\\text{ if and only if }~a_1 =b_1, \\;a_2=b_2, \\;a_3=b_3\\)\nAddition: \\(~\\mathbf{a} +\\mathbf{b} =\\left \\langle a_1 +b_1, a_2 +b_2, a_3 +b_3 \\right \\rangle\\)\nScalar multiplication: \\(~k\\mathbf{a} =\\left \\langle k a_1, k a_2, k a_3 \\right \\rangle\\)\nNegative: \\(-\\mathbf{b} =(-1)\\mathbf{b} =\\left \\langle -b_1, -b_2, -b_3 \\right \\rangle\\)\nSubtraction: \\(~\\mathbf{a} -\\mathbf{b} = \\mathbf{a} +(-1)\\mathbf{b} =\\left \\langle a_1 -b_1, a_2 -b_2, a_3 -b_3 \\right \\rangle\\)\nZero vector: \\(~\\mathbf{0} =\\left \\langle 0, 0, 0 \\right \\rangle\\)\nMagnitude: \\(~\\left \\| \\mathbf{a} \\right \\| =\\sqrt{a_1^2 +a_2^2 +a_3^2}\\)\n\n\n\\(~\\)\nExample \\(\\,\\) Describe the locus of points \\(P(x, y, z)\\) that satisfy the given equations\n\n\\(xyz=0\\)\n\\((x+1)^2 +(y-2)^2 +(z+3)^2 = 0\\)\n\\((x-2)(z-8)=0\\)\n\\(z^2 -25=0\\)\n\\(x=y=z\\)\n\n\\(~\\)",
    "crumbs": [
      "**Second Semester**",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Vectors</span>"
    ]
  },
  {
    "objectID": "ch_07_Vectors.html#sec-7-3",
    "href": "ch_07_Vectors.html#sec-7-3",
    "title": "9  Vectors",
    "section": "9.3 Dot Product (Inner or Scalar Product)",
    "text": "9.3 Dot Product (Inner or Scalar Product)\n\nThe dot product of \\(\\mathbf{a}=\\left \\langle a_1, a_2, a_3 \\right \\rangle\\) and \\(\\mathbf{b}=\\left \\langle b_1, b_2, b_3 \\right \\rangle\\) is\n\\(\\phantom{x}\\mathbf{a} \\cdot \\mathbf{b} = a_1 b_1 + a_2 b_2 + a_3 b_3\\)\nProperties of the Dot Product\n\n\\(\\mathbf{a}\\cdot\\mathbf{b} = 0~\\) if \\(~\\mathbf{a}=\\mathbf{0}~\\) or \\(~\\mathbf{b}=\\mathbf{0}\\)\n\\(\\mathbf{a}\\cdot\\mathbf{b} = \\mathbf{b}\\cdot\\mathbf{a}\\)\n\\(\\mathbf{a}\\cdot(\\mathbf{b} +\\mathbf{c}) = \\mathbf{a}\\cdot\\mathbf{b} +\\mathbf{a}\\cdot\\mathbf{c}\\)\n\\(\\mathbf{a}\\cdot(k\\mathbf{b}) = (k\\mathbf{a})\\cdot\\mathbf{b} =k(\\mathbf{a}\\cdot\\mathbf{b})\\)\n\\(\\mathbf{a}\\cdot\\mathbf{a} \\geq 0\\)\n\\(\\mathbf{a}\\cdot\\mathbf{a} =\\left \\| \\mathbf{a} \\right \\|^2\\)\n\nAlternative Form\nThe dot product of two vectors \\(\\mathbf{a}\\) and \\(\\mathbf{b}\\) is\n\\(\\phantom{xx}\\mathbf{a}\\cdot\\mathbf{b} =\\left\\| \\mathbf{a} \\right\\| \\left\\| \\mathbf{b} \\right\\| \\cos\\theta\\)\nwhere \\(\\theta\\) is the angle between the vectors, \\(\\,0 \\leq \\theta \\leq \\pi\\)\nAngle between two vectors\n\\(\\displaystyle\\phantom{xx}\\cos\\theta = \\frac{a_1 b_1 +a_2 b_2 + a_3 b_3}{\\left\\| \\mathbf{a}\\right\\| \\left\\| \\mathbf{b}\\right\\|}\\)\n\nDirection cosines\n\n\n\n\n\n\n\\(\\phantom{xx}\\displaystyle\\cos\\alpha=\\frac{a_1}{\\left\\|\\mathbf{a}\\right\\|}\\), \\(~\\displaystyle\\cos\\beta=\\frac{a_2}{\\left\\|\\mathbf{a}\\right\\|}\\), \\(~\\displaystyle\\cos\\gamma=\\frac{a_3}{\\left\\|\\mathbf{a}\\right\\|}\\)\nOrthogonal vectors\nTwo nonzero vectors \\(\\mathbf{a}\\) and \\(\\mathbf{b}\\) are orthogonal if and only if \\(~\\mathbf{a}\\cdot\\mathbf{b}=0\\)\nComponent of \\(\\mathbf{a}\\) on \\(\\mathbf{b}\\)\nTo find the component of \\(\\mathbf{a}\\) on a vector \\(\\mathbf{b}\\), \\(~\\)we dot \\(\\mathbf{a}\\) with a unit vector in the direction of \\(\\mathbf{b}\\)\n\n\n\n\n\n\\[\\text{comp}_{\\mathbf{b}} \\mathbf{a} =\\left\\| \\mathbf{a} \\right\\| \\cos\\theta\n=\\left\\| \\mathbf{a} \\right\\| \\cos\\theta \\frac{\\left\\| \\mathbf{b} \\right\\|}{\\left\\| \\mathbf{b} \\right\\|}\n=\\frac{\\mathbf{a}\\cdot\\mathbf{b}}{\\left\\| \\mathbf{b} \\right\\|}\n=\\mathbf{a} \\cdot\\frac{\\mathbf{b}}{\\left\\| \\mathbf{b} \\right\\|}\\]\nProjection of \\(\\mathbf{a}\\) onto \\(\\mathbf{b}\\)\n\n\n\n\n\n\\[\\text{proj}_{\\mathbf{b}} \\mathbf{a} = \\left(\\text{comp}_{\\mathbf{b}} \\mathbf{a} \\right) \\left(\\frac{\\mathbf{b}}{\\left\\| \\mathbf{b} \\right\\|} \\right)=\\color{red}{\\left(\\frac{\\mathbf{a}\\cdot\\mathbf{b}}{\\mathbf{b}\\cdot\\mathbf{b}}\\right) \\mathbf{b}}\\]\n\n\\(~\\)\nExample \\(\\,\\) Verify that the vector \\(\\displaystyle \\mathbf{c}=\\mathbf{b} - \\left(\\frac{\\mathbf{a} \\cdot \\mathbf{b}}{\\mathbf{a} \\cdot \\mathbf{a}}\\right) \\mathbf{a}\\,\\) is orthogonal to the vector \\(\\mathbf{a}\\)\n\\(~\\)\nExample \\(\\,\\) In the methane molecule \\(\\mathrm{CH}_4\\), the hydrogen atoms are located at the four vertices of a regular tetrahedron. The distance between the center of a hydrogen atom and the center of a carbon atom is 1.10 angstroms, and the hydrogen-carbon-hydrogen bond angle is \\(\\,\\theta=109.5^\\circ\\). Using vector methods only, find the distance between two hydrogen atoms\n\\(~\\)",
    "crumbs": [
      "**Second Semester**",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Vectors</span>"
    ]
  },
  {
    "objectID": "ch_07_Vectors.html#sec-7-4",
    "href": "ch_07_Vectors.html#sec-7-4",
    "title": "9  Vectors",
    "section": "9.4 Cross Product",
    "text": "9.4 Cross Product\n\nCross product of Two vectors\n\\(\\phantom{xx}\\mathbf{a} \\times \\mathbf{b}\n=\\left|\\begin{matrix} \\mathbf{i} & \\mathbf{j} & \\mathbf{k}\\\\ a_1 &  a_2 & a_3\\\\ b_1 &  b_2 & b_3 \\end{matrix}\\right|\n=\\left|\\begin{matrix} a_2 & a_3\\\\ b_2 & b_3 \\end{matrix}\\right| \\mathbf{i}\n-\\left|\\begin{matrix} a_1 & a_3\\\\ b_1 & b_3 \\end{matrix}\\right| \\mathbf{j}\n+\\left|\\begin{matrix} a_1 & a_2\\\\ b_1 & b_2 \\end{matrix}\\right| \\mathbf{k}\\)\nAlternative Form and Magnitude of the Cross Product\n\\(\\phantom{xx}\\mathbf{a}\\times\\mathbf{b} =(\\left\\| \\mathbf{a} \\right\\| \\left\\| \\mathbf{b} \\right\\| \\sin\\theta)\\,\\mathbf{n} =\\left\\| \\mathbf{a}\\times\\mathbf{b} \\right\\| \\mathbf{n}\\)\nRight-Hand Rule\nThe vectors \\(\\mathbf{a}\\), \\(\\mathbf{b}\\), and \\(~\\mathbf{a}\\times\\mathbf{b}~\\) form a right-hand system:\n\n\n\n\n\nParallel Vectors\nTwo nonzero vectors \\(\\mathbf{a}\\) and \\(\\mathbf{b}\\) are parallel if and only if \\(~\\mathbf{a}\\times\\mathbf{b}=\\mathbf{0}\\)\nProperties of the Cross Product \\(~\\)\n\n\\(\\mathbf{a}\\times\\mathbf{b} = \\mathbf{0}~\\) if \\(~\\mathbf{a}=\\mathbf{0}~\\) or \\(~\\mathbf{b}=\\mathbf{0}\\)\n\\(\\mathbf{a}\\times\\mathbf{b} =-\\mathbf{b}\\times\\mathbf{a}\\)\n\\(\\mathbf{a}\\times(\\mathbf{b} +\\mathbf{c}) = (\\mathbf{a}\\times\\mathbf{b}) +(\\mathbf{a}\\times\\mathbf{c})\\)\n\\((\\mathbf{a} +\\mathbf{b}) \\times\\mathbf{c} = (\\mathbf{a}\\times\\mathbf{c}) +(\\mathbf{b}\\times\\mathbf{c})\\)\n\n\\(\\mathbf{a}\\times(k\\mathbf{b}) = (k\\mathbf{a})\\times\\mathbf{b} =k(\\mathbf{a}\\times\\mathbf{b})\\)\n\\(\\mathbf{a}\\times\\mathbf{a} =\\mathbf{0}\\)\n\\(\\mathbf{a}\\cdot(\\mathbf{a}\\times\\mathbf{b})=0\\)\n\\(\\mathbf{b}\\cdot(\\mathbf{a}\\times\\mathbf{b})=0\\)\n\nAreas of a parallelogram and a Trianlge\n\\(\\text{(a)}\\) \\(~A =\\left\\|\\mathbf{a} \\times \\mathbf{b} \\right\\|\\)\n\\(\\text{(b)}\\) \\(~A =\\frac{1}{2}\\left\\|\\mathbf{a} \\times \\mathbf{b} \\right\\|\\)\n\\(~\\)\n\n\n\n\n\nVolume of a Parallelepiped\n\\(\\phantom{xx}V=\\left| \\mathbf{a} \\cdot (\\mathbf{b} \\times \\mathbf{c})\\right|\\)\n\\(~\\)\n\n\n\n\n\nSpecial Products\n\\(\\phantom{xx}\\mathbf{a} \\cdot (\\mathbf{b} \\times \\mathbf{c}) =\n\\left|\\begin{matrix}\n  a_1 & a_2 & a_3\\\\\n  b_1 & b_2 & b_3\\\\\n  c_1 & c_2 & c_3\n\\end{matrix}\\right| = (\\mathbf{a} \\times \\mathbf{b}) \\cdot \\mathbf{c}\\)\n\\(~\\)\n\\(\\phantom{xx}\\mathbf{a} \\times (\\mathbf{b} \\times \\mathbf{c}) = (\\mathbf{a} \\cdot \\mathbf{c}) \\mathbf{b}\n-(\\mathbf{a} \\cdot \\mathbf{b}) \\mathbf{c}\\)\n\n\\(~\\)\nExample \\(\\,\\) Prove or disprove\n\n\\(~\\mathbf{a} \\times (\\mathbf{b} \\times \\mathbf{c}) = (\\mathbf{a} \\cdot \\mathbf{c}) \\mathbf{b}\n-(\\mathbf{a} \\cdot \\mathbf{b}) \\mathbf{c}\\)\n\\(~\\mathbf{a} \\times (\\mathbf{b} \\times \\mathbf{c}) =(\\mathbf{a} \\times \\mathbf{b}) \\times \\mathbf{c}\\)\n\\(~\\mathbf{a} \\times (\\mathbf{b} \\times \\mathbf{c}) +\\mathbf{b} \\times (\\mathbf{c} \\times \\mathbf{a})\n  +\\mathbf{c} \\times (\\mathbf{a} \\times \\mathbf{b})=\\mathbf{0}\\)\nLagrange’s identity \\(\\| \\mathbf{a}\\times\\mathbf{b}\\|^2 = \\|\\mathbf{a}\\|^2 \\|\\mathbf{b}\\|^2 - (\\mathbf{a}\\cdot\\mathbf{b})^2\\)\n\n\\(~\\)",
    "crumbs": [
      "**Second Semester**",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Vectors</span>"
    ]
  },
  {
    "objectID": "ch_07_Vectors.html#sec-7-5",
    "href": "ch_07_Vectors.html#sec-7-5",
    "title": "9  Vectors",
    "section": "9.5 Lines and Planes in 3-Space",
    "text": "9.5 Lines and Planes in 3-Space\n\nLines\n\n\n\n\n\n\\[\n\\begin{aligned}\n  \\mathbf{r} -\\mathbf{r}_2  &= t(\\mathbf{r}_2 -\n    \\mathbf{r}_1) = t\\mathbf{a} \\\\\n   &\\Rightarrow \\;\\displaystyle t = \\frac{x - x_2}{a_1} = \\frac{y - y_2}{a_2} = \\frac{z - z_2}{a_3}\n\\end{aligned}\\]\nPlanes\nIf the normal vector is \\(\\mathbf{n}=n_1\\mathbf{i} +n_2\\mathbf{j} +n_3\\mathbf{k}\\), \\(~\\)then\n\\[\n  \\begin{aligned}\n     \\mathbf{n}&\\cdot(\\mathbf{r} -\\mathbf{r}_1) =0 \\\\\n     &\\Rightarrow~ n_1(x -x_1) +n_2 (y -y_1) +n_3(z- z_1)=0\n  \\end{aligned}\n\\]\n\n\n\n\n\n\n\n\n\n\n\n\\(~\\)\nExample \\(\\,\\) Find parametric equation for the line that contains \\((-4, 1, 7)\\) and is perpendicular to the plane \\[-7x+2y+3z=1\\]\n\\(~\\)\nExample \\(\\,\\) Find parametric equations for the line of intersection of the given planes\n\\[\n\\begin{aligned}\n  5x - 4y -9z &= 8\\\\\n  x + 4y +3z &= 4\n\\end{aligned}\\]\n\\(~\\)\nExample \\(\\,\\) Find an equation of the plane that contains the given line and is orthogonal to the indicated plane\n\\[\\frac{2-x}{3}=\\frac{y+2}{5}=\\frac{z-8}{2}; \\quad 2x - 4y-z + 16=0\\]\n\\(~\\)",
    "crumbs": [
      "**Second Semester**",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Vectors</span>"
    ]
  },
  {
    "objectID": "ch_07_Vectors.html#sec-7-6",
    "href": "ch_07_Vectors.html#sec-7-6",
    "title": "9  Vectors",
    "section": "9.6 Vector Spaces",
    "text": "9.6 Vector Spaces\n\n\\(n\\)-Space\n\nA vector in n-space is any ordered \\(n\\)-tuple \\(\\,\\mathbf{a}=\\left\\langle a_1, a_2, \\cdots, a_n \\right\\rangle\\) of real numbers called the components of \\(~\\mathbf{a}\\)\nThe set of all vectors in \\(n\\)-space is denoted by \\(\\mathbb{R}^n\\)\nThe component definitions in 3-space carry over to \\(\\mathbb{R}^n\\) in a natural way\n\nThe standard dot or inner product of two \\(n\\)-vectors is the real number defined by\n\\(\\phantom{xx}\\begin{aligned}\n     \\mathbf{a} \\cdot \\mathbf{b} &= \\left\\langle a_1, a_2, \\cdots, a_n \\right\\rangle \\cdot \\left\\langle b_1, b_2, \\cdots, b_n \\right\\rangle \\\\\n      &= a_1 b_1 +a_2 b_2 + \\cdots + a_n b_n\n  \\end{aligned}\\)\nTwo nonzero vectors \\(\\mathbf{a}\\) and \\(\\mathbf{b}\\) in \\(\\mathbb{R}^n\\) are said to be orthogonal if and only if \\(~\\mathbf{a} \\cdot \\mathbf{b} = 0\\)\nVector Space\nLet \\(V\\) be a set of elements on which two operations called vector addition and scalar multiplication are defined. Then \\(V\\) is said to be a vector space if the following 10 properties are satisfied\nAxioms for Vector Addition\n\nIf \\(\\mathbf{x}\\) and \\(\\mathbf{y}\\) are in \\(V\\), \\(~\\)then \\(\\mathbf{x} +\\mathbf{y}\\) is in \\(V\\)\nFor all \\(\\mathbf{x}\\), \\(\\mathbf{y}\\) in \\(V\\), \\(~\\)\\(\\mathbf{x} +\\mathbf{y} = \\mathbf{y} +\\mathbf{x}\\)\nFor all \\(\\mathbf{x}\\), \\(\\mathbf{y}\\), \\(\\mathbf{z}\\) in \\(V\\), \\(~\\)\\(\\mathbf{x} +(\\mathbf{y} +\\mathbf{z}) = (\\mathbf{x} +\\mathbf{y}) +\\mathbf{z}\\)\nThere is a unique vector \\(\\mathbf{0}\\) in \\(V\\) such that \\(~\\)\\(\\mathbf{0} +\\mathbf{x} = \\mathbf{x} +\\mathbf{0}\\)\nFor each \\(\\mathbf{x}\\) in \\(V\\), \\(~\\)there exists a vector \\(=\\mathbf{x}~\\) such that \\(\\mathbf{x} +(-\\mathbf{x}) = (-\\mathbf{x}) +\\mathbf{x} =\\mathbf{0}\\)\n\nAxioms for Scalar Multiplication\n\nIf \\(k\\) is any scalar and \\(\\mathbf{x}\\) is in \\(V\\), \\(~\\)then \\(~k\\mathbf{x}~\\) is in \\(V\\)\n\\(k(\\mathbf{x} +\\mathbf{y}) =k\\mathbf{x} + k\\mathbf{y}\\)\n\\((k_1 +k_2)\\mathbf{x} =k_1\\mathbf{x} + k_2\\mathbf{x}\\)\n\\(k_1(k_2\\mathbf{x})=(k_2k_2)\\mathbf{x}\\)\n\\(1\\mathbf{x}=\\mathbf{x}\\)\n\nHere are some important vector spaces:\n\nThe set \\(\\mathbb{R}\\) of real numbers\nThe set \\(\\mathbb{R}^2\\) of ordered pairs\nThe set \\(\\mathbb{R}^3\\) of ordered triples\nThe set \\(\\mathbb{R}^n\\) of ordered \\(n\\)-tuples\nThe set \\(P_n\\) of polynomials of degree less than or equal to \\(n\\)\nThe set \\(P\\) of all polynomials\nThe set of real-valued functions \\(~f\\) defined on the set \\(\\mathbb{R}\\)\nThe set \\(C[a,b]\\) of real-valued functions \\(~f\\) continuous on the closed interval \\([a,b]\\)\nThe set \\(C(-\\infty,\\infty)\\) of real-valued functions \\(~f\\) continuous on the set \\(\\mathbb{R}\\)\nThe set \\(C^n[a,b]\\) of all real-valued functions \\(~f\\), \\(\\,\\) for which \\(f\\), \\(f'\\), \\(f''\\), \\(\\cdots\\), \\(f^{(n)}\\) exist and are continuous on the closed interval \\([a,b]\\)\n\n\n\\(~\\)\nExample \\(\\,\\) Consider the set \\(V\\) of positive real numbers. If \\(x\\) and \\(y\\) denote positive real numbers, then we write vectors in \\(V\\) as \\(\\mathbf{x}=x~\\) and \\(~\\mathbf{y}=y\\)\n\nNow, \\(~\\) addition of vectors is defined by\n\\(\\phantom{xx}\\mathbf{x} +\\mathbf{y} =xy\\)\nand scalar multiplication is defined by\n\\(\\phantom{xx}k\\mathbf{x}=x^k\\)\nDetermine whether \\(V\\) is a vector space\n\n\\(~\\)\n\nSubspace\n\nA nonempty subset \\(W\\) of a vector space \\(V\\) is a subspace of \\(V~\\) if and only if \\(~W\\) is closed under vector addition and scalar multiplication defined on \\(V\\):\n\nIf \\(\\mathbf{x}\\) and \\(\\mathbf{y}\\) are in \\(W\\), \\(~\\)then \\(\\mathbf{x} +\\mathbf{y}\\) is in \\(W\\)\nIf \\(\\mathbf{x}\\) is in \\(W\\) and \\(k\\) is any scalar, \\(~\\)then \\(k\\mathbf{x}\\) is in \\(W\\)\n\n\nLinear Independence\nA set of vectors \\(\\left\\{ \\mathbf{x}_1, \\mathbf{x}_2, \\cdots, \\mathbf{x}_n \\right\\}\\) is said to be linearly indpenedent if the only constants satisfying the equation\n\\[k_1\\mathbf{x}_1 +k_2\\mathbf{x}_2 +\\cdots +k_n\\mathbf{x}_n=\\mathbf{0}\\]\nare \\(k_1=k_2=\\cdots=k_n=0\\). If the set of vectors is not linearly indpenedent, \\(~\\)then it is said to be linearly dependent\nBasis\nConsider a set of vectors \\(B=\\left\\{\\mathbf{x}_1, \\mathbf{x}_2,\\cdots,\\mathbf{x}_n\\right\\}\\,\\) in a vector space \\(V\\). \\(~\\)If the set \\(\\,B\\,\\) is linearly independent and if every vector in \\(V\\) can be expressed as a linear combination of these vectors, \\(~\\)then \\(B\\) is said to be a basis in \\(V\\)\nDimension\nThe number of vectors in a basis \\(\\,B\\,\\) for a vector space \\(~V\\) is said to be the dimension of the space\nSpan\nIf \\(S\\) denotes any set of vectors \\(\\left\\{\\mathbf{x}_1, \\mathbf{x}_2,\\cdots,\\mathbf{x}_n\\right\\}\\) in a vector space \\(V\\), \\(\\,\\)then the set of all linear combinations of the vector \\(\\mathbf{x}_1,\\mathbf{x}_2,\\cdots,\\mathbf{x}_n\\) in \\(S\\),\n\\[\\left\\{ k_1\\mathbf{x}_1 +k_2\\mathbf{x}_2+ \\cdots+k_n\\mathbf{x}_n\\right\\}\\]\nwhere the \\(k_i\\) are scalars, \\(\\,\\)is called the span of the vectors and written \\(\\mathrm{span}(S)\\) or \\(\\mathrm{span}(\\mathbf{x}_1,\\mathbf{x}_2,\\cdots,\\mathbf{x}_n)\\)\n\\(~\\)\nIf \\(\\,V=\\text{span}(S)\\), \\(\\,\\)then we say that \\(\\,S\\,\\) is a spanning set for the vector space \\(V\\), \\(\\,\\)or that \\(\\,S\\,\\) spans \\(\\,V\\)\n\n\\(~\\)\nExample \\(\\,\\) Determine whether the given set is a vector space:\n\nThe set of complex numbers \\(a + bi\\), \\(\\,\\)where \\(i^2=-1\\), \\(\\,\\)addition and scalar multiplication defined by\n\\[\n\\begin{aligned}\n  (a_1 + b_1 i) +(a_2 +b_2 i) &= (a_1 +a_2) +(b_1 +b_2)i \\\\\n  k(a+bi) &= ka +kb i\n\\end{aligned}\\]\nwhere \\(k\\) is a real number\nThe set of arrays of real numbers \\(\\begin{pmatrix}\n  a_{11} & a_{12} \\\\\n  a_{21} & a_{22}\n\\end{pmatrix}\\), \\(~\\) addition and scalar multiplication defined by\n\n\\[\n  \\begin{aligned}\n    \\begin{pmatrix}\n      a_{11} & a_{12} \\\\\n      a_{21} & a_{22}\n    \\end{pmatrix} +\n    \\begin{pmatrix}\n      b_{11} & b_{12} \\\\\n      b_{21} & b_{22}\n    \\end{pmatrix} &= \\begin{pmatrix}\n      a_{12} +b_{12} & a_{11}+b_{11} \\\\\n      a_{22} +b_{22} & a_{21}+b_{21}\n    \\end{pmatrix}\\\\\n    k\\begin{pmatrix}\n       a_{11} & a_{12} \\\\\n       a_{21} & a_{22}\n     \\end{pmatrix}&= \\begin{pmatrix}\n       ka_{11} & ka_{12} \\\\\n       ka_{21} & ka_{22}\n      \\end{pmatrix}\n  \\end{aligned}\\]\n\\(~\\)",
    "crumbs": [
      "**Second Semester**",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Vectors</span>"
    ]
  },
  {
    "objectID": "ch_07_Vectors.html#sec-7-7",
    "href": "ch_07_Vectors.html#sec-7-7",
    "title": "9  Vectors",
    "section": "9.7 Gram-Schmidt Orthogonalization Process",
    "text": "9.7 Gram-Schmidt Orthogonalization Process\n\nEvery vector \\(\\mathbf{u}\\) in \\(\\mathbb{R}^n\\) can be written as a linear combination of the vectors in the standard basis \\(B=\\left\\{ \\mathbf{e}_1, \\mathbf{e}_2, \\cdots, \\mathbf{e}_n \\right\\}\\):\n\\[\n\\begin{aligned}\n  \\mathbf{e}_1 &=\\left\\langle 1,0,0,\\cdots,0 \\right\\rangle \\\\\n  \\mathbf{e}_2 &=\\left\\langle 0,1,0,\\cdots,0 \\right\\rangle \\\\\n  &\\;\\;\\vdots \\\\\n  \\mathbf{e}_n &=\\left\\langle 0,0,0,\\cdots,1 \\right\\rangle\n\\end{aligned}\\]\nThis standard basis \\(B=\\left\\{ \\mathbf{e}_1, \\mathbf{e}_2, \\cdots, \\mathbf{e}_n \\right\\}\\,\\) is also an example of an orthonormal basis\n\\[\\mathbf{e}_i \\cdot \\mathbf{e}_j = 0, \\,i \\neq j ~\\text{ and}~ \\left\\| \\mathbf{e}_i \\right\\|=1, ~i=1,2,\\cdots,n\\]\nCoordinates Relative to an Orthonormal Basis\nSuppose \\(B=\\left\\{ \\mathbf{w}_1, \\mathbf{w}_2, \\cdots, \\mathbf{w}_n \\right\\}\\) is an orthonormal basis for \\(\\mathbb{R}^n\\). \\(\\,\\)If \\(\\mathbf{u}\\) is any vector in \\(\\mathbb{R}^n\\), \\(~\\)then\n\\[\\mathbf{u} = (\\mathbf{u}\\cdot\\mathbf{w}_1)\\mathbf{w}_1 +(\\mathbf{u}\\cdot\\mathbf{w}_2)\\mathbf{w}_2 + \\cdots +(\\mathbf{u}\\cdot\\mathbf{w}_n)\\mathbf{w}_n\\]\nConstructing an Orthogonal Basis for \\(\\mathbb{R}^2\\)\n\\[\\begin{aligned} \\mathbf{v}_1 &= \\mathbf{u}_1 \\\\ \\mathbf{v}_2 &= \\mathbf{u}_2 -\\left( \\frac{\\mathbf{u}_2 \\cdot \\mathbf{v}_1}{\\mathbf{v}_1 \\cdot \\mathbf{v}_1}\\right) \\mathbf{v}_1\\end{aligned}\\]\n\n\n\n\n\nConstructing an Orthogonal Basis for \\(\\mathbb{R}^3\\)\n\\[\\begin{aligned} \\mathbf{v}_1 &= \\mathbf{u}_1 \\\\\n\\mathbf{v}_2 &= \\mathbf{u}_2 -\\left( \\frac{\\mathbf{u}_2 \\cdot \\mathbf{v}_1}{\\mathbf{v}_1 \\cdot \\mathbf{v}_1}\\right) \\mathbf{v}_1 \\\\\n\\mathbf{v}_3 &= \\mathbf{u}_3 -\\left( \\frac{\\mathbf{u}_3 \\cdot \\mathbf{v}_1}{\\mathbf{v}_1 \\cdot \\mathbf{v}_1}\\right) \\mathbf{v}_1\n-\\left( \\frac{\\mathbf{u}_3 \\cdot \\mathbf{v}_2}{\\mathbf{v}_2 \\cdot \\mathbf{v}_2}\\right) \\mathbf{v}_2 \\end{aligned}\\]\n\n\n\n\n\n\n\nGram-Schmidt Orthogonalization Process\nLet \\(\\,B=\\left\\{ \\mathbf{u}_1, \\mathbf{u}_2, \\cdots, \\mathbf{u}_m \\right\\}\\), \\(~m \\leq n\\), \\(~\\)be a basis for a subspace \\(W_m\\) of \\(~\\mathbb{R}^n\\). \\(\\,\\)Then \\(B'=\\left\\{ \\mathbf{v}_1, \\mathbf{v}_2, \\cdots, \\mathbf{v}_m \\right\\}\\) , \\(\\,\\)where\n\\[\n\\begin{aligned}\n    \\mathbf{v}_1 & = \\mathbf{u}_1 \\\\\n    \\mathbf{v}_2 & = \\mathbf{u}_2 -\\left( \\frac{\\mathbf{u}_2 \\cdot \\mathbf{v}_1}{\\mathbf{v}_1 \\cdot \\mathbf{v}_1}\\right) \\mathbf{v}_1 \\\\\n   \\mathbf{v}_3 & = \\mathbf{u}_3 -\\left( \\frac{\\mathbf{u}_3 \\cdot \\mathbf{v}_1}{\\mathbf{v}_1 \\cdot \\mathbf{v}_1}\\right) \\mathbf{v}_1\n-\\left( \\frac{\\mathbf{u}_3 \\cdot \\mathbf{v}_2}{\\mathbf{v}_2 \\cdot \\mathbf{v}_2}\\right) \\mathbf{v}_2 \\\\\n&\\;\\vdots \\\\\n  \\mathbf{v}_m & = \\mathbf{u}_m -\\left( \\frac{\\mathbf{u}_m \\cdot \\mathbf{v}_1}{\\mathbf{v}_1 \\cdot \\mathbf{v}_1}\\right) \\mathbf{v}_1\n-\\left( \\frac{\\mathbf{u}_m \\cdot \\mathbf{v}_2}{\\mathbf{v}_2 \\cdot \\mathbf{v}_2}\\right) \\mathbf{v}_2\n-\\cdots -\\left( \\frac{\\mathbf{u}_m \\cdot \\mathbf{v}_{m-1}}{\\mathbf{v}_{m-1} \\cdot \\mathbf{v}_{m-1}}\\right) \\mathbf{v}_{m-1}     \n\\end{aligned}\\]\nis an orthogonal basis for \\(W_m\\)\nAn orthonormal basis for \\(W_m\\) is\n\\[B''=\\left\\{ \\mathbf{w}_1, \\mathbf{w}_2, \\cdots, \\mathbf{w}_m \\right\\}=\\left\\{ \\frac{1}{\\left\\| \\mathbf{v}_1 \\right\\|} \\mathbf{v}_1,\n\\frac{1}{\\left\\| \\mathbf{v}_2 \\right\\|} \\mathbf{v}_2, \\cdots, \\frac{1}{\\left\\| \\mathbf{v}_m \\right\\|} \\mathbf{v}_m \\right\\}\\]\n\n\\(~\\)\nExample \\(\\,\\)An inner product defined on the vector space \\(P_2\\) of all polynomials of degree less than or equal to \\(2\\), is given by\n\\[\\left\\langle p, q \\right \\rangle =\\int_{-1}^{1} p(x)\\, q(x)\\, dx\\]\nUse the Gram-Schmidt orthogonalization process to transform the given basis for \\(P_2\\) into an orthogonal basis \\(B'\\)\n\\[B = \\{1, x, x^2\\}\\]\n\\(~\\)",
    "crumbs": [
      "**Second Semester**",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Vectors</span>"
    ]
  },
  {
    "objectID": "ch_07_Vectors.html#worked-exercises",
    "href": "ch_07_Vectors.html#worked-exercises",
    "title": "9  Vectors",
    "section": "Worked Exercises",
    "text": "Worked Exercises\n1. \\(~\\) Determine whether the given vectors are linearly independent or linearly dependent:\n\\[1, (x+1), (x+1)^2, x^2 \\text{ in } P_2\\]\nSolution\nStep 1: \\(~\\) Understand the space \\(P_2\\)\nThe vector space \\(P_2\\) consists of all real polynomials of degree at most 2. So any element in \\(P_2\\) has the form:\n\\[a_0 + a_1 x + a_2 x^2\\]\nThat means the dimension of \\(P_2\\) is 3. Therefore, any set of more than 3 vectors in \\(P_2\\) must be linearly dependent\nStep 2: \\(~\\) Count the number of vectors\nWe are given 4 vectors:\n\\[1, \\quad (x+1), \\quad (x+1)^2, \\quad x^2\\]\nSince \\(\\dim P_2 = 3\\), and we are given 4 vectors, they must be linearly dependent\n(Optional) Step 3: \\(~\\) Verify dependence explicitly\nTo see this more concretely, write each vector in terms of the standard basis \\(\\{1, x, x^2\\}\\):\n\n\\(1 = 1 + 0x + 0x^2 \\Rightarrow (1, 0, 0)\\)\n\\(x+1 = 1x + 1 \\Rightarrow (1, 1, 0)\\)\n\\((x+1)^2 = x^2 + 2x + 1 \\Rightarrow (1, 2, 1)\\)\n\\(x^2 = x^2 \\Rightarrow (0, 0, 1)\\)\n\nWrite these as column vectors:\n\\[\\begin{bmatrix}\n1\\\\0\\\\0\n\\end{bmatrix}, \\quad\n\\begin{bmatrix}\n1\\\\1\\\\0\n\\end{bmatrix}, \\quad\n\\begin{bmatrix}\n1\\\\2\\\\1\n\\end{bmatrix}, \\quad\n\\begin{bmatrix}\n0\\\\0\\\\1\n\\end{bmatrix}\\]\nYou can now see, for instance:\n\\[(x+1)^2 = x^2 + 2(x) + 1 = x^2 + 2(x+1) - 1\n\\Rightarrow (x+1)^2 - 2(x+1) + x^2 - 1 = 0\\]\nSo there is a nontrivial linear combination:\n\\[1\\cdot(x+1)^2 - 2\\cdot(x+1) + 1\\cdot x^2 - 1 = 0\\]\n\\(~\\)\n2. \\(~\\) Explain why \\(\\displaystyle f(x)=\\frac{x}{x^2 +4x +3}\\) is a vector in \\(C[0, 3]\\) but not a vector in \\(C[-3,0]\\)\nSolution\nStep 1: \\(~\\) Understand the vector space \\(C[a,b]\\)\n\n\\(C[a,b]\\) is the vector space of all continuous real-valued functions defined on the interval \\([a,b]\\)\nSo, a function \\(f(x)\\) is a vector in \\(C[a,b]\\) if and only if it is continuous on the entire interval \\([a,b]\\)\n\nStep 2: \\(~\\) Analyze \\(f(x) = \\dfrac{x}{x^2 + 4x + 3}\\)\nFactor the denominator:\n\\[x^2 + 4x + 3 = (x+1)(x+3)\\]\nSo the function becomes:\n\\[f(x) = \\frac{x}{(x+1)(x+3)}\\]\nThis function is undefined at \\(x = -1\\) and \\(x = -3\\), since the denominator becomes 0 at those points.\nStep 3: \\(~\\) Behavior on \\([0, 3]\\)\n\nThe interval \\([0,3]\\) lies entirely to the right of both singularities \\(x = -1\\) and \\(x = -3\\)\nOn this interval, \\(f(x)\\) is:\n\nDefined (denominator never zero),\nContinuous (since it’s a rational function with no discontinuities on this domain)\n\n\nStep 4: \\(~\\) Behavior on \\([-3, 0]\\)\n\nThis interval includes the point \\(x = -1\\), where the denominator becomes \\(0\\).\nAt \\(x = -1\\), the function is undefined, so it is not continuous on the entire interval\n\nConclusion:\n\n\\(f(x) \\in C[0,3]\\) — it is a continuous function on that interval, so it is a vector in \\(C[0,3]\\)\n\\(f(x) \\notin C[-3,0]\\) — because it is not defined (and hence not continuous) at \\(x = -1\\), so it is not a vector in \\(C[-3,0]\\)\n\n\\(~\\)\n3. \\(~\\) The given vectors span a subspace \\(W\\) of \\(\\mathbb{R}^4\\). Use the Gram-Schmidt orthogonalization process to construct an orthonormal basis for the subspace\n\\[\\mathbf{u}_1=\\left \\langle 4, 0, 2, -1 \\right \\rangle, \\; \\mathbf{u}_2=\\left \\langle 2, 1,-1,1 \\right \\rangle, \\;\\text{and}\\; \\mathbf{u}_3=\\left \\langle 1,1,-1,0 \\right \\rangle\\]\nSolution\n\\[\\begin{aligned}\n     \\mathbf{v}_1 &= \\mathbf{u}_3=\\left \\langle 1,1,-1,0 \\right \\rangle \\\\\n     \\mathbf{v}_2 &= \\mathbf{u}_2 -\\left( \\frac{\\mathbf{u}_2 \\cdot \\mathbf{v}_1}{\\mathbf{v}_1 \\cdot \\mathbf{v}_1}\\right) \\mathbf{v}_1 \\\\\n     &= \\left \\langle 2, 1,-1,1 \\right \\rangle\n        -\\frac{4}{3}\\left \\langle 1,1,-1,0 \\right \\rangle = \\frac{1}{3} \\left \\langle 2,-1,1,3 \\right \\rangle \\\\\n        &\\Rightarrow \\; \\left \\langle 2,-1,1,3 \\right \\rangle\\\\\n    \\mathbf{v}_3 &= \\mathbf{u}_1 -\\left( \\frac{\\mathbf{u}_1 \\cdot \\mathbf{v}_1}{\\mathbf{v}_1 \\cdot \\mathbf{v}_1}\\right) \\mathbf{v}_1\n   -\\left( \\frac{\\mathbf{u}_1 \\cdot \\mathbf{v}_2}{\\mathbf{v}_2 \\cdot \\mathbf{v}_2}\\right) \\mathbf{v}_2 \\\\\n   &= \\left \\langle 4, 0, 2, -1 \\right \\rangle -\\frac{2}{3} \\left \\langle 1, 1, -1, 0 \\right \\rangle\n    -\\frac{7}{15} \\left \\langle 2, -1, 1, 3 \\right \\rangle \\\\\n    &= \\frac{1}{5}  \\left \\langle 12, -1, 11, -12 \\right \\rangle \\;\\Rightarrow\\; \\left\\langle 12, -1, 11, -12 \\right \\rangle \\\\\n    &\\Downarrow \\\\\n    \\mathbf{w}_1 &= \\frac{1}{\\sqrt{3}}\\left \\langle 1,1,-1,0 \\right \\rangle \\\\\n    \\mathbf{w}_2 &= \\frac{1}{\\sqrt{15}}\\left \\langle 2,-1,1,3 \\right \\rangle \\\\\n    \\mathbf{w}_3 &= \\frac{1}{\\sqrt{410}} \\left\\langle 12, -1, 11, -12 \\right \\rangle\n  \\end{aligned}\\]\n\\(~\\)\n4. \\(~\\) The set of vectors \\(\\{ \\mathbf{u}_1, \\mathbf{u}_2, \\mathbf{u}_3 \\}\\), where\n\\[\\mathbf{u}_1=\\left \\langle 1, 1, 3 \\right \\rangle, \\; \\mathbf{u}_2=\\left \\langle 1,4,1 \\right \\rangle, \\;\\text{and}\\; \\mathbf{u}_3=\\left \\langle 1,10,-3 \\right \\rangle\\]\nis linearly dependent in \\(\\mathbb{R}^3\\). Carry out the Gram-Schmidt orthogonalization process and then discuss that result\nSolution\n\\[\\begin{aligned}\n     \\mathbf{v}_1 &= \\mathbf{u}_1=\\left \\langle 1,1,3\\right \\rangle \\\\\n     \\mathbf{v}_2 &= \\mathbf{u}_2 -\\left( \\frac{\\mathbf{u}_2 \\cdot \\mathbf{v}_1}{\\mathbf{v}_1 \\cdot \\mathbf{v}_1}\\right) \\mathbf{v}_1 = \\left \\langle 1,4,1 \\right \\rangle\n        -\\frac{1+4+3}{1+1+9}\\left \\langle 1,1,3 \\right \\rangle \\\\\n        &= \\frac{1}{11} \\left \\langle 3,36,-13 \\right \\rangle\n        \\;\\;\\Rightarrow \\;\\; \\left \\langle 3,36,-13 \\right \\rangle\\\\\n    \\mathbf{v}_3 &= \\mathbf{u}_1 -\\left( \\frac{\\mathbf{u}_1 \\cdot \\mathbf{v}_1}{\\mathbf{v}_1 \\cdot \\mathbf{v}_1}\\right) \\mathbf{v}_1\n   -\\left( \\frac{\\mathbf{u}_1 \\cdot \\mathbf{v}_2}{\\mathbf{v}_2 \\cdot \\mathbf{v}_2}\\right) \\mathbf{v}_2 \\\\\n   &= \\left \\langle 1,10,-3 \\right \\rangle -\\frac{1+10-9}{1+1+9} \\left \\langle 1, 1, 3 \\right \\rangle\n    -\\frac{93+360+39}{9+36^2+13^2} \\left \\langle 3,36,-13 \\right \\rangle \\\\\n    &= \\left\\langle 0, 0, 0 \\right \\rangle \\\\\n    &\\Downarrow \\\\\n    \\mathbf{w}_1 &= \\frac{1}{\\sqrt{11}}\\left \\langle 1,1,3 \\right \\rangle \\\\\n    \\mathbf{w}_2 &= \\frac{1}{\\sqrt{1474}}\\left \\langle 3,36,-13 \\right \\rangle\n   \\end{aligned}\\]",
    "crumbs": [
      "**Second Semester**",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Vectors</span>"
    ]
  },
  {
    "objectID": "ch_x2_Hyperbolic_PDEs.html",
    "href": "ch_x2_Hyperbolic_PDEs.html",
    "title": "13  Hyperbolic Partial Differential Equations",
    "section": "",
    "text": "13.1 The One-dimensional Wave Equation\nWe will now begin to study the second major class of PDEs, \\(\\,\\)hyperbolic equations",
    "crumbs": [
      "**Second Semester**",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Hyperbolic Partial Differential Equations</span>"
    ]
  },
  {
    "objectID": "ch_x2_Hyperbolic_PDEs.html#sec-x2-18",
    "href": "ch_x2_Hyperbolic_PDEs.html#sec-x2-18",
    "title": "13  Hyperbolic Partial Differential Equations",
    "section": "",
    "text": "Vibrating-String Problem\nWe consider the small vibrations of a string that is fastened at each end\n\n\n\n\n\nTo mathematically describe the vibrations of this string, \\(\\,\\)we consider all the forces acting on a small section \\(\\Delta x\\) of the string\n\n\n\n\n\nIf the horizontal component of tension is constant \\(T\\), \\(\\,\\)then the tension acting on each side of the string segment is given by\n\\[\n\\begin{aligned}\n  T_1 \\cos\\alpha &\\approx T \\\\\n  T_2 \\cos\\beta &\\approx T\n\\end{aligned}\\]\nIn the vertical component of Newton’s second law, \\(\\,\\)the mass of this piece \\(\\,\\rho\\Delta x\\,\\) times its acceleration \\(\\,u_{tt}\\) will be equal to the net force on the piece:\n\\[\\begin{aligned}\n\\rho\\Delta x u_{tt} &= T_2 \\sin\\beta +T_1 \\sin\\alpha \\\\\n&\\Downarrow \\\\\n\\frac{\\rho\\Delta x}{T} u_{tt} &= \\frac{T_2 \\sin\\beta}{T_2 \\cos\\beta}\n   +\\frac{T_1 \\sin\\alpha}{T_1 \\cos\\alpha} = \\tan\\beta +\\tan\\alpha \\\\\n&= u_x(x+\\Delta x) -u_x(x) \\\\\n&\\Downarrow \\\\\nu_{tt} &=\\frac{T}{\\rho} \\frac{u_x(x+\\Delta x) -u_x(x)}{\\Delta x} \\\\\n&\\Downarrow\\; c^2 = T/\\rho,\\;\\Delta x \\to 0 \\\\\nu_{tt} &= c^2 u_{xx}\n\\end{aligned}\\]\nThis is the wave equation for \\(\\,u(x,t)\\,\\) and \\(\\,c\\) is the speed of propagation of the wave in the string",
    "crumbs": [
      "**Second Semester**",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Hyperbolic Partial Differential Equations</span>"
    ]
  },
  {
    "objectID": "ch_x2_Hyperbolic_PDEs.html#sec-x2-19",
    "href": "ch_x2_Hyperbolic_PDEs.html#sec-x2-19",
    "title": "13  Hyperbolic Partial Differential Equations",
    "section": "13.2 \\(\\,\\)D’Alembert Solution of the Wave Equation",
    "text": "13.2 \\(\\,\\)D’Alembert Solution of the Wave Equation\n\nIf the student recalls the parabolic case, \\(\\,\\)we started solving diffusion problems when the space variable was bounded (by separation of variables), \\(\\,\\)and then went on to solve the unbounded case (where \\(-\\infty &lt; x &lt;\\infty\\)) by the Fourier transform\nIn the hyperbolic case (wave equation), \\(\\,\\)we will do the opposite. \\(\\,\\)We start by solving the one-dimensional wave equation in free space:\n\n\\[\n\\begin{aligned}\n  u_{tt} &=c^2 u_{xx} && -\\infty &lt; x &lt; \\infty,\\; 0 &lt; t &lt; \\infty \\\\\n  \\begin{array}{r}\n    u(x, 0) \\\\\n    u_t(x, 0)\n  \\end{array}\n   &\n  \\begin{array}{c}\n    = f(x) \\\\\n    = g(x)\n  \\end{array} && -\\infty &lt; x &lt;\\infty\n\\end{aligned} \\tag{DA}\\label{eq:DA}\\]\n\nWe could solve this problem by using the Fourier transform (transforming \\(x\\)) or the Laplace transform (transforming \\(t\\)), \\(\\,\\)but we will introduce yet a new technique (canonical coordinate), \\(\\,\\)which will introduce the reader to several new and exciting ideas\n\nSTEP 1 \\(\\,\\)Replacing \\(\\,(x,t)\\) by new canonical coordinates \\((\\xi,\\eta)\\)\n\\[\\begin{aligned}\nu_{tt}&=c^2 u_{xx} \\\\\n&\\Downarrow\\;\\color{red}{\\xi=x+ct,\\;\\eta=x-ct} \\\\\nu_t&= u_\\xi \\xi_t+u_\\eta \\eta_t =c(u_\\xi -u_\\eta) \\\\\nu_{tt}&= c(u_{\\xi\\xi} -u_{\\eta\\xi})\\xi_t +c(u_{\\xi\\eta} -u_{\\eta\\eta})\\eta_t\\\\\n&=c^2(u_{\\xi\\xi}-2u_{\\xi\\eta}+u_{\\eta\\eta})\\\\\nu_x&=u_\\xi \\xi_x+u_\\eta \\eta_x=u_\\xi+u_\\eta \\\\\nu_{xx}&= u_{\\xi\\xi}\\xi_x+u_{\\eta\\xi}\\xi_x+u_{\\xi\\eta}\\eta_x +u_{\\eta\\eta}\\eta_x\\\\\n&=u_{\\xi\\xi}+2u_{\\xi\\eta}+u_{\\eta\\eta}\\\\\n&\\Downarrow \\\\\nu_{\\xi\\eta}&=0\n\\end{aligned}\\]\nSTEP 2 \\(\\,\\)Solving the Transformed Equations\n\\[\\begin{aligned}\nu_{\\xi\\eta}&= 0 \\\\\n&\\Downarrow \\\\\n\\text{Integration } &\\text{with respect to }\\xi \\\\\n&\\Downarrow \\\\\nu_{\\eta}(\\xi,\\eta)&=\\varphi(\\eta) \\\\\n&\\Downarrow \\\\\n\\text{Integration } &\\text{with respect to }\\eta \\\\\n&\\Downarrow \\;\\;\\phi=\\int\\varphi\\,d\\eta \\\\\nu(\\xi,\\eta)=\\phi&(\\eta) +\\psi(\\xi) \\\\\n\\end{aligned}\\]\nSTEP 3 \\(\\,\\)Transforming back to the Original Coordinates \\(\\,x\\,\\) and \\(\\,t\\)\n\\[\\begin{aligned}\nu(\\xi,\\eta)&=\\phi(\\eta) +\\psi(\\xi) \\\\\n&\\Downarrow\\; \\xi=x+ct, \\;\\eta=x-ct \\\\\nu(x,t)=\\phi&(x-ct) +\\psi(x+ct)\n\\end{aligned}\\]\nNOTE \\(\\,\\)This is the general solution of the wave equation, \\(\\,\\)and it is interesting in that it physically represents the sum of any two moving waves, \\(\\,\\)each moving in opposite directions with velocity \\(\\,c\\)\nSTEP 4 \\(\\,\\)Substituting the General Solution into the ICs\n\\[\\begin{aligned}\nu(x,t)=\\phi(x-&ct) +\\psi(x+ct) \\\\\n&\\Downarrow \\;\\scriptsize u(x,0)=f(x), \\;u_t(x,0)=g(x) \\\\\n\\scriptsize\\phi(x) +\\psi(x) \\; & \\scriptsize = f(x)\\\\\n\\scriptsize-c\\phi_x(x) +c\\psi_x(x) \\; &\\scriptsize = g(x) \\\\\n\\big\\Downarrow \\;&{\\scriptstyle \\text{integrating the 2}^{nd} \\text{equation} \\text{  from } x_0 \\text{ to } x} \\\\\n\\scriptsize\\phi(x) +\\psi(x) = &\\scriptsize \\, f(x)\\\\\n\\scriptsize-c\\phi(x) +c\\psi(x) = &\\scriptsize \\, \\int_{x_0}^x g(\\alpha)\\,d\\alpha +C \\\\\n&\\Downarrow \\\\\n\\scriptsize\\phi(x)=\\frac{1}{2}f(x) -\\,& \\scriptsize\\frac{1}{2c}\\int_{x_0}^x g(\\alpha)\\,d\\alpha -\\frac{C}{2c} \\\\\n\\scriptsize\\psi(x)=\\frac{1}{2}f(x) +\\,& \\scriptsize\\frac{1}{2c}\\int_{x_0}^x g(\\alpha)\\,d\\alpha +\\frac{C}{2c} \\\\\n   &\\Downarrow \\\\\n\\end{aligned}\\]\n\\[\\color{red}{\\begin{aligned}\nu(x,t) =\\frac{1}{2} \\left[ f(x -ct)\\right. &+ \\left. f(x +ct) \\right]\n   +\\frac{1}{2c} \\int_{x-ct}^{x+ct} g(\\alpha)\\,d\\alpha\n\\end{aligned}}\\tag{DAS}\\label{eq:DAS}\\]\nThis is what we were aiming for, \\(\\,\\)and it is called the D’Alembert solution to \\(\\eqref{eq:DA}\\)\n\nMotion of a Simple Square Wave\n\\[\n\\begin{aligned}\n  u(x,0)&=\n  \\begin{cases}\n    1 & \\;-1 &lt; x &lt; 1 \\\\\n    0 & \\text{everywhere else}  \n  \\end{cases} \\\\\n  u_t(x,0)&=0\n\\end{aligned}\\]\n\n\n\n\n\nInitial Velocity Given\nSuppose now the initial position of the string is at equilibrium and we impose an initial velocity (as in a piano string) of \\(\\,\\sin x\\)\n\\[\\begin{aligned}\nu(x,0)&= 0\\\\\nu_t(x,0)&=\\sin x\n\\end{aligned}\\]\nHere, \\(\\,\\)the solution would be\n\\[\\begin{aligned}\nu(x,t) &= \\frac{1}{2c} \\int_{x-ct}^{x+ct} \\sin\\xi \\, d\\xi\\\\\n  &=\\frac{1}{2c} \\left[ \\cos(x -ct) -\\cos(x +ct)\\right] \\\\\n  &=\\frac{1}{c} \\sin x \\cdot \\sin ct\n\\end{aligned}\\]\n\n\\(~\\)\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom matplotlib import animation, rc\nfrom IPython.display import HTML\n\nplt.rcParams['font.size'] = 12\nplt.rcParams['xtick.direction'] = 'in'\nplt.rcParams['ytick.direction'] = 'in'\n\n\nfig = plt.figure(figsize=(6, 4))\nax = plt.axes(xlim=(-6.0*np.pi, 6.0*np.pi), ylim=(-1.5, 1.5))\n\nax.set_xticks([-6.0*np.pi,-3.0*np.pi, 0, 3.0*np.pi, 6.0*np.pi])\nax.set_xticklabels([r'$-6\\pi$',r'$-3\\pi$','$0$',r'$3\\pi$',r'$6\\pi$'])\nax.set_yticks([-1.2, -0.6, 0, 0.6, 1.2])\nax.set_xlabel('$x$')\nax.set_ylabel('$u(x,t)$')\n\nplt.close()\n\n\ntime_text = ax.text(-1.5, 1.3, '')\nline, = ax.plot([], [], lw=2)\ndef init():\n    time_text.set_text('t = 0.0')\n    line.set_data([], [])\n    return (line,)\n\nc = 1\ndef animate(t):\n    time_text.set_text(f't = {t:3.1f}')\n    x = np.linspace(-6.0*np.pi, 6.0*np.pi, 300)\n    u = 1.0/(2.0*c)*(np.cos(x -c*t) -np.cos(x +c*t))\n    line.set_data(x, u)\n    return (line,)\n\ntt = list(np.linspace(0, 2.0*np.pi/c, 100))\nanim = animation.FuncAnimation(fig, animate, \n        init_func=init, frames=tt, interval=200, blit=True) \nHTML('&lt;center&gt;' + anim.to_html5_video() + '&lt;/center&gt;')\n\n\n  \n  Your browser does not support the video tag.\n\n\nInitial Velocity Given: \\(\\;u_t(x,0)=\\sin t\\)\n\n\n\n\\(~\\)",
    "crumbs": [
      "**Second Semester**",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Hyperbolic Partial Differential Equations</span>"
    ]
  },
  {
    "objectID": "ch_x2_Hyperbolic_PDEs.html#sec-x2-20",
    "href": "ch_x2_Hyperbolic_PDEs.html#sec-x2-20",
    "title": "13  Hyperbolic Partial Differential Equations",
    "section": "13.3 \\(\\,\\)More on the D’Alembert Solution",
    "text": "13.3 \\(\\,\\)More on the D’Alembert Solution\n\nWe proved that in the last section the solution of the pure initial-value problem\n\\[\n\\begin{aligned}\n  u_{tt} &=c^2 u_{xx} && -\\infty &lt; x &lt; \\infty,\\; 0 &lt; t &lt; \\infty \\\\\n  \\begin{array}{r}\n    u(x, 0) \\\\\n    u_t(x, 0)\n  \\end{array}\n  &\n  \\begin{array}{c}\n    = f(x) \\\\\n    = g(x)\n  \\end{array} && -\\infty &lt; x &lt;\\infty\n\\end{aligned}\\]\nis given by\n\\[ u(x,t) =\\frac{1}{2} \\left[\\, f(x -ct) +f(x +ct) \\right] +\\frac{1}{2c} \\int_{x-ct}^{x+ct} g(\\xi)\\,d\\xi \\]\nWe now present an interpretation of this solution in the \\(\\,xt\\)-plane at the two specific cases\n\nCASE 1 \\(\\,\\) Initial Position Given; \\(\\,\\) Initial Velocity Zero\n\nLet’s consider the following initial condition\n\\[\\begin{aligned}\n  u(x,0) &=f(x) \\\\\n  u_t(x,0) &=0\n\\end{aligned},\\quad -\\infty &lt; x &lt; \\infty\\]\nthe D’Alembert solution is\n\\[ u(x,t) =\\frac{1}{2} \\left[ f(x -ct) +f(x +ct) \\right] \\]\nand the solution \\(u(x_0,t_0)\\) can be interpreted as being the average of the initial displacement \\(\\,f(x)\\) at the points \\((x_0-ct_0,0)\\) and \\((x_0 +ct_0,0)\\) found by backtracking along the lines\n\\[\\begin{aligned}\nx -ct&= x_0 -ct_0\\\\\nx +ct&= x_0 +ct_0\n\\end{aligned}\\quad\\color{red}{\\text{characteristic curves}}\\]\n\n\n\n\n\nFor example, \\(\\,\\)using this interpretation, \\(\\,\\)the initial condition\n\\[\\begin{aligned}\nu(x,0) &=\n\\begin{cases}\n   1 & \\;-1 &lt; x &lt; 1 \\\\\n   0 & \\text{everywhere else}\n\\end{cases} \\\\\nu_t(x,0) &=0\n\\end{aligned}\\]\nwould give us the solution in the \\(\\,xt\\)-plane shown in figure\n\n\n\n\n\n\nCASE 2 \\(\\,\\)Initial Displacement Zero; \\(\\,\\) Velocity Arbitrary\n\nConsider now the IC\n\\[\\begin{aligned}\nu(x,0) &=0 \\\\\nu_t(x,0) &=g(x)\n\\end{aligned}, \\quad -\\infty &lt; x &lt; \\infty\\]\nthe solution is\n\\[ u(x,t) =\\frac{1}{2c} \\int_{x -ct}^{x +ct} g(\\xi)\\,d\\xi \\]\nand, \\(\\,\\)hence, \\(\\,\\)the solution \\(\\,u(x_0, t_0)\\,\\) can be interpreted as integrating the initial velocity between \\(x_0 -ct_0\\) and \\(x_0 +ct_0\\) on the initial line \\(t=0\\)\nAgain, \\(\\,\\)using this interpretation, \\(\\,\\)the solution to the initial-value problem\n\\[\\begin{aligned}\n   u(x,0) &= 0 \\quad -\\infty&lt;x&lt;\\infty \\\\\n   u_t(x,0) &= \\begin{cases} 1 & \\;-1 &lt; x &lt; 1 \\\\ 0 & \\text{everywhere else}\n  \\end{cases}\n\\end{aligned}\\]\nhas a solution in the \\(\\,tx\\)-plane in figure\n\n\n\n\n\nTo find the displacement, \\(\\,\\)we compute the D’Alembert solution\n\n\\[\\scriptsize \\begin{aligned}\nu(x,t) &=\\frac{1}{2c}\\int_{x-ct}^{x+ct} g(\\xi)\\,d\\xi & \\\\\n&= \\frac{1}{2c} \\int_{x-ct}^{x+ct} 0\\, d\\xi=0, & (x,t) \\in \\text{Region 1}\\\\\n&= \\frac{1}{2c} \\int_{-1}^{x+ct} 1\\, d\\xi=\\frac{1+x+ct}{2c}, & (x,t) \\in \\text{Region 2}\\\\\n&= \\frac{1}{2c} \\int_{-1}^{1} 1\\, d\\xi=\\frac{1}{c}, & (x,t) \\in \\text{Region 3}\\\\\n&= \\frac{1}{2c} \\int_{x-ct}^{1} 1\\, d\\xi=\\frac{1-x+ct}{2c}, & (x,t) \\in \\text{Region 4}\\\\\n&= \\frac{1}{2c} \\int_{x-ct}^{x+ct} 0\\, d\\xi=0, & (x,t) \\in \\text{Region 5}\\\\\n&= \\frac{1}{2c} \\int_{x-ct}^{x+ct} 1\\, d\\xi=t, & (x,t) \\in \\text{Region 6}\n\\end{aligned}\\]\n\\(~\\)\n\nfig = plt.figure(figsize=(6, 4))\nax = plt.axes(xlim=(-15, 15), ylim=(-0.1, 1.5))\n\nax.set_xticks([-15, -10, -5, 0, 5, 10, 15])\nax.set_yticks([0, 0.5, 1.0, 1.5])\nax.set_xlabel('$x$')\nax.set_ylabel('$u(x,t)$')\n\nplt.close()\n\ntime_text = ax.text(-2, 1.3, '')\nline, = ax.plot([], [], lw=2)\ndef init():\n    time_text.set_text('t = 0.0')\n    line.set_data([], [])\n    return (line,)\n\n\nc = 1\ndef animate(t):\n    time_text.set_text(f't = {t:3.1f}')    \n    xx = np.linspace(-15, 15, 300)   \n    uu = np.zeros_like(xx)\n    \n    for i, x in enumerate(xx):\n        ch1 = x +c*t\n        ch2 = x -c*t\n        if ch1 &lt;-1.0 or ch2 &gt; 1.0:\n            uu[i] = 0.0\n        elif t &lt; 1.0 /c:\n            if ch2 &lt;-1.0:\n                uu[i] = (1.0 +ch1) /(2.0*c)\n            elif ch1 &lt; 1.0:\n                uu[i] = t\n            else:\n                uu[i] = (1.0 -ch2) /(2.0*c)           \n        else:\n            if ch1 &lt; 1.0:\n                uu[i] = (1.0 +ch1) /(2.0*c)\n            elif ch2 &lt;-1.0:\n                uu[i]=1.0 /c\n            else:\n                uu[i] = (1.0 -ch2) /(2.0*c) \n                       \n    line.set_data(xx, uu)\n    return (line,)\n\n\ntt = list(np.linspace(0, 20, 50))\nanim = animation.FuncAnimation(fig, animate, init_func=init, \n    frames=tt, interval=300, blit=True)\n\nHTML('&lt;center&gt;' + anim.to_html5_video() + '&lt;/center&gt;')\n\n\n  \n  Your browser does not support the video tag.\n\n\nInitial Velocity Given: \\(\\;u_t(x,0)=\\begin{cases} 1 & \\;-1 &lt; x &lt; 1 \\\\ 0 & \\text{everywhere else} \\end{cases}\\)\n\n\n\n\\(~\\)\n\nSolution of the Semi-Infinite String via the D’Alembert Formula\n\nIn the remainder of the section, \\(\\,\\)we will solve the initial-boundary-value problem for the semi-infinite string\n\\[\n\\begin{aligned}\n  u_{tt}\n   &=c^2 u_{xx}\n   && \\color{red}{0 &lt; x &lt; \\infty}, \\; 0 &lt; t &lt; \\infty \\\\\n  u(0, t) &= 0 && 0 &lt; t &lt; \\infty \\\\\n\\begin{array}{r}\n    u(x, 0) \\\\\n    u_t(x, 0)\n  \\end{array}\n  &\n  \\begin{array}{c}\n    = f(x) \\\\\n    = g(x)\n  \\end{array} && 0 &lt; x &lt;\\infty\n\\end{aligned}\\]\nWe proceed in a manner similar to that used with the infinite string, \\(\\,\\)which is to find the general solution to the PDE\n\\[ u(x,t) = \\phi(x -ct) +\\psi(x +ct) \\tag{GS}\\label{eq:GS}\\]\nIf we now substitute this general solution into the initial conditions, \\(\\,\\)we arrive at\n\\[{\\begin{aligned}\n   \\phi(x - ct)=\\frac{1}{2}f(x -ct) \\,-\\,&\\frac{1}{2c}\\int_{x_0}^{x -ct}\n      g(\\xi)\\,d\\xi -\\frac{C}{2c} \\\\\n   \\psi(x +ct)=\\frac{1}{2}f(x +ct) \\,+\\,&\\frac{1}{2c}\\int_{x_0}^{x +ct}\n      g(\\xi)\\,d\\xi +\\frac{C}{2c} \\\\\n  \\end{aligned}} \\tag{IM}\\label{eq:IM}\\]\nWe now have a problem we didn’t encounter when dealing with the infinite string. \\(\\,\\)Since we are looking for the solution \\(\\,u(x,t)\\,\\) everywhere in the first quadrant \\((x&gt;0,\\;t&gt;0)\\,\\) of the \\(\\,tx\\) plane, \\(\\,\\)it is obvious that we must find\n\\[\\begin{aligned}\n  \\phi(x -ct)\n    &\\;\\;\\; \\color{red}{\\text{for all }\n    -\\infty &lt; x -ct &lt; \\infty} \\\\\n  \\psi(x +ct)&\\;\\;\\; \\text{for all }\n   \\phantom{xxx} 0 &lt; x +ct &lt; \\infty\n\\end{aligned}\\]\nUnfortunately, \\(\\,\\)the first equation only gives us \\(\\,\\phi(x -ct)\\,\\) for \\(\\,x -ct \\geq 0\\), \\(\\,\\)since our initial data \\(\\,f(x)\\) and \\(g(x)\\) are only known for positive arguments\nAs long as \\(x -ct \\geq 0\\), \\(\\,\\)we have no problem, \\(\\,\\)since we can substitute \\(\\eqref{eq:IM}\\) into the general solution \\(\\eqref{eq:GS}\\) to get\n\\[ {u(x,t) =\\frac{1}{2} \\left[ f(x -ct) +f(x +ct) \\right] +\\frac{1}{2c} \\int_{x-ct}^{x+ct} g(\\xi)\\,d\\xi,\\;\\;\\; x \\geq ct} \\]\nThe question is, \\(\\,\\)what to do when \\(x &lt; ct\\)?\nWhen \\(x &lt; ct\\), \\(\\,\\)substituting the general solution \\(\\eqref{eq:GS}\\) into the BC \\(\\,u(0,t)=0\\,\\) gives\n\\[\\color{blue}{\\phi(-ct)=-\\psi(ct)}\\]\nand, \\(\\,\\)hence, \\(\\,\\)by functional substitution\n\\[ {\\phi(x -ct)={\\color{red}{-}}\\frac{1}{2}f({\\color{red}{ct -x}}) {\\color{red}{-}}\\frac{1}{2c}\\int_{x_0}^{{\\color{red}{ct -x}}} g(\\xi)\\,d\\xi {\\color{red}{-}}\\frac{C}{2c}} \\]\nSubstituting this value of \\(\\phi\\) into the general solution \\(\\eqref{eq:GS}\\) gives\n\\[ {u(x,t) =\\frac{1}{2} \\left[ f(x +ct) -f(ct -x) \\right] +\\frac{1}{2c} \\int_{ct -x}^{x+ct} g(\\xi)\\,d\\xi,\\;\\;\\;0&lt;x&lt;ct} \\]\nFor \\(x \\geq ct\\), \\(\\,\\)the solution is the same as the D’Alembert solution for the infinite wave, while for \\(x &lt; ct\\), \\(\\,\\)the solution \\(u(x,t)\\) is modified as a result of the wave reflecting from the boundary (The sign of the wave is changed when it’s reflected)\nThe straight lines\n\\[\\begin{aligned}\n  x +ct &= \\text{constant}\\\\\n  x -ct &= \\text{constant}\n\\end{aligned}\\]\nare known as characteristics, \\(\\,\\)and it is along these lines that disturbances are propagated. \\(\\,\\)Characteristics are generally associated with hyperbolic equations\n\n\n\n\n\n\n\n\\(~\\)\nExample \\(\\,\\)Solve the semi-infinite string problem\n\\(~\\)\n\\[\n  \\begin{aligned}\n    u_{tt} &=c^2 u_{xx} && 0 &lt; x &lt; \\infty,\\; 0 &lt; t &lt; \\infty \\\\\n    u(0,t) &=0 && 0 &lt; t &lt; \\infty \\\\\n    \\begin{array}{r}\n      u(x, 0) \\\\\n      u_t(x, 0)\n    \\end{array}\n    &\\,\n    \\begin{array}{l}\n      = f(x) \\\\\n      = 0\n    \\end{array} && 0 &lt; x &lt;\\infty\n  \\end{aligned}\\]\n\n\n\n\n\n\\(~\\)\n\nfig = plt.figure(figsize=(6, 4))\nax = plt.axes(xlim=(0, 5), ylim=(-1.0, 1.5))\n\nax.set_xticks([0, 2.5, 5])\nax.set_yticks([-1.0, -0.5, 0.0, 0.5, 1.0, 1.5])\nax.set_xlabel('$x$')\nax.set_ylabel('$u(x,t)$')\n\nplt.close()\n\ntime_text = ax.text(2.2, 1.3, '')\nline, = ax.plot([], [], lw=2)\ndef init():\n    time_text.set_text('t = 0.0')\n    line.set_data([], [])\n    return (line,)\n\n\nc = 1\ndef f_i(x):\n    if x &gt;= 1 and x &lt;= 2:\n        u = 1.0\n    else:\n        u = 0.0\n    return u\n    \ndef animate(t):\n    time_text.set_text(f't = {t:3.1f}')        \n    xx = np.linspace(0, 5, 400)   \n    uu = np.zeros_like(xx)\n    \n    for i, x in enumerate(xx):\n        ch1 = x +c*t\n        ch2 = x -c*t\n        if ch2 &gt;= 0:\n            uu[i] = 0.5*(f_i(ch1) +f_i(ch2))\n        else:\n            uu[i] = 0.5*(f_i(ch1) -f_i(-ch2))\n                       \n    line.set_data(xx, uu)\n    return (line,)\n\n\ntt = list(np.linspace(0, 7, 100))\nanim = animation.FuncAnimation(fig, animate, \n        init_func=init, frames=tt, interval=750, blit=True)\nHTML('&lt;center&gt;' + anim.to_html5_video() + '&lt;/center&gt;')\n\n\n  \n  Your browser does not support the video tag.\n\n\nSemi-infinite String\n\n\n\n\\(~\\)\nExample \\(\\,\\)Solve the semi-infinite string problem\n\\[\n  \\begin{aligned}\n    u_{tt} &=c^2 u_{xx} && 0 &lt; x &lt; \\infty,\\; 0 &lt; t &lt; \\infty \\\\\n    u_x(0,t) & =0 && 0 &lt; t &lt; \\infty \\\\\n    \\begin{array}{r}\n      u(x, 0) \\\\\n      u_t(x, 0)\n    \\end{array}\n    &\\;\n    \\begin{array}{l}\n      = f(x) \\\\\n      = 0\n    \\end{array} && 0 &lt; x &lt;\\infty\n  \\end{aligned}\\]\nin a manner analogous to the way the semi-infinite string problem was solved in the section\nSolution \\(\\,\\)For \\(x \\geq ct\\), \\(\\,\\)the solution is the same as the D’Alembert solution for the infinite wave,  \\(\\,\\)while for \\(x &lt; ct\\), \\(\\,\\)the solution \\(u(x,t)\\) is modified as a result of the wave reflecting from the boundary\n\\[ {u(x,t) =\\frac{1}{2} \\left[ f(x +ct) +f(ct -x) \\right] +\\frac{1}{2c} \\left[ \\int_0^{ct -x} g(\\xi)\\,d\\xi + \\int_0^{x +ct} g(\\xi)\\,d\\xi \\right] ,\\;\\; 0 &lt; x &lt; ct} \\]\n\\(~\\)\n\nThe Nonhomogeneous Wave Equation\n\nWe consider now the pure nonhomogeneous wave equation:\n\\[\n\\begin{aligned}\nu_{tt} &\\! =c^2 u_{xx} +\\color{red}{F(x, t)}\n  && -\\infty &lt; x &lt; \\infty,\\; 0 &lt; t &lt; \\infty \\\\\n\\begin{array}{r}\n  u(x, 0) \\\\\n  u_t(x, 0)\n\\end{array}\n&\n\\begin{array}{c}\n  = 0 \\\\\n  = 0\n\\end{array} && -\\infty &lt; x &lt;\\infty\n\\end{aligned}\\]\nwhich includes the forcing term \\(F(x, t)\\), \\(~\\)but zero initial conditions\nWe again make the change of variables \\(\\,\\xi=x +ct\\,\\) and \\(\\,\\eta=x -ct\\). \\(\\,\\)The differential equation then becomes\n\\[\n\\begin{aligned}\n  & u_{\\xi\\eta} = \\color{red}{-\\frac{1}{4c^2} F(\\xi, \\eta)} \\\\ \\\\\n  &\\color{blue}\n  {\\left.\\begin{matrix}\n    u = 0\\\\\n    u_\\xi = u_\\eta\n    \\end{matrix} \\;\\;\\right\\} \\;\\text{ at } \\xi=\\eta \\;\\; \\leftarrow\n   \\left.\\begin{matrix}\n     u = 0 \\\\\n     u_t = 0\n   \\end{matrix} \\;\\;\\right\\} \\text{ at } t=0}\n\\end{aligned}\\]\n\n\\[\n\\begin{aligned}\n\\\\[5pt]\n&\\Downarrow\n   {\\scriptstyle\\text{ integrating with respect to }\n    \\xi \\text{ and } \\eta, \\text{ respectively} }\\\\[5pt]\nu_\\eta \\;&{\\scriptsize= -\\frac{1}{4c^2}\n  \\int_\\eta^\\xi F\\left(\\bar{\\xi}, \\eta \\right)\n   \\,d\\bar{\\xi} +c_1}, \\:\\:\nu_\\xi \\;{\\scriptsize= -\\frac{1}{4c^2}\n   \\int_\\xi^\\eta F\\left(\\xi, \\bar{\\eta} \\right)\n    \\,d\\bar{\\eta}  +c_1} \\\\[5pt]\n&\\Downarrow\n{\\scriptstyle\\text{ integrating with respect to }\n  \\eta \\text{ and } \\xi, \\text{ respectively}}\\\\[5pt]\n  u \\;&{\\scriptsize= -\\frac{1}{4c^2} \\int_\\eta^\\xi\n    \\left[ \\int_{\\bar{\\eta}}^\\xi F\\left( \\bar{\\xi},\n     \\bar{\\eta} \\right) \\,d\\bar{\\xi} \\right ]\n      \\,d\\bar{\\eta} +c_1 \\eta +c_2} \\\\\n      \\;&{\\scriptsize= -\\frac{1}{4c^2} \\int_\\xi^\\eta\n       \\left[ \\int_{\\bar{\\xi}}^\\eta F\\left(\n        \\bar{\\xi}, \\bar{\\eta} \\right)\n        \\,d\\bar{\\eta} \\right ] \\,d\\bar{\\xi}\n        +c_1 \\xi +c_3}\n\\end{aligned}\\]\n\nWe let \\(\\,\\bar{\\xi}=\\beta +c\\tau\\;\\) and \\(\\,\\bar{\\eta}=\\beta -c\\tau\\), \\(\\,\\)(\\(t \\ge \\tau\\), \\(\\,\\beta=x\\)). \\(\\,\\)The domain of integration \\(\\,\\eta \\leq \\bar{\\eta} \\leq \\bar{\\xi} \\leq \\xi\\;\\) becomes\n\\[\n\\begin{aligned}\n  \\eta \\leq \\beta -c\\tau &\\leq \\beta +c\\tau \\leq \\xi \\\\\n  &\\Downarrow \\\\\n  \\eta +c\\tau \\leq &\\;\\beta \\leq\\xi -c \\tau \\\\\n  0 \\leq &\\;\\tau \\leq \\frac{1}{2c} (\\xi -\\eta)\n\\end{aligned}\\]\nThe transformation from \\(\\,(\\bar{\\xi}, \\,\\bar{\\eta})\\,\\) to \\(\\,(\\beta, \\,\\tau)\\,\\) gives the solution formula\n\\[\n\\begin{aligned}\nu &= -\\frac{1}{4c^2} \\int_\\eta^\\xi\n\\left[ \\int_{\\bar{\\eta}}^\\xi F\\left( \\bar{\\xi}, \\bar{\\eta} \\right)\n\\,d\\bar{\\xi} \\right ] \\,d\\bar{\\eta} \\\\[5pt]\n&\\big\\Downarrow {\\scriptstyle \\;\\;d\\bar{\\xi}\\,d\\bar{\\eta} \\,=\\,  \\begin{vmatrix}\n\\frac{\\partial \\bar{\\xi}}{\\partial \\beta}& \\frac{\\partial \\bar{\\xi}}{\\partial \\tau} \\\\\n\\frac{\\partial \\bar{\\eta}}{\\partial \\beta}& \\frac{\\partial \\bar{\\eta}}{\\partial \\tau}\n\\end{vmatrix}  \\,d\\beta\\,d\\tau \\,=\\, -2c \\,d\\beta\\,d\\tau}\\\\[5pt]\n&=\\frac{1}{2c} \\int_0^{(\\xi -\\eta)/2c} \\left[ \\int_{\\eta +c\\tau}^{\\xi -c\\tau} F\\left( \\beta,\\tau \\right) \\,d\\beta \\right ] \\,d\\tau \\\\[5pt]\n&\\Downarrow {\\scriptstyle  \\;\\xi=x +ct, \\;\\eta=x -ct} \\\\\n{\\color{red}{u}}\\; &{\\color{red}{= \\frac{1}{2c} \\int_0^t \\left[ \\int_{x -c(t -\\tau)}^{x +c(t -\\tau)} F\\left( \\beta,\\tau \\right) \\,d\\beta \\right ] \\,d\\tau}}\n\\end{aligned}\n\\]\n\n\n\\(~\\)",
    "crumbs": [
      "**Second Semester**",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Hyperbolic Partial Differential Equations</span>"
    ]
  },
  {
    "objectID": "ch_x2_Hyperbolic_PDEs.html#sex-x2-21",
    "href": "ch_x2_Hyperbolic_PDEs.html#sex-x2-21",
    "title": "13  Hyperbolic Partial Differential Equations",
    "section": "13.4 The Wave Equation in Two and Three Dimensions (Free Space)",
    "text": "13.4 The Wave Equation in Two and Three Dimensions (Free Space)\nThe problem of this section is to generalize the D’Alembert solution to two and three dimensions\n\nWaves in Three Dimensions\nWe start by considering spherical waves in three dimensions that have given ICs; \\(\\,\\)that is, \\(\\,\\)we would like to solve the initial value problem\n\\[\\begin{aligned}\n  u_{tt} &=c^2(u_{xx} +u_{yy} +u_{zz}),\\quad\n  \\begin{cases}\n    -\\infty &lt; x &lt;\\infty \\\\\n    -\\infty &lt; y &lt;\\infty \\\\\n    -\\infty &lt; z &lt;\\infty\n  \\end{cases} \\\\\n  u(&x,y,z,0)=\\phi(x,y,z) \\\\\n  u_t(&x,y,z,0)=\\psi(x,y,z)\n\\end{aligned} \\tag{3D}\\label{eq:3D}\\]\n\nTo solve this problem, \\(\\,\\)we first solve the simpler one (set \\(\\phi=0\\))\n\\[\\color{red}{\\begin{aligned}\n  u_{tt}&=c^2(u_{xx} +u_{yy} +u_{zz}),\\quad (x,y,z) \\in \\mathbb{R}^3\\\\\n  u(&x,y,z,0)=0 \\\\\n  u_t(&x,y,z,0)=\\psi(x,y,z)\n\\end{aligned}} \\tag{3Dv}\\label{eq:3Dv}\\]\nThis problem can be solved by the Fourier transform\n\\[\\begin{aligned}\n  \\mathcal{F}(u)\n    &=\\frac{1}{(2\\pi)^{3/2}}\\iiint_{-\\infty}^\\infty\n      u(x,y,z,t) e^{-i(\\omega_1 x+\\omega_2 y+\\omega_3 z)}\\,dx\\,dy\\,dz\n      =U(\\omega_1,\\omega_2,\\omega_3,t)\\\\\n  \\mathcal{F}(\\psi)&=\\frac{1}{(2\\pi)^{3/2}}\\iiint_{-\\infty}^\\infty\n    \\psi(x,y,z) e^{-i(\\omega_1 x+\\omega_2 y+\\omega_3 z)}\\,dx\\,dy\\,dz\n    =\\Psi(\\omega_1,\\omega_2,\\omega_3)\n\\end{aligned}\\]\n\n\\[\\scriptsize \\begin{aligned}\n&\\Downarrow\\;\\; \\mathcal{F}^{-1} \\\\ \\\\\nu(x,y,z,t)= \\frac{1}{(2\\pi)^{3/2}}\n  \\lim_{L\\to\\infty} \\underset{\\omega_1^2 +\\omega_2^2 +\\omega_3^2 &lt; L^2}{\\iiint}\n  \\Psi(\\omega_1,\\omega_2,\\omega_3) &\n  \\frac{\\sin ct \\sqrt{ \\omega_1^2 +\\omega_2^2 +\\omega_3^2 }}{c\\sqrt{\\omega_1^2\n  +\\omega_2^2 +\\omega_3^2 }} \\,e^{i(\\omega_1 x +\\omega_2 y +\\omega_3 z)}\\,d\\omega_1 \\,d\\omega_2 \\,d\\omega_3 \\\\\n= \\frac{1}{(2\\pi)^{3/2}}\n\\iiint_{-\\infty}^\\infty \\psi(\\xi,\\eta,\\zeta)\\;\n  {\\tiny \\left[ \\frac{1}{(2\\pi)^{3/2}}  \\lim_{L\\to\\infty}\n    \\underset{\\omega_1^2 +\\omega_2^2 +\\omega_3^2 &lt; L^2}{\\iiint} \\right. } &\n  {\\tiny \\left. e^{-i\\left[\\omega_1 (\\xi-x) +\\omega_2 (\\eta-y) +\\omega_3 (\\zeta-z) \\right]} \\;\n  \\frac{\\sin ct \\sqrt{ \\omega_1^2 +\\omega_2^2 +\\omega_3^2 }}{c\\sqrt{\\omega_1^2 +\\omega_2^2 +\\omega_3^2 }}\\,\n  \\,d\\omega_1 \\,d\\omega_2 \\,d\\omega_3 \\,\\right] } \\,d\\xi \\,d\\eta \\,d\\zeta\n\\end{aligned}\\]\n\nWe introduce spherical coordinates \\((\\varrho,\\varphi,\\vartheta)\\) in the \\((\\omega_1,\\omega_2,\\omega_3)\\) space with the north pole \\(\\varphi=0\\) in the direction of the vector \\(\\left \\langle \\xi-x,\\eta-y,\\zeta-z \\right \\rangle\\). \\(\\,\\)Then the integral in \\((\\omega_1,\\omega_2,\\omega_3)\\) becomes\n\\[\\scriptsize\\begin{aligned}\n  \\frac{1}{(2\\pi)^{3/2}}  \\lim_{L\\to\\infty}\n  \\underset{\\omega_1^2 +\\omega_2^2 +\\omega_3^2 &lt; L^2}{\\iiint}\n  &e^{-i\\left[\\omega_1 (\\xi-x) +\\omega_2 (\\eta-y)\n  +\\omega_3 (\\zeta-z)\\right]}\\;\n  \\frac{\\sin ct \\sqrt{ \\omega_1^2 +\\omega_2^2 +\\omega_3^2 }}{c\\sqrt{\\omega_1^2 +\\omega_2^2 +\\omega_3^2 }}\\,\n  \\,d\\omega_1 \\,d\\omega_2 \\,d\\omega_3 \\\\\n  &\\;\\;\\Downarrow \\;\n    {\\tiny r=|\\mathbf{r}|=\\sqrt{(\\xi-x)^2 +(\\eta -y)^2 +(\\zeta -z)^2},\\;\\;\n      \\varrho =|\\boldsymbol{\\varrho}|\n  = \\sqrt{\\omega_1^2 +\\omega_2^2 +\\omega_3^2} }\\\\\n    &=\\frac{1}{(2\\pi)^{3/2}} \\lim_{L\\to\\infty}\n    \\int_0^L \\int_0^{2\\pi}  \\int_0^\\pi\n  \\;e^{-ir \\varrho\\cos \\varphi} \\;\\frac{\\sin ct \\varrho}{c\\varrho}\\;\n  \\varrho^2 \\sin\\varphi \\,d\\varphi\\,d\\vartheta\\,d\\varrho \\\\\n    &\\;\\;\\Downarrow\n    \\; {\\tiny \\alpha=\\cos\\varphi, \\;d\\alpha=-\\sin\\varphi d\\varphi } \\\\\n    &=\\frac{1}{(2\\pi)^{3/2}}\n    \\lim_{L\\to\\infty} \\frac{2\\pi}{c} \\int_0^L \\; \\varrho \\sin ct\\varrho\n  \\underbrace{\\left[ \\frac{e^{-ir\\varrho \\alpha}}{-ir\\varrho}\n    \\right]_{-1}^1 }_{\\frac{2}{r\\varrho}\\sin r\\varrho} \\,d\\varrho \\\\\n    &=\\frac{1}{(2\\pi)^{3/2}} \\lim_{L\\to\\infty} \\frac{4\\pi}{cr} \\int_0^L  \n    \\;\\sin ct \\varrho \\cdot \\sin r\\varrho \\,d\\varrho\n\\end{aligned}\\]\nTo treat the integration with respect to \\(\\xi\\), \\(\\,\\eta\\), and \\(\\,\\zeta\\), \\(~\\)we introduce spherical coordinates \\((r,\\phi,\\theta)\\) in the \\((\\xi,\\eta,\\zeta)\\) space with their origin at \\((x,y,z)\\), \\(\\,\\)so that\n\\[\\begin{aligned}\n\\xi-x &=r\\sin\\phi \\cos\\theta \\\\\n\\eta-y&=r\\sin\\phi \\sin\\theta \\\\\n\\zeta-z&=r\\cos\\phi\n\\end{aligned}\\]\nThen the solution becomes\n\\[\\scriptsize\\begin{aligned}\n{\\normalsize\\color{red}{u(x,y,z,t)}}\n&= {\\tiny \\frac{4\\pi}{(2\\pi)^3 c} \\lim_{L\\to \\infty} \\int_0^\\infty \\int_0^{2\\pi} \\int_0^\\pi  \n  \\;\\psi(x +r\\sin\\phi\\cos\\theta,y +r\\sin\\phi\\sin\\theta, z+r\\cos\\phi)\\; \\frac{1}{r}\\;\\left[ \\int_0^L  \n  \\sin ct \\varrho \\sin r\\varrho\\, d\\varrho \\right] r^2\\sin\\phi \\, d\\phi \\, d\\theta \\,dr }\\\\\n  &= {\\tiny \\frac{1}{2\\pi^2 c} \\lim_{L\\to \\infty} \\int_0^L \\sin ct\\varrho \\left[\\int_0^\\infty \\sin r\\varrho\n  \\underbrace{\\left( \\int_0^{2\\pi} \\int_0^\\pi  \\psi(x +r\\sin\\phi\\cos\\theta,y +r\\sin\\phi\\sin\\theta, z+r\\cos\\phi)\n  \\,r\\,\\sin\\phi \\,d\\phi\\,d\\theta  \\right) }_{g(r)} dr \\right] d\\varrho } \\\\\n  &= {\\tiny \\frac{1}{2\\pi^2 c} \\frac{\\pi}{2} \\int_0^\\infty \\sin ct \\varrho \\;\\underbrace{\\left[\\frac{2}{\\pi} \\int_0^\\infty\n  \\sin r\\varrho \\,g(r)\\,dr \\right]}_{G(\\varrho)} d\\varrho  \n  = \\frac{1}{4\\pi c} \\underbrace{\\int_0^\\infty \\sin ct \\varrho \\;G(\\rho)\\,d\\varrho}_{g(ct)} } \\\\\n  &= {\\tiny \\frac{t}{4\\pi (ct)^2} \\int_0^{2\\pi}\\int_0^\\pi  \\psi(x +ct\\sin\\phi\\cos\\theta,y +ct\\sin\\phi\\sin\\theta, z+ct\\cos\\phi)\n  \\,(ct)^2\\sin\\phi \\,d\\phi \\,d\\theta  }\n  = {\\normalsize\\color{red}{t\\bar{\\psi}}}\n\\end{aligned}\\]\n where \\(\\bar{\\psi}\\) is the average of the initial disturbance \\(\\psi\\) over the surface of the sphere of radius \\(\\,ct\\,\\) centered at \\((x,y,z)\\)\nThe interpretation of this solution is that the initial disturbance \\(\\,\\psi\\,\\) radiates outward spherically (velocity \\(c\\)) at each point, \\(\\,\\)so that after so many seconds, \\(\\,\\)the point \\(\\,(x,y,z)\\) will be influenced by those initial disturbances on a sphere (of radius \\(\\,ct\\)) around that point\n\n\n\n\n\nNow, \\(\\,\\)to finish the problem, \\(\\,\\)what about the other half; \\(\\,\\)that is\n\\[\\color{red}{\\begin{aligned}\nu_{tt}&=c^2(u_{xx} +u_{yy} +u_{zz}),\\quad (x,y,z) \\in \\mathbb{R}^3 \\\\\nu(&x,y,z,0)=\\phi(x,y,z) \\\\\nu_t(&x,y,z,0)=0\n\\end{aligned}} \\tag{3Dp}\\label{eq:3Dp}\\]\nThis is easy: \\(\\,\\)A famous theorem developed by Stokes says all we have to do to solve this problem is to change the ICs to \\(\\,v=0\\), \\(\\,v_t=\\phi\\), \\(\\,\\)and then differentiate this solution with respect to time:\n\\[ v=\\int_0^t u\\,dt,\\;v_t=u, \\;u_{tt}=\\frac{\\partial}{\\partial t} v_{tt}, \\;u_{xx}=\\frac{\\partial}{\\partial t} v_{xx}\\]\nIn other words, \\(\\,\\)we solve\n\\[\\begin{aligned}\nv_{tt}&=c^2(v_{xx} +v_{yy} +v_{zz}),\\quad (x,y,z) \\in \\mathbb{R}^3 \\\\\nv(&x,y,z,0)=0 \\\\\nv_t(&x,y,z,0)=\\phi(x,y,z)\n\\end{aligned}\\]\nto get \\(v=t\\bar{\\phi}\\,\\) and then differentiate with respect to time. \\(\\,\\)This gives us the solution\n\\[\\color{red}{u=\\frac{\\partial}{\\partial t} \\left[ t\\bar{\\phi} \\right]}\\]\nto problem \\(\\eqref{eq:3Dp}\\)\nWe now have the solution to our general three-dimensional problem \\(\\eqref{eq:3D}\\). \\(\\,\\)It’s just\n\\[ \\color{red}{u(x,y,z,t)=\\frac{\\partial}{\\partial t} \\left[ t\\bar{\\phi} \\right]+t\\bar{\\psi}}\\]\nwhere \\(\\,\\bar{\\phi}\\,\\) and \\(\\,\\bar{\\psi}\\,\\) are the averages of the functions \\(\\,\\phi\\,\\) and \\(\\,\\psi\\,\\) over the sphere of radius \\(\\,ct\\,\\) centered at \\(\\,(x,y,z)\\)\n\\(~\\)\n\n\n\n\n\nSuppose now the initial disturbances \\(\\,\\phi\\,\\) and \\(\\,\\psi\\,\\) are zero except for a small sphere. \\(\\,\\)As time increases, \\(\\,\\)the radius of the sphere around \\((x,y,z)\\) increases with velocity \\(\\,c\\)\nand so after \\(\\,t_2\\) seconds, \\(\\,\\)it will finally intersect the initial disturbance region, \\(\\,\\)and, \\(\\,\\)hence, \\(\\,u(x,y,z,t)\\,\\) becomes nonzero\nFor \\(\\,t_2 &lt; t &lt; t_3\\), \\(\\,\\)the solution at \\(\\,(x,y,z)\\,\\) will be nonzero, \\(\\,\\)since the sphere intersects the disturbance region\nbut when \\(\\,t=t_3\\), \\(\\,\\)the solution at \\(\\,(x,y,z)\\,\\) abruptly becomes zero again. \\(\\,\\)In other words, \\(\\,\\)the wave disturbance originating from the initial-disturbance region has a sharp trailing edge. \\(\\,\\)This general principle is known as Huygen’s principle for the three dimensions, \\(\\,\\)and \\(\\,\\)it is the reason why sound waves in three dimensions stimulate our ears but die off instanteously when the wave has passed\n\n\n\\(~\\)\nExample \\(\\,\\)Illustrate by picture and words the spherical wave solution of the three-dimensional problem\n\\[\\begin{aligned}\nu_{tt}&=c^2(u_{xx} +u_{yy} +u_{zz}), \\quad\n\\begin{cases}\n-\\infty &lt; x &lt;\\infty \\\\\n-\\infty &lt; y &lt;\\infty \\\\\n-\\infty &lt; z &lt;\\infty\n\\end{cases} \\\\\nu(&x,y,z,0)=0 \\\\\nu_t(&x,y,z,0)=\n\\begin{cases}\n\\;1 & x^2+y^2+z^2 \\leq 1 \\\\\n\\;0 & \\text{elsewhere}\n\\end{cases}\n\\end{aligned}\\]\n\\(~\\)\n\nTwo-Dimensional Wave Equation\nTo solve the two-dimensional problem\n\\[\\begin{aligned}\n  u_{tt}&=c^2(u_{xx} +u_{yy}),\\quad\n  \\begin{cases}\n  -\\infty &lt; x &lt;\\infty \\\\\n  -\\infty &lt; y &lt;\\infty\n  \\end{cases} \\\\\n  u(&x,y,0)=\\phi(x,y) \\\\\n  u_t(&x,y,0)=\\psi(x,y)\n\\end{aligned} \\tag{2D}\\label{eq:2D}\\]\nwe merely let the initial disturbances \\(\\,\\phi\\,\\) and \\(\\,\\psi\\,\\) in the three-dimensional problem depend on only the two variables \\(\\,x\\,\\) and \\(\\,y\\). \\(\\,\\)Doing this, the three-dimensional formula\n\\[ u = \\frac{\\partial}{\\partial t} [t\\bar{\\phi}] +t\\bar{\\psi} \\]\nfor \\(\\,u\\,\\) will describe cylindrical waves and, \\(\\,\\)hence, \\(\\,\\)give us the solution for the two-dimensional problem\n\nThis technique is called the method of descent. \\(\\,\\)Carrying out the computations, \\(\\,\\)we get\n\\[\\scriptstyle\\begin{aligned}\n\\bar{\\psi}(x,y,t) &= \\frac{1}{4\\pi (ct)^2} \\color{red}{\\int_0^{2\\pi}  \\int_{0}^\\pi}\n  \\psi(x +ct\\sin\\phi\\cos\\theta,y+ct\\sin\\phi\\sin\\theta)\\, \\color{red}{(ct)^2 \\sin\\phi \\, d\\phi \\, d\\theta} \\;\\;\\leftarrow \\text{spherical surface}\\\\\n&\\Downarrow \\;\\; {\\tiny r = ct \\,\\sin \\phi, \\; \\,dr =ct \\, \\cos\\phi \\,d\\phi\\;\\;\\rightarrow\\;\\; \\int_0^{\\pi} ct \\sin\\phi\\,d\\phi = 2\\int_0^{ct}\\frac{r}{\\sqrt{(ct)^2 -r^2}} dr } \\\\\n&= \\frac{1}{2\\pi ct} \\, \\color{red}{\\int_0^{2\\pi} \\int_0^{ct}} \\psi(x +r\\cos\\theta, y+r\\sin\\theta)\\,\\frac{1}{\\sqrt{(ct)^2 -r^2}}\\,\\color{red}{r\\,dr\\,d\\theta} \\;\\;\\leftarrow \\text{circle interior}\\\\\n\\bar{\\phi}(x,y,t) &= \\frac{1}{2\\pi ct} \\, \\color{red}{\\int_0^{2\\pi} \\int_0^{ct}}\n\\phi(x +r\\cos\\theta, y+r\\sin\\theta)\\,\\frac{1}{\\sqrt{(ct)^2 -r^2}}\\,\\color{red}{r\\,dr\\,d\\theta}\n\\end{aligned}\\]\nNote that in this solution, \\(\\,\\)the two integrals of the initial conditions \\(\\,\\phi\\,\\) and \\(\\,\\psi\\,\\) are integrated over the interior of a circle with center \\(\\,(x,y)\\,\\) and radius \\(\\,ct\\). \\(\\,\\)We see that initial disturbances give rise to sharp leading waves, \\(\\,\\)but not to sharp trailing waves. \\(\\,\\)Thus, Huygen’s principle doesn’t hold in two dimensions\n\nOne-Dimensional Wave Equation\nFinally, \\(\\,\\)if we assume the initial conditions \\(\\,\\phi\\,\\) and \\(\\,\\psi\\,\\) depend only on one variable, \\(\\,\\)this gives rise to plane waves and, \\(\\,\\)hence, \\(\\,\\)the preceding equation descends one more dimension\n\\[\\begin{aligned}\n\\bar{\\psi}(x,t) &= {\\tiny \\frac{1}{2\\pi ct} \\, \\int_0^{2\\pi} \\int_0^{ct}\n  \\frac{\\psi(x +r\\cos\\theta)}{\\sqrt{(ct)^2 -r^2}}\\,r\\,dr\\,d\\theta }\\\\\n&\\Downarrow {\\tiny r^2=x^2 + y^2, \\; (ct)^2 -x^2 = \\alpha^2 }\\\\\n&={\\tiny \\frac{1}{2\\pi ct}\\, \\int_{x -ct}^{x +ct} \\underbrace{\\int_{-\\alpha}^{\\alpha}\n  \\frac{1}{\\sqrt{\\alpha^2 -y^2}}\\,dy}_{ \\pi} \\;\\psi(x) \\,dx }\n= \\frac{1}{2ct} \\int_{x-ct}^{x+ct} \\psi(x)\\,dx \\\\\n\\bar{\\phi}(x,t) &= \\frac{1}{2ct} \\int_{x-ct}^{x+ct} \\phi(x)\\,dx\n\\end{aligned}\\]\nto the well-known D’Alembert solution. \\(\\,\\)Note in the D’Alembert solution, \\(\\,\\)the initial position \\(\\,\\phi\\,\\) gives rise to sharp trailing edges, \\(\\,\\)but the initial velocity \\(\\,\\psi\\,\\) does not. \\(\\,\\)In other words, \\(\\,\\)one dimension is a little unusual in that the initial position satisfies Huygen’s principle, \\(\\,\\)but the initial velocity does not\n\n\\(~\\)\nExample \\(\\,\\)What is the two dimensional solution of the analogous cylindrical-wave problem\n\\[\\begin{aligned}\nu_{tt}&=c^2(u_{xx} +u_{yy}),\\quad\n\\begin{cases}\n-\\infty &lt; x &lt;\\infty \\\\\n-\\infty &lt; y &lt;\\infty\n\\end{cases} \\\\\nu(&x,y,0)=0 \\\\\nu_t(&x,y,0)=\n\\begin{cases}\n\\;1 & x^2+y^2 \\leq 1 \\\\\n\\;0 & \\text{elsewhere}\n\\end{cases}\n\\end{aligned}\\]\n\\(~\\)",
    "crumbs": [
      "**Second Semester**",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Hyperbolic Partial Differential Equations</span>"
    ]
  },
  {
    "objectID": "ch_x2_Hyperbolic_PDEs.html#sec-x2-22",
    "href": "ch_x2_Hyperbolic_PDEs.html#sec-x2-22",
    "title": "13  Hyperbolic Partial Differential Equations",
    "section": "13.5 Boundary Conditions Associated with the Wave Equation",
    "text": "13.5 Boundary Conditions Associated with the Wave Equation\nThe purpose of this section is to discuss some of the various types of BCs that are associated with physical problems of wave motions. \\(\\,\\)Here, \\(\\,\\)we will stick to one-dimensional problems where the BCs (linear ones) are generally groupded into one of three kinds\n\nFirst Kind: \\(\\,\\)Controlled End Points\nWe are now involved with problems like\n\\[\n\\begin{aligned}\n  u_{tt} &=c^2 u_{xx} && \\color{red}{0 &lt; x &lt; 1},\\;\\; 0 &lt; t &lt; \\infty \\\\ \\\\\n  \\begin{array} {r}\n    u(0, t) \\\\ u(1,t)\n  \\end{array}\n  &\n  \\begin{array} {r}\n    =g_1(t) \\\\ =g_2(t)\n  \\end{array}\n  && 0 &lt; t &lt; \\infty \\\\ \\\\\n  \\begin{array} {r}\n    u(x, 0) \\\\ u_t(x, 0)\n  \\end{array}\n  &\n  \\begin{array} {r}\n    =f(x) \\\\ =g(x)\n  \\end{array}\n  && 0 \\leq x \\leq L\n\\end{aligned}\\]\nwhere we control the end points so that they move in a given manner\n\n\n\n\n\nA typical problem of this kind would involve suddenly twisting (at \\(t=1\\)) the right end of a fastened rod so many degrees and observing the resulting tortional vibration\n\n\n\n\n\nSecond Kind: \\(\\,\\)Force Given on the Boundaries\nIn as much as the vertical forces on the string at the left and right ends are given by \\(\\,Tu_x(0,t)\\,\\) and \\(\\,Tu_x(1,t)\\), \\(\\,\\)respectively, \\(\\,\\)by allowing the ends of the string to slide vertically on frictionless sleeves, \\(\\,\\)the boundary conditions become\n\\[\\begin{aligned}\nu_x(0,t)&=0 \\\\\nu_x(1,t)&=0\n\\end{aligned}, \\quad 0 &lt; t &lt; \\infty\\]\n\n\n\n\n\nIf a vertical force \\(f(t)\\) is applied at the end \\(x=1\\), \\(\\,\\)then the BC would be\n\\[u_x(1,t)=\\frac{1}{T}\\,f(t)\\]\nThird Kind: \\(\\,\\)Elastic Attachment on the Boundaries\nConsider finally a violin string whose ends are attached to an elastic arrangement like the one shown in figure\n\n\n\n\n\nHere, \\(\\,\\)the spring attachments at each end give rise to vertical forces proportional to the displacements \\(\\,-u(0,t)\\,\\) and \\(\\,-u(1,t)\\)\nSetting the vertical tensions of the spring at the two ends \\(\\,-Tu_x(0,t)\\,\\) and \\(\\,Tu_x(1,t)\\,\\) equal to these displacements (multiplied by the spring constant \\(h\\)) gives us our desired BCs\n\\[\\begin{aligned}\n-T u_x(0,t) &= -hu(0,t)\\\\\nTu_x(1,t) &= -hu(1,t)\n\\end{aligned}\\]\nWe can rewrite these two homogeneous BCs as\n\\[\\begin{aligned}\nu_x(0,t) -\\frac{h}{T} u(0,t) &=0\\\\\nu_x(1,t) +\\frac{h}{T} u(1,t) &=0\n\\end{aligned}\\]\nIf the two spring attachments are displaced according to the functions \\(\\,\\theta_1(t)\\,\\) and \\(\\,\\theta_2(t)\\), \\(\\,\\)we would have the nonhomogeneous BCs\n\\[\\begin{aligned}\nu_x(0,t) &=\\frac{h}{T} \\left[u(0,t) -\\theta_1(t) \\right]\\\\\nu_x(1,t) &=-\\frac{h}{T} \\left[u(1,t) -\\theta_2(t) \\right]\n\\end{aligned}\\]\n\n\n\n\n\n\n\\(~\\)\nNOTE \\(\\,\\)What is the general nature of BC\n\\[ u_x(0,t) =\\frac{h}{T} \\left[u(0,t) -\\theta_1(t) \\right]\\]\nwhen \\(~h \\to \\infty\\) and \\(~h \\to 0\\)\n\\(~\\)",
    "crumbs": [
      "**Second Semester**",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Hyperbolic Partial Differential Equations</span>"
    ]
  },
  {
    "objectID": "ch_x2_Hyperbolic_PDEs.html#sec-x2-23",
    "href": "ch_x2_Hyperbolic_PDEs.html#sec-x2-23",
    "title": "13  Hyperbolic Partial Differential Equations",
    "section": "13.6 The Finite Vibrating String (Standing Waves)",
    "text": "13.6 The Finite Vibrating String (Standing Waves)\n\nSo far, \\(\\,\\)we have studied the wave equation \\(\\,u_{tt}=c^2 u_{xx}\\,\\) for the unbounded domain and have found D’Alembert solutions to be certain traveling waves (moving in opposite directions)\nWhen we study the same wave equation in a bounded region of space \\(0 &lt; x &lt; L\\), \\(\\,\\)we find that the waves no longer appear to be moving due to their repeated interaction with the boundaries and, in fact, often appear to be what are known as standing waves\nSeparation of Variables Solution to the Finite Vibrating String\nConsider what happens when a guitar string (fixed at both ends \\(x=0,\\,L\\)) described by the simple hyperbolic IBVP is set in motion\n\\[\n\\begin{aligned}\n  u_{tt} &=c^2 u_{xx} && \\color{red}{0 &lt; x &lt; L},\\;\\; 0 &lt; t &lt; \\infty \\\\ \\\\\n  \\begin{array} {r}\n    u(0, t) \\\\ u(1,t)\n  \\end{array}\n  &\\;\n  \\begin{array} {r}\n    =0 \\\\ =0\n  \\end{array}\n  && 0 &lt; t &lt; \\infty \\\\ \\\\\n  \\begin{array} {r}\n    u(x, 0) \\\\ u_t(x, 0)\n  \\end{array}\n  &\\;\n  \\begin{array} {r}\n    =f(x) \\\\ =g(x)\n  \\end{array}\n  && 0 \\leq x \\leq L\n\\end{aligned} \\tag{FS}\\label{eq:FS}\\]\n\nWhat happens is that the traveling wave solution to the PDE and IC keeps reflecting from the boundaries in such a way that the wave motion does not to be moving, \\(\\,\\)but, \\(\\,\\)in fact, appears to be vibrating in one position:\n\n\n\n\n\nWe start by seeking standing-wave solutions to the PDE; \\(\\,\\)that is solutions of the form\n\\[u(x,t)=X(x)T(t)\\]\nSubstituting this expression into the wave equation and separating variables gives us the two ODEs\n\\[\\begin{aligned}\n  \\frac{\\;\\;T''}{c^2 T}\n  &= \\frac{\\;\\;X''}{X} = - \\lambda &lt;0\\\\\n  &\\Downarrow \\\\\n  T'' +c^2\\lambda T &=0\\\\\n  X'' +\\lambda X &=0\n\\end{aligned}\\]\nin which only positive values of \\(\\lambda\\) give feasible (nonzero and bounded) solutions\nThe solutions of these two ODEs yield\n\\[\\begin{aligned}\nT(t) &= a \\cos c\\sqrt{\\lambda} t +b\\sin c\\sqrt{\\lambda}t\\\\\nX(x)&=d \\cos \\sqrt{\\lambda}x +e\\sin\\sqrt{\\lambda}x\n\\end{aligned}\\]\nPlugging this equation into \\(u(0,t)=u(L,t)=0\\,\\) gives\n\\[\\scriptsize\n\\begin{aligned}\n  u(0,t)\n  & = d \\cdot \\,T(t)=0 \\;\\;\\Rightarrow \\;\\; T(t) \\neq 0,\\;\\;d=0\\\\\n  u(L,t)\n  & = e\\sin\\sqrt{\\lambda}L \\cdot T(t)=0\\;\\; \\Rightarrow T(t) \\neq 0,\\;\\; e\\neq0, \\;\\sin\\sqrt{\\lambda}L=0\\;\\;\\\\\n&\\normalsize\\Rightarrow \\;\\;\\lambda_n=\\left( \\frac{n\\pi}{L}\\right)^2,\\;\\;n=1,2,3,\\cdots\n\\end{aligned}\\]\nHence, \\(\\,\\)we have now found a sequence of simple vibrations (which we subscript with \\(n\\))\n\\[ \\begin{aligned} u_n(x,t)&= T_n(t)X_n(x) \\\\=&\\left[ a_n \\cos\\frac{n\\pi c t}{L} +b_n \\sin \\frac{n\\pi c t}{L} \\right]\\,\\sin \\frac{n\\pi x}{L}, \\;\\;n=1,2,3,\\cdots \\end{aligned}\\]\nAll of which satisfy the wave equation and the BCs, \\(\\,\\)and constitute a family of standing waves. \\(\\,\\)Since any sum of these vibrations is also a solution to the PDE and BCs (since the PDE and BCs are linear and homogeneous), \\(\\,\\)we add them together in such a way that the resulting sum also agrees with the ICs. \\(\\,\\)Substituting the sum\n\\[ u(x,t)= \\sum_{n=1}^\\infty \\left[ a_n \\cos\\frac{n\\pi c t}{L} +b_n \\sin \\frac{n\\pi c t}{L} \\right]\\,\\sin \\frac{n\\pi x}{L}\\]\ninto the ICs\n\\[u(x,0)=f(x), \\;\\; u_t(x,0)=g(x)\\]\ngives the two equations \\[ \\sum_{n=1}^\\infty a_n \\sin \\frac{n\\pi x}{L}=f(x), \\;\\;\n\\sum_{n=1}^\\infty b_n \\frac{n\\pi c}{L}\\sin\\frac{n\\pi x}{L}=g(x)\\]\nand using the orthogonality condition\n\\[\\int_0^L \\sin \\frac{m\\pi x}{L}\\, \\sin \\frac{n\\pi x}{L}\\,dx=\\begin{cases}\n\\;\\;0& m \\neq n \\\\\nL/2& m = n\n\\end{cases}\\]\nwe can find the coefficients \\(\\,a_n\\,\\) and \\(\\,b_n\\)\n\\[\\begin{aligned}\na_n&= \\frac{2}{L} \\int_0^L f(x)\\, \\sin \\frac{n\\pi x}{L}\\,dx\\\\\nb_n&= \\frac{2}{n\\pi c} \\int_0^L g(x)\\, \\sin \\frac{n\\pi x}{L}\\,dx\n\\end{aligned}\\]\n\n\nNOTE\n\\[\\scriptsize\\begin{aligned}\n\\normalsize u(x,t) \\;&\\normalsize= \\sum_{n=1}^\\infty \\left[ a_n \\cos\\frac{n\\pi c t}{L} +b_n \\sin \\frac{n\\pi c t}{L} \\right]\\,\\sin \\frac{n\\pi x}{L}\\\\[5pt]\n&= \\frac{1}{2} \\sum_{n=1}^\\infty a_n\\left[\\sin \\frac{n\\pi}{L} (x+ct) +\\sin \\frac{n\\pi}{L} (x-ct)\\right ]\n\\\\&\\quad-\\frac{1}{2} \\sum_{n=1}^\\infty b_n\\left[\\cos \\frac{n\\pi}{L} (x+ct) -\\cos \\frac{n\\pi}{L} (x-ct)\\right ]\\\\[5pt]\n&\\;\\;\\Big\\Downarrow \\;\\; {\\tiny\\sum_{n=1}^\\infty a_n \\sin \\frac{n\\pi x}{L}=f(x), \\;\\;\\sum_{n=1}^\\infty b_n \\frac{n\\pi c}{L}\\sin\\frac{n\\pi x}{L}=g(x) \\;\\rightarrow\\, -\\sum_{n=1}^\\infty b_n\\, c\\cos \\frac{n\\pi x}{L} +\\sum_{n=1}^\\infty b_n c=\\int^x_0 g(\\alpha) d\\alpha} \\\\[5pt]\n&\\normalsize= \\frac{1}{2}\\left[ f(x+ct) +f(x-ct)\\right] +\\frac{1}{2c} \\int_{x-ct}^{x+ct} g(\\alpha)\\,d\\alpha\n\\end{aligned}\\]\n\\(~\\)\nExample \\(\\,\\)Find the solution to the vibrating-string problem \\(\\eqref{eq:FS}\\) if the ICs are given by\n\\[\\begin{aligned}\nu(x,0) &= \\sin \\frac{\\pi x}{L} +0.5 \\sin\\frac{3\\pi x}{L}\\\\\nu_t(x,0) &= 0\n\\end{aligned}\\]\n\\(~\\)\nExample \\(\\,\\)What is the solution of the vibrating-string problem \\(\\eqref{eq:FS}\\) if the ICs are\n\\[\\begin{aligned}\nu(x,0)&= 0\\\\\nu_t(x,0)&= \\sin\\frac{3\\pi x}{L}\n\\end{aligned}\\]\n\\(~\\)\nExample \\(\\,\\)Solve the damped vibrating-string problem\n\\[\n\\begin{aligned}\n  u_{tt} &=c^2 u_{xx} -\\beta u_t && \\color{red}{0 &lt; x &lt; 1},\\;\\; 0 &lt; t &lt; \\infty \\\\ \\\\\n  \\begin{array} {r}\n    u(0, t) \\\\ u(1,t)\n  \\end{array}\n   &\n  \\begin{array} {r}\n    =0 \\\\ =0\n  \\end{array}\n  && 0 &lt; t &lt; \\infty \\\\ \\\\\n  \\begin{array} {r}\n    u(x, 0) \\\\ u_t(x, 0)\n  \\end{array}\n   &\n  \\begin{array} {r}\n    =f(x) \\\\ =0\\phantom{(x)}\n  \\end{array}\n  && 0 \\leq x \\leq 1\n\\end{aligned}\\]\n\\(~\\)",
    "crumbs": [
      "**Second Semester**",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Hyperbolic Partial Differential Equations</span>"
    ]
  },
  {
    "objectID": "ch_x2_Hyperbolic_PDEs.html#sec-x2-24",
    "href": "ch_x2_Hyperbolic_PDEs.html#sec-x2-24",
    "title": "13  Hyperbolic Partial Differential Equations",
    "section": "13.7 Solution of Nonhomogeneous Wave Equation via the Finite Transform",
    "text": "13.7 Solution of Nonhomogeneous Wave Equation via the Finite Transform\n\nConsider the nonhomogeneous wave equation\n\\[\n\\begin{aligned}\n  u_{tt} &=c^2 u_{xx} +\\sin\\pi x && 0 &lt; x &lt; 1,\\;\\; 0 &lt; t &lt; \\infty \\\\ \\\\\n  \\begin{array} {r}\n    u(0, t) \\\\ u(1,t)\n  \\end{array}\n  &\n  \\begin{array} {r}\n    =0 \\\\ =0\n  \\end{array}\n  && 0 &lt; t &lt; \\infty \\\\ \\\\\n  \\begin{array} {r}\n    u(x, 0) \\\\ u_t(x, 0)\n  \\end{array}\n  &\n  \\begin{array} {r}\n    =1 \\\\ =0\n  \\end{array}\n  && 0 \\leq x \\leq 1\n\\end{aligned}\\]\n\nSTEP 1 \\(\\,\\) Determine the transform\nSince the \\(x\\)-variable ranges from \\(0\\) to \\(1\\), \\(\\,\\)we use a finite transform. \\(\\,\\)Also, \\(\\,\\)you will see why, \\(\\,\\)in this case, we use the sine transform. \\(\\,\\)We could solve this problem with the Laplace transform by transforming \\(\\,t\\)\nSTEP 2 \\(\\,\\) Carry out the transformation\n\\[\\begin{aligned}\n\\mathcal{F}_s [u_{tt}]&= c^2\\mathcal{F}_s [u_{xx}] +\\mathcal{F}_s [\\sin\\pi x] \\\\\n&\\Downarrow \\\\\n\\frac{d^2 U_n(t)}{dt^2}&= -(n\\pi c)^2 U_n(t) +2n\\pi c^2\\left[ u(0,t) -(-1)^n u(1,t) \\right] +S_n(t) \\\\\n&\\Downarrow\\; u(0,t)=0, \\;\\;u(1,t)=0 \\\\\n\\frac{d^2 U_n(t)}{dt^2}&= -(n\\pi c)^2 U_n(t) +S_n(t)\n\\end{aligned}\\]\nwhere\n\\[ S_n(t) = 2 \\int_0^1 \\sin \\pi x \\, \\sin n\\pi x \\,dx =\n\\begin{cases}\n1 & n=1 \\\\\n0 & n=2,3,\\cdots\n\\end{cases}\\]\nIf we now transform the initial conditions, \\(\\,\\)we will arrive at the initial conditions for our ODE\n\\[\\begin{aligned}\n\\mathcal{F}_s [u(x,0)]&= U_n(0) =\n\\begin{cases}\n\\displaystyle \\frac{4}{n\\pi} & n=1,3,\\cdots \\\\\n\\;\\,0 & n=2,4,\\cdots\n\\end{cases}\\\\\n\\mathcal{F}_s [u_t(x,0)]&= \\frac{dU_n(0)}{dt} =0\n\\end{aligned}\\]\nSo, \\(\\,\\)solving our new initial-value problem, \\(\\,\\)we have\n\\[\\begin{aligned}\nU_1(t)&= \\left( \\frac{4}{\\pi} -\\frac{1}{\\pi^2} \\right) \\,\\cos\\pi c t +\\frac{1}{\\pi^2}\\\\\nU_n(t) &= \\begin{cases}\n\\quad\\;\\; 0 & n=2,4,\\cdots \\\\\n\\displaystyle \\frac{4}{n\\pi}\\cos n\\pi c t & n=3,5,\\cdots\n\\end{cases}\n\\end{aligned}\\]\nHence, \\(\\,\\)the solution \\(\\,u(x,t)\\) of the problem is\n\\[ u(x,t)=\\left[ \\left( \\frac{4}{\\pi} -\\frac{1}{\\pi^2} \\right) \\,\\cos\\pi c t +\\frac{1}{\\pi^2} \\right]\\,\\sin \\pi x\n+\\frac{4}{\\pi} \\sum_{n=1}^\\infty \\frac{1}{2n +1} \\cos(2n +1)\\pi c t \\, \\sin(2n +1)\\pi x\\]",
    "crumbs": [
      "**Second Semester**",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Hyperbolic Partial Differential Equations</span>"
    ]
  },
  {
    "objectID": "ch_x2_Hyperbolic_PDEs.html#sec-x2-25",
    "href": "ch_x2_Hyperbolic_PDEs.html#sec-x2-25",
    "title": "13  Hyperbolic Partial Differential Equations",
    "section": "13.8 The Vibrating Drumhead (Wave Equation in Polar Coordinates)",
    "text": "13.8 The Vibrating Drumhead (Wave Equation in Polar Coordinates)\n\nThe problem is to find \\(\\,u(r,\\theta,t)\\) (which stands for the height of the drumhead from the plane) that satisfies\n\\[\\color{red}{\\begin{aligned}\nu_{tt} &= c^2 \\left( u_{rr} +\\frac{1}{r} u_r +\\frac{1}{r^2} u_{\\theta\\theta} \\right) && \\\\\nu(1, \\theta, t) &= 0 && 0 &lt; t &lt; \\infty \\\\\nu(r, \\theta, 0) &= f(r,\\theta) && 0 &lt; r &lt; 1\\\\\nu_t(r, \\theta, 0) &= g(r,\\theta) &&\n\\end{aligned}}\\]\nWe will look for solutions of the form\n\\[\\color{blue}{u(r,\\theta,t)=U(r,\\theta)T(t)}\\]\nThis gives the shape \\(\\,U(r,\\theta)\\) of the vibrations times the oscillatory factor \\(\\,T(t)\\)\nCarrying out this substitution, \\(\\,\\)we arrive at the two equations\n\\[\\scriptsize\\begin{aligned}\n{ u_{tt}} &{ = c^2 \\left( u_{rr} +\\frac{1}{r} u_r +\\frac{1}{r^2} u_{\\theta\\theta} \\right)}\\\\\n&\\Downarrow \\\\\n{ UT''} &= { c^2 \\left( U_{rr} +\\frac{1}{r}U_r +\\frac{1}{r^2}U_{\\theta\\theta} \\right) T}  \\\\\n&\\Downarrow \\\\\n{ \\frac{\\;\\;T''}{c^2T}} &={\\frac{\\displaystyle U_{rr} +\\frac{1}{r}U_r +\\frac{1}{r^2}U_{\\theta\\theta}}{U} =-\\lambda &lt; 0} \\\\\n&\\Downarrow \\\\\n\\normalsize\\color{red}{T''} &\\normalsize\\color{red}{+ c^2 \\lambda T =0} && \\text{Simple Harmonic Motion}\\\\\n\\normalsize\\color{red}{U_{rr}} &\\normalsize\\color{red}{+\\frac{1}{r}U_r +\\frac{1}{r^2}U_{\\theta\\theta} +\\lambda U=0} && \\text{Helmholtz Equation}\n\\end{aligned}\\]\nFor the next step, \\(\\,\\)we want to solve the Helmholtz equation, \\(\\,\\)but, \\(\\,\\)first, \\(\\,\\)it needs a boundary condition. \\(\\,\\)To find it, \\(\\,\\)we substitute \\(\\,u(r,\\theta,t)=U(r,\\theta)T(t)\\) \\(\\,\\)into the boundary condition of the drumhead to get\n\\[\\color{red}{u(1,\\theta,t)=U(1,\\theta)T(t)=0 \\;\\;\\Rightarrow\\;\\;U(1,\\theta)=0}\\]\n\n\\(~\\)\n\nSolution of the Helmholtz Eigenvalue Problem\n\nTo solve\n\\[\\color{blue}{\\begin{aligned}\n&U_{rr} +\\frac{1}{r}U_r +\\frac{1}{r^2}U_{\\theta\\theta} +\\lambda U=0\\\\\n&U(1,\\theta)=0\n\\end{aligned}}\\]\nwe let\n\\[\\color{red}{U(r,\\theta)=R(r)\\Theta(\\theta)}\\]\nand plug it into the Helmholtz BVP\n\\[\\begin{aligned}\nU_{rr} +\\frac{1}{r}U_r &+\\frac{1}{r^2}U_{\\theta\\theta} +\\lambda U=0\\\\\n&\\Downarrow U(r,\\theta)=R(r)\\Theta(\\theta)\\\\\nR''\\Theta +\\frac{1}{r}R' \\Theta &+\\frac{1}{r^2}R \\Theta'' +\\lambda R\\Theta = 0 \\\\\n&\\Downarrow \\\\\n-\\frac{r^2 R'' +rR' +\\lambda r^2R}{R} &= \\frac{\\;\\Theta''}{\\Theta} = -\\mu &lt;0 \\\\\n&\\Downarrow \\\\\n\\color{red}{\\Theta'' +\\mu \\Theta} &\\color{red}{=0} \\\\\n\\color{red}{r^2 R'' +rR'} &\\color{red}{+(\\lambda r^2 -\\mu) R =0}\n\\end{aligned}\\]\nWe get a new separation constant \\(\\,\\mu\\)\nIt’s obvious that the drumhead is periodic with period \\(\\,2\\pi\\,\\) in \\(\\,\\theta.\\) \\(\\,\\)Thus\n\\[\\scriptsize\\begin{aligned}\n\\Theta'' &+\\mu\\Theta = 0 \\\\\n&\\Downarrow \\\\\n\\Theta(\\theta) &= a \\cos\\sqrt{\\mu}\\theta +b\\sin\\sqrt{\\mu}\\theta \\\\\n&\\Downarrow \\;{\\scriptstyle \\Theta(0)=\\Theta(2\\pi),\\;\\;\\Theta'(0)=\\Theta'(2\\pi)} \\\\\n\\begin{bmatrix}\n  1 -\\cos 2\\pi\\sqrt{\\mu} & -\\sin 2\\pi\\sqrt{\\mu} \\\\\n  \\sqrt{\\mu} \\sin 2\\pi\\sqrt{\\mu} & \\sqrt{\\mu}\\left(1 -\\cos 2\\pi\\sqrt{\\mu}\\right)\n\\end{bmatrix} &\\begin{bmatrix} a \\\\ b \\end{bmatrix} = \\begin{bmatrix} 0 \\\\ 0 \\end{bmatrix} \\\\ \\;\\;&\\overset{\\text{non-trivial solution}}{\\longrightarrow} \\;\\;\n  {\\tiny \\sqrt{\\mu}(1 -\\cos 2\\pi\\sqrt{\\mu})^2 +\\sqrt{\\mu} \\sin^2 2\\pi\\sqrt{\\mu}=0 \\;\\rightarrow\\;1=\\cos 2\\pi\\sqrt{\\mu} } \\\\\n&\\Downarrow \\\\\n\\color{red}{\\mu}\\, &\\color{red}{= n^2, \\;\\;n=0,1,2,\\cdots} \\\\\n&\\Downarrow \\\\\n\\color{red}{\\Theta_n(\\theta)}\\, &\\color{red}{= a_n \\cos n\\theta +b_n\\sin n\\theta} \\\\\n&=\\sqrt{a_n^2 +b_n^2} \\left(\\cos n\\theta \\cos \\phi +\\sin n\\theta \\sin \\phi \\right),\\;\\;\\tan \\phi=\\frac{b_n}{a_n} \\\\\n&\\color{red}{=c_n \\cos (n\\theta -\\phi)}\n\\end{aligned}\\]\nSo, in order to solve the Helmholtz equation, \\(~\\)we must solve the following differential equation\n\\[\\color{red}{\\begin{aligned}\n  r^2 R_n''\\, +\\,&rR_n' +(\\lambda r^2 -n^2) R_n =0, \\;\\;\\;0&lt;r&lt;1 \\\\\n  \\left |  R_n(0)\\right| &&lt; \\infty \\\\\n  R_n(1)\\phantom{|} &= 0\n\\end{aligned}}\\]\nIt is the parameterized Bessel’s equation and has two linearly independent solutions \\(\\color{red}{J_n(\\sqrt{\\lambda}r)}\\) and \\(\\color{red}{Y_n(\\sqrt{\\lambda}r)}\\)\n\n\n\\(~\\)\n\nfrom scipy.special import jv, yv, jn_zeros\n\nfig = plt.figure(figsize=(6, 8))\nax1 = fig.add_subplot(211)\nax2 = fig.add_subplot(212)\n\nax1.axis((0, 10, -0.6, 1))    \nax1.set_ylabel('$J_n$')\n\nax2.axis((0, 10, -3, 1))    \nax2.set_xlabel(r'$\\sqrt{\\lambda}$')\nax2.set_ylabel('$Y_n$')\n\nax1.plot([0, 10], [0, 0], 'k:')\nax2.plot([0, 10], [0, 0], 'k:')\n\nx = np.linspace(0, 10, 200)\nfor n in range(4):\n    y1 = jv(n, x)\n    y2 = yv(n, x)\n    ax1.plot(x, y1, label=rf'$J_{n:d}(\\sqrt{{\\lambda}})$')\n    ax2.plot(x, y2, label=rf'$Y_{n:d}(\\sqrt{{\\lambda}})$')\n\nax1.legend(bbox_to_anchor=(1.01, 1), loc='upper left')\nax2.legend(bbox_to_anchor=(1.01, 1), loc='upper left')\n\nplt.show()\n\n\n\n\n\n\n\nFigure 13.1: Bessel’s functions\n\n\n\n\n\n\\(~\\)\n\n\nSince the functions \\(Y_n(\\sqrt{\\lambda} r)\\) are unbounded at \\(r=0\\), \\(\\,\\)we choose as our solution \\(J_n(\\sqrt{\\lambda} r)\\). \\(\\,\\)The last step in finding \\(R_n(r)\\) is to use the boundary condition \\(R_n(1)=0~\\) to find \\(\\lambda\\). \\(\\,\\)We must pick \\(\\lambda\\) to be one of the roots of \\(\\,\\color{red}{J_n(\\sqrt{\\lambda})=0}\\); \\(\\,\\)that is\n\\[\\color{red}{\\sqrt{\\lambda_{nm}}=\\alpha_{nm}}\\]\nwhere \\(\\,\\alpha_{nm}\\) is the \\(m\\)-th root of \\(\\,J_n(r)=0\\), \\(~m=1,2,3,\\cdots\\). \\(\\,\\)With these roots, \\(\\,\\)we have just solved the Helmholtz eigenvalue problem\nThe eigenvalues \\(\\,\\lambda_{nm}\\) are \\(\\,\\alpha_{nm}^2\\); \\(\\,\\)and the corresponding eigenfunctions are\n\\[\\color{red}{U_{nm}(r,\\theta) =c_n J_n(\\alpha_{nm} r) \\cos (n\\theta -\\phi), \\;\\;\\;n=0,1,2,\\cdots, \\;\\;m=1,2,3,\\cdots}\\]\nThe general shape of \\(\\,U_{nm}(r,\\theta)\\,\\) is the same for different values of the constants \\(\\,c_n\\,\\) and \\(\\,\\phi.\\) \\(\\,\\)We plot these functions for the different values of \\(\\,n\\) and \\(\\,m\\) with \\(\\,c_n=1\\,\\) and \\(\\,\\phi=0\\)\n\n\n\\(~\\)\n\nncols = 4\nnrows = 4\n    \nfig, axs = plt.subplots(ncols=ncols, \n    nrows=ncols, figsize=(7, 7), \n    subplot_kw=dict(projection='polar'))\n\nfor ax in axs.flat:\n    ax.set(xticks=[], yticks=[])\n\nazimuths = np.radians(np.linspace(0, 360, 100))\nzeniths = np.linspace(0, 1, 100)\n\ntheta, r = np.meshgrid(azimuths, zeniths)\n\nfor n in range(ncols):\n    alpha_nm = jn_zeros(n, nrows)\n    for m in range(nrows):\n        U_nm = jv(n, alpha_nm[m] *r) *np.cos(n *theta)\n        axs[n, m].contourf(theta, r, U_nm, levels=[0, 10])\n        if m == 0:\n            axs[n, 0].set_ylabel(f'n = {n}') \n        if n == 0:\n            axs[nrows -1, m].set_xlabel(f'm = {m +1}')\n  \nplt.show()\n\n\n\n\n\n\n\nFigure 13.2: Helmholtz Eigenfunctions: \\(\\,U_{nm}(r, \\theta)\\)\n\n\n\n\n\n\\(~\\)\n\nNow that we have solved the Helmholtz equation for the basic shapes \\(\\,U_{nm}(r,\\theta)\\), \\(\\,\\)the final step is to multiply by the time factor\n\\[\\color{red}{T_{nm}(t)=A_{nm}\\cos c\\alpha_{nm}t +B_{nm}\\sin c\\alpha_{nm} t}\\]\nand add the products in such a way that the initial conditions are satisfied. \\(\\,\\)That is, \\(\\,\\)the solution to our problem will be\n\\[ \\color{red}{u(r,\\theta,t) = \\sum_{n=0}^\\infty \\sum_{m=1}^\\infty J_n(\\alpha_{nm}r) \\cos n\\theta \\left[ A_{nm}\\cos c\\alpha_{nm}t +B_{nm}\\sin c\\alpha_{nm}t \\right]}\\]\nRather than going through the complicated process of finding \\(\\,A_{nm}\\,\\) and \\(\\,B_{nm}\\,\\) for the general case, \\(\\,\\)we will find the solution for the situation where \\(\\,u\\,\\) is independent of \\(\\,\\theta\\) (very common, \\(\\,n=0\\)). \\(\\,\\)In other words, \\(\\,\\)we will assume that the initial position of the drumhead depends only on \\(\\,r\\)\nIn particular, \\(\\,\\)we consider\n\\[\\begin{aligned}\nu(r,\\theta,0) &= \\color{red}{f(r)}\\\\\nu_t(r,\\theta,0) &= 0\n\\end{aligned}\\]\n(It’s just easy to do the case where \\(\\,u_t \\neq 0\\)). \\(\\,\\)With these assumptions, \\(\\,\\)the solution now becomes\n\\[ u(r,t) = \\sum_{m=1}^\\infty A_{\\color{red}{0}m} J_{\\color{red}{0}}(\\alpha_{\\color{red}{0}m}r) \\cos c\\alpha_{\\color{red}{0}m}t\\]\nand our goal is to find \\(\\,A_{0m}\\,\\) so that\n\\[ \\color{blue}{f(r) = \\sum_{m=1}^\\infty \\color{red}{A_{0m}} J_0(\\alpha_{0m}r)} \\tag{IC}\\label{eq:IC}\\]\nTo find the constants \\(\\,A_{0m}\\), \\(\\,\\)we use the orthogonality condition of the Bessel functions:\n\\[\\int_0^1 rJ_0(\\alpha_{0i} r)\\,J_0(\\alpha_{0j} r) \\,dr =\n\\begin{cases}\n0 & i \\neq j \\\\\n\\frac{1}{2}J_1^2(\\alpha_{0i}) & i=j\n\\end{cases}\\]\nHence, \\(\\,\\)we multiply each side of equation \\(\\eqref{eq:IC}\\) by \\(\\,rJ_0(\\alpha_{0j}r)\\,\\) and integrate from \\(\\,0\\,\\) to \\(\\,1\\,\\), \\(\\,\\)giving\n\\[ \\int_0^1 rf(r)J_0(\\alpha_{0j}r)\\,dr = A_{0j} \\int_0^1 r J_0^2(\\alpha_{0j}r)\\,dr \\]\nfrom which we can solve for \\(\\,A_{0j}\\)\n\\[ \\color{red}{A_{0j}=\\frac{2}{J_1^2(\\alpha_{0j})} \\int_0^1 rf(r)J_0(\\alpha_{0j}r)\\,dr, \\;\\;j =1,2,3,\\cdots} \\]\nFor example \\(\\,\\)the solution to the vibrating drumhead with initial conditions\n\\[\\begin{aligned}\nu(r,\\theta,0) &= J_0(2.405 r) +0.5 J_0(8.654 r)\\\\\nu_t(r,\\theta,0) &= 0\n\\end{aligned}\\]\nwould be\n\\[ u(r,t) = J_0(2.405 r) \\cos(2.405 ct) +0.5 J_0(8.654 r)\\cos(8.654 ct) \\]\nWe start by drawing \\(\\,J_0(\\sqrt{\\lambda})\\). \\(\\,\\)Now, \\(\\,\\)in order to graph the functions\n\\[J_0(\\alpha_{01}r), \\,J_0(\\alpha_{02}r), \\,\\cdots,\\, J_0(\\alpha_{0m}r)\\]\nwe rescale the \\(\\,r\\)-axis so that \\(\\,m\\)-th root passes through \\(\\,r=1\\)\n\n\\(~\\)\n\nfrom mpl_toolkits.axisartist.axislines import SubplotZero\n\nfig = plt.figure(figsize=(7, 8))\nax1 = SubplotZero(fig, 211)\nax2 = SubplotZero(fig, 212)\n\nfig.add_subplot(ax1)\nax1.axis[\"left\", \"right\", \"bottom\", \"top\"].set_visible(False)\nax1.axis[\"xzero\", \"yzero\"].set_visible(True)\nax1.axis[\"xzero\", \"yzero\"].set_axisline_style(\"-|&gt;\")\n\nax1.set_xticks([2, 4, 6, 8, 10])\nax1.set_ylim(-0.5, 1.1)\nax1.set_yticks([-0.5, 0.5, 1.0])\nax1.text(11, -0.03, r'$\\sqrt{\\lambda}$', fontsize=14)\n\nfig.add_subplot(ax2)\nax2.axis[\"left\", \"right\", \"bottom\", \"top\"].set_visible(False)\nax2.axis[\"xzero\", \"yzero\"].set_visible(True)\nax2.axis[\"xzero\", \"yzero\"].set_axisline_style(\"-|&gt;\")\n\nax2.set_xticks([0.2, 0.4, 0.6, 0.8, 1.0])\nax2.set_ylim(-0.5, 1.1)   \nax2.set_yticks([-0.5, 0.5, 1.0])\nax2.text(1.1, -0.03, '$r$', fontsize=12)\n\nmm = 3\nalpha_0m =jn_zeros(0, mm)\n\nr1 = np.linspace(0, 10, 100)\ny1 = jv(0, r1)\nax1.plot(r1, y1, label=r'$J_0(\\sqrt{\\lambda})$')\nax1.plot(alpha_0m, np.zeros(mm), 'ro')\n\nfor m in range(mm):\n    ax1.text(alpha_0m[m] -0.2, 0.1, rf'$\\alpha_{m +1}$', fontsize=12)\nax1.legend()\n\nr2 = np.linspace(0, 1, 100)\nfor m in range(mm):   \n    y2 = jv(0, alpha_0m[m]*r2)\n    ax2.plot(r2, y2, label=rf'$J_0(\\alpha_{m +1}r)$')\n    ax2.plot(alpha_0m[:m +1] /alpha_0m[m], np.zeros(m +1), 'x')\nax2.legend()\n\nplt.show()  \n\n\n\n\n\n\n\n\n\\(~\\)\n\nIn general, \\(\\,\\)what we would do is to expand the initial position \\(\\,f(r)\\,\\) into the sum of these basic shapes \\(\\,J_0(\\alpha_{0m} r)\\), \\(\\,\\)observe the vibration for each one and then add them up\n\n\nfig = plt.figure(figsize=(7, 5))\nax = plt.axes(xlim=(0, 1), ylim=(-1.5, 1.5))\n\nax.set_xticks([0.0, 0.2, 0.4, 0.6, 0.8, 1.0])\nax.set_yticks([-1.5, -1.0, -0.5, 0, 0.5, 1.0, 1.5])\nax.tick_params(pad=10)\nax.set_xlabel('$r$')\nax.set_ylabel('$u(r,t)$')\n\nplt.close()\n\n\nline1, = ax.plot([], [], lw=1, ls=':')\nline2, = ax.plot([], [], lw=1, ls=':')\nline_sum, = ax.plot([], [], lw=4)\n\ntime_text = ax.text(0.45, 1.3, '')\ndef init():\n    time_text.set_text('t = 0.0')\n    line1.set_data([], [])\n    line2.set_data([], [])\n    line_sum.set_data([], [])\n    return (line1, line2, line_sum)\n\nc = 1 \nalpha_0m =jn_zeros(0, 3)\n\ndef animate(t):\n    time_text.set_text(f't = {t:.2f}')        \n    r = np.linspace(0, 1, 100)   \n\n    u1 = jv(0, alpha_0m[0] *r) *np.cos(c *alpha_0m[0] *t)\n    u2 = 0.5 *jv(0, alpha_0m[2] *r) *np.cos(c *alpha_0m[2] *t)\n    u_sum = u1 +u2    \n    \n    line1.set_data(r, u1)\n    line2.set_data(r, u2)\n    line_sum.set_data(r, u_sum)\n    \n    return (line1, line2, line_sum)\n\ntt = np.linspace(0, 10, 100)\nanim = animation.FuncAnimation(fig, animate, \n         init_func=init, frames=tt, interval=200, blit=True)\n\nHTML('&lt;center&gt;' + anim.to_html5_video() + '&lt;/center&gt;')\n\n\n  \n  Your browser does not support the video tag.\n\n\n\n\\(~\\)\nExample \\(\\,\\)Solve the vibrating drum head problem\n\\[\n\\begin{aligned}\n  u_{tt} &=\\nabla^2 u && 0 &lt; r &lt; 1, \\;\\; 0 &lt; t &lt; \\infty \\\\ \\\\\n  u(1,\\theta,t) &=0 && 0 &lt; t &lt; \\infty \\\\ \\\\\n  \\begin{array}{r}\n    u(r,\\theta,0) \\\\ u_t(r,\\theta,0)\n  \\end{array}\n  &\n  \\begin{array}{l}\n    = 1 -r^2 \\\\ =0 \\phantom{-r^2}\n  \\end{array} && 0 \\leq r \\leq 1\n\\end{aligned}\\]\n\\(~\\)",
    "crumbs": [
      "**Second Semester**",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Hyperbolic Partial Differential Equations</span>"
    ]
  },
  {
    "objectID": "ch_x2_Hyperbolic_PDEs.html#sec-x2-26",
    "href": "ch_x2_Hyperbolic_PDEs.html#sec-x2-26",
    "title": "13  Hyperbolic Partial Differential Equations",
    "section": "13.9 Dimensionless Problems",
    "text": "13.9 Dimensionless Problems\n\nThe basic idea behind dimensionless analysis is that by introducing new (dimensionless) variables in a problem, \\(\\,\\)the problem becomes purely mathematical and contains none of the physical constants that originally characterized it\nIn this way, \\(\\,\\)many different equations in physics, biology, engineering, \\(\\,\\)and chemistry that contain special nuances via physical parameters are all transformed into the same simple form\nConverting a Diffusion Problem to Dimensionless Form\n\nSuppose we start with the initial-boundary-value problem:\n\\[\n\\begin{aligned}\n  u_t &=\\alpha u_{xx} && 0 &lt; x &lt; L, \\;\\; 0 &lt; t &lt; \\infty \\\\[5pt]\n  \\begin{array}{r}\n    u(0,t) \\\\ u(L,t)\n  \\end{array}\n  &\n  \\begin{array}{l}\n    = T_1 \\\\ = T_2\n  \\end{array} && 0 &lt; t &lt; \\infty \\\\[5pt]\n  u(x,0) &= \\sin \\frac{\\pi x}{L} && 0 \\leq x \\leq L  \n\\end{aligned}\\]\nOur goal is to change this problem to a new equivalent formulation that has the properties\n  1. No physical parameters (like \\(\\alpha\\)) in the new equation\n  2. The initial and boundary conditions are simpler\n\\(~\\)\nTransforming the Dependent Variable \\(~u \\to U =\\displaystyle\\frac{u(x,t) -T_1}{T_2 -T_1}\\)\n\\[\n\\begin{aligned}\n  U_t &=\\alpha U_{xx} && 0 &lt; x &lt; L, \\;\\; 0 &lt; t &lt; \\infty \\\\[5pt]\n  \\begin{array}{r}\n    U(0,t) \\\\ U(L,t)\n  \\end{array}\n  &\n  \\begin{array}{l}\n    = 0 \\\\ = 1\n  \\end{array} && 0 &lt; t &lt; \\infty \\\\[5pt]\n  U(x,0) &= \\frac{\\displaystyle \\sin \\frac{\\pi x}{L} -T_1}{T_2 -T_1} && 0 \\leq x \\leq L  \n\\end{aligned}\\]\n\\(~\\)\nTransforming the Space Variable \\(~x \\to \\xi=\\;^{\\displaystyle x}/_{\\displaystyle L}\\)\n\\[\n\\begin{aligned}\n  U_t &=\\frac{\\alpha}{L^2} U_{\\xi\\xi} && 0 &lt; \\xi &lt; 1, \\; 0 &lt; t &lt; \\infty \\\\[5pt]\n  \\begin{array}{r}\n    U(0,t) \\\\ U(L,t)\n  \\end{array}\n  &\n  \\begin{array}{l}\n    = 0 \\\\ = 1\n  \\end{array} && 0 &lt; t &lt; \\infty \\\\[5pt]\n  U(\\xi,0) &= \\frac{\\sin\\pi\\xi -T_1}{T_2 -T_1} && 0 \\leq \\xi \\leq 1  \n\\end{aligned}\\]\n\\(~\\)\nTransforming the Time Variable \\(\\;\\; t \\to \\tau\\)\nSince our goal is to eliminate the constant \\(\\displaystyle\\frac{\\alpha}{L^2}\\) from the PDE, \\(\\,\\)we proceed as follows:\n\nTry a transformation of the form \\(\\tau=ct\\), \\(\\,\\)where \\(c\\) is an unknown constant\nCompute \\(U_t=U_\\tau\\tau_t=cU_\\tau\\)\nSubstitute this derivative into the the PDE to obtain\n\\[ cU_\\tau =\\frac{\\alpha}{L^2} U_{\\xi\\xi}\\]\nand, \\(\\,\\)hence, \\(\\,\\)pick \\(\\displaystyle c=\\frac{\\alpha}{L^2}\\).\n\nThis give us our new time\n\\[ \\tau=\\frac{\\alpha}{L^2} t \\]\nWe now have the completely dimensionless problem\n\\[\n\\begin{aligned}\n  U_\\tau &= U_{\\xi\\xi} && 0 &lt; \\xi &lt; 1, \\; 0 &lt; \\tau &lt; \\infty \\\\[5pt]\n  \\begin{array}{r}\n    U(0,\\tau) \\\\ U(L,\\tau)\n  \\end{array}\n  &\n  \\begin{array}{l}\n    = 0 \\\\ = 1\n  \\end{array} && 0 &lt; t &lt; \\infty \\\\[5pt]\n  U(\\xi,0) &= \\frac{\\sin\\pi\\xi -T_1}{T_2 -T_1} && 0 \\leq \\xi \\leq 1  \n\\end{aligned}\\]\n\n\n\\(~\\)\n\nTransforming a Hyperbolic Problem to Dimensionless Form\nConsider the vibrating string\n\\[\n\\begin{aligned}\n  u_{tt} &=c^2 u_{xx} && 0 &lt; x &lt; L,\\;\\; 0 &lt; t &lt; \\infty \\\\[5pt]\n  \\begin{array}{r}\n    u(0,t) \\\\ u(L,t)\n  \\end{array}\n  &\n  \\begin{array}{l}\n    = 0 \\\\ = 0\n  \\end{array} && 0 &lt; t &lt; \\infty \\\\[5pt]\n  \\begin{array}{r}\n    u(x,0) \\\\ u_t(x,0)\n  \\end{array}\n  &\n  \\begin{array}{l}\n    = \\sin \\frac{\\pi x}{L} +0.5\\sin \\frac{3\\pi x}{L} \\\\ = 0\n  \\end{array} && 0 &lt; x &lt; L\n\\end{aligned}\\]\n\nBy transforming the independent variables (no need to transform \\(u\\)) into a new ones\n\\[ \\xi=\\frac{x}{L}\\; \\text{ and } \\;\\tau=\\frac{c}{L} t \\]\nwe get the new problem\n\\[\n\\begin{aligned}\n  u_{\\tau\\tau} & =u_{\\xi\\xi} && 0 &lt; \\xi &lt; 1,\\;\\; 0 &lt; \\tau &lt; \\infty \\\\[5pt]\n  \\begin{array}{r}\n    u(0,\\tau) \\\\ u(1,\\tau)\n  \\end{array}\n  &\n  \\begin{array}{l}\n    = 0 \\\\ = 0\n  \\end{array} && 0 &lt; \\tau &lt; \\infty \\\\[5pt]\n  \\begin{array}{r}\n    u(\\xi,0) \\\\ u_\\tau(\\xi,0)\n  \\end{array}\n  &\n  \\begin{array}{l}\n    = \\sin \\pi\\xi +0.5\\sin 3\\pi\\xi \\\\ = 0\n  \\end{array} && 0 &lt; \\xi &lt; 1 \\\\  \n\\end{aligned}\\]\nwhich has the solution\n\\[ u(\\xi,\\tau)=\\cos \\pi \\tau \\sin\\pi\\xi +0.5 \\cos 3\\pi \\tau \\sin 3\\pi \\xi \\]\n\n\n\\(~\\)\nExample \\(\\,\\)How could you pick a new space variable \\(\\,\\xi\\,\\) so that \\(\\,v\\) is eliminated in the equation\n\\[ u_t +vu_x=0 \\]\n\\(~\\)",
    "crumbs": [
      "**Second Semester**",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Hyperbolic Partial Differential Equations</span>"
    ]
  },
  {
    "objectID": "ch_x2_Hyperbolic_PDEs.html#sec-x2-27",
    "href": "ch_x2_Hyperbolic_PDEs.html#sec-x2-27",
    "title": "13  Hyperbolic Partial Differential Equations",
    "section": "13.10 \\(~\\)Classification of PDEs (Cannonical Form of the Hyperbolic Equation)",
    "text": "13.10 \\(~\\)Classification of PDEs (Cannonical Form of the Hyperbolic Equation)\n\nThe purpose here is to classify the second-order linear PDE\n\\[ \\color{red}{A}u_{xx} +\\color{red}{B}u_{xy} +\\color{red}{C} u_{yy} +Du_x +Eu_y +Fu = G \\tag{SL}\\label{eq:SL}\\]\nin which \\(A, B, C, D, E, F,\\) and \\(\\,G\\) are, in general, functions of \\(x\\) and \\(y\\) in two independent variables and could be constants, as\n\nHyperbolic if \\(\\,B^2 -4AC &gt; 0~\\) at \\(\\,(x,y)\\) \nParabolic \\(~\\,\\) if \\(\\,B^2 -4AC = 0~\\) at \\(\\,(x,y)\\) \nElliptic \\(~~~~~\\,\\) if \\(\\,B^2 -4AC &lt; 0~\\) at \\(\\,(x,y)\\) \n\nand depending on which is true, \\(\\,\\)to transform the equation into a corresponding cannonical (simple) form by introducing new coordinates \\(\\,\\xi=\\xi(x,y)\\) \\(\\,\\)and \\(\\,\\eta=\\eta(x,y)\\)\n\\[\\begin{aligned}\n  u_{\\xi\\eta} = \\Phi(\\xi,\\eta,u,u_\\xi,u_\\eta)\n  \\; &\\text{ or } \\;\n  u_{\\xi\\xi} -u_{\\eta\\eta} =\n   \\Psi(\\xi,\\eta,u,u_\\xi,u_\\eta) \\\\\n  u_{\\eta\\eta} &= \\Phi(\\xi,\\eta,u,u_\\xi,u_\\eta)\\\\\n  u_{\\xi\\xi} +u_{\\eta\\eta} &= \\Phi(\\xi,\\eta,u,u_\\xi,u_\\eta)\n\\end{aligned}\\]\nThe student should note that whether equation \\(\\eqref{eq:SL}\\) is hyperbolic, parabolic, or elliptic depends only on the coefficients of the second derivatives. \\(\\,\\)We now come to the major portion of this section; \\(\\,\\)rewriting hyperbolic equations in their canonical form\nThe Canonical Form of the Hyperbolic Equation\nThe objective here is to introduce new coordinates \\(\\,\\xi=\\xi(x,y)\\,\\) and \\(\\,\\eta=\\eta(x,y)\\,\\) so that the general PDE contains only one second derivative \\(\\,u_{\\xi\\eta}\\,\\). \\(\\,\\)First of all, \\(\\,\\)we compute the partial derivatives\n\\[\\begin{aligned}\nAu_{xx} +Bu_{xy} +C u_{yy} &+Du_x +Eu_y +Fu = G \\\\\n&\\Downarrow\\;\\;\n\\color{red}{\\xi=\\xi(x,y), \\;\\eta=\\eta(x,y)} \\\\\nu_x =\\; u_\\xi &\\xi_x +u_\\eta \\eta_x \\\\\nu_y =\\; u_\\xi &\\xi_y +u_\\eta \\eta_y \\\\  \nu_{xx} =\\; u_{\\xi\\xi}& \\xi_x^2\n  +2u_{\\xi\\eta}\\xi_x\\eta_x\n  +u_{\\eta\\eta}\\eta_x^2\n  +u_\\xi\\xi_{xx}\n  +u_\\eta\\eta_{xx}\\\\\nu_{xy} =\\;u_{\\xi\\xi}& \\xi_x\\xi_y\n  +u_{\\xi\\eta}(\\xi_x\\eta_y +\\xi_y\\eta_x)\n  +u_{\\eta\\eta}\\eta_x\\eta_y\n  +u_\\xi\\xi_{xy}\n  +u_\\eta\\eta_{xy}\\\\\nu_{yy} =\\;u_{\\xi\\xi}& \\xi_y^2\n  +2u_{\\xi\\eta}\\xi_y\\eta_y\n  +u_{\\eta\\eta}\\eta_y^2\n  +u_\\xi\\xi_{yy}\n  +u_\\eta\\eta_{yy}\\\\[5pt]\n  &\\Downarrow \\\\\n\\end{aligned}\\]\n\\[\n\\begin{aligned}\n  {\\color{red}{\\bar{A}}}u_{\\xi\\xi}\n+{\\color{red}{\\bar{B}}}u_{\\xi\\eta}\n+&{\\color{red}{\\bar{C}} u_{\\eta\\eta}}\n+\\bar{D}u_\\xi\n+\\bar{E}u_\\eta\n+\\bar{F}u = \\bar{G} \\\\[5pt]\n\\text{in which} & \\\\[5pt]\n\\bar{A}=&\\;A\\xi_x^2 +B\\xi_x \\xi_y +C\\xi_y^2 \\\\\n\\bar{B}=&\\;2A\\xi_x\\eta_x\n  +B(\\xi_x \\eta_y+\\xi_y\\eta_x) +2C\\xi_y\\eta_y \\\\\n\\bar{C}=&\\;A\\eta_x^2 +B\\eta_x \\eta_y +C\\eta_y^2 \\\\\n%  \\bar{D}=&\\;A\\xi_{xx} +B\\xi_{xy}\n%    +C\\xi_{yy} +D\\xi_x +E\\xi_y \\\\\n%  \\bar{E}=&\\;A\\eta_{xx} +B\\eta_{xy}\n%    +C\\eta_{yy} +D\\eta_x +E\\eta_y \\\\\n%  \\bar{F}=&\\;F\\\\\n%  \\bar{G}=&\\;G\\\\\n\\Downarrow& \\\\\n\\begin{pmatrix}\n  2\\bar{A}& \\bar{B}\\\\\n  \\bar{B}& 2\\bar{C}\n\\end{pmatrix} =& \\;\n\\begin{pmatrix}\n  \\xi_x & \\xi_y\\\\\n  \\eta_x & \\eta_y\n\\end{pmatrix}\n\\begin{pmatrix}\n  2A & B\\\\\n    B & 2C\n\\end{pmatrix}\n\\begin{pmatrix}\n  \\xi_x & \\xi_y\\\\\n  \\eta_x & \\eta_y\n\\end{pmatrix}^T \\\\\n\\Downarrow& \\\\  \n\\bar{B}^2 -4\\bar{A}\\bar{C}\n  =& \\; (\\xi_x\\eta_y-\\xi_y\\eta_x)^2(B^2 -4AC) \\\\\n  =& \\; J^2(B^2 -4AC)\n\\end{aligned}\\]\nin which \\(\\,J\\,\\) is the Jacobian of the transformation and we select the transformation \\(\\,(\\xi,\\eta)\\,\\) such that \\(\\,J \\neq 0\\)\nThe next step in our process is to set the coefficients \\(\\,\\bar{A}\\,\\) and \\(\\,\\bar{C}\\,\\) to zero and solve for the tranformation \\(\\,\\xi\\,\\) and \\(\\,\\eta\\,\\). \\(\\,\\)This will give us the coordinates that reduce the original PDE to canonical form:\n\\[\\begin{aligned}\n\\bar{A}=&\\;A\\xi_x^2 +B\\xi_x \\xi_y +C\\xi_y^2 =0\\\\\n\\bar{C}=&\\;A\\eta_x^2 +B\\eta_x \\eta_y +C\\eta_y^2 =0\\\\\n&\\Downarrow \\\\\nA\\left[ \\frac{\\xi_x}{\\xi_y} \\right]^2 &+B\\left[ \\frac{\\xi_x}{\\xi_y} \\right] +C=0,\\;\\;\nA\\left[ \\frac{\\eta_x}{\\eta_y} \\right]^2 +B\\left[ \\frac{\\eta_x}{\\eta_y} \\right] +C=0 \\\\\n&\\Downarrow \\\\\n\\left[ \\frac{\\xi_x}{\\xi_y} \\right] =&\\frac{-B +\\sqrt{\\color{blue}{B^2-4AC}}}{2A},\\;\\;\n\\left[ \\frac{\\eta_x}{\\eta_y} \\right] =\\frac{-B -\\sqrt{\\color{blue}{B^2-4AC}}}{2A}\n\\end{aligned}\\]\nWe have now reduced the problem to finding the two functions \\(\\,\\xi(x,y)\\,\\) and \\(\\,\\eta(x,y)\\,\\) so that their ratio \\(\\,[\\xi_x/\\xi_y]\\,\\) and \\(\\,[\\eta_x/\\eta_y]\\,\\) satisfy the above equation. \\(\\,\\)Finding functions satisfying these conditions is really quite easy once we look for a few moments at figure\n\n\n\n\n\n\\[\\begin{aligned}\n  &\\big\\Downarrow \\;\\;{\\scriptsize \\xi = \\text{constant}, \\eta = \\text{constant}}\\\\\n  \\frac{dy}{dx} &=-\\left[ \\frac{\\xi_x}{\\xi_y} \\right]=\\frac{B -\\sqrt{B^2-4AC}}{2A} =g_1(x,y)\\\\\n  \\frac{dy}{dx} &=-\\left[ \\frac{\\eta_x}{\\eta_y} \\right]=\\frac{B +\\sqrt{B^2-4AC}}{2A} =g_2(x,y)\\\\\n  &\\Downarrow \\;\\;{\\scriptsize \\text{integration and arrangement}}\\\\\n  G_1(&x,y) = c_1 \\\\\n  G_2(&x,y) = c_2 \\\\\n  &\\Downarrow \\\\\n  \\xi &=G_1(x,y) \\\\\n  \\eta &=G_2(x,y)\n\\end{aligned}\\]\n\n\\(~\\)\nExample \\(\\,\\)Consider the simple equation\n\\[u_{xx} -4u_{yy} +u_x =0,\\;\\;\\;B^2 -4AC=16&gt;0\\]\nwhose characteristic equations are\n\\[\\begin{aligned}\n\\frac{dy}{dx} &=-\\left[ \\frac{\\xi_x}{\\xi_y} \\right]=\\frac{B -\\sqrt{B^2-4AC}}{2A} =-2\\\\\n\\frac{dy}{dx} &=-\\left[ \\frac{\\eta_x}{\\eta_y} \\right]=\\frac{B +\\sqrt{B^2-4AC}}{2A} =2\\\\\n\\end{aligned}\\]\nTo find \\(\\xi\\) and \\(\\eta\\), \\(\\,\\)we first integrate for \\(x\\) and solve for the constants \\(c_1\\) and \\(c_2\\), \\(\\,\\)getting\n\\[\\begin{array}{l}\n    y =-2x+c_1 \\\\\n    y = 2x+c_2 \\\\\n  \\end{array}\n  \\;\\;\\Rightarrow\\;\\;\n  \\begin{array}{l}\n   \\xi =y+2x=c_1 \\\\\n   \\eta =y-2x=c_2\n\\end{array}\\]\n\n\n\n\n\n\\(~\\)\nExample \\(\\,\\)Suppose we start with the equation\n\\[y^2u_{xx} -x^2 u_{yy}=0,\\;\\;\\;x&gt;0,\\;y&gt;0\\]\nwhich is a hyperbolic equation in the first quadrant. \\(\\,\\)We consider the problem of finding new coordinates that will change the original equation to canonical form for \\(\\,x\\,\\) and \\(\\,y\\,\\) in the first quadrant\nSTEP 1 \\(\\,\\) Solve the two characteristic equations\n\\[\\begin{aligned}\n\\frac{dy}{dx} &=-\\left[ \\frac{\\xi_x}{\\xi_y} \\right]=\\frac{B -\\sqrt{B^2-4AC}}{2A} =-\\frac{x}{y}\\\\\n\\frac{dy}{dx} &=-\\left[ \\frac{\\eta_x}{\\eta_y} \\right]=\\frac{B +\\sqrt{B^2-4AC}}{2A} =\\frac{x}{y}\\\\\n\\end{aligned}\\]\nIntegrating these two equations by the ODE technique of separating variables gives the implicit relationship\n\\[\\begin{aligned}\ny^2 &-x^2 = c_1 \\\\\ny^2 &+x^2 = c_2 \\\\\n&\\Downarrow \\\\\n\\xi= \\,&y^2 -x^2 \\\\\n\\eta = \\,&y^2 +x^2\n\\end{aligned}\\]\n\n\n\n\n\nWe substitute this new coordinate into the equation to find the new equation\n\\[\\begin{aligned}\n  y^2 u_{xx} &-x^2 u_{yy}= 0\\\\\n  &\\Downarrow\\;\\; \\xi=y^2 -x^2,\\; \\eta=y^2 +x^2 \\\\\n  -16x^2y^2 u_{\\xi\\eta} -2(x^2 +y^2) &u_\\xi +2(y^2 -x^2) u_\\eta=0\n\\end{aligned}\\]\nSTEP 2 \\(\\,\\) Finally, \\(\\,\\)solving for \\(\\,x\\,\\) and \\(\\,y\\,\\) in terms of \\(\\,\\xi\\,\\) and \\(\\,\\eta\\), \\(\\,\\)we get\n\\[ u_{\\xi\\eta}=\\frac{\\eta u_\\xi -\\xi u_\\eta}{2(\\xi^2 -\\eta^2)}\\]\n\\(~\\)\nExample \\(\\,\\)Find the new characteristic coordinates for\n\\[u_{xx} + 4u_{xy} = 0\\]\nSolve the transformed equation in the new coordinate system and then transform back to the original coordinates to find the solution to the original problem\n\\(~\\)\nNOTES\n\nThe general hyperbolic equation actually has two canonical forms; \\(\\,\\)the other one can be found by making yet another transformation\n\\[\\begin{aligned}\n  \\alpha &=\\xi +\\eta \\\\\n  \\beta &=\\xi -\\eta\n\\end{aligned}\\]\nand rewriting the first canonical form in terms of \\(\\,\\alpha\\,\\) and \\(\\,\\beta\\). \\(\\,\\)The partial derivatives are transformed as follows:\n\\[\\begin{aligned}\n  u_\\xi&= u_\\alpha \\alpha_\\xi +u_\\beta\n   \\beta_\\xi=u_\\alpha +u_\\beta\\\\\n  u_\\eta&= u_\\alpha \\alpha_\\eta +u_\\beta\n   \\beta_\\eta=u_\\alpha -u_\\beta\\\\\n  u_{\\xi\\eta}&= u_{\\alpha\\alpha}\\alpha_\\eta\n   +u_{\\alpha\\beta}\\beta_\\eta\n    +u_{\\beta\\alpha}\\alpha_\\eta +u_{\\beta\\beta}\n     \\beta_\\eta =u_{\\alpha\\alpha} -u_{\\beta\\beta}\n\\end{aligned}\\]\nIn the general parabolic equation, \\(\\,\\)we obtain only a single canonical transformation as\n\\[ \\bar{A}=0, \\;\\;\\frac{dy}{dx}=-\\left [ \\frac{\\xi_x}{\\xi_y} \\right ],\\;\\;\\;\\text{or}\\;\\;\\; \\bar{C}=0, \\;\\;\\frac{dy}{dx}=-\\left [ \\frac{\\eta_x}{\\eta_y} \\right ]\\]\nSuppose \\(\\,\\xi\\,\\) is calculated. \\(\\,\\)What could be the possible choice for \\(\\,\\eta\\,\\)? \\(\\,\\)We can select \\(\\,\\eta\\,\\) as any arbitrary function of \\(\\,x\\,\\) and \\(\\,y\\,\\) such that \\(\\,\\eta\\,\\) is independent of \\(\\,\\xi\\,\\) (\\(J \\neq 0\\))\nIn the general elliptic case, \\(\\,A\\lambda^2 +B\\lambda +C=0\\,\\) leads to complex conjugate canonical tramsformation \\(\\,\\xi\\,\\) and \\(\\,\\eta\\). \\(\\,\\)Since \\(\\,\\xi\\,\\) and \\(\\,\\eta\\,\\) are complex, \\(\\,\\)we introduce new real variables\n\\[\\begin{aligned}\n   \\alpha &= \\frac{1}{2}(\\xi +\\eta)\\\\\n   \\beta &= \\frac{1}{2i}(\\xi -i\\eta)\n  \\end{aligned}\\]\nUnder the transformation \\(\\,(x, y)\\to (\\alpha, \\beta)\\), \\(\\,\\)the canonical form is given by\n\\[ u_{\\alpha\\alpha} +u_{\\beta\\beta}=\\Phi(\\alpha,\\beta,u,u_\\alpha,u_\\beta)\\]\nThe three major classifications of linear PDEs as hyperbolic, parabolic, and elliptic equations essentially classify physical problems into three basic physical types:\n\nWave propagation\nDiffusion\nSteady-state problems\n\nThe mathematical solutions of these three types of equations are quite different\n\n\\(~\\)",
    "crumbs": [
      "**Second Semester**",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Hyperbolic Partial Differential Equations</span>"
    ]
  },
  {
    "objectID": "ch_x2_Hyperbolic_PDEs.html#sec-x2-28",
    "href": "ch_x2_Hyperbolic_PDEs.html#sec-x2-28",
    "title": "13  Hyperbolic Partial Differential Equations",
    "section": "13.11 \\(~\\)First-Order Equations (Method of Characteristics)",
    "text": "13.11 \\(~\\)First-Order Equations (Method of Characteristics)\n\nIf the student recalls, \\(\\,\\)when we solved the diffusion equation\n\\[ u_t=\\alpha u_{xx} - vu_x, \\quad -\\infty &lt; x &lt; \\infty, \\;\\; 0 &lt; t &lt; \\infty\\]\nthe constant \\(\\,\\alpha\\,\\) stood for the amount of diffusion, \\(\\,\\)while \\(v\\) stood for the velocity of the medium\nHence if \\(\\,\\alpha=0\\) (no diffusion), \\(\\,\\)it is clear (since we only have convection) that the solution will travel along the \\(\\,x\\)-axis with velocity \\(\\,v\\). \\(\\,\\)In other words, \\(\\,\\)if the initial solution is \\(\\,u(x,0)=\\phi(x)\\), \\(\\,\\)then the solution to\n\\[u_t=-vu_x\\]\nwould be \\(\\,u(x,t)=\\phi(x-vt)\\)\nSo we can think of the first-order equation\n\\[a(x,t)u_x +b(x,t)u_t = 0\\]\nas the concentration along a stream where the velocity is given by\n\\[ v=\\frac{a(x,t)}{b(x,t)}\\]\nThe purpose of this section is to introduce the notion of first-order partial differential equations and method of characteristics. \\(\\,\\)The problem we will solve is the initial-value problem\n\\[ a(x,t) u_x +b(x,t) u_t +c(x,t)u=0 \\]\n\\[u(x,0)=\\phi(x), \\;\\; {-\\infty&lt;x&lt;\\infty,\\;\\; 0&lt; t &lt; \\infty}\\]\nThe solution to this equation is based on a physical fact, \\(\\,\\)that an initial disturbance at some point \\(\\,x\\,\\) propagates along a line (curve) in the \\(\\,tx\\)-plane (called a characteristic)\n\n\n\n\n\nWith this in mind, \\(\\,\\)the idea is to introduce two new coordinates \\(\\,s\\,\\) and \\(\\,\\tau\\) (to replace \\(\\,x\\,\\) and \\(\\,t\\)) that have the properties\n\n\\(s\\,\\) will change along the characteristic curves\n\\(\\tau\\,\\) will change along the initial curve (most likely the line \\(\\,t=0\\))\n\n\\[\\begin{aligned}\n  a(x,t) u_x +\\, & b(x,t) u_t +c(x,t)u = 0\\\\\n  &\\big\\Downarrow\\;\\; {\\scriptsize\\text{characteristic curves } \\{ \\left( x(s), t(s) \\right) : 0&lt;s&lt;\\infty \\}} \\\\\n  \\frac{dx}{ds} = a(x,t), &\\;\n  \\frac{dt}{ds} = b(x,t) \\\\\n  &\\Downarrow \\\\\n  \\frac{du}{ds}= u_x \\frac{dx}{ds} +u_t\\frac{dt}{ds}&=a(x,t) u_x +b(x,t) u_t \\\\\n  &\\Downarrow \\\\\n  \\frac{du}{ds} +\\, &c(x,t)u =0\n\\end{aligned}\\]\n\n\\(~\\)\nExample \\(\\,\\)Solve the following IVP with constant coefficients:\n\\[\n\\begin{aligned}\n  u_x +u_t +2u &=0 && -\\infty &lt; x &lt; \\infty,\\;\\; 0 &lt; t &lt; \\infty \\\\\n  u(x,0) &=\\sin x && -\\infty &lt; x &lt; \\infty\n\\end{aligned}\\]\nSTEP 1 \\(\\,\\) Find the characteristics (along which the initial data propagate)\n\\[\\begin{aligned}\n  \\frac{dx}{ds}=1,\\;\\;\n   \\frac{dt} {ds}&=1\\quad 0&lt;s&lt;\\infty \\\\\n  &\\Downarrow \\\\\n  x(s) = &\\;s +c_1 \\\\\n  t(s) = &\\;s +c_2 \\\\\n  &\\Downarrow \\;\\;x(0)= \\tau, \\;\\;t(0)=0 \\\\\n  x = &\\;s +\\tau \\\\\n  t = &\\;s \\\\\n  &\\Downarrow \\\\\n  x - t = \\tau, \\;\\;\\;&-\\infty &lt; \\tau &lt; \\infty\n\\end{aligned}\\]\n\n\n\n\n\nSTEP 2 \\(\\,\\) Using this coordinate system, \\(\\,\\)we change the PDE to the ODE\n\\[\\begin{aligned}\n\\frac{du}{ds} +2u&= 0, \\;\\; u(0)= \\sin\\tau \\\\\n&\\Downarrow \\\\\nu(s,\\tau) &=\\sin \\tau \\cdot e^{-2s}\n\\end{aligned}\\]\nSTEP 3 \\(\\,\\) We now solve the transformation for \\(\\,s\\,\\) and \\(\\,\\tau\\,\\) in terms of \\(\\,x\\,\\) and \\(\\,t\\):\n\\[u(x,t) = \\sin(x -t) \\cdot e^{-2t}\\]\n\n\n\n\n\n\\(~\\)\nExample \\(\\,\\)Solve the following IVP with variable coefficients:\n\\[\n\\begin{aligned}\n  x u_x +u_t +tu &=0 && -\\infty &lt; x &lt; \\infty,\\; 0 &lt; t &lt; \\infty \\\\\n  u(x,0) &=F(x) && -\\infty &lt; x &lt; \\infty, \\;\\;\\text{an arbitrary initial wave}\n\\end{aligned}\\]\nSTEP 1 \\(\\,\\) Find the characteristics (along which the initial data propagate)\n\\[\\begin{aligned}\n  \\frac{dx}{ds} =x,\\;\\;&\\frac{dt}{ds} =1\n    \\quad 0&lt;s&lt;\\infty \\\\\n  &\\Downarrow \\\\\n  x(s) &=\\; c_1 e^s\\\\\n  t(s) &=\\; s +c_2 \\\\\n  &\\Downarrow \\;\\; x(0)= \\tau, \\;\\;t(0)=0 \\\\\n  x &=\\;\\tau e^s \\\\\n  t &=\\;s\n\\end{aligned}\\]\nSTEP 2 \\(\\,\\) Using this coordinate system, \\(\\,\\)we change the PDE to the ODE\n\\[\\begin{aligned}\n  \\frac{du}{ds} +su&= 0, \\;\\; u(0)= F(\\tau) \\\\\n  &\\Downarrow \\\\\n  u(s,\\tau) &=F(\\tau) \\cdot e^{-s^2/2}\n\\end{aligned}\\]\nSTEP 3 \\(\\,\\) We now solve the transformation for \\(\\,s\\,\\) and \\(\\,\\tau\\,\\) in terms of \\(\\,x\\,\\) and \\(\\,t\\):\n\\[u(x,t) = F(xe^{-t}) \\cdot e^{-t^2/2}\\]\n\\(~\\)\nExample \\(\\,\\) Solve the problem in higher dimensions (surface waves)\n\\[\n\\begin{aligned}\n   a u_x +b u_y +c u_t +du &=0 && \\scriptsize -\\infty &lt; x &lt; \\infty, \\;-\\infty &lt; y &lt; \\infty, \\; 0&lt; t &lt; \\infty \\\\\n   u(x,y,0) &=e^{-(x^2 +y^2)} && \\scriptsize -\\infty &lt; x &lt; \\infty, \\;-\\infty &lt; y &lt; \\infty\n\\end{aligned}\\]\n\\(~\\)",
    "crumbs": [
      "**Second Semester**",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Hyperbolic Partial Differential Equations</span>"
    ]
  },
  {
    "objectID": "ch_x2_Hyperbolic_PDEs.html#sec-x2-29",
    "href": "ch_x2_Hyperbolic_PDEs.html#sec-x2-29",
    "title": "13  Hyperbolic Partial Differential Equations",
    "section": "13.12 \\(~\\)Nonlinear First-Order Equations (Conservation Equations)",
    "text": "13.12 \\(~\\)Nonlinear First-Order Equations (Conservation Equations)\n\nOne of the most useful partial differential equations in mathematics is the conservation equation\n\\[u_t +f_x=0\\]\nThis equation says that the increase of a physical quantity \\(\\,u_t\\,\\) is equal to the change in flux \\(\\,-f_x\\,\\) of that quantity across the boundary\nDerivation of the Conservation Equation\nSuppose we have a stretch of highway on which cars are moving from left to right, \\(\\,\\)and we assume there are no exit and entrance ramps. \\(\\,\\)We let\n\\[\n\\begin{aligned}\n  u(x,t) &=\\text{density of cars at } x\\;\\;\n    (\\text{cars per unit length at } x) \\\\[5pt]\n  f(x,t) &=\\text{flux of cars at } x \\;\\;\n    (\\text{cars per minute passing the point } x)\n\\end{aligned}\\]\nThen, \\(\\,\\)it is fairly obvious that for a road segment \\(\\,[a,b]\\), \\(\\,\\)the change in the number of cars (with respect to time) is given by both of the following expressions:\n\\[\\begin{aligned}\n  \\text{change in the number of cars in } [a,b]\n    &= \\frac{d}{dt} \\int_a^b u(x,t)\\,dx \\\\\n  \\text{change in the number of cars in } [a,b]\n    &= f(a,t) -f(b,t)\n    = -\\int_a^b \\frac{\\partial f}{\\partial x}(x,t)\\,dx\n  \\end{aligned}\\]\nSetting these two integrals equal to each other, \\(\\,\\)we have\n\\[ \\int_a^b \\frac{\\partial u}{\\partial t}(x,t)\\,dx = -\\int_a^b \\frac{\\partial f}{\\partial x}(x,t)\\,dx\\]\nFinally, \\(\\,\\)since the integral \\(\\,[a,b]\\,\\) was arbitrary, \\(\\,\\)we have the one-dimensional conservation equation\n\\[u_t +f_x=0\\]\nIf we carried out a similar analysis in two or three dimensions, \\(\\,\\)we would have\n\\[u_t +f_x +f_y=0, \\;\\; u_t +f_x +f_y +f_z=0\\]\n\n\\(~\\)\n\nConservation Equation Applied to the Traffic Problem\n\nWe are now ready to see how the conservation equation \\(\\,u_t +f_x=0\\,\\) can be applied to the traffic problem\nTo do this, \\(\\,\\)we rewrite the change in flux \\(\\,f_x\\,\\) as\n\\[f_x = \\frac{df}{du} u_x,\\;\\;\\text{chain rule}\\]\nand substitute it in the conservation equation. \\(\\,\\)Doing this, \\(\\,\\)we arrive at\n\\[ u_t +\\frac{df}{du}u_x=u_t+g(u)u_x=0\\]\nwhere we can imagine a car moving upstream or downstream with velocity \\(\\,g(u)\\)\nHence, after \\(\\,t\\,\\) minutes, the position \\(\\,x\\,\\) of the car will be\n\\[x = x_0 +g(u)t, \\quad\\text{characteristic equation}\\]\nHence, after \\(\\,t\\,\\) minutes, the position \\(\\,x\\,\\) of the car will be\n\\[x = x_0 +g(u)t, \\quad\\text{characteristic equation}\\]\nRemembering that the concentration \\(\\,u(x,t)\\,\\) will not change along each characteristic, \\(\\,\\)if we know the initial density \\(\\,u(x_0,0)\\), then the characteristic equation for \\(\\,x\\,\\) can be written\n\\[x=x_0+g[u(x_0,0)]t\\]\nIt is clear that by knowing these characteristic curves starting from each point (and knowing the solution doesn’t change along these curves), \\(\\,\\)we can piece together the solution \\(\\,u(x,t)\\,\\) for all time \\(\\,t\\). \\(\\,\\)We won’t actually find the explicit equation for the solution \\(\\,u(x,t)\\,\\) in terms of \\(\\,x\\,\\) and \\(\\,t\\,\\) but will use our knowledge of the characteristics of the equation to solve some interesting problems\n\n\n\n\n\n\n\n\\(~\\)\n\nTraffic-Flow Problem\n\nWe will now solve a traffic-flow problem where the flux is given by \\(\\,f(u)=u^2\\) and the initial density of cars is given. \\(\\,\\)In other words, \\(\\,\\)we have\n\n\\[ u_t +2uu_x =0 \\quad -\\infty &lt; x &lt; \\infty,\n\\; 0&lt; t &lt; \\infty\\]\n\\[u(x,0) =\n\\begin{cases}\n\\;\\;\\; 1 & x \\leq 0\\\\\n1 -x     & 0 &lt; x &lt; 1, \\;-\\infty &lt; x &lt;\\infty \\\\\n\\;\\;\\; 0 & 1 \\leq x\n\\end{cases}\\]\n\n\n\n\n\n\nSTEP 1\nWe begin by finding the characteristics starting from each initial point \\(\\,(x_0,0)\\). \\(\\,\\)For \\(\\,x_0 &lt; 0\\), \\(\\,\\)the characteristics are\n\\[\\begin{aligned}\n  x &= x_0 +g[u(x_0,0)] t \\\\\n  &= x_0 +g[1] t\\\\\n  &= x_0 +2t \\\\\n  &\\Downarrow \\\\\n  t &=\\frac{1}{2} (x -x_0)\n\\end{aligned}\\]\nNow consider those initial points \\(\\,0 &lt; x_0 &lt; 1\\). \\(\\,\\)Here, \\(\\,\\)the characteristic curves are\n\\[\\begin{aligned}\nx &= x_0 +g[u(x_0,0)] t \\\\\n&= x_0 +g[1 -x_0] t\\\\\n&= x_0 +2(1 -x_0)t \\\\\n&\\Downarrow \\\\\nt &=\\frac{x -x_0}{2(1 -x_0)}\n\\end{aligned}\\]\nFinally, \\(\\,\\)for \\(\\,1 \\leq x_0 &lt; \\infty\\), \\(\\,\\)we have the characteristics\n\\[\\begin{aligned}\nx &= x_0 +g[u(x_0,0)] t \\\\\n&= x_0 +g[0] t\\\\\n&= x_0\n\\end{aligned}\\]\nwhich represents vertical lines starting at \\(\\,x_0\\)\n\n\n\n\n\nThe solution remains constant along each characteristic line. \\(\\,\\)Now the solution for \\(\\,x \\in \\mathbb{R}\\), \\(\\,0 \\leq t &lt;\\textstyle\\frac{1}{2}\\,\\) is given by\n\\[u(x,t)=\n\\begin{cases}\n\\quad 1 & \\; x \\leq 2t,\\qquad 0 \\leq t &lt; \\frac{1}{2}\\\\\n\\displaystyle\\frac{1 -x}{1 -2t} & 2t \\leq x &lt; 1, \\;\\,\n   0 \\leq t &lt; \\frac{1}{2} \\\\\n\\quad 0 & \\; 1 \\leq x, \\qquad \\; 0 \\leq t &lt; \\frac{1}{2}\n\\end{cases}\\]\nThe difficulty in this example is that some characteristic lines will intersect each other. \\(\\,\\)In other words, \\(\\,\\)at the intersection point \\(\\,(1,\\textstyle \\frac{1}{2})\\,\\) of the two characteristic lines, \\(\\,\\)the solution becomes multi-valued. \\(\\,\\)This pathology means that the solution in the usual sense fails to exist after \\(\\,t \\geq \\textstyle \\frac{1}{2}\\)\n\n\n\n\n\nSTEP 2\n\nHence, \\(\\,\\)to find the solution past \\(\\,t=\\textstyle \\frac{1}{2}\\), \\(\\,\\)we must use another type of analysis. \\(\\,\\)When characteristics run together, \\(\\,\\)we have the phenomenon of shock waves (discontinuous solutions), \\(\\,\\)and what we must ask is, \\(\\,\\)how fast does the leading edge of the shork wave propagate along the road?\nBefore proceeding further, \\(\\,\\)it is necessary to introduce the concept of a jump condition: a condition that holds at a discontinuity or abrupt change. \\(\\,\\)Consider one dimensional problem where there is a jump in the conserved quantity \\(\\,u\\):\n\\[u_t +f_x=0\\]\nLet the solution exhibit a jump (or shork) at \\(\\,x=x_s(t)\\,\\) and integrate over the partial domain, \\(\\,x \\in (x_1, x_2)\\), \\(\\,\\)where \\(\\,x_1 &lt; x_s(t) &lt; x_2\\),\n\\[\\scriptsize \\begin{aligned}\n\\frac{d}{dt} \\left( \\int_{x_1}^{x_s(t)}u\\,dx +\\int_{x_s(t)}^{x_2}u \\,dx\\right)&= -\\int_{x_1}^{x_2}\n  \\frac{\\partial } {\\partial x} f(u)\\,dx\\\\\n&\\Downarrow \\\\\n  \\int_{x_1}^{x_s(t)} u_t \\,dx +u^-(x_s(t),t)\\frac{dx_s}{dt} +\\int_{x_s(t)}^{x_2} u_t \\,dx\n  &-u^+(x_s(t),t)\\frac{dx_s}{dt}= -\\left. \\underset{}{f(u)}\\, \\right|_{x_1}^{x_2}\\\\\nx_1 \\to x_s(t),\\;x_2 \\to x_s(t) \\;\\; &\\Downarrow \\\\\n\\left[ u^-(x_s(t),t) -u^+(x_s(t),t) \\right] \\frac{d x_s}{dt} &= f(u^-(x_s(t),t)) -f(u^+(x_s(t),t)) \\\\\n&\\Downarrow \\\\\ns=\\frac{d x_s}{dt} &=\\frac{f(u^-(x_s(t),t)) -f(u^+(x_s(t),t))}{ u^-(x_s(t),t) -u^+(x_s(t),t)}\n\\end{aligned}\\]\nin which \\(\\,s\\,\\) is the speed of the discontinuity and represents the jump condition for conservation eqaution\nA shork situation arises in a system where its characteristics intersect, \\(\\,\\)and under these conditions a requirement for a unique single-valued solution is that the solution should satisfy the Lax entropy condition\n\\[f'(u^+(x_s(t),t)) &lt; s &lt; f'(u^-(x_s(t),t))\\]\nwhere \\(f'(u^+(x_s(t),t))\\,\\) and \\(\\,f'(u^-(x_s(t),t))\\,\\) represent characteristic speeds at upstream and downstream conditions, respectively\nSo, \\(\\,\\)in our example, \\(\\,\\)the speed of the wave front would be(remembering that \\(\\,f(u)=u^2\\))\n\\[ s = \\frac{1 -0}{1 -0}=1\\]\nTherefore, \\(\\,\\)a shock is formed at \\((x,t)=(1,\\textstyle\\frac{1}{2})\\): \n\nAfter that, \\(\\,\\)it moves with constant speed \\(1\\), \\(\\,\\)along the line \\(\\,x=t +\\textstyle \\frac{1}{2}\\), \\(\\,\\)for \\(\\,t \\geq \\textstyle \\frac{1}{2}\\).\nIn conclusion, \\(\\,\\)the solution takes different forms in the three different regions in the space-time plane\n\n\n\\[u(x,t)=\n\\begin{cases}\n\\quad 1 & x \\leq 2t, \\qquad\\; 0 \\leq t &lt; \\frac{1}{2} \\;\\;\\text{or}\\;\\; x-\\frac{1}{2} \\leq t, \\;\\;t \\geq \\frac{1}{2}\\\\\n\\displaystyle\\frac{1 -x}{1 -2t} & 2t \\leq x &lt; 1, \\;\\, 0 \\leq t &lt; \\frac{1}{2} \\\\\n\\quad 0 & x \\geq 1, \\qquad\\;\\; 0 \\leq t &lt; \\frac{1}{2} \\;\\;\\text{or}\\;\\; x-\\frac{1}{2} &gt; t, \\;\\;t \\geq \\frac{1}{2}\n\\end{cases}\\]\n\\(~\\)\nExample \\(\\,\\) Solve\n\\[\n\\begin{aligned}\n  u_t +uu_x &=0 \\qquad -\\infty &lt; x &lt; \\infty, \\; 0 &lt; t &lt; \\infty \\\\\n  u(x,0) &=\n    \\begin{cases}\n      3(1 -|x|) & |x| \\leq 1\\\\\n      \\quad\\; 0 & |x| &gt; 1\n    \\end{cases}  \n\\end{aligned}\\]\n\\(~\\)\nExample \\(\\,\\) Verify that \\(\\,u=\\phi[x-g(u)t]\\,\\) is an implicit solution of the nonlinear problem\n\\[\n\\begin{aligned}\n  u_t +g(u)u_x &=0 \\quad -\\infty &lt; x &lt; \\infty, \\; 0 &lt; t &lt; \\infty \\\\\n  u(x,0) &= \\phi(x)\n\\end{aligned}\\]\n\\(~\\)",
    "crumbs": [
      "**Second Semester**",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Hyperbolic Partial Differential Equations</span>"
    ]
  },
  {
    "objectID": "ch_x2_Hyperbolic_PDEs.html#sec-x2-30",
    "href": "ch_x2_Hyperbolic_PDEs.html#sec-x2-30",
    "title": "13  Hyperbolic Partial Differential Equations",
    "section": "13.13 \\(~\\)Systems of PDEs",
    "text": "13.13 \\(~\\)Systems of PDEs\n\nIn many areas of science, \\(\\,\\)several unknown quantities (and their derivatives) are modeled by a system of interlocking equations. \\(\\,\\)And we seek to find all of these functions simultaneously\nThere are other reasons for studying systems of equations. \\(\\,\\)Although things are not quite so simple in PDE theory, \\(\\,\\)it is often possible to write a single higher-order PDE as a system of first-order equations\nFor example, \\(\\,\\)the telegraph equation\n\\[ u_{tt}=c^2u_{xx} +au_t +bu \\]\ncan be rewritten in equivalent form as three first-order PDEs by introducing the three variables:\n\\[\\begin{aligned}\n  u_1 &= u \\\\\n  u_2 &= u_x \\\\\n  u_3 &= u_t\n\\end{aligned}\\]\nIt is a simple matter to see that the telegraph equation is equivalent to the three equations:\n\\[\\begin{aligned}\n\\frac{\\partial u_1}{\\partial x} &= u_2 \\\\[2pt]\n\\frac{\\partial u_2}{\\partial t} &= u_3 \\\\[2pt]\n\\frac{\\partial u_3}{\\partial t} &= c^2\n  \\frac{\\partial u_2}{\\partial x} +au_3 +bu_1\n\\end{aligned}\\]\nSolution of the Linear System \\(\\,\\mathbf{u}_t +\\mathbf{A}\\mathbf{u}_x=\\mathbf{0}\\)\n\nConsider the initial-value problem consisting of two PDEs and two ICs\n\\[\\begin{aligned}\n  \\frac{\\partial u_1}{\\partial t} +8\n    &\\frac{\\partial u_2}{\\partial x} = 0 \\\\[2pt]\n  \\frac{\\partial u_2}{\\partial t} +2\n    &\\frac{\\partial u_1}{\\partial x} = 0 \\\\[5pt]\n  u_1(x,0)& = f(x) \\\\\n  u_2(x,0)& = g(x)\n\\end{aligned}\\]\nWe start by writing the two PDEs in matrix form\n\\[\\begin{aligned}\n  \\begin{bmatrix}\n  \\frac{\\partial u_1}{\\partial t} \\\\\n  \\frac{\\partial u_2}{\\partial t}\n  \\end{bmatrix}\n+&\\begin{bmatrix}\n    0 & 8 \\\\ 2 & 0\n  \\end{bmatrix}\n  \\begin{bmatrix}\n  \\frac{\\partial u_1}{\\partial x} \\\\\n  \\frac{\\partial u_2}{\\partial x}\n  \\end{bmatrix}\n  = \\begin{bmatrix} 0 \\\\ 0 \\end{bmatrix} \\\\\n\\end{aligned}\\]\n\\[\\begin{aligned}\n&\\Downarrow \\\\[2pt]\n  \\mathbf{u}_t +\n  &\\mathbf{A}\\mathbf{u}_x = \\mathbf{0} \\\\[2pt]\n&\\Downarrow \\;\\;\n  \\mathbf{u}=\\mathbf{P}\\mathbf{v}\\\\[2pt]\n  \\mathbf{v}_t +&\\mathbf{P}^{-1}\n  \\mathbf{A}\\mathbf{P}\\mathbf{v}_x\n  = \\mathbf{0} \\\\[2pt]\n&\\Downarrow \\\\[2pt]\n  \\mathbf{v}_t +\n  &\\boldsymbol{\\Gamma}\\mathbf{v}_x = \\mathbf{0}\n\\end{aligned}\\]\nwhere\n\\[\\mathbf{P}=\n\\begin{bmatrix}\n2 & -2 \\\\\n1 & \\;\\;\\,1\n\\end{bmatrix},\\;\\;\n\\boldsymbol{\\Gamma}=\n\\begin{bmatrix}\n4 & \\;\\;\\,0 \\\\\n0 & -4\n\\end{bmatrix}\\qquad\\]\nThese are the two uncoupled equations that we can solve independently\n\\[\\begin{aligned}\n  &\\frac{\\partial v_1}{\\partial t} +4\\frac{\\partial v_1}{\\partial x} = 0 \\\\\n  &\\frac{\\partial v_2}{\\partial t} -4\\frac{\\partial v_2}{\\partial x} = 0\n\\end{aligned}\\]\nThese equations have travelling-wave solutions\n\\[\\begin{aligned}\nv_1(x,t) &= \\phi(x -4t) \\\\\nv_2(x,t) &= \\psi(x +4t)\n\\end{aligned}\\]\nwhere \\(\\,\\phi\\,\\) and \\(\\,\\psi\\,\\) are arbitrary differentiable functions\nTo obtain our general solution \\(\\,u\\), \\(\\,\\)we merely compute\n\\[\\mathbf{u}=\\mathbf{P}\\mathbf{v}=\n\\begin{bmatrix}\n  2\\phi(x -4t) -2\\psi(x+4t)\\\\\n  \\phi(x -4t) +\\psi(x+4t)\n\\end{bmatrix}\\]\nWe now substitute general solution into the ICs to get\n\\[\\begin{aligned}\n2\\phi(x) &-2\\psi(x) = f(x) \\\\\n\\phi(x) &+\\psi(x) = g(x) \\\\\n&\\Downarrow \\\\\n\\phi(x) &= \\frac{1}{4} \\left[ f(x) +2g(x) \\right] \\\\\n\\psi(x) &= \\frac{1}{4} \\left[ 2g(x) -f(x) \\right]\n\\end{aligned}\\]\nand, \\(\\,\\)hence, \\(\\,\\)the solution to IVP is\n\\[\\begin{aligned}\nu_1(x,t) &= \\frac{1}{2} \\left[ f(x -4t) +2g(x -4t) \\right] -\\frac{1}{2}\\left[ 2g(x +4t) -f(x +4t) \\right]\\\\\nu_2(x,t) &= \\frac{1}{4} \\left[ f(x -4t) +2g(x -4t) \\right] +\\frac{1}{4}\\left[ 2g(x +4t) -f(x +4t) \\right]\n\\end{aligned}\\]\n\n\n\\(~\\)\nExample \\(\\,\\) What is the general solution of the system\n\\[\\begin{aligned}\n&\\frac{\\partial u_1}{\\partial t} +\\frac{\\partial u_1}{\\partial x} +\\frac{\\partial u_2}{\\partial x} = 0\\\\\n&\\frac{\\partial u_2}{\\partial t} +4\\frac{\\partial u_1}{\\partial x} +\\frac{\\partial u_2}{\\partial x} = 0\n\\end{aligned}\\]\n\\(~\\)",
    "crumbs": [
      "**Second Semester**",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Hyperbolic Partial Differential Equations</span>"
    ]
  },
  {
    "objectID": "ch_x2_Hyperbolic_PDEs.html#worked-exercises",
    "href": "ch_x2_Hyperbolic_PDEs.html#worked-exercises",
    "title": "13  Hyperbolic Partial Differential Equations",
    "section": "Worked Exercises",
    "text": "Worked Exercises\n1. \\(~\\) Find \\(u(0,0,0,t)\\)\n\\[\n\\begin{aligned}\nu_{tt} &= \\nabla^2 u, \\;\\; t &gt; 0\\\\\n&u(x, y, z, 0) = 0, \\;\nu_t(x, y, z, 0) = x^2 +y^2 +z^2\n\\end{aligned}\n\\]\nSolution\nStep 1: \\(~\\) Use the Kirchhoff’s formula (solution to 3D wave equation)\nFor the 3D wave equation (with wave speed \\(c= 1\\)), the solution with initial conditions \\(u(\\mathbf{x},0) = f(\\mathbf{x})\\), \\(u_t(\\mathbf{x},0) = g(\\mathbf{x})\\) is:\n\\[u(\\mathbf{x},t) = \\frac{1}{4\\pi t} \\int_{|\\xi - \\mathbf{x}| = t} g(\\xi) \\, dS(\\xi) + \\frac{1}{4\\pi t^2} \\int_{|\\xi - \\mathbf{x}| = t} f(\\xi) \\, dS(\\xi) \\]\nwhere the integrals are over the sphere of radius \\(t\\) centered at \\(\\mathbf{x}\\)\nIn our problem:\n\n\\(f(x,y,z) = 0\\)\n\\(g(x,y,z) = x^2 + y^2 + z^2 = |\\xi|^2\\)\nPoint of interest: \\(\\mathbf{x} = (0,0,0)\\)\n\nThus:\n\\[u(0,0,0,t) = \\frac{1}{4\\pi t} \\int_{|\\xi| = t} |\\xi|^2 \\, dS(\\xi)\\]\nStep 2: \\(~\\) Evaluate the integral\nOn the sphere \\(|\\xi| = t\\), we have \\(|\\xi|^2 = t^2\\) constant\nThen:\n\\[u(0,0,0,t) = \\frac{1}{4\\pi t} \\int_{|\\xi| = t} t^2 \\, dS(\\xi) = \\frac{t^2}{4\\pi t} \\cdot \\text{Surface area of sphere of radius } t\\]\nThe surface area of a sphere of radius \\(t\\) is \\(4\\pi t^2\\), so:\n\\[u(0,0,0,t) = \\frac{t^2}{4\\pi t} \\cdot 4\\pi t^2 = t^2 \\cdot t = \\boxed{t^3}\\]\n\\(~\\)\n2. \\(~\\) Find \\(u(x,y,z,t)\\)\n\\[\n\\begin{aligned}\nu_{tt} &= \\nabla^2 u, \\;\\; t &gt; 0\\\\\n&u(x, y, z, 0) = 0, \\;\nu_t(x, y, z, 0) = x\n\\end{aligned}\n\\]\nSolution\nStep 1: \\(~\\) Use Kirchhoff’s formula (3D wave equation)\nFor the 3D wave equation \\(u_{tt} = \\nabla^2 u\\) with wave speed \\(c = 1\\), the general solution with initial data\n\\[u(\\mathbf{x},0) = f(\\mathbf{x}), \\quad u_t(\\mathbf{x},0) = g(\\mathbf{x})\\]\nis given by:\n\\[u(\\mathbf{x},t) = \\frac{1}{4\\pi t} \\int_{|\\xi - \\mathbf{x}| = t} g(\\xi) \\, dS(\\xi) + \\frac{1}{4\\pi t^2} \\int_{|\\xi - \\mathbf{x}| = t} f(\\xi) \\, dS(\\xi)\\]\nIn our case:\n\n\\(f(x,y,z) = 0\\)\n\\(g(x,y,z) = x\\) (we’ll write this as \\(\\xi_1\\) to distinguish from the field point \\(\\mathbf{x} = (x_1,x_2,x_3)\\))\nTherefore:\n\n\\[u(\\mathbf{x},t) = \\frac{1}{4\\pi t} \\int_{|\\xi - \\mathbf{x}| = t} \\xi_1 \\, dS(\\xi)\\]\nStep 2: \\(~\\) Use coordinate shift\nLet’s simplify by performing a change of variables to center the sphere\nLet \\(\\mathbf{x} = (x_1, x_2, x_3)\\), and let \\(\\xi = \\mathbf{x} + t\\omega\\), where \\(\\omega \\in S^2\\) is a unit vector. Then \\(|\\xi - \\mathbf{x}| = t\\), and \\(dS(\\xi) = t^2\\, d\\omega\\). So:\n\\[u(\\mathbf{x},t) = \\frac{1}{4\\pi t} \\int_{|\\xi - \\mathbf{x}| = t} \\xi_1 \\, dS(\\xi)\n= \\frac{1}{4\\pi t} \\int_{S^2} (x_1 + t\\omega_1) \\cdot t^2 \\, d\\omega\\]\nSimplify:\n\\[u(\\mathbf{x},t) = \\frac{t}{4\\pi} \\int_{S^2} (x_1 + t\\omega_1) \\, d\\omega\n= \\frac{t}{4\\pi} \\left[ x_1 \\int_{S^2} d\\omega + t \\int_{S^2} \\omega_1 \\, d\\omega \\right]\\]\nNow compute the integrals:\n\n\\(\\displaystyle \\int_{S^2} d\\omega = 4\\pi\\)\n\\(\\displaystyle \\int_{S^2} \\omega_1 \\, d\\omega = 0~\\) (odd function over symmetric sphere)\n\nSo:\n\\[u(\\mathbf{x},t) = \\frac{t}{4\\pi} \\cdot x_1 \\cdot 4\\pi = x_1 t = x t\\]\n\\(~\\)\n3. \\(~\\) Solve the vibrations of a membrane which is in the \\(xy\\) plane:\n\\[\n\\begin{aligned}\nu_{tt} &= \\frac{\\rho}{T} \\left[ \\frac{\\partial^2 u}{\\partial x^2} +\\frac{\\partial^2 u}{\\partial y^2} \\right] , \\;\\; 0 &lt; x &lt; \\pi, \\;0 &lt; y &lt; \\pi \\\\\nu&(0, y, t) =0, \\; u(\\pi, y, t) =0 \\\\\nu&(x, 0, t) =0, \\; u(x, \\pi, t) =0 \\\\\nu&(x,y,0) = \\sin 2x \\sin 3y +\\sin 3x \\sin2y. \\;\nu_t(x,y,0) =0\n\\end{aligned}\\]\nSolution\nStep 1: \\(~\\) Separation of Variables\nAssume a solution of the form: \\[u(x, y, t) = X(x) Y(y) T(t)\\] Plugging into the wave equation:\n\\[X Y T'' = c^2 \\left( X'' Y T + X Y'' T \\right)\\]\nDivide both sides by \\(XYT\\):\n\\[\\frac{T''}{c^2 T} = \\frac{X''}{X} + \\frac{Y''}{Y} = -\\lambda\\]\nWe separate variables: \\[\\frac{X''}{X} + \\frac{Y''}{Y} = -\\lambda \\Rightarrow\n\\begin{cases}\nX’’ + \\mu X = 0 \\\\\nY’’ + (\\lambda - \\mu) Y = 0\n\\end{cases}\\]\nWith the boundary conditions: \\[\\begin{aligned}\nX(0) &= X(\\pi) = 0 \\Rightarrow X_n(x) = \\sin(nx),\\quad \\mu = n^2 \\\\\nY(0) &= Y(\\pi) = 0 \\Rightarrow Y_m(y) = \\sin(my),\\quad \\lambda - \\mu = m^2 \\Rightarrow \\lambda = n^2 + m^2\n\\end{aligned}\\]\nThen the time part: \\[\\begin{aligned}\nT'' &+ c^2(n^2 + m^2)T = 0\\\\\n&\\Rightarrow\nT_{n,m}(t) = A_{n,m} \\cos(c\\sqrt{n^2 + m^2}t) + B_{n,m} \\sin(c\\sqrt{n^2 + m^2}t)\n\\end{aligned}\\]\nStep 2: \\(~\\) General Solution\n\\[\n\\begin{aligned}\nu(x, y, t) = \\sum_{n=1}^{\\infty} \\sum_{m=1}^{\\infty} &\\left[ A_{n,m} \\cos(c\\sqrt{n^2 + m^2} t)\\right. \\\\\n&\\left.+ B_{n,m} \\sin(c\\sqrt{n^2 + m^2} t) \\right] \\sin(nx) \\sin(my)\n\\end{aligned}\n\\]\nStep 3: \\(~\\) Apply Initial Conditions\nInitial displacement:\n\\[u(x, y, 0) = \\sum_{n,m} A_{n,m} \\sin(nx)\\sin(my) = \\sin 2x \\sin 3y + \\sin 3x \\sin 2y\\]\nSo \\(A_{2,3} = 1\\), \\(A_{3,2} = 1\\), all others zero\nInitial velocity: \\[u_t(x, y, 0) = -c\\sum_{n,m} c\\sqrt{n^2 + m^2} B_{n,m} \\sin(nx)\\sin(my) = 0 \\Rightarrow B_{n,m} = 0\\]\nFinal Answer\n\\[u(x, y, t) = \\cos\\left(c \\sqrt{13}\\, t\\right) \\sin(2x) \\sin(3y)\n    +\\cos\\left(c \\sqrt{13}\\, t\\right) \\sin(3x) \\sin(2y)\\]\nwhere \\(c = \\sqrt{\\rho/T}\\)\n\\(~\\)\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\ndef plot_rectangle_membrane_normal_modes(n, m):\n    \n    fig, ax = plt.subplots(figsize=(3, 3))\n\n    ax.set(xticks=[], yticks=[])\n\n    x_ = np.linspace(0, np.pi, 100)\n    y_ = np.linspace(0, np.pi, 100)\n\n    X_, Y_ = np.meshgrid(x_, y_)\n    U_ = np.sin(n*X_) *np.sin(m*Y_) +np.sin(m*X_) *np.sin(n*Y_) \n    \n    ax.contourf(X_, Y_, U_, levels=[0, 10])\n    \n    ax.set_title(f'$n={n}$, $m={m}$')\n    \n    plt.show()\n\n\nplot_rectangle_membrane_normal_modes(2, 3)\n\n\n\n\n\n\n\n\n\\(~\\)\n4. \\(~\\) Solve the vibrations of a membrane which is in the \\(xy\\) plane:\n\\[\\begin{aligned}\nu_{tt} &= \\frac{\\rho}{T} \\left[ \\frac{\\partial^2 u}{\\partial x^2} +\\frac{\\partial^2 u}{\\partial y^2} \\right] , \\;\\; 0 &lt; x &lt; \\pi, \\;0 &lt; y &lt; \\pi \\\\\nu&(0, y, t) =0, \\; u(\\pi, y, t) =0 \\\\\nu&(x, 0, t) =0, \\; u(x, \\pi, t) =0 \\\\\nu&(x,y,0) = \\sin x \\sin 4y +\\sin 4x \\sin y, \\;\nu_t(x,y,0) =0\n\\end{aligned}\\]\nSolution\nStep 1: \\(~\\) Use Separation of Variables\nAssume a solution of the form:\n\\[\n\\begin{aligned}\nu(x,y,t) = \\sum_{n=1}^\\infty \\sum_{m=1}^\\infty &\\left[ A_{nm} \\cos(c\\sqrt{n^2 + m^2} \\, t)\\right.\\\\\n&+\\left. B_{nm} \\sin(c\\sqrt{n^2 + m^2} \\, t) \\right] \\sin(nx) \\sin(my)\n\\end{aligned}\\]\nFrom the initial displacement: \\[u(x,y,0) = \\sum_{n,m} A_{nm} \\sin(nx)\\sin(my) = \\sin x \\sin 4y + \\sin 4x \\sin y\\]\nSo the only nonzero coefficients are: \\[A_{1,4} = 1,\\quad A_{4,1} = 1\\]\nFrom the initial velocity condition: \\[u_t(x,y,0) = 0 \\Rightarrow B_{nm} = 0\\]\nStep 2: \\(~\\) Final Solution\n\\[\\begin{aligned}\nu(x, y, t) &= A_{1,4} \\cos(c \\sqrt{1^2 + 4^2} \\, t) \\sin x \\sin 4y\\\\\n    &\\quad+ A_{4,1} \\cos(c \\sqrt{4^2 + 1^2} \\, t) \\sin 4x \\sin y \\\\\n&= \\cos(c\\sqrt{17} \\, t) \\sin x \\sin 4y + \\cos(c\\sqrt{17} \\, t) \\sin 4x \\sin y\n\\end{aligned}\\]\nThus, the solution is:\n\\[\nu(x, y, t) = \\cos(c\\sqrt{17} \\, t) \\left[ \\sin x \\sin 4y + \\sin 4x \\sin y \\right]\n\\]\nwhere \\(c = \\sqrt{\\frac{\\rho}{T}}\\)\n\\(~\\)\n\nplot_rectangle_membrane_normal_modes(1, 4)\n\n\n\n\n\n\n\n\n\\(~\\)\n5. \\(~\\) Suppose the vibration of a string is described by \\(u_{tt}=u_{xx}\\) and has an initial displacement as given by the following diagram:\n\\((a)\\)\n\n\n\n\n\n\\((b)\\)\n\n\n\n\n\nAssuming the initial velocity \\(u_t(x,0)=0\\), describe the solution of this problem in the \\(xt\\)-plane and plot the solution at \\(t=4\\) in the \\(xu\\)-plane\nSolution\n\n# Re-import libraries due to code state reset\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport matplotlib.animation as animation\nfrom IPython.display import HTML\n\n# Define the piecewise function f(x)\ndef fa(x):\n    x = np.array(x)\n    result = np.zeros_like(x)\n    result[(x &gt;= -3) & (x &lt; -2)] = 1\n    result[(x &gt; -2) & (x &lt; -1)] = 2\n    result[(x &gt;= -1) & (x &lt; 1)] = 1\n    result[(x &gt;= 1) & (x &lt; 2)] = 2\n    result[(x &gt;= 2) & (x &lt; 3)] = 1\n    # result is already 0 elsewhere\n    return result\n\ndef fb(x):\n    x = np.array(x)\n    result = np.zeros_like(x)\n    result[(x &gt;= -3) & (x &lt; -2)] = 1\n    result[(x &gt; -2) & (x &lt; -1)] = 2\n    result[(x &gt;= -1) & (x &lt; 1)] = 3\n    result[(x &gt;= 1) & (x &lt; 2)] = 2\n    result[(x &gt;= 2) & (x &lt; 3)] = 1\n    # result is already 0 elsewhere\n    return result    \n\n\n# Create meshgrid\nx = np.linspace(-10, 10, 400)\nt = np.linspace(0, 6, 200)\nX, T = np.meshgrid(x, t)\n\nUa = 0.5 *(fa(X -T) +fa(X +T))\nUb = 0.5 *(fb(X -T) +fb(X +T))\n\n# Plot u\nfig, axes = plt.subplots(2, 1, figsize=(7, 7))\n\ncba = axes[0].pcolormesh(x, t, Ua, vmin=0, vmax=3, cmap='gray_r')\ncbb = axes[1].pcolormesh(x, t, Ub, vmin=0, vmax=3, cmap='gray_r')\nplt.colorbar(cba, shrink=0.9, ticks=[0, 0.5, 1, 1.5, 2, 2.5, 3], label='u')\nplt.colorbar(cbb, shrink=0.9, ticks=[0, 0.5, 1, 1.5, 2, 2.5, 3], label='u')\n\naxes[0].axhline(y=4, color=\"black\", linestyle=\":\", label='$t=4$')\naxes[1].axhline(y=4, color=\"black\", linestyle=\":\", label='$t=4$')\n\n# Plot characteristics\nfor x0 in [-3, -2, -1, 1, 2, 3]:\n    axes[0].plot(x0 +t, t, 'red', linestyle=':')\n    axes[0].plot(x0 -t, t, 'red', linestyle=':')\n    axes[1].plot(x0 +t, t, 'red', linestyle=':')\n    axes[1].plot(x0 -t, t, 'red', linestyle=':')    \n\naxes[0].set_title('Characteristic Lines and $u(x,t)$ in $x$-$t$ Plane')\naxes[1].set_xlabel('x')\naxes[0].set_ylabel('t')\naxes[1].set_ylabel('t')\n\nplt.legend(loc=4)\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\n# Define u(x, t) = 1/2 [f(x -t) +f(x +t)] at t = 4\n\nt = 4\nx_vals = np.linspace(-10, 10, 1000)\nua_vals = 0.5 *(fa(x_vals -t) +fa(x_vals +t))\nub_vals = 0.5 *(fb(x_vals -t) +fb(x_vals +t))\n\n# Plot u(x, 4)\nfig, axes = plt.subplots(2, 1, figsize=(7, 6))\n\naxes[0].plot(x_vals, ua_vals, color='blue')\naxes[1].plot(x_vals, ub_vals, color='blue')\n\naxes[0].set_title('Displacement $u(x, 4)$ in the $xu$-plane')\naxes[1].set_xlabel('x')\naxes[0].set_ylabel('u(x, 4)')\naxes[1].set_ylabel('u(x, 4)')\naxes[0].grid(True)\naxes[1].grid(True)\n\naxes[0].set_ylim([-0.1, 2.1])\naxes[1].set_ylim([-0.1, 2.1])\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\n# Create grid\nx = np.linspace(-10, 10, 400)\nt_vals_light = np.linspace(0, 6, 40)\n\n# Set up the figure\nfig, axes = plt.subplots(2, 1, figsize=(7, 6))\n\nline0, = axes[0].plot([], [], lw=2)\nline1, = axes[1].plot([], [], lw=2)\naxes[0].set_xlim(-10, 10)\naxes[1].set_xlim(-10, 10)\n\naxes[0].set_ylim(-0.1, 3.1)\naxes[1].set_ylim(-0.1, 3.1)\n\naxes[1].set_xlabel('x')\naxes[0].set_ylabel('u(x,t)')\naxes[1].set_ylabel('u(x,t)')\n#axes[0].set_title('Wave Propagation Over Time')\n\nplt.close()\n\n# Animation functions\ndef init():\n    line0.set_data([], [])\n    line1.set_data([], [])    \n    return line0, line1\n\ndef animate(i):\n    t = t_vals_light[i]\n    ua = 0.5 *(fa(x -t) +fa(x +t))\n    ub = 0.5 *(fb(x -t) +fb(x +t))    \n    line0.set_data(x, ua)\n    line1.set_data(x, ub)    \n    axes[0].set_title(f'Wave at t = {t:.2f}')\n    return line0, line1\n\n# Create and save animation\nani = animation.FuncAnimation(fig, animate, frames=len(t_vals_light),\n                              init_func=init, blit=True, interval=400)\n\nHTML('&lt;center&gt;' + ani.to_html5_video() + '&lt;/center&gt;')\n\n\n  \n  Your browser does not support the video tag.\n\n\n\n\\(~\\)\n6. \\(~\\) Find \\(u\\left( \\frac{3}{8}, 3 \\right)\\)\n\\[\n\\begin{aligned}\nu_{tt} &= u_{xx}, \\;\\;0&lt;x&lt;1, \\; 0 &lt; t &lt; \\infty\\\\\nu_x(0, t) &= 0\\\\\nu(1, t) &= 0\\\\\nu(x, 0) &=(1-x)^3 \\\\\nu_t(x, 0) &= (1-x)^2\n\\end{aligned}\n\\]\nSolution\nMethod of Solution: D’Alembert’s Formula with Function Extension\nStep 1: \\(~\\) D’Alembert Solution on the Infinite Domain\nThe general D’Alembert solution for the wave equation on the entire real line is:\n\\[u(x,t) = \\frac{1}{2}\\left[f(x + t) + f(x - t)\\right] + \\frac{1}{2} \\int_{x - t}^{x + t} g(s)\\, ds\\]\nThis solution assumes that u is defined on the entire real line with no boundary conditions. To use this method on the interval \\(0 &lt; x &lt; 1\\), we extend \\(f(x)\\) and \\(g(x)\\) to the whole real line by reflecting them in such a way that the boundary conditions are satisfied\nStep 2: \\(~\\) Extend \\(f(x)\\) and \\(g(x)\\) to the Real Line\nWe extend \\(f(x)\\) and \\(g(x)\\) as follows:\nBoundary Conditions:\n\nNeumann at \\(x = 0 \\;\\Rightarrow\\;\\) Extend as an even function about \\(x\\) = 0\nDirichlet at \\(x = 1 \\;\\Rightarrow\\;\\) Extend as an odd function about \\(x = 1\\)\n\nThis leads to a 2-periodic extension over \\(\\mathbb{R}\\). The steps:\n\nReflect \\(f(x)\\), \\(g(x)\\) evenly about \\(x = 0\\)\nReflect them oddly about \\(x = 1\\)\nPeriodically extend with period 4\n\nWe denote the extended functions as \\(\\tilde{f}(x)\\), \\(\\tilde{g}(x)\\), defined for all \\(x \\in \\mathbb{R}\\) and this pattern repeats every 4 units:\n\\[\\tilde{f}(x + 4) = \\tilde{f}(x)\\]\nStep 3: \\(~\\) Apply D’Alembert Formula\nUsing the extended functions, the D’Alembert solution becomes:\n\\[u(x,t) = \\frac{1}{2} \\left[\\tilde{f}(x + t) + \\tilde{f}(x - t)\\right] + \\frac{1}{2} \\int_{x - t}^{x + t} \\tilde{g}(s)\\, ds\\]\nNow substitute \\(x = \\frac{3}{8}\\), \\(t = 3\\):\n\\[u\\left( \\frac{3}{8}, 3 \\right) = \\frac{1}{2} \\left[ \\tilde{f}\\left(\\frac{3}{8} + 3\\right) + \\tilde{f}\\left(\\frac{3}{8} - 3\\right) \\right] + \\frac{1}{2} \\int_{\\frac{3}{8} - 3}^{\\frac{3}{8} + 3} \\tilde{g}(s)\\, ds\\]\n\nConstruction of \\(\\tilde{f}(x)\\) on \\([0, 4)\\):\n\n\n\n\n\n\n\n\n\nInterval\nDefinition of \\(\\tilde{f}(x)\\)\nReasoning\n\n\n\n\n\\(0 \\leq x &lt; 1\\)\n\\((1 - x)^3\\)\nOriginal definition\n\n\n\\(1 \\leq x &lt; 2\\)\n\\((1 - x)^3\\)\nOdd reflection about \\(x = 1\\)\n\n\n\\(2 \\leq x &lt; 3\\)\n\\((x - 3)^3\\)\nEven reflection about \\(x = 2\\) of previous term\n\n\n\\(3 \\leq x &lt; 4\\)\n\\((x - 3)^3\\)\nOdd reflection about \\(x = 3\\) of previous term\n\n\n\n\nConstruction of \\(\\tilde{g}(x)\\) on \\([0, 4)\\):\n\n\n\n\n\n\n\n\n\nInterval\nDefinition of \\(\\tilde{g}(x)\\)\nReasoning\n\n\n\n\n\\(0 \\leq x &lt; 1\\)\n\\(\\phantom{-}(1 - x)^2\\)\nOriginal definition\n\n\n\\(1 \\leq x &lt; 2\\)\n\\(-(1 - x)^2\\)\nOdd reflection about \\(x = 1\\)\n\n\n\\(2 \\leq x &lt; 3\\)\n\\(-(x - 3)^2\\)\nEven reflection about \\(x = 2\\) of previous term\n\n\n\\(3 \\leq x &lt; 4\\)\n\\(\\phantom{-}(x - 3)^2\\)\nOdd reflection about \\(x = 3\\) of previous term\n\n\n\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\ndef tilde_fg(x):\n    x = np.array(x)\n    f = np.zeros_like(x)\n    g = np.zeros_like(x)    \n    \n    # (0, 1): original\n    mask1 = (0 &lt;= x) & (x &lt; 1)\n    f[mask1] = (1 -x[mask1])**3\n    g[mask1] = (1 -x[mask1])**2    \n\n    # (1, 2): odd reflection about x = 1\n    mask2 = (1 &lt;= x) & (x &lt; 2)\n    f[mask2] = (1 -x[mask2])**3\n    g[mask2] = -(1 -x[mask2])**2      \n\n    # (2, 3): even reflection about x = 2 \n    mask3 = (2 &lt;= x) & (x &lt; 3)\n    f[mask3] = (x[mask3] -3)**3  # no sign change\n    g[mask3] = -(x[mask3] -3)**2\n\n    # (3, 4): odd reflection about x = 3\n    mask4 = (3 &lt;= x) & (x &lt;= 4)\n    f[mask4] = (x[mask4] -3)**3\n    g[mask4] = (x[mask4] -3)**2\n\n    return f, g\n\n# Domain\nx_vals = np.linspace(0, 4, 1000)\nf_vals, g_vals = tilde_fg(x_vals)\n\n# Plot\nplt.figure(figsize=(6, 4))\nplt.plot(x_vals, f_vals, color='red', label=r'$\\tilde{f}$')\nplt.plot(x_vals, g_vals, color='blue', label=r'$\\tilde{g}$')\n\nplt.title(r'$\\tilde{f}(x), \\tilde{g}(x)$ on $[0, 4)$')\nplt.xlabel('x')\nplt.ylabel(r'$\\tilde{f}(x), \\tilde{g}(x)$')\nplt.legend()\nplt.grid(True)\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\nStep 4: \\(~\\) Evaluate \\(\\tilde{f}(x \\pm t)\\)\n\\[x = \\frac{3}{8} = 0.375,\\quad t = 3 \\Rightarrow x + t = 3.375,\\quad x - t = -2.625\\]\nUsing 4-periodicity:\n\n\\(3.375 \\mod 4 = 3.375\\)\n\\(-2.625 \\mod 4 = 1.375\\)\n\nThus:\n\n\\(\\tilde{f}(3.375) = (3.375 - 3)^3 = (0.375)^3 = \\frac{27}{512}\\)\n\\(\\tilde{f}(1.375) = (1 -1.375)^3 = - (0.375)^3 = -\\frac{27}{512}\\)\n\nSo the average is:\n\\[\\frac{1}{2} \\left[ \\tilde{f}(3.375) + \\tilde{f}(-2.625) \\right] = \\frac{1}{2} \\left( \\frac{27}{512} - \\frac{27}{512} \\right) = 0\\]\nStep 5: \\(~\\) Evaluate the Integral Term\nWe now compute:\n\\[\\int_{-2.625}^{3.375} \\tilde{g}(s)\\, ds =\n\\underbrace{\\int_{-2.625}^{-2} \\tilde{g}(s)\\, ds}_{A}\n    +\\underbrace{\\int_{-2}^{2} \\tilde{g}(s)\\, ds}_{0}\n    +\\underbrace{\\int_{2}^{3.375} \\tilde{g}(s)\\, ds}_{B}\\]\nWe computed the tails:\n\n\\(\\displaystyle A = \\int_{-2.625}^{-2} \\tilde{g}(s)\\, ds = -\\int_{1.375}^{2} (s - 1)^2 ds = \\boxed{-\\frac{485}{1536}}\\)\n\\(\\displaystyle B = \\int_{2}^{3.375} \\tilde{g}(s)\\, ds\\)\n\nWe split this into two parts, because the definition of \\(\\tilde{g}(s)\\) changes at \\(s = 3\\):\n\nOn \\([2, 3)\\): \\(\\tilde{g}(s) = (3 - s)^2\\)\n\\[\\int_{2}^{3} (3 - s)^2 ds = \\int_1^0 u^2 (-du) = \\int_0^1 u^2 du = \\left[\\frac{u^3}{3}\\right]_0^1 = \\frac{1}{3}\\]\nOn \\([3, 3.375]\\): \\(\\tilde{g}(s) = -(s - 3)^2\\)\n\n\\[\\int_{3}^{3.375} - (s - 3)^2 ds = - \\int_0^{3/8} v^2 dv = - \\left[\\frac{v^3}{3}\\right]_0^{3/8}\n= - \\frac{(3/8)^3}{3} = - \\frac{27}{1536}\\]\nSo:\n\\[\\int_{2}^{3.375} \\tilde{g}(s)\\, ds = \\frac{1}{3} - \\frac{27}{1536} = \\frac{512 - 27}{1536} = \\boxed{\\frac{485}{1536}}\\]\nFinal Answer:\nRecall that from the previous step:\n\\[\\frac{1}{2} [\\tilde{f}(x+t) + \\tilde{f}(x - t)] = 0\n\\quad\\text{and}\\quad \\text{(integral term)} = 0\\]\n\\[\\boxed{u\\left( \\frac{3}{8}, 3 \\right) = 0}\\]\n\\(~\\)\n7. \\(~\\) Suppose the vibration of a string is described by \\(u_{tt}=u_{xx}\\) and has an initial displacement as given by the following diagram:\n\nAssuming the initial velocity \\(u_t(x,0)=0\\), describe the solution of this problem in the \\(xt\\)-plane",
    "crumbs": [
      "**Second Semester**",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Hyperbolic Partial Differential Equations</span>"
    ]
  },
  {
    "objectID": "x_PDE_dedalus.html#coordinates-distributors-and-bases",
    "href": "x_PDE_dedalus.html#coordinates-distributors-and-bases",
    "title": "Appendix J — DEDALUS",
    "section": "",
    "text": "J.1.1 Coordinates\nIn Dedalus, the spatial coordinates of a PDE are represented by coordinate objects. For simple 1D problems, you can define a coordinate directly using the Coordinate class. For higher-dimensional problems, you’ll usually combine multiple coordinates into a CoordinateSystem.\nDedalus currently includes several built-in coordinate systems:\n\nCartesianCoordinates: works in any number of dimensions\nPolarCoordinates: with azimuth and radius\nS2Coordinates: with azimuth and colatitude\nSphericalCoordinates: with azimuth, colatitude, and radius\n\nWhen you create a CoordinateSystem, you just provide the names you’d like to use for each coordinate, in the order shown above. For example, let’s walk through how to set up a problem in 3D Cartesian coordinates\n\ncoords = d3.CartesianCoordinates('x', 'y', 'z')\n\n\n\nJ.1.2 Distributors\nA distributor object handles the parallel decomposition of fields and problems. Every problem in Dedalus needs a distributor, even if you’re just running in serial\nTo create a distributor, you provide the coordinate system for your PDE, specify the datatype of the fields you’ll be working with, and, if needed, supply a process mesh to control parallelization\n\n# No mesh for serial / automatic parallelization\ndist = d3.Distributor(coords, dtype=np.float64) \n\nParallelization & process meshes\nWhen you run Dedalus under MPI, it parallelizes computations using block-distributed domain decompositions. By default, Dedalus spreads the work across a 1-dimensional mesh of all available MPI processes—this is called a slab decomposition. If you want more flexibility, you can specify a custom process mesh with the mesh keyword when creating a domain. This allows pencil decompositions, where the domain is split along more than one direction. Just keep in mind that the total number of MPI processes must match the product of the process mesh shape you provide\nThere’s also an important restriction: the mesh dimension can’t be larger than the number of separable coordinates in the linear part of your PDE. In practice, this usually means you can parallelize over periodic or angular coordinates. For fully separable problems—like a fully periodic box or a simulation on the sphere—the mesh dimension must be strictly less than the total dimension\nLayouts\nThe distributor object sets up the machinery needed to allocate and transform fields in parallel. A key part of this is an ordered set of Layout objects, which describe how the data should be represented and distributed as it moves between coefficient space and grid space. Moving from one layout to another involves two types of operations: spectral transforms (done locally) and global transposes (which shuffle data across the process mesh to put it in the right place)\nThe basic algorithm works like this:\n\nWe start in coefficient space (layout 0), where the last axis is local (not distributed)\nThen we transform that last axis into grid space (layout 1)\nIf needed, we perform a global transpose so that the next axis becomes local, and transform that axis into grid space\nThis process repeats until all axes have been transformed into grid space (the final layout)\n\nLet’s take a look at the layouts for the domain we just built. Since this is a serial computation, no global transposes are required—all axes are already local. So the layout transitions are just coefficient-to-grid transforms, working backwards from the last axis\n\nfor layout in dist.layouts:\n  print(f'Layout {layout.index}:  Grid space: {layout.grid_space}  Local: {layout.local}')\n\nLayout 0:  Grid space: [False False False]  Local: [ True  True  True]\nLayout 1:  Grid space: [False False  True]  Local: [ True  True  True]\nLayout 2:  Grid space: [False  True  True]  Local: [ True  True  True]\nLayout 3:  Grid space: [ True  True  True]  Local: [ True  True  True]\n\n\nTo get a sense of how things work in a distributed simulation, we’ll change the process mesh shape and rebuild the layout objects. For this example, we’ll bypass the usual internal checks on the number of available processes and related settings, just so we can see how the layouts are constructed in a parallel setup\n\n# Don't do this. For illustration only\ndist.mesh = np.array([4, 2])\ndist.comm_coords = np.array([0, 0])\ndist._build_layouts(dry_run=True)\n\n\nfor layout in dist.layouts:\n  print(f'Layout {layout.index}:  Grid space: {layout.grid_space}  Local: {layout.local}')\n\nLayout 0:  Grid space: [False False False]  Local: [False False  True]\nLayout 1:  Grid space: [False False  True]  Local: [False False  True]\nLayout 2:  Grid space: [False False  True]  Local: [False  True False]\nLayout 3:  Grid space: [False  True  True]  Local: [False  True False]\nLayout 4:  Grid space: [False  True  True]  Local: [ True False False]\nLayout 5:  Grid space: [ True  True  True]  Local: [ True False False]\n\n\nWe can see that there are now two additional layouts, corresponding to the transposed states of the mixed-transform layouts. Two global transposes are necessary here in order for the \\(x\\) and \\(y\\) axes to be stored locally, which is required to perform the respective spectral transforms. Here’s a sketch of the data distribution in the different layouts:\n\nIn most cases, you won’t need to interact with layout objects directly. However, it’s useful to understand this system, since it controls how data is distributed and transformed. Being aware of it will help when working with field objects, as we’ll see in later sections\n\n\n\n\n\n\nCoefficient Space\n\n\n\n\nThis is the representation of a function in terms of its spectral coefficients\nFunctions are expressed as linear combinations of basis functions (Fourier, Chebyshev, Legendre, etc.)\n\nExamples: \\[ f(x) \\approx \\sum_{k=-N/2}^{N/2} \\hat{f}k e^{ikx} \\quad (\\text{Fourier})\\] \\[f(x) \\approx \\sum_{n=0}^{N} \\hat{f}_n T_n(x) \\quad (\\text{Chebyshev})\\]\n\nHere, \\(\\hat{f}_k\\), \\(\\hat{f}_n\\) are the data in coefficient space\nDifferentiation and integration are simple in this representation:\n\nFourier: differentiation \\(\\rightarrow\\) multiply coefficients by \\(ik\\)\nChebyshev: differentiation \\(\\rightarrow\\) apply a simple linear recurrence\n\nAnalytical operations are most efficient in coefficient space\n\n\n\n\n\n\n\n\n\nGrid Space\n\n\n\n\nThis is the representation of a function by its values on discrete grid points\nIt’s what you use when you want to “see the shape” of a function or set initial conditions\n\nExamples:\n\nFourier basis \\(\\rightarrow\\) values on uniformly spaced grid points\nChebyshev basis \\(\\rightarrow\\) values on non-uniform grid points (clustered near the boundaries)\nNonlinear operations (products, \\(f(u)\\)) are natural and efficient in grid space\n\n\n\n\n\nJ.1.3 Bases\nCreating a basis\nEach type of basis in Dedalus is represented by a separate class. These classes define the corresponding spectral operators as well as transforms between the grid space and coefficient space representations of functions in that basis. The most commonly used bases are:\n\nRealFourier for real periodic functions on an interval using cosine & sine modes\nComplexFourier for complex periodic functions on an interval using complex exponentials\nChebyshev for functions on an interval\nJacobi for functions on an interval under a more general inner product (usually Chebyshev is best for performance)\nDiskBasis for functions on a full disk in polar coordinates\nAnnulusBasis for functions on an annulus in polar coordinates\nSphereBasis for functions on the 2-sphere in S2 or spherical coordinates\nBallBasis for functions on a full ball in spherical coordinates\nShellBasis for functions on a spherical shell in spherical coordinates\n\nFor one-dimensional or Cartesian bases, you create a basis by specifying:\n\nthe corresponding coordinate object\nthe number of modes for the basis\nthe coordinate bounds of the basis interval\n\nFor multidimensional or curvilinear bases, you provide:\n\nthe corresponding coordinate system\nthe multidimensional mode shape for the basis\nthe radial extent of the basis\nthe data type (dtype) for the problem\n\nOptionally, for any basis, you can also specify dealiasing scale factors for each axis. These factors control how much to pad the modes when transforming to grid space. For example, to properly dealias quadratic nonlinearities, you would typically use a scaling factor of 3/2\n\nxbasis = d3.RealFourier(coords['x'], size=32, bounds=(0,1), dealias=3/2)\nybasis = d3.RealFourier(coords['y'], size=32, bounds=(0,1), dealias=3/2)\nzbasis = d3.Chebyshev(coords['z'], size=32, bounds=(0,1), dealias=3/2)\n\nSome bases also accept additional arguments that let you tweak their internal behavior. For more details, check the basis.py API documentation\nBasis grids and scale factors\nEach basis comes with a corresponding coordinate or collocation grid—or multiple grids for multidimensional bases. These grids are useful for tasks like initializing and plotting fields\nYou can access the global (non-distributed) grids using the basis object’s global_grid method (or global_grids for multidimensional bases). These methods also allow you to provide scale factors, which control how many points are included in the grid relative to the number of basis modes\nFor example, let’s take a look at Chebyshev grids with scaling factors of 1 and 3/2\n\ngrid_normal = zbasis.global_grid(dist, scale=1).ravel()\ngrid_dealias = zbasis.global_grid(dist, scale=3/2).ravel()\n\nplt.figure(figsize=(6, 1.5), dpi=100)\n\nplt.plot(grid_normal, 0*grid_normal +1, 'o', markersize=5)\nplt.plot(grid_dealias, 0*grid_dealias -1, 'o', markersize=5)\n\nplt.xlabel('z')\nplt.title('Chebyshev grid with scales 1 and 3/2')\nplt.ylim([-2, 2])\nplt.gca().yaxis.set_ticks([])\nplt.tight_layout()\n\n\n\n\n\n\n\n\nNote that Chebyshev grids are non-equispaced: the points cluster quadratically near the ends of the interval. This behavior is especially useful for resolving sharp features, such as boundary layers\nDistributed grid and element arrays\nTo make it easier to create field data, the distributor provides the local portions of the coordinate grids and mode numbers (wavenumbers or polynomial degrees). You can access the local grids—which are distributed according to the last layout or the full grid space—using the dist.local_grid method (or dist.local_grids for multidimensional bases)\nWhen calling these methods, you need to specify the basis and optionally a scale factor (which defaults to 1)\n\nlocal_x = dist.local_grid(xbasis)\nlocal_y = dist.local_grid(ybasis)\nlocal_z = dist.local_grid(zbasis)\n\nprint('Local x shape:', local_x.shape)\nprint('Local y shape:', local_y.shape)\nprint('Local z shape:', local_z.shape)\n\nLocal x shape: (32, 1, 1)\nLocal y shape: (1, 8, 1)\nLocal z shape: (1, 1, 16)\n\n\nThe local \\(x\\) grid corresponds to the full Fourier grid for the \\(x\\)-basis and is the same on all processes, because the first axis is local in grid space\nThe local \\(y\\) and local \\(z\\) grids, on the other hand, usually differ across processes. These grids contain only the local portions of the \\(y\\) and \\(z\\) basis grids, distributed according to the process mesh—for example, 4 blocks in \\(y\\) and 2 blocks in \\(z\\)\nYou can access the local modes—which are distributed according to layout 0 (the full coefficient space)—using the dist.local_modes method. When calling this method, you just need to specify the basis\n\nlocal_kx = dist.local_modes(xbasis)\nlocal_ky = dist.local_modes(ybasis)\nlocal_nz = dist.local_modes(zbasis)\n\nprint('Local kx shape:', local_kx.shape)\nprint('Local ky shape:', local_ky.shape)\nprint('Local nz shape:', local_nz.shape)\n\nLocal kx shape: (8, 1, 1)\nLocal ky shape: (1, 16, 1)\nLocal nz shape: (1, 1, 32)\n\n\nThe local \\(k_x\\) and local \\(k_y\\) arrays now differ across processes, because they contain only the local portions of the \\(x\\) and \\(y\\) wavenumbers, which are distributed in coefficient space\nThe local \\(n_z\\) array, on the other hand, includes the full set of Chebyshev modes, which are always local in coefficient space\nThese local arrays can be used to create parallel-safe initial conditions for fields, either in grid space or coefficient space, as we’ll explore in the next section",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>J</span>  <span class='chapter-title'>DEDALUS</span>"
    ]
  },
  {
    "objectID": "x_PDE_dedalus.html#fields-and-operators",
    "href": "x_PDE_dedalus.html#fields-and-operators",
    "title": "Appendix J — DEDALUS",
    "section": "J.2 Fields and Operators",
    "text": "J.2 Fields and Operators\nThis tutorial covers the basics of setting up and working with field and operator objects in Dedalus. Dedalus uses these abstractions to implement a symbolic algebra system, which allows us to represent mathematical expressions and PDEs in a convenient and flexible way\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nimport dedalus.public as d3\nfrom dedalus.extras.plot_tools import plot_bot_2d\n\nfigkw = {'figsize': (6, 4), 'dpi': 100}\n\n\nJ.2.1 Fields\nCreating a field\nIn Dedalus, field objects represent scalar-valued fields defined over a set of bases (or a domain). You can create a field directly from the Field class by providing a distributor, a list of bases, and, optionally, a name. Alternatively, you can create a field using the dist.Field method\nLet’s try setting up a 2D domain and building a field\n\ncoords = d3.CartesianCoordinates('x', 'y')\ndist = d3.Distributor(coords, dtype=np.float64)\n\nxbasis = d3.RealFourier(coords['x'], 64, bounds=(-np.pi, np.pi), dealias=3/2)\nybasis = d3.Chebyshev(coords['y'], 64, bounds=(-1, 1), dealias=3/2)\n\nf = dist.Field(name='f', bases=(xbasis, ybasis))\n\nThis field \\(f\\) depends on both \\(x\\) and \\(y\\), since it is defined using both xbasis and ybasis. If you want to create a field that depends on only \\(x\\) or only \\(y\\), you can pass bases=xbasis or bases=ybasis, respectively. To create a spatially constant field—one that does not vary with \\(x\\) or \\(y\\)—simply do not provide any bases\nVector and tensor fields\nBy default, the Field class creates a scalar-valued field, which can also be instantiated using the ScalarField constructor\nIf you want a vector-valued field, use the VectorField constructor and provide the coordinate system corresponding to the components of the vector. Technically, this is specifying the vector bundle of the field to be the tangent bundle on the chosen coordinate system\nSimilarly, you can create arbitrary-order tensor fields using the TensorField constructor by passing a tuple of coordinate systems. This defines the tensor bundle of the field\nThe bases of a field describe its spatial variation, while the vector/tensor bundle describes the components of the field. For example, you could have a 2D vector with \\(x\\) and \\(y\\) components that varies only along the \\(x\\) direction—so it would only need an \\(x\\) basis\nLet’s go ahead and build such a vector field on our domain\n\nu = dist.VectorField(coords, name='u', bases=xbasis)\n\nManipulating field data\nField objects provide several methods for transforming their data between different layouts—this includes grid space, coefficient space, and all intermediate layouts\nEach field has a layout attribute, which points to the layout object describing its current transform and distribution state\nBy default, fields are instantiated in coefficient space\n\nf.layout.grid_space\n\narray([False, False])\n\n\nYou can assign and retrieve field data in any layout by indexing a field with that layout object. In most cases, you won’t need the mixed layouts—the full grid and full coefficient layouts are usually enough. You can also use the shortcuts 'g' and 'c' to access these layouts easily\nWhen working in parallel, each process only manipulates its local portion of the globally distributed data. This means that you can safely set a field’s grid data in a parallel-safe way using the local grids provided by the domain\n\nx = dist.local_grid(xbasis)\ny = dist.local_grid(ybasis)\n\nf['g'] = np.exp((1 -y**2) *np.cos(x +np.cos(x) *y**2)) *(1 +0.05 *np.cos(10 *(x +2 *y)))  \n\n# Plot grid values\nplot_bot_2d(f, figkw=figkw, title=\"f['g']\");\n\n\n\n\n\n\n\n\nYou can convert a field to spectral coefficients by accessing its data in coefficient space. Internally, this triggers an in-place multidimensional spectral transform on the field’s data\n\nf['c']\n\n# Plot log magnitude of spectral coefficients\nlog_mag = lambda xmesh, ymesh, data: (xmesh, ymesh, np.log10(np.abs(data) +1.e-20))\nplot_bot_2d(\n  f, \n  func=log_mag, \n  clim=(-20, 0), \n  cmap='viridis', \n  title=\"log10(abs(f['c'])\", \n  figkw={'figsize': (5, 5), 'dpi': 100});\n\n\n\n\n\n\n\n\nExamining the spectral coefficients of a field is very useful, because the amplitude of the highest modes indicates the truncation errors in the spectral discretization\nIf the amplitudes of these modes are small, as in this example, we can conclude that the field is well-resolved\nVector and tensor components\n\nu['g'].shape\n\n(2, 64, 1)\n\n\nVector and Tensor Field Data Shapes\n\nThe first axis of the data array corresponds to the field’s components\nFor example, in a 2D Cartesian vector field, this axis has size 2, representing the \\(x\\)- and \\(y\\)-components\nThe remaining axes correspond to the physical shape of the domain\nIn our example, the field is constant along the \\(y\\)-direction, since it was only defined with an \\(x\\)-basis\n\nGrid Space vs. Coefficient Space\n\nIn grid space, vector and tensor components align with the unit vectors of the tangent space (e.g., \\(\\hat{x}\\), \\(\\hat{y}\\) in Cartesian coordinates)\nIn coefficient space, the same is true for Cartesian domains\nHowever, in curvilinear domains (polar, spherical, etc.), the components may be recombined during spectral transforms, making the coefficient-space data harder to interpret component-wise\n\nPractical Tip\nBecause of this, it’s generally recommended to initialize vector and tensor fields in grid space, where the components correspond directly to the familiar unit vectors\nField scale factors\nChanging Field Resolution with change_scales\n\nThe change_scales method on a field lets you modify the scaling factors used when transforming the field’s data into grid space\nIf you set field data using grid arrays, make sure that the field and grid use the same scale factors, otherwise you’ll get shape errors\n\nPractical Uses\n\nLarge scale factors \\(( &gt; 1)\\): Interpolate the field data onto a higher-resolution grid\nSmall scale factors \\(( &lt; 1)\\): View the field on a lower-resolution grid\nBut beware: if the scale factor is less than \\(1\\), you’ll actually lose data during the transform to grid space\n\nHigh-Resolution Sampling with change_scales\nWe can sample a field on a higher-resolution grid by increasing its scale factors using the change_scales method. This effectively interpolates the field data, giving us a finer view of its structure in grid space\n\nf.change_scales(4)\n\n# Plot grid values\nf['g']\nplot_bot_2d(f, title=\"f['g']\", figkw=figkw);\n\n\n\n\n\n\n\n\nScale Factors in Data Access\nScale factors can also be passed as a second argument when setting or retrieving field data through the __getitem__ / __setitem__ interface:\n\nfield['g', 2] \\(\\;\\rightarrow\\;\\) get the grid-space data at 2× resolution\nfield['g', 0.5] \\(\\;\\rightarrow\\;\\) get the grid-space data at half resolution\nfield['g', 2] = ... \\(\\;\\rightarrow\\;\\) set the grid-space data at 2× resolution\n\nThis provides a convenient way to work with data at different resolutions without calling change_scales explicitly\n\nprint(f['g', 1].shape)\nprint(f['g', 2].shape)\n\n(64, 64)\n(128, 128)\n\n\n\n\nJ.2.2 Operators\nArithmetic with fields\nOperator Classes\n\nIn Dedalus, mathematical operations on fields—such as arithmetic, differentiation, integration, and interpolation—are represented by Operator classes\nAn operator instance corresponds to a specific mathematical expression\nOperators provide an interface for deferred evaluation, meaning the expression is stored symbolically and can be evaluated later, even if its arguments evolve over time\n\nArithmetic Operators\n\nDedalus lets you write arithmetic operations between fields (or between fields and scalars) using Python’s infix operators (+, -, *, /, **)\nThis makes expressions look natural, just like standard math notation\n\n\ng_op = 1 -2 *f\nprint(g_op)\n\nC(C(1)) + -1*2*f\n\n\nOperator Objects and Evaluation\n\nWhen we perform arithmetic with fields, the result is not a field, but an operator object\nFor example, the expression might symbolically represent “add 1 to the product of -1, 2, and our field”\nTo actually compute the operation, we call the operator’s .evaluate() method\nThis returns a new field containing the numerical result\n💡Important: the dealiasing scale factors set when the basis was instantiated are always applied during operator evaluation\n\n\ng = g_op.evaluate()\n\n# Plot grid values\ng['g']\nplot_bot_2d(g, title=\"g['g']\", figkw=figkw);\n\n\n\n\n\n\n\n\nBuilding expressions\nBuilding Expression Trees\n\nOperator instances can themselves be passed as arguments to other operators\nThis allows us to build expression trees that represent more complicated mathematical formulas in a natural, symbolic way\n\n\nh_op = 1 /np.cosh(g_op +2.5)\nprint(h_op)\n\nPow(cosh(C(C(1)) + -1*2*f + C(C(2.5))), -1)\n\n\nVisualizing Operator Structures\n\nOperator signatures can become hard to read when expressions get complicated\nTo make this easier, Dedalus provides a helper in dedalus.tools that lets us plot the operator’s structure\nThis visualization shows the tree of operations (e.g. additions, multiplications, derivatives), making it clear how the overall expression is built\n\n\nfrom dedalus.tools.plot_op import plot_operator\n\nplot_operator(h_op, figsize=6, fontsize=14, opsize=0.4)\n\n\n\n\n\n\n\n\nEvaluating Operators\n\nOnce an operator is constructed (even a complex expression tree), you can evaluate it to get a field containing the result\nThis is done using the .evaluate() method of the operator\n\n\nh = h_op.evaluate()\n\n# Plot grid values\nh['g']\nplot_bot_2d(h, title=\"h['g']\", figkw=figkw);\n\n\n\n\n\n\n\n\nDeferred evaluation\nDeferred Evaluation with Operators\n\nOperator objects in Dedalus symbolically represent an operation on their field arguments\nThey use deferred evaluation, meaning that the operation is not computed immediately\nIf the data of the field arguments changes, re-evaluating the operator with .evaluate() produces a new result that reflects the updated field data\n💡Key insight: This allows you to reuse the same operator object on different field states, which is especially useful in time-dependent simulations where fields evolve over time\n\n\n# Change scales back to 1 to build new grid data\nf.change_scales(1)\nf['g'] = 3 *np.cos(1.5 *np.pi *y)**2 *np.cos(x /2)**4 +3 *np.exp(-((2 *x +2)**2 +(4 *y +4 /3)**2)) +3 *np.exp(-((2 *x +2)**2 + (4 *y -4 /3)**2))\n\n# Plot grid values\nf['g']\nplot_bot_2d(f, title=\"f['g']\", figkw=figkw);\n\n\n\n\n\n\n\n\n\nh = h_op.evaluate()\n\n# Plot grid values\nh['g']\nplot_bot_2d(h, title=\"h['g']\", figkw=figkw);\n\n\n\n\n\n\n\n\nDifferential operators\n\nIn Dedalus, operators are also used to compute derivatives of fields\nFor one-dimensional bases, partial derivatives are implemented using the Differentiate operator\nTo compute a derivative, you specify the coordinate with respect to which you want to differentiate\n\n\nfx = d3.Differentiate(f, coords['x'])\n\nVector Calculus Operators in Dedalus\nFor multidimensional problems, Dedalus provides built-in vector calculus operators:\n\n\n\n\n\n\n\n\nOperator\nApplicable Fields\nDescription\n\n\n\n\nGradient\nArbitrary fields\nComputes the gradient of a scalar or vector field\n\n\nDivergence\nVector or tensor fields\nComputes the divergence\n\n\nCurl\nVector fields\nComputes the curl\n\n\nLaplacian\nArbitrary fields\nComputes the Laplacian, defined as the divergence of the gradient\n\n\n\n\nThese operators return operator objects that can be evaluated to produce fields with the computed results\n\nThey provide a high-level, symbolic interface for common PDE operations, simplifying multidimensional derivations\n\nOptional Arguments for Vector Calculus Operators\n\nA coordinate system can optionally be specified as the tangent bundle for the Gradient and Laplacian operators.\n\nIf not provided, they default to the distributor’s coordinate system\n\nA tensor index can optionally be specified for the Divergence operator\n\nDefaults to 0 if not provided.\n\nTensor ranks propagate naturally:\n\nThe gradient of a rank-0 (scalar) field is rank-1 (vector)\nThe gradient of a rank-1 (vector) field is rank-2 (tensor), and so on\n\n\n\nlap_f = d3.Laplacian(f).evaluate()\ngrad_f = d3.Gradient(f).evaluate()\n\nprint('f shape:', f['g'].shape)\nprint('Grad(f) shape:', grad_f['g'].shape)\nprint('Lap(f) shape:', lap_f['g'].shape)\n\ndiv_grad_f = d3.Divergence(d3.Gradient(f)).evaluate()\n\nprint('Lap(f) is Div(Grad(f)):', np.allclose(lap_f['g'], div_grad_f['g']))\n\nf shape: (96, 96)\nGrad(f) shape: (2, 96, 96)\nLap(f) shape: (96, 96)\nLap(f) is Div(Grad(f)): True\n\n\nOperators for Tensor Field Components\nDedalus provides several operators specifically for manipulating components of tensor fields:\n\n\n\n\n\n\n\nOperator\nDescription\n\n\n\n\nTrace\nContracts two specified indices of a tensor (defaults to indices 0 and 1)\n\n\nTransposeComponents\nSwaps two specified indices of a tensor (defaults to indices 0 and 1)\n\n\nSkew\nApplies a 90-degree positive rotation to 2D vector fields\n\n\n\n\ngrad_u = d3.Gradient(u)\ntranspose_grad_u = d3.TransposeComponents(grad_u)\n\nIntegrals and Averages\n\nDedalus provides operators for computing integrals and averages of scalar fields:\n\n\n\n\n\n\n\n\nOperator\nDescription\n\n\n\n\nIntegrate\nComputes the integral of a scalar field over specified coordinates or a coordinate system\n\n\nAverage\nComputes the average of a scalar field over specified coordinates or a coordinate system\n\n\n\n\n# Total integral of the field\nf_int = d3.Integrate(f, ('x', 'y'))\nprint('f integral:', f_int.evaluate()['g'])\n\n# Average of the field\nf_ave = d3.Average(f, ('x', 'y'))\nprint('f average:', f_ave.evaluate()['g'])\n\nf integral: [[9.42458659]]\nf average: [[0.74998477]]\n\n\nInterpolation\n\nDedalus provides the Interpolate operator for interpolating field data along a coordinate\nInterpolation can also be done using the __call__ method on fields or operators, with keywords specifying the coordinate and position\nFor convenience, the strings 'left', 'right', and 'center' can be used to refer to the endpoints and middle of 1D intervals\n\n\nf00 = f(x=0, y=0)\nprint('f(0,0):', f00.evaluate()['g'])\n\nf(0,0): [[3.01857352]]",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>J</span>  <span class='chapter-title'>DEDALUS</span>"
    ]
  },
  {
    "objectID": "x_PDE_dedalus.html#problems-and-solvers",
    "href": "x_PDE_dedalus.html#problems-and-solvers",
    "title": "Appendix J — DEDALUS",
    "section": "J.3 Problems and Solvers",
    "text": "J.3 Problems and Solvers\nThis tutorial covers the basics of setting up and solving problems in Dedalus. Dedalus can symbolically formulate and solve a wide range of problems, including:\n\nInitial value problems (IVPs)\nBoundary value problems (BVPs)\nEigenvalue problems (EVPs)\n\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nimport dedalus.public as d3\nfrom dedalus.extras.plot_tools import plot_bot_2d\n\nfigkw = {'figsize':(6, 4), 'dpi':150}\n\n\nJ.3.1 Problems\n\nJ.3.1.1 Standard Formulation of Problems in Dedalus\nDedalus standardizes the formulation of all initial value problems (IVPs) by taking systems of:\n\nSymbolically specified equations\nBoundary conditions\n\nand rewriting them into the following generic form:\n\\[\\mathcal{M} \\cdot \\partial_t \\mathcal{X} + \\mathcal{L} \\cdot \\mathcal{X} = \\mathcal{F}(\\mathcal{X}, t)\\]\nwhere:\n\n\\(\\mathcal{M}\\), \\(\\mathcal{L}\\): matrices of linear differential operators\n\\(\\mathcal{X}\\): state vector of the unknown fields\n\\(\\mathcal{F}(\\mathcal{X}, t)\\): vector of general nonlinear expressions\n\nInterpretation\n\nThis form encapsulates both:\n\nPrognostic/evolution equations (contain time derivatives)\nDiagnostic/algebraic constraints (no time derivatives)\n\nThese apply in both the interior of the domain and on the boundaries\n\nRestrictions\n\nLeft-hand side (LHS):\n\nMust be first-order in temporal derivatives\nMust be linear in the problem variables\n\nRight-hand side (RHS):\n\nMay contain nonlinear and time-dependent terms\nMust not contain temporal derivatives\n\n\nOther Standard Problem Types\nIn addition to IVPs, Dedalus defines similar standard forms for:\n\nGeneralized eigenvalue problems (EVPs): \\(\\sigma \\mathcal{M} \\cdot \\mathcal{X} + \\mathcal{L} \\cdot \\mathcal{X} = 0\\)\nLinear boundary value problems (LBVPs): \\(\\mathcal{L} \\cdot \\mathcal{X} = \\mathcal{G}\\)\nNonlinear boundary value problems (NLBVPs): \\(\\mathcal{L} \\cdot \\mathcal{X} = \\mathcal{F}(\\mathcal{X})\\)\n\nThese correspond to the Dedalus classes:\n\nIVP\nEVP\nLBVP\nNLBVP\n\n\n\nJ.3.1.2 Creating a Problem Object\nWhen creating a problem object, you must provide a list of the field variables to be solved. You can also pass a dictionary via the namespace argument to make substitutions (operators or functions) available when parsing the equations\n👉 Typically, we suggest passing locals(), so that all script-level definitions are accessible inside the problem\nThe Complex Ginzburg–Landau Equation\nWe will solve the complex Ginzburg–Landau equation (CGLE) for a variable \\(u(x,t)\\) on a finite interval \\(x \\in [0, 300]\\), subject to homogeneous Dirichlet boundary conditions:\n\\[\\partial_t u = u + (1 + i b) \\, \\partial_{xx} u - (1 + i c) |u|^2 u\\]\nwith boundary conditions:\n\\[u(x=0) = u(x=300) = 0\\]\nDiscretization\n\nBasis: We discretize \\(x\\) using a Chebyshev basis\nDealiasing: We use a dealiasing factor of \\(2\\) to correctly resolve the cubic nonlinearity\n\nBoundary Conditions and Tau Terms\nThe current version of Dedalus requires explicitly adding tau terms as unknowns to enforce boundary conditions\n\nEach Dirichlet boundary condition requires one tau term\nSince this problem has two endpoint conditions, we must include two constant tau terms\n\n\n# Bases\nxcoord = d3.Coordinate('x')\ndist = d3.Distributor(xcoord, dtype=np.complex128)\nxbasis = d3.Chebyshev(xcoord, 1024, bounds=(0, 300), dealias=2)\n\n# Fields\nu = dist.Field(name='u', bases=xbasis)\ntau1 = dist.Field(name='tau1')\ntau2 = dist.Field(name='tau2')\n\n# Problem\nproblem = d3.IVP([u, tau1, tau2], namespace=locals())\n\n\n\nJ.3.1.3 Substitutions in Dedalus\nSubstitutions allow you to use non-variable objects inside a problem. These can include:\n\nOther fields (e.g. forcing terms or non-constant coefficients (NCCs))\nOperator aliases (e.g. differentiation, magnitudes, helper functions)\nParameters or constants used in the PDE\n\nNon-Constant Coefficients (NCCs)\n\nIf an NCC appears on the LHS of an equation, it will couple the dimensions associated with its bases\nTherefore, you should only use bases corresponding to the non-constant dimensions\n\nFor example, consider a 3D domain with:\n\nFourier basis in \\(x\\)\nFourier basis in \\(y\\)\nChebyshev basis in \\(z\\)\n\nWe can add a simple non-constant coefficient in \\(z\\) as:\n\n#ncc = dist.Field(bases=zbasis)\n#ncc['g'] = z**2\n\nOperator Aliases and Helper Substitutions\nSubstitutions can also define aliases for operators or helper functions to make equation entry cleaner. For example:\n\n# Substitutions\ndx = lambda A: d3.Differentiate(A, xcoord)\nmagsq_u = u *np.conj(u)\nb = 0.5\nc = -1.76\n\nTau Polynomials\nTau polynomials (see the Tau Method documentation) can also be defined as substitutions\n\n# Tau polynomials\ntau_basis = xbasis.derivative_basis(2)\np1 = dist.Field(bases=tau_basis)\np2 = dist.Field(bases=tau_basis)\n\n# Define tau modes\np1['c'][-1] = 1\np2['c'][-2] = 2\n\nHere:\n\ntau_basis is a derivative basis used to enforce boundary conditions\np1 and p2 are tau polynomial fields with coefficients set explicitly\n\n\n\nJ.3.1.4 Entering Equations in Dedalus\nDedalus allows equations to be entered in two equivalent ways:\nAs operator pairs:\n\nproblem.add_equation((LHS, RHS))\n\nwhere both LHS and RHS are symbolic operator expressions\nAs strings:\n\nproblem.add_equation(\"LHS = RHS\")\n\nwhere string parsing recognizes:\n\nSubstitutions (from the namespace argument)\nAll built-in operators\nSome operator abbreviations (e.g. dt for time derivative)\n\nWe add the main PDE and enforce homogeneous Dirichlet boundary conditions:\n\n# Main PDE:\n#   ∂_t u = u + (1 + i b) ∂_xx u - (1 + i c)|u|²u\n#\n# `dt(u)`: time derivative of $u$\n# `dx(dx(u))`: second derivative in $x$\n# `tau1 *p1 +tau2 *p2`: tau terms used to enforce boundary conditions\n#   `magsq_u`: substitution for $|u|^2$\n\nproblem.add_equation(\n  \"dt(u) -u -(1 +1j*b) *dx(dx(u)) +tau1 *p1 +tau2 *p2 = - (1 +1j*c) *magsq_u *u\"\n)\n\n# Boundary conditions:\nproblem.add_equation(\"u(x='left') = 0\")\nproblem.add_equation(\"u(x='right') = 0\")\n\n{'eqn': interp(&lt;Field 6250088944&gt;, x=right),\n 'LHS': interp(&lt;Field 6250088944&gt;, x=right),\n 'RHS': 0,\n 'condition': 'True',\n 'tensorsig': (),\n 'dtype': numpy.complex128,\n 'valid_modes': array([ True]),\n 'M': 0,\n 'L': interp(&lt;Field 6250088944&gt;, x=right),\n 'F': &lt;Field 6250090176&gt;,\n 'domain': &lt;dedalus.core.domain.Domain at 0x17488c260&gt;,\n 'matrix_dependence': array([ True]),\n 'matrix_coupling': array([ True])}\n\n\nThis means Dedalus:\n\nConverts the equation into symbolic operators\nTracks how it couples to the problem variables\nStores metadata for matrix assembly and solver setup\n\n\n\n\nJ.3.2 Solvers\nBuilding a solver\nEach problem type (IVP, EVP, LBVP, and NLBVP) has a corresponding solver class that carries out the solution steps for that problem. Solvers are created using the problem.build_solver method\nFor IVPs, a time-stepping method must be chosen when building the solver. Several multistep and Runge–Kutta IMEX schemes are available (see the list in the timesteppers module), and they can be selected by name\n\n# Build solver\nsolver = problem.build_solver(d3.RK222)\n\n2025-09-27 10:13:39,634 subsystems 0/1 INFO :: Building subproblem matrices 1/1 (~100%) Elapsed: 0s, Remaining: 0s, Rate: 1.2e+01/s\n\n\nFor IVPs, the stopping criteria that halt time evolution are specified by setting the attributes solver.stop_iteration, solver.stop_wall_time (seconds since solver instantiation), and/or solver.stop_sim_time\nFor example, to stop at 500 in simulation units:\n\n# Stopping criteria\nsolver.stop_sim_time = 500\n\nFor IVPs and nonlinear BVPs, the initial conditions are specified by directly modifying the state variable data before starting the simulation\n\n# Setup a sine wave\nx = dist.local_grid(xbasis)\nu['g'] = 1e-3 *np.sin(5 *np.pi *x /300)\n\nSolving/iterating a problem\nIVPs are advanced in time using the solver.step method with a specified timestep. EVPs are solved using the solver.solve_dense or solver.solve_sparse methods. LBVPs are solved using the solver.solve method. NLBVPs are iterated using the solver.newton_iteration method\nThe control logic for the main loop of a Dedalus IVP simulation is implemented explicitly in the simulation script. The solver.proceed property will automatically switch from True to False once any of the defined stopping criteria have been satisfied\nIn the following example, we advance our problem until the halting condition is reached, saving the grid values of u every few iterations. We explicitly retrieve the grid values of u with a scale factor of 1, since field data is internally converted to the dealiasing scales when the RHS operators are evaluated at each step. We also copy the grid values explicitly, because the integrator updates the field’s data in place. This run should take only a few seconds to complete\n\n# Setup storage\nu_list = [u['g', 1].copy()]\nt_list = [solver.sim_time]\n\n# Main loop\ntimestep = 0.05\nwhile solver.proceed:\n    solver.step(timestep)\n    if solver.iteration % 10 == 0:\n        u_list.append(u['g', 1].copy())\n        t_list.append(solver.sim_time)\n    if solver.iteration % 1000 == 0:\n        print(f'Completed iteration {solver.iteration}')\n\n# Convert storage lists to arrays\nu_array = np.array(u_list)\nt_array = np.array(t_list)\n\nCompleted iteration 1000\nCompleted iteration 2000\nCompleted iteration 3000\nCompleted iteration 4000\nCompleted iteration 5000\nCompleted iteration 6000\nCompleted iteration 7000\nCompleted iteration 8000\nCompleted iteration 9000\nCompleted iteration 10000\n2025-09-27 10:13:46,537 solvers 0/1 INFO :: Simulation stop time reached.\n\n\nNext, we generate a space–time plot of the solution magnitude:\n\n# Plot solution\nplt.figure(figsize=(6, 7), dpi=100)\n\nplt.pcolormesh(x, t_array, np.abs(u_array), shading='nearest')\n\nplt.colorbar()\nplt.xlabel('x')\nplt.ylabel('t')\nplt.title('Hole-defect chaos in the CGLE: |u|')\nplt.tight_layout()",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>J</span>  <span class='chapter-title'>DEDALUS</span>"
    ]
  },
  {
    "objectID": "x_PDE_dedalus.html#analysis-and-post-processing",
    "href": "x_PDE_dedalus.html#analysis-and-post-processing",
    "title": "Appendix J — DEDALUS",
    "section": "J.4 Analysis and Post-processing",
    "text": "J.4 Analysis and Post-processing\nThis tutorial introduces the basics of data analysis and post-processing in Dedalus. Analysis tasks can be defined symbolically and are automatically saved to distributed HDF5 files\n\nimport subprocess\nimport h5py\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nimport dedalus.public as d3\n\n\nfrom pathlib import Path\nimport shutil\n\n# Set the path for the analysis output folder\nanalysis_dir = Path('dedalus/analysis')\n\n# 1. Safely remove the existing folder if it exists\nif analysis_dir.exists() and analysis_dir.is_dir():\n    shutil.rmtree(analysis_dir)\n    print(f\"Deleted existing folder: {analysis_dir}\")\n\n# 2. Create a new folder\nanalysis_dir.mkdir(parents=True, exist_ok=True)\nprint(f\"Created new folder: {analysis_dir}\")\n\nDeleted existing folder: dedalus/analysis\nCreated new folder: dedalus/analysis\n\n\n\nJ.4.1 Analysis\nDedalus provides a framework for defining, evaluating, and saving arbitrary analysis tasks while an initial value problem is running. To get started, let’s set up the complex Ginzburg–Landau problem from the previous tutorial\n\n# Basis\nxcoord = d3.Coordinate('x')\ndist = d3.Distributor(xcoord, dtype=np.complex128)\nxbasis = d3.Chebyshev(xcoord, 1024, bounds=(0, 300), dealias=2)\n\n# Fields\nu = dist.Field(name='u', bases=xbasis)\ntau1 = dist.Field(name='tau1')\ntau2 = dist.Field(name='tau2')\n\n# Substitutions\ndx = lambda A: d3.Differentiate(A, xcoord)\nmagsq_u = u *np.conj(u)\nb = 0.5\nc = -1.76\n\n# Tau polynomials\ntau_basis = xbasis.derivative_basis(2)\np1 = dist.Field(bases=tau_basis)\np2 = dist.Field(bases=tau_basis)\np1['c'][-1] = 1\np2['c'][-2] = 2\n\n# Problem\nproblem = d3.IVP([u, tau1, tau2], namespace=locals())\nproblem.add_equation(\n  \"dt(u) -u -(1 +1j*b) *dx(dx(u)) +tau1 *p1 +tau2 *p2 = -(1 +1j *c) *magsq_u *u\"\n)\nproblem.add_equation(\"u(x='left') = 0\")\nproblem.add_equation(\"u(x='right') = 0\")\n\n# Solver\nsolver = problem.build_solver(d3.RK222)\nsolver.stop_sim_time = 500\n\n# Initial conditions\nx = dist.local_grid(xbasis)\nu['g'] = 1e-3 *np.sin(5 *np.pi *x /300)\n\n2025-09-27 10:13:47,472 subsystems 0/1 INFO :: Building subproblem matrices 1/1 (~100%) Elapsed: 0s, Remaining: 0s, Rate: 1.1e+01/s\n\n\nAnalysis handlers\nThe explicit evaluation of analysis tasks during timestepping is managed by the solver.evaluator object. Various handler objects can be attached to the evaluator to control when the evaluator computes their tasks and what happens to the resulting data\nFor example, an internal SystemHandler directs the evaluator to compute the RHS expressions at every iteration and uses the resulting data for the explicit part of the timestepping algorithm\nFor simulation analysis, the most commonly used handler is the FileHandler, which periodically computes tasks and writes the results to HDF5 files. When setting up a file handler, you specify the output directory or file path, as well as the cadence at which the handler’s tasks should be evaluated. This cadence can be specified in terms of any combination of:\n\nSimulation time, with sim_dt\nWall time, with wall_dt\nIteration number, with iter\n\nTo limit file sizes, the output from a file handler is divided into different sets over time, each containing a limited number of writes, which can be controlled using the max_writes keyword when constructing the file handler\nFor example, to evaluate a file handler every 10 iterations:\n\nanalysis = solver.evaluator.add_file_handler(analysis_dir, iter=10, max_writes=400)\n\nYou can attach multiple file handlers to save different sets of tasks at different cadences and to different files\nAdding analysis tasks\nAnalysis tasks are attached to a given handler using the add_task method. Tasks can be specified as operator expressions or as plain text, and are parsed using the same namespace as for equation entry. For each task, you can optionally set the output layout, scaling factors, and a reference name\nFor example, to track the average magnitude of the solution:\n\nanalysis.add_task(d3.Integrate(np.sqrt(magsq_u), 'x') /300, layout='g', name='&lt;|u|&gt;')\n\nFor checkpointing, you can also save all state variables at once using add_tasks:\n\nanalysis.add_tasks(solver.state, layout='g')\n\nOnce the tasks are added, you can run the simulation just as in the previous tutorial. You no longer need to manually save any data inside the main loop. The evaluator will automatically compute and save the specified analysis tasks at the defined cadence\n\n# Main simulation loop\ntimestep = 0.05\nwhile solver.proceed:\n  solver.step(timestep)\n  if solver.iteration % 1000 == 0:\n    print(f'Completed iteration {solver.iteration}')\n\nCompleted iteration 1000\nCompleted iteration 2000\nCompleted iteration 3000\nCompleted iteration 4000\nCompleted iteration 5000\nCompleted iteration 6000\nCompleted iteration 7000\nCompleted iteration 8000\nCompleted iteration 9000\nCompleted iteration 10000\n2025-09-27 10:14:02,505 solvers 0/1 INFO :: Simulation stop time reached.\n\n\n\n\nJ.4.2 Post-processing\nFile Arrangement\nBy default, the output files for each FileHandler are organized as follows:\n\nBase folder\n\n\nNamed according to the first argument provided when constructing the file handler, e.g., dedalus/analysis/\n\n\nSet files\n\n\nWithin the base folder, HDF5 files are created for each output set\nEach file has the same base name with a set number appended, e.g., analysis_s1.h5\n\n\nParallel execution\n\n\nWhen running in parallel, each set may contain subfolders with individual HDF5 files for each process, e.g., dedalus/analysis_s1/analysis_s1_p0.h5\nThese process files are virtually merged into the corresponding set files and generally do not need to be accessed directly\nIf relocating the data, the set folders and process files should be moved or copied along with the set files\n\nLet’s examine the output files from our example problem. Since we have a total of 1000 outputs and max_writes=400 per file, we should see three output sets\n\nprint(subprocess.check_output(\"find dedalus/analysis | sort\", shell=True).decode())\n\ndedalus/analysis\ndedalus/analysis/analysis_s1.h5\ndedalus/analysis/analysis_s2.h5\ndedalus/analysis/analysis_s3.h5\n\n\n\nHandling data\nDedalus produces HDF5 files that can be accessed directly using h5py or loaded into xarray. Here, we first focus on directly interacting with the HDF5 files via h5py\nEach HDF5 file contains a tasks group, which includes a dataset for each task assigned to the file handler\n\nThe first dimension of each dataset corresponds to time\nSubsequent dimensions correspond to vector/tensor components of the task (if applicable)\nThe remaining dimensions correspond to the spatial axes of the task\n\nThe HDF5 datasets are self-describing, with dimensional scales attached to each axis:\n\nTime axis: simulation time, wall time, iteration number, and write number\nSpatial axes: grid points or modes, depending on the task layout\n\nFor more details, see the h5py documentation\nNow, let’s open the first analysis set file and plot a time series of the average magnitude of the solution\n\nwith h5py.File(analysis_dir/\"analysis_s1.h5\", mode='r') as file:\n  # Load datasets\n  mag_u = file['tasks']['&lt;|u|&gt;']\n  t = mag_u.dims[0]['sim_time']\n  \n  # Plot data\n  fig = plt.figure(figsize=(6, 4), dpi=100)\n  \n  plt.plot(t[:], mag_u[:].real)\n  plt.xlabel('t')\n  plt.ylabel('&lt;|u|&gt;')\n\n\n\n\n\n\n\n\nIt is also possible to load analysis tasks directly into an xarray.DataArray, and then utilize xarray’s plotting utilities for visualization\n\ntasks = d3.load_tasks_to_xarray(analysis_dir/\"analysis_s1.h5\")\n\nfig = plt.figure(figsize=(6, 4), dpi=100)\ntasks['&lt;|u|&gt;'].real.plot();\n\n\n\n\n\n\n\n\nNow let’s examine the saved solution over space and time. This time, instead of plotting the amplitude, we will plot the phase of the solution\n\nwith h5py.File(analysis_dir/\"analysis_s1.h5\", mode='r') as file:\n  # Load datasets\n  u = file['tasks']['u']\n  t = u.dims[0]['sim_time']\n  x = u.dims[1][0]\n  \n  # Plot data\n  u_phase = np.arctan2(u[:].imag, u[:].real)\n  \n  plt.figure(figsize=(6, 7), dpi=100)\n  \n  plt.pcolormesh(x[:], t[:], u_phase, shading='nearest', cmap='twilight_shifted')\n  plt.colorbar(label='phase(u)')\n\n  plt.xlabel('x')\n  plt.ylabel('t')\n  plt.title('Hole-defect chaos in the CGLE')\n\n\n\n\n\n\n\n\nAgain, this process can be simplified by loading the data into xarray and using its plotting utilities\n\ntasks = d3.load_tasks_to_xarray(analysis_dir/\"analysis_s1.h5\")\nu_phase = np.arctan2(tasks['u'].imag, tasks['u'].real)\nu_phase.name = \"phase(u)\"\n\nplt.figure(figsize=(6, 7), dpi=100)\n\nu_phase.plot(x='x', y='t', cmap='twilight_shifted')\nplt.title('Hole-defect chaos in the CGLE');",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>J</span>  <span class='chapter-title'>DEDALUS</span>"
    ]
  }
]