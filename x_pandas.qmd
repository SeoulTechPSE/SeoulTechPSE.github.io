---
title: "Pandas"
jupyter: python3
---

# Pandas {#sec-pandas}

[![](figures/pandas_logo.png){width="40%" fig-align="center"}](https://https://pandas.pydata.org)

```{python}
import numpy as np

import pyarrow as pa
import pandas as pd
string_pa = pd.ArrowDtype(pa.string())

print('NumPy:  ', np.__version__)
print('PyArrow:', pa.__version__)
print('Pandas: ', pd.__version__)
```

## Of Types Python, NumPy, and Pyarrow

### Integer Types

```{python}
small_values = [1, 99, 127]
large_values = [2**31, 2**63, 2**100]
missing_values = [None, 1, -45]
```

* In Pandas 1.x, we could use the `pd.Series` constructor to create a series of integers:

```{python}
small_ser = pd.Series(small_values)
small_ser
```

* Note that the type of this series is `int64`. This is a `NumPy` type. Pandas 1.x uses `NumPy` to store the data

* Because these numbers don’t exceed 127, we could use the `int8` type instead. This would save memory and make the code run faster. We can use the `astype` method to convert the series to `int8`:

```{python}
small_ser.astype('int8')
```

```{python}
small_ser = pd.Series(small_values, dtype='int8')
small_ser
```

* We add the string `[pyarrow]` to the dtype to indicate that we want to use the PyArrow extension type

```{python}
small_ser_pa = pd.Series(small_values, dtype='int8[pyarrow]')
small_ser_pa
```

* Let’s convert the `large_values` list to both a `NumPy` and a `PyArrow`-backed series. Here’s NumPy:

```{python}
large_ser = pd.Series(large_values)
large_ser
```

* And here’s `PyArrow`:

```{python}
try:
  large_ser = pd.Series(large_values, dtype='int64[pyarrow]')
except Exception as e:
  print(e)
```

* Note that the `NumPy`-backed series has a type of `object`. Because the numbers are larger than an `int64`, `NumPy` will store the values but store them as Python objects. This will make the code run slower and use more memory.

* The `PyArrow` backend doesn’t gracefully fall back to Python objects. Instead, it will raise an error

* Now let’s look at the `missing_values` list. We will again convert it to both a `NumPy` and `PyArrow`-backed series:

```{python}
missing_ser = pd.Series(missing_values)
missing_ser
```

```{python}
try:
  missing_ser = pd.Series(missing_values, dtype='int8')
except Exception as e:
  print(e)
```

* Here’s the PyArrow-backed series of the missing_values list:

```{python}
missing_ser_pa = pd.Series(missing_values,
                  dtype='int8[pyarrow]')
missing_ser_pa
```

* One more thing to note about type conversion. The `NumPy` series will let you convert to other sizes, but the value will overflow if it is too large:

```{python}
medium_values = [2**15+5, 2**31-8, 2**63]

medium_ser = pd.Series(medium_values)
medium_ser
```

```{python}
medium_ser.astype('int8')
```

* However, the PyArrow series will raise an error if the value is too large:

```{python}
try:
  medium_ser.astype('int8[pyarrow]')
except Exception as e:
  print(e)
```

### Floating Point Types

* Here are some Python lists with floating-point data. One has missing values, one doesn’t, and a final one has text values (like meteorological data with 'T' for trace amounts of rain):

```{python}
float_vals = [1.5, 2.7, 127.0]
float_missing = [None, 1.5, -45.0]
float_rain = [1.5, 2.7, 0.0, 'T', 1.5, 0]
```

* Let’s convert the first two to Pandas 1.x series:

```{python}
pd.Series(float_vals)
```

```{python}
pd.Series(float_missing)
```

* Let’s convert the list to both a PyArrow and backed series:

```{python}
pd.Series(float_vals, dtype='float64[pyarrow]')
```

```{python}
pd.Series(float_missing, dtype='float64[pyarrow]')
```

* Let’s look at the `float_rain` list. It has the string value `'T'` for trace amounts of rain. Let’s try to convert it to a numeric series

```{python}
pd.Series(float_rain)
```

```{python}
pd.Series(float_rain).replace('T', '0.0').astype('float64')
```

* Let’s do the same for PyArrow:

```{python}
(
  pd.Series(float_rain)
  .replace('T', '0.0')
  .astype('float')
  .astype('float64[pyarrow]')
)
```

### String Data

* In this section, we will focus on columns with free-form text

* Sadly, we can’t just use `string[pyarrow]` as a type to get the new Pandas 2 `pyarrow` types. This is because this type was introduced back in Pandas 1.5 era and the operations on it will generally return legacy `NumPy` typed data

  ```python
           string_pa = pd.ArrowDtype(pa.string())
  ```

```{python}
text_freeform = ['My name is Jeff', 
                 'I like pandas', 
                 'I like programming']

text_with_missing = ['My name is Jeff', 
                      None, 
                     'I like programming']               
```

* First, the Pandas 1.x series:

```{python}
pd.Series(text_freeform)
```

```{python}
pd.Series(text_with_missing)
```

* Pandas 1.x stores the str type as Python objects. This is because `NumPy` doesn’t support strings

* Let’s use the new Pandas 2.0 string type:

```{python}
pd.Series(text_freeform, dtype=string_pa)
```

```{python}
pd.Series(text_with_missing, dtype=string_pa)
```

* Notice that the type of the series is `string[pyarrow]`. This is a big improvement over Pandas 1.x. It uses less memory, and it is faster than Pandas 1.x

### Categorical Data

* Categorical data is string data that has a low cardinality. `Pandas` stores categorical data in a particular way. It keeps the unique values in a separate array and then stores the values as integers that refer to the individual values. This can save memory and can make operations faster

```{python}
states = ['CA', 'NY', 'TX']
months = ['Jan', 'Feb', 'Mar', 'Apr', 
          'May', 'Jun', 'Jul', 'Aug', 
          'Sep', 'Oct', 'Nov', 'Dec']
```

* `NumPy` doesn’t support categorical data natively, but because it is so common, pandas 1.x added support for categorical data. Let’s convert a list of strings to a categorical series:

```{python}
pd.Series(states, dtype='category')
```
* Let’s make an ordered categorical series from the months of the year:

```{python}
pd.Series(months, dtype='category')
```

* This is not ordered. If we sort the series, it will sort alphabetically:

```{python}
pd.Series(months, dtype='category').sort_values()
```

* To sort the data, we need to first create an ordered categorical type and pass that type in as the `dtype` parameter:

```{python}
month_cat = pd.CategoricalDtype(categories=months, ordered=True)
pd.Series(months, dtype=month_cat).sort_values()
```

* `PyArrow` doesn’t have “categorical” type, but it does have a “dictionary” type. However, the dictionary type isn’t exposed directly in pandas 2. Instead, if you want to use a categorical type, I suggest you use the Pandas 1.x categorical type

```{python}
pd.Series(months, dtype=string_pa).astype(month_cat)
```

### Dates and Times

* `Pandas` makes it convenient to work with dates and times. It has several data types that can be used to represent dates and times. However, we generally want to use `datetime64[ns]` for Pandas 1.x and `timestamp[ns][pyarrow]` for Pandas 2.0

```{python}
import datetime as dt

dt_list = [dt.datetime(2020, 1, 1, 4, 30), 
           dt.datetime(2020, 1, 2), 
           dt.datetime(2020, 1, 3)]

string_dates = ['2020-01-01 04:30:00', 
                '2020-01-02 00:00:00', 
                '2020-01-03 00:00:00']

string_dates_missing = ['2020-01-01 4:30', 
                        None, 
                        '2020-01-03']

epoch_dates = [1577836800, 
               1577923200, 
               1578009600]
```

* Let’s try and convert these to Pandas 1.x series. Most of these convert with little issue:

```{python}
pd.Series(dt_list)
```

```{python}
pd.Series(string_dates, dtype='datetime64[ns]')
```

```{python}
pd.Series(string_dates_missing, dtype='datetime64[ns]')
```

* Let’s use `datetime64[s]` to convert from seconds (instead of nanoseconds):

```{python}
pd.Series(epoch_dates, dtype='datetime64[s]')
```

* Let’s convert these to Pandas 2.0 series:

```{python}
pd.Series(dt_list, dtype='timestamp[ns][pyarrow]')
```

```{python}
pd.Series(string_dates, dtype='timestamp[ns][pyarrow]')
```

```{python}
pd.Series(epoch_dates).astype('timestamp[s][pyarrow]')
```

## Series

* This section introduces the `Series` object, the first of the two core `pandas` objects. The `Series` is a one-dimensional array-like object that is used to model a single column or row of data

### The pandas Series

```{python}
songs1 = pd.Series([145, 142, 38, 13], name='counts')
songs1
```

* You will want to use the `pyarrow` backend as it is more efficient for both computation and memory usage

```{python}
songs2 = pd.Series([145, 142, 38, 13], name='counts', 
              dtype='int64[pyarrow]')
songs2
```

```{python}
songs2.index
```

```{python}
songs2_ = pd.Series([145, 142, 38, 13],
  name='counts',
  index=['Paul', 'John', 'George', 'Ringo'],
  dtype='int64[pyarrow]')

songs2_
```

```{python}
songs2_.index
```

### The NA value

* If you are using the `NumPy` backend, you will see `NaN `instead of `<NA>`. The `NumPy` backend only supports missing values for floating point types

```{python}
nan_series = pd.Series([2, np.nan], 
      index=['Ono', 'Clapton'])

nan_series      
```

* If we create this same series using the `pyarrow` backend, the integer type is preserved

```{python}
nan_series2 = pd.Series([2, np.nan], 
      index=['Ono', 'Clapton'],
      dtype='int64[pyarrow]')

nan_series2  
```

```{python}
nan_series2.count()
```

```{python}
nan_series2.size
```

### Similar to NumPy

```{python}
numpy_ser = np.array([145, 142, 38, 13])
```

```{python}
numpy_ser[1]
```

```{python}
songs2_.iloc[1]
```

```{python}
numpy_ser.mean()
```

```{python}
songs2_.mean()
```

* We can use set operations to determine the methods that are common to both types:

```{python}
len(set(dir(numpy_ser)) & set(dir(songs2_)))
```

* In this example, we will make a mask(*a boolean array*):

```{python}
mask = songs2_ > songs2_.median()  # boolean array
mask
```

```{python}
songs2_[mask]
```

```{python}
numpy_ser[numpy_ser > np.median(numpy_ser)]
```

### Categorical Data

* To create a category, we pass `dtype="category"` into the `Series` constructor. Alternatively, we can call the `.astype("category")` method on a series:

```{python}
s1 = pd.Series(['s', 'm', 'l'], dtype='category')
s1
```

* By default, categories don’t have an ordering. We can verify this by inspecting the `.cat` attribute that has various properties:

```{python}
s1.cat.ordered
```

* We can create a type with the `CategoricalDtype` constructor and the appropriate parameters to convert a non-categorical series to an ordered category. Then we pass this type into the `.astype` method:

```{python}
s2 = pd.Series(['m', 'l', 'xs', 's', 'xl'], dtype=string_pa)

size_type = pd.CategoricalDtype(
    categories=['s', 'm', 'l'], ordered=True)

s3 = s2.astype(size_type)
s3
```

* In this case, we limited the categories to `'s'`, `'m'`, and `'l'`, but the data had values that were not in those categories. Converting the data to a category type replaces those extra values with `NaN`

* If we have ordered categories, we can make comparisons on them:

```{python}
s3 > 's'
```

* The prior example created a new `Series` from existing data that was not categorical. We can also add ordering information to categorical data. We need to make sure that we specify all of the members of the category, or `pandas` will throw a `ValueError`:

```{python}
try:
  s1.cat.reorder_categories(['xs', 's', 'm', 'l', 'xl'], 
                             ordered=True)
except ValueError as e:
  print(e)
```

* This error is because we are adding ordering to new categories. We can list the current categories with the `.cat.categories` attribute:

```{python}
s1.cat.categories
```

* We need to make sure that the category is aware of all of the valid values. We can do this by calling the `.add_categories` method before calling `.reorder_categories`:

```{python}
( s1
  .cat.add_categories(['xs', 'xl', ])
  .cat.reorder_categories(['xs', 's', 'm', 'l', 'xl'],
    ordered=True)
)
```

### Loading the Data and `dir` Function

* We use the engine backend so that `pandas` uses the `pyarrow` library to parse the CSV. This library tends to read files more quickly than the standard `pandas` implementation

```{python}
url = 'data/vehicles.csv.zip'
df = pd.read_csv(url, 
                 dtype_backend='pyarrow', 
                 engine='pyarrow')
```

* The first columns in the dataset we will investigate are `city08` and `highway08`, which provide information on miles per gallon usage while driving around in the city and highway, respectively:

```{python}
city_mpg = df.city08
highway_mpg = df.highway08
```

```{python}
city_mpg
```

```{python}
highway_mpg
```

* The built-in `dir` function will list the attributes of an object. Let’s examine how many attributes there are on a series:

```{python}
len(dir(city_mpg))
```

## Operators

### Dunder Methods

```{python}
(city_mpg +highway_mpg) /2
```

```{python}
city_mpg.__add__(highway_mpg).__truediv__(2)
```

* Because most `pandas` methods do not mutate data in place but instead return a new object, we can keep tacking on method calls to the returned object

```{python}
city_mpg.add(highway_mpg).truediv(2)
```

* *Chaining* makes the code easy to read and understand. We can chain with operators as well, but we must wrap the operation with parentheses

```{python}
(
  city_mpg
  .add(highway_mpg)
  .truediv(2)  
)
```

### Index Alignment

* Because of index alignment, you will want to make sure that the indexes:

  * Are unique (no duplicates)
  * Are common to both series

```{python}
s1 = pd.Series([10, 20, 30], index=[1,2,2])
s2 = pd.Series([35, 44, 53], index=[2,2,4], name='s2')
```

```{python}
s1
```

```{python}
s2
```

```{python}
s1 +s2
```

* The dunder methods have a `fill_value` parameter that changes this behavior. If one of the operands is missing, it will use the fill_value instead

```{python}
s1.add(s2, fill_value=0)
```

### Broadcasting

```{python}
s2 +5
```

* With many math operations, these are optimized and happen very quickly in the CPU. This is called *vectorization*

## Aggregate Methods

* Aggregate methods collapse the values of a series down to a scalar

### Agregations

```{python}
city_mpg.mean()
```

* There are also a few aggregate properties

```{python}
city_mpg.is_unique
```

```{python}
city_mpg.is_monotonic_increasing
```

* One method to be aware of is the `.quantile` method

```{python}
city_mpg.quantile()
```

```{python}
city_mpg.quantile(.9)
```

```{python}
city_mpg.quantile([.1, .5, .9])
```

### Count and Mean of an Attribute

* If we want the count and percentage of cars with mileage greater than 20, we can use the following code:

```{python}
(
  city_mpg
  .gt(20)
  .sum()
)
```

* If you want to calculate the percentage of values that meet some criteria, you can apply the `.mean` method:

```{python}
(
  city_mpg
  .gt(20)
  .mean()
) * 100
```

### `.agg` and Aggregation Strings

* You can use `.agg` to calculate the mean:

```{python}
city_mpg.agg('mean')
```

* However, that is easier with `city_mpg.mean()`. Where `.agg` shines is in the ability to perform multiple aggregations

```{python}
def second_to_last(s):
  return s.iloc[-2]
```

```{python}
city_mpg.agg(['mean', 'var', 'max', second_to_last])
```

## Conversion Methods

```{python}
city_mpg.astype('int16[pyarrow]')
```

```{python}
try:
  city_mpg.astype('int8[pyarrow]')
except Exception as e:
  print(e)
```

```{python}
np.iinfo('uint8')
```

```{python}
np.finfo('float32')
```

### Memory Usage

```{python}
city_mpg.nbytes
```

```{python}
city_mpg.astype('int16').nbytes
```

```{python}
city_mpg.astype('int16[pyarrow]').nbytes
```

* Using `.nbytes` with `object` types only shows how much memory the Pandas object is taking. The `make` of the `autos` has `pyarrow` strings. If we convert it back to a Pandas 1 string column, the type becomes `object`

* Here we inspect the Pandas 2 memory usage:

```{python}
make = df.make
make.nbytes
```

```{python}
make.memory_usage()
```

```{python}
make.memory_usage(deep=True)
```

* Now, let’s convert it to Pandas 1

```{python}
make.astype(str).memory_usage()
```

```{python}
make.astype(str).memory_usage(deep=True)
```

* Notice that the Pandas 1 memory usage is much higher than the Pandas 2. In this example, it is using almost 6 times the memory!

### String and Category Types

* You can use the `.astype` method to convert to a category:

```{python}
(
  make
  .astype('category')
  .memory_usage(deep=True)
)
```

* We save another 5x versus the `pyarrow` string type when we convert the `make` column to a category

* When we convert strings to categories, we retain the ability to use the string methods via the `.str` accessor. We can also use the `.cat` accessor to use category methods


### Ordered Categories

* To create ordered categories, you need to define your own `CategoricalDtype`

```{python}
values = pd.Series(sorted(set(city_mpg)))
city_type = pd.CategoricalDtype(categories=values, ordered=True)
city_mpg.astype(city_type)
```

### Converting to Other Types

* The `.to_numpy` method (or the `.values` property) will give us a `NumPy` array of values, and the `.to_list` will return a Python `list` of values

* When you want a dataframe with a single column, you can use the `.to_frame` method

```{python}
city_mpg.to_frame()
```

* To convert to a `datetime`, use the `to_datetime` function in `pandas`

## Manipulation Methods

* The messy data you get from the real world must be cleaned up and then cleaned up again. Luckily, `pandas` has a lot of tools to help you with this

### `.apply` and `.where`

* `.apply` allows you to apply a function to an entire series or element-wise to every value. If it does the latter, you are taking the data out of the optimized and fast storage and pulling it into Python. This is a slow operation

```{python}
def gt20(val):
  return val > 20
```

```{python}
%%timeit
city_mpg.apply(gt20)
```

* In contrast, if we use the broadcasted .gt method, it runs almost 50 times faster:

```{python}
%%timeit
city_mpg.gt(20)
```

* Here’s another example. I will look at the `make` column from my dataset.  I might want to limit my dataset to show the top five makes and label everything else as `Other` 

```{python}
make = df.make
make
```

```{python}
make.value_counts()
```

* The first five entries in the index are the values I want to keep. I want to replace everything else with `Other`. Here is an example using `.apply`:

```{python}
top5 = make.value_counts().index[:5]
top5
```

```{python}
def generalize_top5(val):
  if val in top5:
    return val
  return 'Other'
```

```{python}
make.apply(generalize_top5)
```

```{python}
%%timeit
make.apply(generalize_top5)
```

* `generalize_top5` is called once for every value. A faster, more conversational manner of doing this is using the `.where` method

```{python}
make.where(make.isin(top5), other='Other')
```

```{python}
%%timeit
make.where(make.isin(top5), other='Other')
```

* The complement of the `.where` method is the `.mask` method

```{python}
make.mask(~make.isin(top5), other='Other')
```

### `.apply` with `NumPy` Functions

* There are cases where `.apply` is helpful. If you are working with a `NumPy` function that works on arrays, then `.apply` will *broadcast* the operation to the entire series

```{python}
import math
```

```{python}
%%timeit
city_mpg.apply(math.log)
```

```{python}
%%timeit
city_mpg.apply(np.log)
```

### If Else with Pandas

```{python}
vc = make.value_counts()

top5 = vc.index[:5]
top10 = vc.index[:10]

def generalize(val):
  if val in top5:
    return val
  elif val in top10:
    return 'Top10'
  else:
    return 'Other'
```

```{python}
make.apply(generalize).head(15)
```

* One of the new features of pandas 2.2 is the `.case_when` method

```{python}
(
  make
  .case_when(
    caselist=[
      (make.isin(top5), make),
      (make.isin(top10), 'Top10'),
      (pd.Series(True, index=make.index), 'Other')
    ]
  )
).head(15)
```

* The `.case_when` code runs about 6x faster

```{python}
(
  make
  .where(make.isin(top5), 'Top10')  
  .where(make.isin(top10), 'Other')
).head(15)
```

---

```{python}
%%timeit
make.apply(generalize)
```

```{python}
%%timeit
(
  make
  .case_when(
    caselist=[
      (make.isin(top5), make),
      (make.isin(top10), 'Top10'),
      (pd.Series(True, index=make.index), 'Other')
    ]
  )
)
```

```{python}
%%timeit
(
  make
  .where(make.isin(top5), 'Top10')
  .where(make.isin(top10), 'Other')
)
```

### Missing Data

* The `cylinders` column has missing values

```{python}
cyl = df.cylinders

(
  cyl
  .isna()
  .sum()
)
```

* First, let’s find the index where the values are missing in the `cylinders` column and then show what those `makes` are:

```{python}
missing = cyl.isna()
make.loc[missing]
```

### Filling In Missing Data

```{python}
cyl[cyl.isna()]
```

```{python}
cyl.fillna(0).loc[7136:7141]
```

### Interpolating Data

* Another option for replacing missing data is the `.interpolate` method

```{python}
temp = pd.Series([32, 40, None, 42, 39, 32], 
        dtype='float[pyarrow]')
temp
```

```{python}
temp.dropna()
```

```{python}
temp.ffill()
```

```{python}
temp.bfill()
```

```{python}
temp.fillna(temp.mean())
```

```{python}
temp.interpolate()
```

### Clipping Data

* If you have outliers in your data, you might want to use the `.clip` method

```{python}
city_mpg.loc[:446]
```

* We can trim the values to be between the 5th (11.0) and 95th quantile (27.0) with the following code:

```{python}
(
  city_mpg
  .loc[:446]
  .clip(lower=city_mpg.quantile(.05),
        upper=city_mpg.quantile(.95))
)
```

### Sorting Values

* The `.sort_values` method will sort the values in ascending order and also rearrange the index accordingly:

```{python}
city_mpg.sort_values()
```

```{python}
(city_mpg.sort_values() + highway_mpg) / 2
```

### Sorting the Index

* If you want to sort the index of a series, you can use the `.sort_index` method

```{python}
city_mpg.sort_values().sort_index()
```

### Dropping Duplicates

* Many datasets have duplicate entries. The `.drop_duplicates` method will remove values that appear more than once

* You can determine whether to keep the first or last duplicate value using the `keep` parameter

```{python}
city_mpg.drop_duplicates()
```

* There are only 105 results (down from 41144) now that duplicates are removed:

```{python}
city_mpg.drop_duplicates(keep='last')
```

```{python}
city_mpg.drop_duplicates(keep=False)
```

### Ranking Data

* The `.rank` method will return a series that keeps the original index but uses the ranks of values from the original series. You can control how ranking occurs with the `method` parameter

```{python}
city_mpg.rank()
```

* By default, if two values are the same, their rank will be the average of the positions they take

* You can specify `'min'` to put equal values in the same rank. If the first three values are equal, they will all be ranked 1, and the next value will be ranked 4

```{python}
city_mpg.rank(method='min')
```

* If you use `method='dense'`, the ranks will be consecutive integers and dense (i.e., to not skip any positions). For example, if the first three values are equal, they will all be ranked 1, and the next value will be ranked 2:

```{python}
city_mpg.rank(method='dense')
```

### Replacing Data

* The `.replace` method allows you to map whole values to new values. There are many ways to specify how to replace the values. You can specify an entire string to replace a string or use a dictionary to map old to new values. This example uses the former:

```{python}
make.replace('Hyundai', '현대').value_counts().loc[['현대', 'Kia']]
```

```{python}
make.replace(to_replace=['Hyundai', 'Kia'],
      value=['현대', '기아'] ).value_counts().loc[['현대', '기아']]
```

```{python}
make.replace(to_replace={'Hyundai': '현대',
            'Kia': '기아'}).value_counts().loc[['현대', '기아']]
```

* The `to_replace` parameter’s value can contain a regular expression if you provide the `regex=True` parameter

```{python}
make.replace(r'(Fer)ra(r.*)', value=r'\2-other-\1', regex=True)
```

### Binning Data

* The following code creates 10 bins of equal width for the `city` column

```{python}
pd.cut(city_mpg, 10)
```

* Each of the ten bins would have approximately 10 percent of the data. The quantiles of the data determine the bin edges

```{python}
pd.qcut(city_mpg, 10)
```

```{python}
pd.qcut(city_mpg, 10, labels=list(range(1,11)))
```

## Indexing Operations

### Prepping the Data and Renaming the Index

* We will use the `.rename` method to change the index labels. We can pass in a dictionary to map the previous index label to the new label:

```{python}
city2 = city_mpg.rename(make.to_dict())
city2
```

* To view the index, you can access the `.index` attribute:

```{python}
city2.index
```

```{python}

```

```{python}

```

```{python}

```

```{python}

```

```{python}

```

```{python}

```

```{python}

```

```{python}

```

```{python}

```

```{python}

```

```{python}

```

```{python}

```

```{python}

```

```{python}

```

```{python}

```

```{python}

```

```{python}

```

```{python}

```

```{python}

```

```{python}

```

```{python}

```

```{python}

```

```{python}

```

```{python}

```

```{python}

```

```{python}

```

```{python}

```

```{python}

```

```{python}

```

```{python}

```

```{python}

```

```{python}

```

```{python}

```

```{python}

```
